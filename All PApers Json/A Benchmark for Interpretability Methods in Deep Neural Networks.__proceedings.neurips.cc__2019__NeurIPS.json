{
  "pdf": "A Benchmark for Interpretability Methods in Deep Neural Networks.__proceedings.neurips.cc__2019__NeurIPS",
  "text": "A Benchmark for Interpretability Methods in Deep\nNeural Networks\nSara Hooker , Dumitru Erhan, Pieter-Jan Kindermans, Been Kim\nGoogle Brain\nshooker,dumitru,pikinder,beenkim@google.com\nAbstract\nW e propose an empirical measure of the approximate accuracy of feature impor-\ntance estimates in deep neural networks. Our results across several large-scale\nimage classiﬁcation datasets show that many popular interpretability methods pro-\nduce estimates of feature importance that are not better than a random designation\nof feature importance. Only certain ensemble based approaches—V arGrad and\nSmoothGrad-Squared—outperform such a random assignment of importance. The\nmanner of ensembling remains critical, we show that some approaches do no better\nthen the underlying method but carry a far higher computational burden.\n1 Introduction\nIn a machine learning setting, a question of great interest is estimating the inﬂuence of a given input\nfeature to the prediction made by a model. Understanding what f",
  "values": {
    "Interpretable (to users)": "Yes",
    "Respect for Law and public interest": "Yes",
    "Non-maleficence": "Yes",
    "Autonomy (power to decide)": "Yes",
    "Not socially biased": "Yes",
    "Transparent (to users)": "Yes",
    "Privacy": "Yes",
    "Respect for Persons": "Yes",
    "User influence": "Yes",
    "Justice": "Yes",
    "Fairness": "Yes",
    "Explicability": "Yes",
    "Critiqability": "Yes",
    "Collective influence": "Yes",
    "Beneficence": "Yes",
    "Deferral to humans": "Yes"
  }
}