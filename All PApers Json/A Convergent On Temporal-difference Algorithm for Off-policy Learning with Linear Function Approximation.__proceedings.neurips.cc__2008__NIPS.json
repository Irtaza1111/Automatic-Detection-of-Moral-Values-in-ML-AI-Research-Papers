{
  "pdf": "A Convergent On Temporal-difference Algorithm for Off-policy Learning with Linear Function Approximation.__proceedings.neurips.cc__2008__NIPS",
  "text": "A ConvergentO(n) Algorithm\nfor Off-policy Temporal-difference Learning\nwith Linear Function Approximation\nRichard S. Sutton, Csaba Szepesv´ari∗, Hamid Reza Maei\nReinforcement Learning and Artiﬁcial Intelligence Laboratory\nDepartment of Computing Science\nUniversity of Alberta\nEdmonton, Alberta, Canada T6G 2E8\nAbstract\nWe introduce the ﬁrst temporal-difference learning algorithm that is stable with\nlinear function approximation and off-policy training, for any ﬁnite Markov de-\ncision process, behavior policy, and target policy, and whose complexity scales\nlinearly in the number of parameters. We consider an i.i.d. policy-evaluation set-\nting in which the data need not come from on-policy experience. The gradient\ntemporal-difference (GTD) algorithm estimates the expected update vector of the\nTD(0) algorithm and performs stochastic gradient descent on its L2 norm. We\nprove that this algorithm is stable and convergent under the usual stochastic ap-\nproximation conditions to the same least-s",
  "values": {
    "Interpretable (to users)": "Yes",
    "Respect for Law and public interest": "Yes",
    "Not socially biased": "Yes",
    "Transparent (to users)": "Yes",
    "Autonomy (power to decide)": "Yes",
    "Respect for Persons": "Yes",
    "Justice": "Yes",
    "Privacy": "Yes",
    "Non-maleficence": "Yes",
    "User influence": "Yes",
    "Fairness": "Yes",
    "Collective influence": "Yes",
    "Beneficence": "Yes",
    "Critiqability": "Yes",
    "Explicability": "Yes",
    "Deferral to humans": "Yes"
  }
}