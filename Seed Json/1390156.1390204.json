{
  "pdf": "1390156.1390204",
  "title": "1390156.1390204",
  "author": "Unknown",
  "paper_id": "1390156.1390204",
  "text": "Grassmann Discriminant Analysis:\na Unifying View on Subspace-Based Learning\nJihun Hamm jhham@seas.upenn.edu\nDaniel D. Lee ddlee@seas.upenn.edu\nGRASP Laboratory, University of Pennsylvania, Philadelphia, PA 19104 USA\nAbstract\nIn this paper we propose a discriminant\nlearning framework for problems in which\ndata consist of linear subspaces instead of\nvectors. By treating subspaces as basic el-\nements, we can make learning algorithms\nadapt naturally to the problems with lin-\near invariant structures. We propose a uni-\nfying view on the subspace-based learning\nmethod by formulating the problems on the\nGrassmann manifold, which is the set of\nﬁxed-dimensional linear subspaces of a Eu-\nclidean space. Previous methods on the prob-\nlem typically adopt an inconsistent strategy:\nfeature extraction is performed in the Eu-\nclidean space while non-Euclidean distances\nare used. In our approach, we treat each sub-\nspace as a point in the Grassmann space, and\nperform feature extraction and classiﬁcation\nin the same space. We show feasibility of\nthe approach by using the Grassmann kernel\nfunctions such as the Projection kernel and\nthe Binet-Cauchy kernel. Experiments with\nreal image databases show that the proposed\nmethod performs well compared with state-\nof-the-art algorithms.\n1. Introduction\nWe often encounter learning problems in which the ba-\nsic elements of the data are sets of vectors instead of\nvectors. Suppose we want to recognize a person from\nmultiple pictures of the individual, taken from diﬀer-\nent angles, under diﬀerent illumination or at diﬀerent\nplaces. When comparing such sets of image vectors, we\nare free to deﬁne the similarity between sets based on\nAppearing in Proceedings of the 25 th International Confer-\nence on Machine Learning , Helsinki, Finland, 2008. Copy-\nright 2008 by the author(s)/owner(s).\nthe similarity between image vectors (Shakhnarovich\net al., 2002; Kondor & Jebara, 2003; Zhou & Chel-\nlappa, 2006).\nIn this paper, we speciﬁcally focus on those data that\ncan be modeled as a collection of linear subspaces. In\nthe example above, let’s assume that the set of images\nof a single person is well approximated by a low di-\nmensional subspace (Turk & Pentland, 1991), and the\nwhole data is the collection of such subspaces. The\nbeneﬁts of using subspaces are two-fold: 1) compar-\ning two subspaces is cheaper than comparing two sets\ndirectly when those sets are very large, and 2) it is\nmore robust to missing data since the subspace can\n‘ﬁll-in’ the missing pictures. However the advantages\ncome with the challenge of representing and handling\nthe subspaces appropriately.\nWe approach the subspace-based learning problems by\nformulating the problems on the Grassmann manifold,\nthe set of ﬁxed-dimensional linear subspaces of a Eu-\nclidean space. With this unifying framework we can\nmake analytic comparisons of the various distances of\nsubspaces. In particular, we single out those distances\nthat are induced from the Grassmann kernels , which\nare positive deﬁnite kernel functions on the Grassmann\nspace. The Grassmann kernels allow us to use the\nusual kernel-based algorithms on this unconventional\nspace and to avoid ad hoc approaches to the problem.\nWe demonstrate the proposed framework by using the\nProjection metric and the Binet-Cauchy metric and by\napplying kernel Linear Discriminant Analysis to clas-\nsiﬁcation problems with real image databases.\n1.1. Contributions of the Paper\nAlthough the Projection metric and the Binet-Cauchy\nmetric were previously used (Chang et al., 2006; Wolf\n& Shashua, 2003), their potential for subspace-based\nlearning has not been fully explored. In this work, we\nprovide an analytic exposition of the two metrics as\nexamples of the Grassmann kernels, and contrast the\n376\n\nGrassmann Discriminant Analysis\nYi\nYj\nθ 2\nG(m, D )\nu1\nv1\nθ1 , ..., θm\nspan( Yi )\nspan( Yj )\nR D\nFigure 1. Principal angles and Grassmann distances. Let span( Yi) and span( Yj) be two subspaces in the Euclidean space\nRD on the left. The distance between two subspaces span( Yi) and span( Yj) can be measured by the principal angles\nθ = [ θ1, ... , θ m]′ using the usual innerproduct of vectors. In the Grassmann manifold viewpoint, the subspaces span( Yi)\nand span(Yj) are considered as two points on the manifold G(m, D ), whose Riemannian distance is related to the principal\nangles by d(Yi, Y j) = ∥θ∥2. Various distances can be deﬁned based on the principal angles.\ntwo metrics with other metrics used in the literature.\nSeveral subspace-based classiﬁcation methods have\nbeen previously proposed (Yamaguchi et al., 1998;\nSakano, 2000; Fukui & Yamaguchi, 2003; Kim et al.,\n2007). However, these methods adopt an inconsistent\nstrategy: feature extraction is performed in the Eu-\nclidean space when non-Euclidean distances are used.\nThis inconsistency can result in complications and\nweak guarantees. In our approach, the feature ex-\ntraction and the distance measurement are integrated\naround the Grassmann kernel, resulting in a simpler\nand better-understood formulation.\nThe rest of the paper is organized as follows. In Sec. 2\nand 3 we introduce the Grassmann manifolds and de-\nrive various distances on the space. In Sec. 4 we\npresent a kernel view of the problem and emphasize the\nadvantages of using positive deﬁnite metrics. In Sec. 5\nwe propose the Grassmann Discriminant Analysis and\ncompare it with other subspace-based discrimination\nmethods. In Sec. 6 we test the proposed algorithm for\nface recognition and object categorization tasks. We\nconclude in Sec. 7 with a discussion.\n2. Grassmann Manifold and Principal\nAngles\nIn this section we brieﬂy review the Grassmann man-\nifold and the principal angles.\nDeﬁnition 1 The Grassmann manifold G(m, D ) is\nthe set of m-dimensional linear subspaces of the RD.\nThe G(m, D ) is a m(D − m)-dimensional compact Rie-\nmannian manifold. 1 An element of G(m, D ) can be\n1G(m, D ) can be derived as a quotient space of orthog-\nonal groups G(m, D ) = O(D)/ O(m) × O (D − m), where\nrepresented by an orthonormal matrix Y of size D by\nm such that Y ′Y = Im, where Im is the m by m iden-\ntity matrix. For example, Y can be the m basis vectors\nof a set of pictures in RD. However, the matrix rep-\nresentation of a point in G(m, D ) is not unique: two\nmatrices Y1 and Y2 are considered the same if and only\nif span( Y1) = span( Y2), where span( Y ) denotes the\nsubspace spanned by the column vectors of Y . Equiva-\nlently, span(Y1) = span( Y2) if and only if Y1R1 = Y2R2\nfor some R1, R 2 ∈ O (m). With this understanding, we\nwill often use the notation Y when we actually mean\nits equivalence class span( Y ), and use Y1 = Y2 when\nwe mean span( Y1) = span( Y2), for simplicity.\nFormally, the Riemannian distance between two sub-\nspaces is the length of the shortest geodesic connecting\nthe two points on the Grassmann manifold. However,\nthere is a more intuitive and computationally eﬃcient\nway of deﬁning the distances using the principal angles\n(Golub & Loan, 1996).\nDeﬁnition 2 Let Y1 and Y2 be two orthonormal\nmatrices of size D by m. The principal an-\ngles 0 ≤ θ1 ≤ · · · ≤θm ≤ π/ 2 between two subspaces\nspan(Y1) and span(Y2), are deﬁned recursively by\ncos θk = maxuk∈ span(Y1)\nmaxvk∈ span(Y2)\nuk\n′vk, subject to\nuk\n′uk = 1, vk\n′vk = 1,\nuk\n′ui = 0, vk\n′vi = 0, (i = 1, ..., k − 1).\nIn other words, the ﬁrst principal angle θ1 is the small-\nest angle between all pairs of unit vectors in the ﬁrst\nand the second subspaces. The rest of the principal\nO(m) is the group of m by m orthonormal matrices. We\nrefer the readers to (Wong, 1967; Absil et al., 2004) for\ndetails on the Riemannian geometry of the space.\n377\nGrassmann Discriminant Analysis\nangles are similarly deﬁned. It is known (Wong, 1967;\nEdelman et al., 1999) that the principal angles are re-\nlated to the geodesic distance by d2\nG(Y1, Y 2) = ∑\ni θ2\ni\n(refer to Fig. 1.)\nThe principal angles can be computed from the Singu-\nlar Value Decomposition (SVD) of Y ′\n1 Y2,\nY ′\n1 Y2 = U (cos Θ) V ′, (1)\nwhere U = [ u1 ... um], V = [ v1 ... vm], and cos Θ\nis the diagonal matrix cos Θ = diag(cos θ1 ... cos θm).\nThe cosines of the principal angles cos θ1, ... , cos θm\nare also known as canonical correlations.\nAlthough the deﬁnition can be extended to the cases\nwhere Y1 and Y2 have diﬀerent number of columns,\nwe will assume Y1 and Y2 have the same size D by m\nthroughout this paper. Also, we will occasionally use\nG instead of G(m, D ) for simplicity.\n3. Distances for Subspaces\nIn this paper we use the term distance as any assign-\nment of nonnegative values for each pair of points in\na space X . A valid metric is, however, a distance that\nsatisﬁes the additional axioms:\nDeﬁnition 3 A real-valued function d : X × X → R\nis called a metric if\n1. d(x1, x 2) ≥ 0,\n2. d(x1, x 2) = 0 if and only if x1 = x2,\n3. d(x1, x 2) = d(x2, x 1),\n4. d(x1, x 2) + d(x2, x 3) ≤ d(x1, x 3),\nfor all x1, x 2, x 3 ∈ X .\nA distance (or a metric) between subspaces d(Y1, Y 2)\nhas to be invariant under diﬀerent representations\nd(Y1, Y 2) = d(Y1R1, Y 2R2), ∀R1, R 2 ∈ O (m).\nIn this section we introduce various distances for sub-\nspaces derivable from the principal angles.\n3.1. Projection Metric and Binet-Cauchy\nMetric\nWe ﬁrst underline two main distances of this paper.\n1. Projection metric\ndP (Y1, Y 2) =\n( m∑\ni=1\nsin2 θi\n) 1/2\n=\n(\nm −\nm∑\ni=1\ncos2 θi\n) 1/2\n.\n(2)\nThe Projection metric is the 2-norm of the sine\nof principal angles (Edelman et al., 1999; Wang\net al., 2006).\n2. Binet-Cauchy metric\ndBC (Y1, Y 2) =\n(\n1 −\n∏\ni\ncos2 θi\n) 1/2\n. (3)\nThe Binet-Cauchy metric is deﬁned with the\nproduct of canonical correlations (Wolf &\nShashua, 2003; Vishwanathan & Smola, 2004).\nAs the names hint, these two distances are in fact valid\nmetrics satisfying Def. 3. The proofs are deferred until\nSec. 4.\n3.2. Other Distances in the Literature\nWe describe a few other distances used in the liter-\nature. The principal angles are the keys that relate\nthese distances.\n1. Max Correlation\ndMax(Y1, Y 2) =\n(\n1 − cos2 θ1\n) 1/2\n= sin θ1. (4)\nThe max correlation is a distance based on only\nthe largest canonical correlation cos θ1 (or the\nsmallest principal angle θ1). This max correla-\ntion was used in previous works (Yamaguchi et al.,\n1998; Sakano, 2000; Fukui & Yamaguchi, 2003).\n2. Min Correlation\ndMin(Y1, Y 2) =\n(\n1 − cos2 θm\n) 1/2\n= sin θm. (5)\nThe min correlation is deﬁned similarly to the\nmax correlation. However, the min correlation\nis more closely related to the Projection metric:\nwe can rewrite the Projection metric as dP =\n2− 1/2 ∥Y1Y ′\n1 − Y2Y ′\n2 ∥F and the min correlation\nas dMin = ∥Y1Y ′\n1 − Y2Y ′\n2 ∥2.\n3. Procrustes metric\ndCF (Y1, Y 2) = 2\n( m∑\ni=1\nsin2(θi/ 2)\n) 1/2\n. (6)\nThe Procrustes metric is the minimum distance\nbetween diﬀerent representations of two subspaces\nspan(Y1) and span( Y2): (Chikuse, 2003)\ndCF = min\nR1,R2∈O (m)\n∥Y1R1− Y2R2∥F = ∥Y1U − Y2V ∥F ,\nwhere U and V are from (1). By deﬁnition,\nthe distance is invariant of the choice of the\n378\nGrassmann Discriminant Analysis\nbases of span( Y1) and span( Y2). The Procrustes\nmetric is also called chordal distance (Edelman\net al., 1999). We can similarly deﬁne the mini-\nmum distance using other matrix norms such as\ndC2(Y1, Y 2) = ∥Y1U − Y2V ∥2 = 2 sin(θm/ 2).\n3.3. Which Distance to Use?\nThe choice of the best distance for a classiﬁcation task\ndepends on a few factors. The ﬁrst factor is the dis-\ntribution of data. Since the distances are deﬁned with\nparticular combinations of the principal angles, the\nbest distance depends highly on the probability dis-\ntribution of the principal angles of the given data.\nFor example, dMax uses the smallest principal angle θ1\nonly, and may be robust when the data are noisy. On\nthe other hand, when all subspaces are sharply concen-\ntrated on one point, dMax will be close to zero for most\nof the data. In this case, dMin may be more discrimi-\nnative. The Projection metric dP , which uses all the\nprincipal angles, will show intermediate characteristics\nbetween the two distances. Similar arguments can be\nmade for the Procrustes metrics dCF and dC2, which\nuse all angles and the largest angle only, respectively.\nThe second criterion for choosing the distance, is the\ndegree of structure in the distance. Without any struc-\nture a distance can be used only with a simple K-\nNearest Neighbor (K-NN) algorithm for classiﬁcation.\nWhen a distance have an extra structure such as tri-\nangle inequality, for example, we can speed up the\nnearest neighbor searches by estimating lower and up-\nper limits of unknown distances (Farag´ o et al., 1993).\nFrom this point of view, the max correlation is not a\nmetric and may not be used with more sophisticated\nalgorithms. On the other hand, the Min Correlation\nand the Procrustes metrics are valid metrics 2.\nThe most structured metrics are those which are in-\nduced from a positive deﬁnite kernel. Among the met-\nrics mentioned so far, only the Projection metric and\nthe Binet-Cauchy metric belong to this class. The\nproof and the consequences of positive deﬁniteness are\nthe main topics of the next section.\n4. Kernel Functions for Subspaces\nWe have deﬁned a valid metric on Grassmann mani-\nfolds. The next question is whether we can deﬁne a\nkernel function compatible with the metric. For this\npurpose let’s recall a few deﬁnitions. Let X be any\n2The metric properties follow from the properties of\nmatrix 2-norm and F-norm. To check the conditions in\nDef. 3 for Procrustes we use the equality min R1,R2 ∥Y1R1−\nY2R2∥2,F = min R3 ∥Y1 − Y2R3∥2,F for R1, R 2, R 3 ∈ O(m).\nset, and k : X × X → R be a symmetric real-valued\nfunction k(xi, x j) = k(xj, x i) for all xi, x j ∈ X .\nDeﬁnition 4 A real symmetric function is a (resp.\nconditionally) positive deﬁnite kernel function, if∑\ni,jcicjk(xi, x j) ≥ 0, for all x1, ..., x n(xi ∈ X ) and\nc1, ..., c n(ci ∈ R) for any n ∈ N. (resp. for all\nc1, ..., c n(ci ∈ R) such that ∑n\ni=1 ci = 0.)\nIn this paper we are interested in the kernel functions\non the Grassmann space.\nDeﬁnition 5 A Grassmann kernel function is a pos-\nitive deﬁnite kernel function on G.\nIn the following we show that the Projection metric\nand the Binet-Cauchy are induced from the Grass-\nmann kernels.\n4.1. Projection Metric\nThe Projection metric can be understood by associ-\nating a point span( Y ) ∈ G with its projection matrix\nY Y ′ by an embedding:\nΨ P : G(m, D ) → RD× D, span(Y ) ↦→Y Y ′. (7)\nThe image Ψ P (G(m, D )) is the set of rank- m or-\nthogonal projection matrices. This map is in fact\nan isometric embedding (Chikuse, 2003) and the\nprojection metric is simply a Euclidean distance in\nRD× D. The corresponding innerproduct of the space\nis tr [(Y1Y ′\n1 )(Y2Y ′\n2 )] = ∥Y ′\n1 Y2∥2\nF , and therefore\nProposition 1 The Projection kernel\nkP (Y1, Y 2) = ∥Y ′\n1 Y2∥2\nF (8)\nis a Grassmann kernel.\nProof The kernel is well-deﬁned because kP (Y1, Y 2) =\nkP (Y1R1, Y 2R2) for any R1, R 2 ∈ O (m). The positive\ndeﬁniteness follows from the properties of the Frobe-\nnius norm. For all Y1, ..., Y n(Yi ∈ G ) and c1, ..., c n(ci ∈\nR) for any n ∈ N, we have\n∑\nij\ncicj∥Y ′\ni Yj∥2\nF =\n∑\nij\ncicjtr(YiY ′\ni YjY ′\nj )\n= tr(\n∑\ni\nciYiY ′\ni )2 = ∥\n∑\ni\nciYiY ′\ni ∥2\nF ≥ 0.\nWe can generate a family of kernels from the Projec-\ntion kernel. For example, the square-root ∥Y ′\ni Yj∥F is\nalso a positive deﬁnite kernel.\n379\nGrassmann Discriminant Analysis\n4.2. Binet-Cauchy Metric\nThe Binet-Cauchy metric can also be understood from\nan embedding. Let s be a subset of {1, ..., D } with\nm elements s = {r1, ..., r m}, and Y (s) be the m × m\nmatrix whose rows are the r1, ... , r m-th rows of Y . If\ns1, s 2, ..., s n are all such choices of the subset s ordered\nlexicographically, then the Binet-Cauchy embedding is\ndeﬁned as\nΨ BC : G(m, D ) → Rn, Y ↦→\n(\ndet Y (s1), ..., det Y (sn)\n)\n,\n(9)\nwhere n = DCm is the number of choosing m rows out\nof D rows. The natural innerproduct in this case is∑n\nr=1 det Y (si)\n1 det Y (si)\n2 .\nProposition 2 The Binet-Cauchy kernel\nkBC (Y1, Y 2) = (det Y ′\n1 Y2)2 = det Y ′\n1 Y2Y ′\n2 Y1 (10)\nis a Grassmann kernel.\nProof First, the kernel is well-deﬁned because\nkBC (Y1, Y 2) = kBC (Y1R1, Y 2R2) for any R1, R 2 ∈\nO(m). To show that kBC is positive deﬁnite it suﬃces\nto show that k(Y1, Y 2) = det Y ′\n1 Y2 is positive deﬁnite.\nFrom the Binet-Cauchy identity, we have\ndet Y ′\n1 Y2 =\n∑\ns\ndet Y (s)\n1 det Y (s)\n2 .\nTherefore, for all Y1, ..., Y n(Yi ∈ G ) and c1, ..., c n(ci ∈\nR) for any n ∈ N, we have\n∑\nij\ncicj det Y ′\ni Yj =\n∑\nij\ncicj\n∑\ns\ndet Y (s)\ni det Y (s)\nj\n=\n∑\ns\n( ∑\ni\nci det Y (s)\ni\n) 2\n≥ 0.\nWe can also generate another family of kernels\nfrom the Binet-Cauchy kernel. Note that although\ndet Y ′\n1 Y2 is a Grassmann kernel we prefer using\nkBC (Y1, Y 2) = det( Y ′\n1 Y2)2, since it is directly related\nto principal angles det( Y ′\n1 Y2)2 = ∏cos2 θi, whereas\ndet Y ′\n1 Y2 ̸= ∏cos θi in general. 3 Another variant\narcsin kBC (Y1, Y 2) is also a positive deﬁnite kernel 4\nand its induced metric d = (arccos(det Y ′\n1 Y2))1/2 is\na conditionally positive deﬁnite metric.\n4.3. Indeﬁnite Kernels from Other Metrics\nSince the Projection metric and the Binet-Cauchy\nmetric are derived from positive deﬁnite kernels, all\n3det Y ′\n1 Y2 can be negative whereas Q cos θi, the product\nof singular values, is nonnegative by deﬁnition.\n4Theorem 4.18 and 4.19 (Sch¨ olkopf & Smola, 2001).\nthe kernel-based algorithms for Hilbert spaces are at\nour disposal. In contrast, other metrics in the previ-\nous sections are not associated with any Grassmann\nkernel. To show this we can use the following result\n(Schoenberg, 1938; Hein et al., 2005):\nProposition 3 A metric d is induced from a positive\ndeﬁnite kernel if and only if\nˆk(x1, x 2) = − d2(x1, x 2)/ 2, x 1, x 2 ∈ X (11)\nis conditionally positive deﬁnite.\nThe proposition allows us to show a metric’s non-\npositive deﬁniteness by constructing an indeﬁnite ker-\nnel matrix from (11) as a counterexample.\nThere have been eﬀorts to use indeﬁnite kernels for\nlearning (Ong et al., 2004; Haasdonk, 2005), and sev-\neral heuristics have been proposed to make an in-\ndeﬁnite kernel matrix to a positive deﬁnite matrix\n(Pekalska et al., 2002). However, we do not advocate\nthe use of the heuristics since they change the geome-\ntry of the original data.\n5. Grassmann Discriminant Analysis\nIn this section we give an example of the Discriminant\nAnalysis on Grassmann space by using kernel LDA\nwith the Grassmann kernels.\n5.1. Linear Discriminant Analysis\nThe Linear Discriminant Analysis (LDA) (Fukunaga,\n1990), followed by a K-NN classiﬁer, has been success-\nfully used for classiﬁcation.\nLet {x1, ..., xN } be the data vectors and {y1, ..., y N }\nbe the class labels yi ∈ { 1, ..., C }. Without loss of\ngenerality we assume the data are ordered according\nto the class labels: 1 = y1 ≤ y2 ≤ ... ≤ yN = C. Each\nclass c has Nc number of samples.\nLet µc = 1/N c\n∑\n{i|yi=c} xi be the mean of class c, and\nµ = 1 /N ∑\ni xi be the overall mean. LDA searches\nfor the discriminant direction w which maximizes the\nRayleigh quotient L(w) = w′Sbw/ w′Sww where Sb\nand Sw are the between-class and within-class covari-\nance matrices respectively:\nSb = 1\nN\nC∑\nc=1\nNc(µc − µ)(µc − µ)′\nSw = 1\nN\nC∑\nc=1\n∑\n{i|yi=c}\n(xi − µc)(xi − µc)′\nThe optimal w is obtained from the largest eigenvec-\ntor of S− 1\nw Sb. Since S− 1\nw Sb has rank C − 1, there are\n380\nGrassmann Discriminant Analysis\nC − 1-number of local optima W = {w1, ..., wC−1}.\nBy projecting data onto the space spanned by W , we\nachieve dimensionality reduction and feature extrac-\ntion of data onto the most discriminant subspace.\n5.2. Kernel LDA with Grassmann Kernels\nKernel LDA can be formulated by using the kernel\ntrick as follows. Let φ : G → H be the feature map,\nand Φ = [ φ1... φN ] be the feature matrix of the train-\ning points. Assuming w is a linear combination of the\nthose feature vectors, w = Φ α, we can rewrite the\nRayleigh quotient in terms of α as\nL(α) = α ′Φ ′SBΦ α\nα ′Φ ′SW Φ α = α ′K(V − 1N 1′\nN /N )Kα\nα ′(K(IN − V )K + σ2IN )α ,\n(12)\nwhere K is the kernel matrix, 1N is a uniform vector\n[1 ... 1]′ of length N , V is a block-diagonal matrix\nwhose c-th block is the uniform matrix 1Nc 1′\nNc /N c,\nand σ2IN is a regularizer for making the computation\nstable. Similarly to LDA, the set of optimal α’s are\ncomputed from the eigenvectors.\nThe procedures for using kernel LDA with the Grass-\nmann kernels are summarized below:\nAssume the D by m orthonormal bases {Yi} are\nalready computed from the SVD of sets in the data.\nTraining:\n1. Compute the matrix [Ktrain]ij = kP (Yi, Y j) or\nkBC (Yi, Y j) for all Yi, Y j in the training set.\n2. Solve maxα L(α) by eigen-decomposition.\n3. Compute the (C − 1)-dimensional coeﬃcients\nFtrain = α ′Ktrain.\nTesting:\n1. Compute the matrix [Ktest]ij = kP (Yi, Y j) or\nkBC (Yi, Y j) for all Yi in training set and Yj in\nthe test set.\n2. Compute the (C − 1)-dim coeﬃcients Ftest =\nα ′Ktest.\n3. Perform 1-NN classiﬁcation from the Eu-\nclidean distance between Ftrain and Ftest.\nAnother way of applying LDA to subspaces is to use\nthe Projection embedding Ψ P (7) or the Binet-Cauchy\nembedding Ψ BC (9) directly. A subspace is repre-\nsented by a D by D matrix in the former, or by a\nvector of length n = DCm in the latter. However, us-\ning these embeddings to compute Sb or Sw is a waste\nof computation and storage resources when D is large.\n5.3. Other Subspace-Based Algorithms\n5.3.1. Mutual Subspace Method (MSM)\nThe original MSM (Yamaguchi et al., 1998) performs\nsimple 1-NN classiﬁcation with dMax with no feature\nextraction. The method can be extended to any dis-\ntance described in the paper. There are attempts to\nuse kernels for MSM (Sakano, 2000). However, the\nkernel is used only to represent data in the original\nspace, and the algorithm is still a 1-NN classiﬁcation.\n5.3.2. Constrained MSM\nConstrained MSM (Fukui & Yamaguchi, 2003) is a\ntechnique that applies dimensionality reduction to\nbases of the subspaces in the original space. Let\nG = ∑\ni YiY ′\ni be the sum of the projection matrices\nand {v1, ..., vD} be the eigenvectors corresponding to\nthe eigenvalues {λ1 ≤ ... ≤ λD} of G. The authors\nclaim that the ﬁrst few eigenvectors v1, ..., vd of G are\nmore discriminative than the later eigenvectors, and\nthey suggest projecting the basis vectors of each sub-\nspace Y1 onto the span( v1, ..., vl), followed by normal-\nization and orthonormalization. However these proce-\ndure lack justiﬁcations, as well as a clear criterion for\nchoosing the dimension d, on which the result crucially\ndepends from our experience.\n5.3.3. Discriminant Analysis of Canonical\nCorrelations (DCC)\nDCC (Kim et al., 2007) can be understood as a non-\nparametric version of linear discrimination analysis us-\ning the Procrustes metric (6). The algorithm ﬁnds the\ndiscriminating direction w which maximize the ratio\nL(w) = w′SBw/ w′Sww, where Sb and Sw are the\nnonparametric between-class and within-class ‘covari-\nance’ matrices:\nSb =\n∑\ni\n∑\nj∈ Bi\n(YiU − YjV )(YiU − YjV )′\nSw =\n∑\ni\n∑\nj∈ Wi\n(YiU − YjV )(YiU − YjV )′,\nwhere U and V are from (1). Recall that tr( YiU −\nYjV )(YiU − YjV )′ = ∥YiU − YjV ∥2\nF is the squared\nProcrustes metric. However, unlike our method, Sb\nand Sw do not admit a geometric interpretation as\ntrue covariance matrices, and cannot be kernelized ei-\nther. A main disadvantage of the DCC is that the\nalgorithm iterates the two stages of 1) maximizing the\nratio L(w) and of 2) computing Sb and Sw, which\nresults in computational overheads and more parame-\n381\nGrassmann Discriminant Analysis\nters to be determined. This reﬂects the complication\nof treating the problem in a Euclidean space with a\nnon-Euclidean distance.\n6. Experiments\nIn this section we test the Grassmann Discriminant\nAnalysis for 1) a face recognition task and 2) an object\ncategorization task with real image databases.\n6.1. Algorithms\nWe use the following six methods for feature extraction\ntogether with an 1-NN classiﬁer.\n1) GDA1 (with Projection kernel), 2) GDA2 (with\nBinet-Cauchy kernel), 3) Min dist , 4) MSM, 5) cMSM,\nand 6) DCC.\nFor GDA1 and GDA2, the optimal values of σ\nare found by scanning through a range of val-\nues. The results do not seem to vary much as\nlong as σ is small enough. The Min dist is\na simple pairwise distance which is not subspace-\nbased. If Yi and Yj are two sets of basis vectors:\nYi = {yi1, ..., yimi } and Yj = {yj1, ..., yjmj }, then\ndMindist(Yi, Y j) = min k,l∥yik − yjl∥2. For cMSM and\nDCC, the optimal dimension l is found by exhaus-\ntive searching. For DCC, we have used two nearest-\nneighbors for Bi and Wi in Sec. 5.3.3. Since the Sw\nand Sb are likely to be rank deﬁcient, we ﬁrst reduced\nthe dimension of the data to N − C using PCA as\nrecommended. Each optimization is iterated 5 times.\n6.2. T esting Illumination-Invariance with Y ale\nF ace Database\nThe Yale face database and the Extended Yale face\ndatabase (Georghiades et al., 2001) together consist of\npictures of 38 subjects with 9 diﬀerent poses and 45 dif-\nferent lighting conditions. Face regions were cropped\nfrom the original pictures, resized to 24 × 21 pixels\n(D = 504), and normalized to have the same variance.\nFor each subject and each pose, we model the illumi-\nnation variations by a subspace of the size m = 1, ..., 5,\nspanned by the 1 to 5 largest eigenvectors from SVD.\nWe evaluate the recognition rate of subjects with nine-\nfold cross validation, holding out one pose of all sub-\njects from the training set and using it for test.\nThe recognition rates are shown in Fig. 2. The GDA1\noutperforms the other methods consistently. The\nGDA2 also performs well for small m, but performs\nworse as m becomes large. The rates of the others\nalso seem to decrease as m increases. An interpreta-\ntion of the observation is that the ﬁrst few eigenvec-\ntors from the data already have enough information\nand the smaller eigenvectors are spurious for discrim-\ninating the subjects.\n6.3. T esting Pose-Invariance with ETH-80\nDatabase\nThe ETH-80 (Leibe & Schiele, 2003) database con-\nsists of pictures of 8 object categories (‘apple’, ‘pear’,\n‘tomato’, ‘cow’, ‘dog’, ‘horse’, ‘cup’, ‘car’). Each cat-\negory has 10 objects that belong to the category, and\neach object is recorded under 41 diﬀerent poses. Im-\nages were resized to 32 × 32 pixels ( D = 1024) and\nnormalized to have the same variance. For each cate-\ngory and each object, we model the pose variations by\na subspace of the size m = 1 , ..., 5, spanned by the 1\nto 5 largest eigenvectors from SVD. We evaluate the\nclassiﬁcation rate of the categories with ten-fold cross\nvalidation, holding out one object instance of each cat-\negory from the training set and using it for test.\nThe recognition rates are also summarized in Fig. 2.\nThe GDA1 also outperforms the other methods most\nof the time, but the cMSM performs better than GDA2\nas m increases. The rates seem to peak around m =\n4 and then decrease as m increases. This results is\nconsistent with the observation that the eigenvalues\nfrom this database decrease more gradually than the\neigenvalues from the Yale face database.\n7. Conclusion\nIn this paper we have proposed a Grassmann frame-\nwork for problem in which data consist of subspaces.\nBy using the Projection metric and the Binet-Cauchy\nmetric, which are derived from the Grassmann ker-\nnels, we were able to apply kernel methods such as\nkernel LDA to subspace data. In addition to having\ntheoretically sound grounds, the proposed method also\noutperformed state-of-the-art methods in two experi-\nments with real data. As a future work, we are pur-\nsuing a better understanding of probabilistic distribu-\ntions on the Grassmann manifold.\nReferences\nAbsil, P., Mahony, R., & Sepulchre, R. (2004). Riemannian\ngeometry of Grassmann manifolds with a view on algo-\nrithmic computation. Acta Appl. Math. , 80, 199–220.\nChang, J.-M., Beveridge, J. R., Draper, B. A., Kirby, M.,\nKley, H., & Peterson, C. (2006). Illumination face spaces\nare idiosyncratic. IPCV (pp. 390–396).\nChikuse, Y. (2003). Statistics on special manifolds, lecture\nnotes in statistics, vol. 174 . New York: Springer.\nEdelman, A., Arias, T. A., & Smith, S. T. (1999). The\n382\nGrassmann Discriminant Analysis\nFigure 2. Recognition rates of subjects from Yale face database (Left), and classiﬁcation rates of categories in ETH-80\ndatabase (Right). The bars represent the rates of six algorithms (GDA1, GDA2, Min Dist, MSM, cMSM, DCC) evaluated\nfor m = 1 , ...., 5 where m is the number of basis vectors for subspaces. The GDA1 achieves the best rates consistently,\nand the GDA2 also performs competitively for small m.\ngeometry of algorithms with orthogonality constraints.\nSIAM J. Matrix Anal. Appl. , 20, 303–353.\nFarag´ o, A., Linder, T., & Lugosi, G. (1993). Fast nearest-\nneighbor search in dissimilarity spaces. IEEE Trans.\nPattern Anal. Mach. Intell. , 15, 957–962.\nFukui, K., & Yamaguchi, O. (2003). Face recognition using\nmulti-viewpoint patterns for robot vision. Int. Symp. of\nRobotics Res. (pp. 192–201).\nFukunaga, K. (1990). Introduction to statistical pattern\nrecognition (2nd ed.) . San Diego, CA, USA: Academic\nPress Professional, Inc.\nGeorghiades, A. S., Belhumeur, P. N., & Kriegman, D. J.\n(2001). From few to many: Illumination cone models for\nface recognition under variable lighting and pose. IEEE\nTrans. Pattern Anal. Mach. Intell. , 23, 643–660.\nGolub, G. H., & Loan, C. F. V. (1996). Matrix compu-\ntations (3rd ed.) . Baltimore, MD, USA: Johns Hopkins\nUniversity Press.\nHaasdonk, B. (2005). Feature space interpretation of svms\nwith indeﬁnite kernels. IEEE Trans. Pattern Anal.\nMach. Intell. , 27, 482–492.\nHein, M., Bousquet, O., & Sch¨ olkopf, B. (2005). Maximal\nmargin classiﬁcation for metric spaces. J. Comput. Syst.\nSci., 71, 333–359.\nKim, T.-K., Kittler, J., & Cipolla, R. (2007). Discrimi-\nnative learning and recognition of image set classes us-\ning canonical correlations. IEEE Trans. Pattern Anal.\nMach. Intell. , 29, 1005–1018.\nKondor, R. I., & Jebara, T. (2003). A kernel between sets\nof vectors. Proc. of the 20th Int. Conf. on Mach. Learn.\n(pp. 361–368).\nLeibe, B., & Schiele, B. (2003). Analyzing appearance and\ncontour based methods for object categorization. CVPR,\n02, 409.\nOng, C. S., Mary, X., Canu, S., & Smola, A. J. (2004).\nLearning with non-positive kernels. Proc. of 21st Int.\nConf. on Mach. Learn. (p. 81). New York, NY, USA:\nACM.\nPekalska, E., Paclik, P., & Duin, R. P. W. (2002). A gener-\nalized kernel approach to dissimilarity-based classiﬁca-\ntion. J. Mach. Learn. Res. , 2, 175–211.\nSakano, H.; Mukawa, N. (2000). Kernel mutual subspace\nmethod for robust facial image recognition. Proc. of Int.\nConf. on Knowledge-Based Intell. Eng. Sys. and App.\nTech. (pp. 245–248).\nSchoenberg, I. J. (1938). Metric spaces and positive deﬁnite\nfunctions. Trans. Amer. Math. Soc. , 44, 522–536.\nSch¨ olkopf, B., & Smola, A. J. (2001). Learning with ker-\nnels: Support vector machines, regularization, optimiza-\ntion, and beyond . Cambridge, MA, USA: MIT Press.\nShakhnarovich, G., John W. Fisher, I., & Darrell, T.\n(2002). Face recognition from long-term observations.\nProc. of the 7th Euro. Conf. on Computer Vision (pp.\n851–868). London, UK.\nTurk, M., & Pentland, A. P. (1991). Eigenfaces for recog-\nnition. J. Cog. Neurosc. , 3, 71–86.\nVishwanathan, S., & Smola, A. J. (2004). Binet-cauchy\nkernels. Proc. of Neural Info. Proc. Sys. .\nWang, L., Wang, X., & Feng, J. (2006). Subspace distance\nanalysis with application to adaptive bayesian algorithm\nfor face recognition. Pattern Recogn., 39, 456–464.\nWolf, L., & Shashua, A. (2003). Learning over sets using\nkernel principal angles. J. Mach. Learn. Res. , 4, 913–\n931.\nWong, Y.-C. (1967). Diﬀerential geometry of Grassmann\nmanifolds. Proc. of the Nat. Acad. of Sci., Vol. 57 , 589–\n594.\nYamaguchi, O., Fukui, K., & Maeda, K. (1998). Face\nrecognition using temporal image sequence. Proc. of the\n3rd. Int. Conf. on Face & Gesture Recognition (p. 318).\nWashington, DC, USA: IEEE Computer Society.\nZhou, S. K., & Chellappa, R. (2006). From sample similar-\nity to ensemble similarity: Probabilistic distance mea-\nsures in reproducing kernel hilbert space. IEEE Trans.\nPattern Anal. Mach. Intell. , 28, 917–929.\n383",
  "values": {
    "Non-maleficence": "No",
    "Privacy": "No",
    "Deferral to humans": "No",
    "Beneficence": "No",
    "User influence": "No",
    "Justice": "No",
    "Explicability": "No",
    "Critiqability": "No",
    "Collective influence": "No",
    "Not socially biased": "No",
    "Interpretable (to users)": "No",
    "Fairness": "No",
    "Respect for Law and public interest": "No",
    "Respect for Persons": "No",
    "Autonomy (power to decide)": "No",
    "Transparent (to users)": "No"
  }
}