{
  "pdf": "1390156.1390208",
  "title": "1390156.1390208",
  "author": "Unknown",
  "paper_id": "1390156.1390208",
  "text": "A Dual Coordinate Descent Method for Large-scale Linear SVM\nCho-Jui Hsieh b92085@csie.ntu.edu.tw\nKai-W ei Chang b92084@csie.ntu.edu.tw\nChih-Jen Lin cjlin@csie.ntu.edu.tw\nDepartment of Computer Science, National Taiwan University, Taipei 106, Taiwan\nS. Sathiya Keerthi selvarak@yahoo-inc.com\nYahoo! Research, Santa Clara, USA\nS. Sundararajan ssrajan@yahoo-inc.com\nYahoo! Labs, Bangalore, India\nAbstract\nIn many applications, data appear with a\nhuge number of instances as well as features.\nLinear Support Vector Machines (SVM) is\none of the most popular tools to deal with\nsuch large-scale sparse data. This paper\npresents a novel dual coordinate descent\nmethod for linear SVM with L1- and L2-\nloss functions. The proposed method is sim-\nple and reaches an ϵ-accurate solution in\nO(log(1/ϵ)) iterations. Experiments indicate\nthat our method is much faster than state\nof the art solvers such as Pegasos, TRON,\nSVMperf, and a recent primal coordinate de-\nscent implementation.\n1. Introduction\nSupport vector machines (SVM) (Boser et al., 1992)\nare useful for data classiﬁcation. Given a set of\ninstance-label pairs (xi,yi),i = 1,...,l, xi ∈Rn, yi ∈\n{−1, +1}, SVM requires the solution of the following\nunconstrained optimization problem:\nmin\nw\n1\n2wTw +C\nl∑\ni=1\nξ(w;xi,yi), (1)\nwhere ξ(w;xi,yi) is a loss function, and C ≥ 0 is a\npenalty parameter. Two common loss functions are:\nmax(1 −yiwTxi, 0) and max(1 −yiwTxi, 0)2. (2)\nThe former is called L1-SVM, while the latter is L2-\nSVM. In some applications, an SVM problem appears\nAppearing in Proceedings of the 25 th International Confer-\nence on Machine Learning , Helsinki, Finland, 2008. Copy-\nright 2008 by the author(s)/owner(s).\nwith a bias term b. One often deal with this term by\nappending each instance with an additional dimension:\nxT\ni ← [xT\ni , 1] wT ← [wT,b ]. (3)\nProblem (1) is often referred to as the primal form of\nSVM. One may instead solve its dual problem:\nmin\nα\nf(α) = 1\n2αT ¯Qα −eTα\nsubject to 0 ≤αi ≤U, ∀i, (4)\nwhere ¯Q =Q +D, D is a diagonal matrix, and Qij =\nyiyjxT\nixj. For L1-SVM, U =C and Dii = 0, ∀i. For\nL2-SVM, U = ∞ and Dii = 1/(2C), ∀i.\nAn SVM usually maps training vectors into a high-\ndimensional space via a nonlinear function φ(x). Due\nto the high dimensionality of the vector variable w,\none solves the dual problem (4) by the kernel trick\n(i.e., using a closed form of φ(xi)Tφ(xj)). We call\nsuch a problem as a nonlinear SVM. In some applica-\ntions, data appear in a rich dimensional feature space,\nthe performances are similar with/without nonlinear\nmapping. If data are not mapped, we can often train\nmuch larger data sets. We indicate such cases as linear\nSVM; these are often encountered in applications such\nas document classiﬁcation. In this paper, we aim at\nsolving very large linear SVM problems.\nRecently, many methods have been proposed for lin-\near SVM in large-scale scenarios. For L1-SVM, Zhang\n(2004), Shalev-Shwartz et al. (2007), Bottou (2007)\npropose various stochastic gradient descent methods.\nCollins et al. (2008) apply an exponentiated gradi-\nent method. SVMperf (Joachims, 2006) uses a cutting\nplane technique. Smola et al. (2008) apply bundle\nmethods, and view SVMperf as a special case. For\nL2-SVM, Keerthi and DeCoste (2005) propose mod-\niﬁed Newton methods. A trust region Newton method\n(TRON) (Lin et al., 2008) is proposed for logistic re-\n408\n\nA Dual Coordinate Descent Method for Large-scale Linear SVM\ngression and L2-SVM. These algorithms focus on dif-\nferent aspects of the training speed. Some aim at\nquickly obtaining a usable model, but some achieve\nfast ﬁnal convergence of solving the optimization prob-\nlem in (1) or (4). Moreover, among these methods,\nJoachims (2006), Smola et al. (2008) and Collins et al.\n(2008) solve SVM via the dual (4). Others consider the\nprimal form (1). The decision of using primal or dual\nis of course related to the algorithm design.\nVery recently, Chang et al. (2007) propose using co-\nordinate descent methods for solving primal L2-SVM.\nExperiments show that their approach more quickly\nobtains a useful model than some of the above meth-\nods. Coordinate descent, a popular optimization tech-\nnique, updates one variable at a time by minimizing a\nsingle-variable sub-problem. If one can eﬃciently solve\nthis sub-problem, then it can be a competitive opti-\nmization method. Due to the non-diﬀerentiability of\nthe primal L1-SVM, Chang et al’s work is restricted to\nL2-SVM. Moreover, as primal L2-SVM is diﬀerentiable\nbut not twice diﬀerentiable, certain considerations are\nneeded in solving the single-variable sub-problem.\nWhile the dual form (4) involves bound constraints\n0 ≤αi ≤U, its objective function is twice diﬀerentiable\nfor both L1- and L2-SVM. In this paper, we investi-\ngate coordinate descent methods for the dual problem\n(4). We prove that an ϵ-optimal solution is obtained\nin O(log(1/ϵ)) iterations. We propose an implemen-\ntation using a random order of sub-problems at each\niteration, which leads to very fast training. Experi-\nments indicate that our method is more eﬃcient than\nthe primal coordinate descent method. As Chang et al.\n(2007) solve the primal, they require the easy access\nof a feature’s corresponding data values. However, in\npractice one often has an easier access of values per in-\nstance. Solving the dual takes this advantage, so our\nimplementation is simpler than Chang et al. (2007).\nEarly SVM papers (Mangasarian & Musicant, 1999;\nFriess et al., 1998) have discussed coordinate descent\nmethods for the SVM dual form. However, they do not\nfocus on large data using the linear kernel. Crammer\nand Singer (2003) proposed an online setting for multi-\nclass SVM without considering large sparse data. Re-\ncently, Bordes et al. (2007) applied a coordinate de-\nscent method to multi-class SVM, but they focus on\nnonlinear kernels. In this paper, we point out that\ndual coordinate descent methods make crucial advan-\ntage of the linear kernel and outperform other solvers\nwhen the numbers of data and features are both large.\nCoordinate descent methods for (4) are related to the\npopular decomposition methods for training nonlinear\nSVM. In this paper, we show their key diﬀerences and\nexplain why earlier studies on decomposition meth-\nods failed to modify their algorithms in an eﬃcient\nway like ours for large-scale linear SVM. We also dis-\ncuss the connection to other linear SVM works such as\n(Crammer & Singer, 2003; Collins et al., 2008; Shalev-\nShwartz et al., 2007).\nThis paper is organized as follows. In Section 2, we de-\nscribe our proposed algorithm. Implementation issues\nare investigated in Section 3. Section 4 discusses the\nconnection to other methods. In Section 5, we compare\nour method with state of the art implementations for\nlarge linear SVM. Results show that the new method\nis more eﬃcient. Proofs can be found at http://www.\ncsie.ntu.edu.tw/~cjlin/papers/cddual.pdf.\n2. A Dual Coordinate Descent Method\nIn this section, we describe our coordinate descent\nmethod for L1- and L2-SVM. The optimization pro-\ncess starts from an initial pointα0 ∈Rl and generates\na sequence of vectors {αk}∞\nk=0. We refer to the process\nfromαk toαk+1 as an outer iteration. In each outer\niteration we have l inner iterations, so that sequen-\ntially α1,α 2,...,α l are updated. Each outer iteration\nthus generates vectorsαk,i ∈Rl,i = 1,...,l + 1, such\nthatαk,1 =αk,αk,l+1 =αk+1, and\nαk,i = [αk+1\n1 ,...,α k+1\ni−1,αk\ni,...,α k\nl ]T, ∀i = 2,...,l.\nFor updating αk,i to αk,i+1, we solve the following\none-variable sub-problem:\nmin\nd\nf(αk,i +dei) subject to 0 ≤αk\ni +d ≤U, (5)\nwhereei = [0,..., 0, 1, 0,..., 0]T . The objective func-\ntion of (5) is a simple quadratic function of d:\nf(αk,i +dei) = 1\n2\n¯Qiid2 + ∇if(αk,i)d + constant, (6)\nwhere ∇if is the ith component of the gradient ∇f.\nOne can easily see that (5) has an optimum at d = 0\n(i.e., no need to update αi) if and only if\n∇P\ni f(αk,i) = 0, (7)\nwhere ∇Pf(α) means the projected gradient\n∇P\ni f(α) =\n\n\n\n∇if(α) if 0 <α i <U,\nmin(0, ∇if(α)) if αi = 0,\nmax(0, ∇if(α)) if αi =U.\n(8)\nIf (7) holds, we move to the indexi+1 without updat-\ningαk,i\ni . Otherwise, we must ﬁnd the optimal solution\nof (5). If ¯Qii > 0, easily the solution is:\nαk,i+1\ni = min\n(\nmax\n(\nαk,i\ni − ∇if(αk,i)\n¯Qii\n, 0\n)\n,U\n)\n. (9)\n409\nA Dual Coordinate Descent Method for Large-scale Linear SVM\nAlgorithm 1 A dual coordinate descent method for\nLinear SVM\n• Givenα and the corresponding w =∑\niyiαixi.\n• Whileα is not optimal\nFori = 1,...,l\n(a) ¯αi ←αi\n(b) G =yiwTxi − 1 +Diiαi\n(c)\nPG =\n\n\n\nmin(G, 0) if αi = 0,\nmax(G, 0) if αi =U,\nG if 0 <α i <U\n(d) If |PG | ̸= 0,\nαi ← min(max(αi −G/ ¯Qii, 0),U )\nw ←w + (αi − ¯αi)yixi\nWe thus need to calculate ¯Qii and ∇if(αk,i). First,\n¯Qii =xT\nixi +Dii can be precomputed and stored in\nthe memory. Second, to evaluate ∇if(αk,i), we use\n∇if(α) = ( ¯Qα)i − 1 =\n∑l\nj=1\n¯Qijαj − 1. (10)\n¯Q may be too large to be stored, so one calculates ¯Q’s\nith row when doing (10). If ¯n is the average number\nof nonzero elements per instance, and O(¯n) is needed\nfor each kernel evaluation, then calculating theith row\nof the kernel matrix takes O(l¯n). Such operations are\nexpensive. However, for a linear SVM, we can deﬁne\nw =\n∑l\nj=1\nyjαjxj, (11)\nso (10) becomes\n∇if(α) =yiwTxi − 1 +Diiαi. (12)\nTo evaluate (12), the main cost isO(¯n) for calculating\nwTxi. This is much smaller than O(l¯n). To apply\n(12),w must be maintained throughout the coordinate\ndescent procedure. Calculating w by (11) takes O(l¯n)\noperations, which are too expensive. Fortunately, if\n¯αi is the current value and αi is the value after the\nupdating, we can maintain w by\nw ←w + (αi − ¯αi)yixi. (13)\nThe number of operations is only O(¯n). To have the\nﬁrstw, one can use α0 = 0 sow = 0. In the end, we\nobtain the optimalw of the primal problem (1) as the\nprimal-dual relationship implies (11).\nIf ¯Qii = 0, we have Dii = 0, Qii = xT\nixi = 0, and\nhence xi = 0. This occurs only in L1-SVM without\nthe bias term by (3). From (12), if xi = 0, then\n∇if(αk,i) = −1. As U = C < ∞ for L1-SVM, the\nsolution of (5) makes the new αk,i+1\ni = U. We can\neasily include this case in (9) by setting 1 / ¯Qii = ∞.\nBrieﬂy, our algorithm uses (12) to compute ∇if(αk,i),\nchecks the optimality of the sub-problem (5) by (7),\nupdates αi by (9), and then maintains w by (13). A\ndescription is in Algorithm 1. The cost per iteration\n(i.e., from αk to αk+1) is O(l¯n). The main memory\nrequirement is on storing x1,..., xl. For the conver-\ngence, we prove the following theorem using techniques\nin (Luo & Tseng, 1992):\nTheorem 1 For L1-SVM and L2-SVM, {αk,i} gen-\nerated by Algorithm 1 globally converges to an optimal\nsolution α∗. The convergence rate is at least linear:\nthere are 0<µ< 1 and an iteration k0 such that\nf(αk+1) −f(α∗) ≤µ(f(αk) −f(α∗)), ∀k ≥k0. (14)\nThe global convergence result is quite remarkable.\nUsually for a convex but not strictly convex problem\n(e.g., L1-SVM), one can only obtain that any limit\npoint is optimal. We deﬁne an ϵ-accurate solution α\nif f(α) ≤ f(α∗) +ϵ. By (14), our algorithm obtains\nan ϵ-accurate solution in O(log(1/ϵ)) iterations.\n3. Implementation Issues\n3.1. Random Permutation of Sub-problems\nIn Algorithm 1, the coordinate descent algorithm\nsolves the one-variable sub-problems in the order of\nα1,...,α l. Past results such as (Chang et al., 2007)\nshow that solving sub-problems in an arbitrary order\nmay give faster convergence. This inspires us to ran-\ndomly permute the sub-problems at each outer itera-\ntion. Formally, at the kth outer iteration, we permute\n{1,...,l } to {π(1),...,π (l)}, and solve sub-problems\nin the order of απ(1),απ(2),...,α π(l). Similar to Al-\ngorithm 1, the algorithm generates a sequence {αk,i}\nsuch thatαk,1 =αk,αk,l+1 =αk+1,1 and\nαk,i\nt =\n{\nαk+1\nt if π−1\nk (t)<i,\nαk\nt if π−1\nk (t) ≥i.\nThe update from αk,i toαk,i+1 is by\nαk,i+1\nt =αk,i\nt +arg min\n0≤αk,i\nt +d≤U\nf(αk,i+det) ifπ−1\nk (t) =i.\nWe prove that Theorem 1 is still valid. Hence, the new\nsetting obtains anϵ-accurate solution inO(log(1/ϵ)) it-\nerations. A simple experiment reveals that this setting\nof permuting sub-problems is much faster than Algo-\nrithm 1. The improvement is also bigger than that\nobserved in (Chang et al., 2007) for primal coordinate\ndescent methods.\n410\nA Dual Coordinate Descent Method for Large-scale Linear SVM\nAlgorithm 2 Coordinate descent algorithm with ran-\ndomly selecting one instance at a time\n• Givenα and the corresponding w =∑\niyiαixi.\n• Whileα is not optimal\n– Randomly choose i ∈ {1,...,l }.\n– Do steps (a)-(d) of Algorithm 1 to updateαi.\n3.2. Shrinking\nEq. (4) contains constraints 0 ≤ αi ≤ U. If an\nαi is 0 or U for many iterations, it may remain the\nsame. To speed up decomposition methods for non-\nlinear SVM (discussed in Section 4.1), the shrinking\ntechnique (Joachims, 1998) reduces the size of the op-\ntimization problem without considering some bounded\nvariables. Below we show it is much easier to apply this\ntechnique to linear SVM than the nonlinear case.\nIf A is the subset after removing some elements and\n¯A = {1,...,l } \\A, then the new problem is\nmin\nαA\n1\n2αT\nA ¯QAAαA + ( ¯QA ¯Aα ¯A −eA)TαA\nsubject to 0 ≤αi ≤U,i ∈A, (15)\nwhere ¯QAA, ¯QA ¯A are sub-matrices of ¯Q, and α ¯A is\nconsidered as a constant vector. Solving this smaller\nproblem consumes less time and memory. Once (15) is\nsolved, we must check if the vectorα is optimal for (4).\nThis check needs the whole gradient ∇f(α). Since\n∇if(α) = ¯Qi,AαA + ¯Qi, ¯Aα ¯A − 1,\nifi ∈A, and one stores ¯Qi, ¯Aα ¯A before solving (15), we\nalready have ∇if(α). However, for all i /∈A, we must\ncalculate the corresponding rows of ¯Q. This step, re-\nferred to as the reconstruction of gradients in training\nnonlinear SVM, is very time consuming. It may cost\nup to O(l2¯n) if each kernel evaluation is O(¯n).\nFor linear SVM, in solving the smaller problem (15),\nwe still have the vector\nw =\n∑\ni∈A\nyiαixi +\n∑\ni∈ ¯A\nyiαixi\nthough only the ﬁrst part ∑\ni∈Ayiαixi is updated.\nTherefore, using (12), ∇f(α) is easily available. Below\nwe demonstrate a shrinking implementation so that re-\nconstructing the whole ∇f(α) is never needed.\nOur method is related to what LIBSVM (Chang & Lin,\n2001) uses. From the optimality condition of bound-\nconstrained problems,α is optimal for (4) if and only if\n∇Pf(α) = 0, where ∇Pf(α) is the projected gradient\ndeﬁned in (8). We then prove the following result:\nTheorem 2 Letα∗ be the convergent point of {αk,i}.\n1. If α∗\ni = 0 and ∇if(α∗) > 0, then ∃ki such that\n∀k ≥ki, ∀s,αk,s\ni = 0.\n2. If α∗\ni = U and ∇if(α∗) < 0, then ∃ki such that\n∀k ≥ki, ∀s, αk,s\ni =U .\n3. lim\nk→∞\nmax\nj\n∇P\nj f(αk,j)= lim\nk→∞\nmin\nj\n∇P\nj f(αk,j)=0.\nDuring the optimization procedure, ∇Pf(αk) ̸= 0, so\nin general maxj ∇P\nj f(αk)> 0 and minj ∇P\nj f(αk)< 0.\nThese two values measure how the current solution vi-\nolates the optimality condition. In our iterative proce-\ndure, what we have are ∇if(αk,i), i= 1,...,l . Hence,\nat the (k − 1)st iteration, we obtain\nMk−1 ≡ max\nj\n∇P\nj f(αk−1,j),mk−1 ≡ min\nj\n∇P\nj f(αk−1,j).\nThen at each inner step of the kth iteration, before\nupdating αk,i\ni to αk,i+1\ni , this element is shrunken if\none of the following two conditions holds:\nαk,i\ni = 0 and ∇if(αk,i)> ¯Mk−1,\nαk,i\ni =U and ∇if(αk,i)< ¯mk−1,\n(16)\nwhere ¯Mk−1 =\n{\nMk−1 if Mk−1 > 0,\n∞ otherwise,\n¯mk−1 =\n{\nmk−1 if mk−1 < 0\n−∞ otherwise.\nIn (16), ¯Mk−1 must be strictly positive, so we set it be\n∞ ifMk−1 = 0. From Theorem 2, elements satisfying\nthe “if condition” of properties 1 and 2 meet (16) after\ncertain iterations, and are then correctly removed for\noptimization. To have a more aggressive shrinking,\none may multiply both ¯Mk−1 and ¯mk−1 in (16) by a\nthreshold smaller than one.\nProperty 3 of Theorem 2 indicates that with a toler-\nance ϵ,\nMk −mk <ϵ (17)\nis satisﬁed after a ﬁnite number of iterations. Hence\n(17) is a valid stopping condition. We also use it for\nsmaller problems (15). If at the kth iteration, (17)\nfor (15) is reached, we enlarge A to {1,...,l }, set\n¯Mk = ∞, ¯mk = −∞ (so no shrinking at the ( k + 1)st\niteration), and continue regular iterations. Thus, we\ndo shrinking without reconstructing gradients.\n3.3. An Online Setting\nIn some applications, the number of instances is huge,\nso going over all α1,...,α l causes an expensive outer\niteration. Instead, one can randomly choose an index\nik at a time, and update only αik at the kth outer\niteration. A description is in Algorithm 2. The setting\nis related to (Crammer & Singer, 2003; Collins et al.,\n2008). See also the discussion in Section 4.2.\n411\nA Dual Coordinate Descent Method for Large-scale Linear SVM\nTable 1. A comparison between decomposition methods\n(Decomp.) and dual coordinate descent ( DCD). For both\nmethods, we consider that oneαi is updated at a time. We\nassume Decomp. maintains gradients, but DCD does not.\nThe average number of nonzeros per instance is ¯n.\nNonlinear SVM Linear SVM\nDecomp. DCD Decomp. DCD\nUpdate αi O(1) O(l¯n) O(1) O(¯n)\nMaintain ∇f(α) O(l¯n) NA O(l¯n) NA\n4. Relations with Other Methods\n4.1. Decomposition Methods for Nonlinear\nSVM\nDecomposition methods are one of the most popular\napproaches for training nonlinear SVM. As the kernel\nmatrix is dense and cannot be stored in the computer\nmemory, decomposition methods solve a sub-problem\nof few variables at each iteration. Only a small num-\nber of corresponding kernel columns are needed, so the\nmemory problem is resolved. If the number of vari-\nables is restricted to one, a decomposition method is\nlike the online coordinate descent in Section 3.3, but\nit diﬀers in the way it selects variables for updating.\nIt has been shown (Keerthi & DeCoste, 2005) that,\nfor linear SVM decomposition methods are ineﬃcient.\nOn the other hand, here we are pointing out that dual\ncoordinate descent is eﬃcient for linear SVM. There-\nfore, it is important to discuss the relationship between\ndecomposition methods and our method.\nIn early decomposition methods that were ﬁrst pro-\nposed (Osuna et al., 1997; Platt, 1998), variables min-\nimized at an iteration are selected by certain heuristics.\nHowever, subsequent developments (Joachims, 1998;\nChang & Lin, 2001; Keerthi et al., 2001) all use gra-\ndient information to conduct the selection. The main\nreason is that maintaining the whole gradient does not\nintroduce extra cost. Here we explain the detail by as-\nsuming that one variable ofα is chosen and updated at\na time1. To set-up and solve the sub-problem (6), one\nuses (10) to calculate ∇if(α). If O(¯n) eﬀort is needed\nfor each kernel evaluation, obtaining the ith row of\nthe kernel matrix takes O(l¯n) eﬀort. If instead one\nmaintains the whole gradient, then ∇if(α) is directly\navailable. After updating αk,i\ni toαk,i+1\ni , we obtain ¯Q’s\nith column (same as the ith row due to the symmetry\nof ¯Q), and calculate the new whole gradient:\n∇f(αk,i+1) = ∇f(αk,i) + ¯Q:,i(αk,i+1\ni −αk,i\ni ), (18)\nwhere ¯Q:,i is the ith column of ¯Q. The cost is O(l¯n)\nfor ¯Q:,i and O(l) for (18). Therefore, maintaining the\n1Solvers like LIBSVM update at least two variables due\nto a linear constraint in their dual problems. Here (4) has\nno such a constraint, so selecting one variable is possible.\nwhole gradient does not cost more. As using the whole\ngradient implies fewer iterations (i.e., faster conver-\ngence due to the ability to choose for updating the vari-\nable that violates optimality most), one should take\nthis advantage. However, the situation for linear SVM\nis very diﬀerent. With the diﬀerent way (12) to calcu-\nlate ∇if(α), the cost to update oneαi is onlyO(¯n). If\nwe still maintain the whole gradient, evaluating (12) l\ntimes takesO(l¯n) eﬀort. We gather this comparison of\ndiﬀerent situations in Table 1. Clearly, for nonlinear\nSVM, one should use decomposition methods by main-\ntaining the whole gradient. However, for linear SVM,\nifl is large, the cost per iteration without maintaining\ngradients is much smaller than that with. Hence, the\ncoordinate descent method can be faster than the de-\ncomposition method by using many cheap iterations.\nAn earlier attempt to speed up decomposition methods\nfor linear SVM is (Kao et al., 2004). However, it failed\nto derive our method here because it does not give up\nmaintaining gradients.\n4.2. Existing Linear SVM Methods\nWe discussed in Section 1 and other places the dif-\nference between our method and a primal coordinate\ndescent method (Chang et al., 2007). Below we de-\nscribe the relations with other linear SVM methods.\nWe mentioned in Section 3.3 that our Algorithm 2 is\nrelated to the online mode in (Collins et al., 2008).\nThey aim at solving multi-class and structured prob-\nlems. At each iteration an instance is used; then a\nsub-problem of several variables is solved. They ap-\nproximately minimize the sub-problem, but for two-\nclass case, one can exactly solve it by (9). For the\nbatch setting, our approach is diﬀerent from theirs.\nThe algorithm for multi-class problems in (Crammer &\nSinger, 2003) is also similar to our online setting. For\nthe two-class case, it solves (1) with the loss function\nmax(−yiwTxi, 0), which is diﬀerent from (2). They\ndo not study data with a large number of features.\nNext, we discuss the connection to stochastic gradient\ndescent (Shalev-Shwartz et al., 2007; Bottou, 2007).\nThe most important step of this method is the follow-\ning update of w:\nw ←w −η∇w(yi,xi), (19)\nwhere ∇w(yi,xi) is the sub-gradient of the approxi-\nmate objective function:\nwTw/2 +C max(1 −yiwTxi, 0),\nandη is the learning rate (or the step size). While our\nmethod is dual-based, throughout the iterations we\n412\nA Dual Coordinate Descent Method for Large-scale Linear SVM\nTable 2. On the right training time for a solver to reduce the primal objective value to within 1% of the optimal value;\nsee (20). Time is in seconds. The method with the shortest running time is boldfaced. Listed on the left are the statistics\nof data sets: l is the number of instances and n is the number of features.\nData set Data statistics Linear L1-SVM Linear L2-SVM\nl n # nonzeros DCDL1 Pegasos SVM perf DCDL2 PCD TRON\na9a 32,561 123 451,592 0.2 1.1 6.0 0.4 0.1 0.1\nastro-physic 62,369 99,757 4,834,550 0.2 2.8 2.6 0.2 0.5 1.2\nreal-sim 72,309 20,958 3,709,083 0.2 2.4 2.4 0.1 0.2 0.9\nnews20 19,996 1,355,191 9,097,916 0.5 10.3 20.0 0.2 2.4 5.2\nyahoo-japan 176,203 832,026 23,506,415 1.1 12.7 69.4 1.0 2.9 38.2\nrcv1 677,399 47,236 49,556,258 2.6 21.9 72.0 2.7 5.1 18.6\nyahoo-korea 460,554 3,052,939 156,436,656 8.3 79.7 656.8 7.1 18.4 286.1\nmaintainw by (13). Both (13) and (19) use one single\ninstancexi, but they take diﬀerent directionsyixi and\n∇w(yi,xi). The selection of the learning rateη may be\nthe subtlest thing in stochastic gradient descent, but\nfor our method this is never a concern. The step size\n(αi − ¯αi) in (13) is governed by solving a sub-problem\nfrom the dual.\n5. Experiments\nIn this section, we analyze the performance of our dual\ncoordinate descent algorithm for L1- and L2-SVM. We\ncompare our implementation with state of the art lin-\near SVM solvers. We also investigate how the shrink-\ning technique improves our algorithm.\nTable 2 lists the statistics of data sets. Four of them\n(a9a, real-sim, news20, rcv1) are at http://www.csie.\nntu.edu.tw/~cjlin/libsvmtools/datasets. The\nset astro-physic is available upon request from\nThorsten Joachims. Except a9a, all others are from\ndocument classiﬁcation. Past results show that lin-\near SVM performs as well as kernelized ones for such\ndata. To estimate the testing accuracy, we use a strat-\niﬁed selection to split each set to 4/5 training and 1/5\ntesting. We brieﬂy describe each set below. Details\ncan be found in (Joachims, 2006) ( astro-physic) and\n(Lin et al., 2008) (others). a9a is from the UCI “adult”\ndata set. real-sim includes Usenet articles. astro-physic\nincludes documents from Physics Arxiv. news20 is a\ncollection of news documents. yahoo-japan and yahoo-\nkorea are obtained from Yahoo!. rcv1 is an archive of\nmanually categorized newswire stories from Reuters.\nWe compare six implementations of linear SVM. Three\nsolve L1-SVM, and three solve L2-SVM.\nDCDL1 and DCDL2: the dual coordinate descent\nmethod with sub-problems permuted at each outer it-\neration (see Section 3.1). DCDL1 solves L1-SVM while\nDCDL2 solves L2-SVM. We omit the shrinking setting .\nPegasos: the primal estimated sub-gradient solver\n(Shalev-Shwartz et al., 2007) for L1-SVM. The source\nis at http://ttic.uchicago.edu/~shai/code.\nSVMperf (Joachims, 2006): a cutting plane method for\nL1-SVM. We use the latest version 2.1. The source is\nat http://svmlight.joachims.org/svm_perf.html.\nTRON: a trust region Newton method (Lin et al., 2008)\nfor L2-SVM. We use the software LIBLINEAR version\n1.22 with option -s 2 (http://www.csie.ntu.edu.\ntw/~cjlin/liblinear).\nPCD: a primal coordinate descent method for L2-SVM\n(Chang et al., 2007).\nSince (Bottou, 2007) is related to Pegasos, we do not\npresent its results. We do not compare with another\nonline method Vowpal Wabbit (Langford et al., 2007)\neither as it has been made available only very recently.\nThough a code for the bundle method (Smola et al.,\n2008) is available, we do not include it for comparison\ndue to its closeness to SVMperf. All sources used for\nour comparisons are available at http://csie.ntu.\nedu.tw/~cjlin/liblinear/exp.html.\nWe set the penalty parameter C = 1 for comparison2.\nFor all data sets, the testing accuracy does not increase\nafter C ≥ 4. All the above methods are implemented\nin C/C++ with double precision. Some implementa-\ntions such as (Bottou, 2007) use single precision to\nreduce training time, but numerical inaccuracy may\noccur. We do not include the bias term by (3).\nTo compare these solvers, we consider the CPU time\nof reducing the relative diﬀerence between the primal\nobjective value and the optimum to within 0.01:\n|fP (w) −fP (w∗)|/|fP (w∗)| ≤ 0.01, (20)\nwherefP is the objective function of (1), and fP (w∗)\nis the optimal value. Note that for consistency, we use\nprimal objective values even for dual solvers. The ref-\nerence solutions of L1- and L2-SVM are respectively\nobtained by solving DCDL1 and DCDL2 until the du-\nality gaps are less than 10 −6. Table 2 lists the re-\nsults. Clearly, our dual coordinate descent method\n2The equivalent setting for Pegasos is λ = 1/(Cl). For\nSVMperf, its penalty parameter is Cperf = 0.01Cl.\n413\nA Dual Coordinate Descent Method for Large-scale Linear SVM\n(a) L1-SVM: astro-physic\n (b) L2-SVM: astro-physic\n(c) L1-SVM: news20\n (d) L2-SVM: news20\n(e) L1-SVM: rcv1\n (f) L2-SVM: rcv1\nFigure 1. Time versus the relative error (20). DCDL1-S,\nDCDL2-S are DCDL1, DCDL2 with shrinking. The dotted\nline indicates the relative error 0.01. Time is in seconds.\nfor both L1- and L2-SVM is signiﬁcantly faster than\nother solvers. To check details, we choose astro-physic,\nnews20, rcv1, and show the relative error along time\nin Figure 1. In Section 3.2, we pointed out that the\nshrinking technique is very suitable for DCD. In Fig-\nure 1, we also include them ( DCDL1-S and DCDL2-S)\nfor comparison. Like in Table 2, our solvers are eﬃ-\ncient for both L1- and L2-SVM. With shrinking, its\nperformance is even better.\nAnother evaluation is to consider how fast a solver ob-\ntains a model with reasonable testing accuracy. Using\nthe optimal solutions from the above experiment, we\ngenerate the reference models for L1- and L2-SVM. We\nevaluate the testing accuracy diﬀerence between the\ncurrent model and the reference model along the train-\ning time. Figure 2 shows the results. Overall, DCDL1\nand DCDL2 are more eﬃcient than other solvers. Note\nthat we omit DCDL1-S and DCDL2-S in Figure 2, as\nthe performances with/without shrinking are similar.\nAmong L1-SVM solvers, SVMperf is competitive with\nPegasos for small data. But in the case of a huge num-\nber of instances, Pegasos outperforms SVMperf. How-\never, Pegasos has slower convergence than DCDL1. As\ndiscussed in Section 4.2, the learning rate of stochas-\ntic gradient descent may be the cause, but for DCDL1\nwe exactly solve sub-problems to obtain the step size\n(a) L1-SVM: astro-physic\n (b) L2-SVM: astro-physic\n(c) L1-SVM: news20\n (d) L2-SVM: news20\n(e) L1-SVM: rcv1\n (f) L2-SVM: rcv1\nFigure 2. Time versus the diﬀerence of testing accuracy be-\ntween the current model and the reference model (obtained\nusing strict stopping conditions). Time is in seconds.\nin updating w. Also, Pegasos has a jumpy test set\nperformance while DCDL1 gives a stable behavior.\nIn the comparison of L2-SVM solvers,DCDL2 and PCD\nare both coordinate descent methods. The former one\nis applied to the dual, but the latter one to the pri-\nmal. DCDL2 has a closed form solution for each sub-\nproblem, but PCD has not. The cost per PCD outer\niteration is thus higher than that of DCDL2. There-\nfore, while PCD is very competitive (only second to\nDCDL1/DCDL2 in Table 2), DCDL2 is even better.\nRegarding TRON, as a Newton method, it possesses\nfast ﬁnal convergence. However, since it takes signiﬁ-\ncant eﬀort at each iteration, it hardly generates a rea-\nsonable model quickly. From the experiment results,\nDCDL2 converges as fast as TRON, but also performs\nwell in early iterations.\nDue to the space limitation, we give the following ob-\nservations without details. First, Figure 1 indicates\nthat our coordinate descent method converges faster\nfor L2-SVM than L1-SVM. As L2-SVM has the diag-\nonal matrix D with Dii = 1/(2C), we suspect that\nits ¯Q is better conditioned, and hence leads to faster\nconvergence. Second, all methods have slower conver-\ngence whenC is large. However, small C’s are usually\nenough as the accuracy is stable after a threshold. In\npractice, one thus should try from a small C. More-\n414\nA Dual Coordinate Descent Method for Large-scale Linear SVM\nover, if n ≪ l and C is too large, then our DCDL2 is\nslower than TRON or PCD (see problem a9a in Table\n2, where the accuracy does not change afterC ≥ 0.25).\nIf n ≪ l, clearly one should solve the primal, whose\nnumber of variables is justn. Such data are not our fo-\ncus. Indeed, with a small number of features, one usu-\nally maps data to a higher space and train a nonlinear\nSVM. Third, we have checked the online Algorithm 2.\nIts performance is similar to DCDL1 and DCDL2 (i.e.,\nbatch setting without shrinking). Fourth, we have in-\nvestigated real document classiﬁcation which involves\nmany two-class problems. Using the proposed method\nas the solver is more eﬃcient than using others.\n6. Discussion and Conclusions\nWe can apply the proposed method to solve regular-\nized least square problems, which have the loss func-\ntion (1 −yiwTxi)2 in (1). The dual is simply (4) with-\nout constraints, so the implementation is simpler.\nIn summary, we present and analyze an eﬃcient dual\ncoordinate decent method for large linear SVM. It is\nvery simple to implement, and possesses sound op-\ntimization properties. Experiments show that our\nmethod is faster than state of the art implementations.\nReferences\nBordes, A., Bottou, L., Gallinari, P., & Weston, J.\n(2007). Solving multiclass support vector machines\nwith LaRank. ICML.\nBoser, B. E., Guyon, I., & Vapnik, V. (1992). A train-\ning algorithm for optimal margin classiﬁers. COLT.\nBottou, L. (2007). Stochastic gradient descent exam-\nples. http://leon.bottou.org/projects/sgd.\nChang, C.-C., & Lin, C.-J. (2001). LIBSVM: a library\nfor support vector machines . Software available at\nhttp://www.csie.ntu.edu.tw/~cjlin/libsvm.\nChang, K.-W., Hsieh, C.-J., & Lin, C.-J. (2007). Coor-\ndinate descent method for large-scale L2-loss linear\nSVM (Technical Report). http://www.csie.ntu.\nedu.tw/~cjlin/papers/cdl2.pdf.\nCollins, M., Globerson, A., Koo, T., Carreras, X.,\n& Bartlett, P. (2008). Exponentiated gradient al-\ngorithms for conditional random ﬁelds and max-\nmargin markov networks. JMLR. To appear.\nCrammer, K., & Singer, Y. (2003). Ultraconservative\nonline algorithms for multiclass problems. JMLR,\n3, 951–991.\nFriess, T.-T., Cristianini, N., & Campbell, C. (1998).\nThe kernel adatron algorithm: a fast and sim-\nple learning procedure for support vector machines.\nICML.\nJoachims, T. (1998). Making large-scale SVM learning\npractical. Advances in Kernel Methods - Support\nVector Learning. Cambridge, MA: MIT Press.\nJoachims, T. (2006). Training linear SVMs in linear\ntime. ACM KDD.\nKao, W.-C., Chung, K.-M., Sun, C.-L., & Lin, C.-J.\n(2004). Decomposition methods for linear support\nvector machines. Neural Comput., 16, 1689–1704.\nKeerthi, S. S., & DeCoste, D. (2005). A modiﬁed ﬁnite\nNewton method for fast solution of large scale linear\nSVMs. JMLR, 6, 341–361.\nKeerthi, S. S., Shevade, S. K., Bhattacharyya, C., &\nMurthy, K. R. K. (2001). Improvements to Platt’s\nSMO algorithm for SVM classiﬁer design. Neural\nComput., 13, 637–649.\nLangford, J., Li, L., & Strehl, A. (2007). Vowpal Wab-\nbit. http://hunch.net/~vw.\nLin, C.-J., Weng, R. C., & Keerthi, S. S. (2008). Trust\nregion Newton method for large-scale logistic regres-\nsion. JMLR, 9, 623–646.\nLuo, Z.-Q., & Tseng, P. (1992). On the convergence of\ncoordinate descent method for convex diﬀerentiable\nminimization. J. Optim. Theory Appl. , 72, 7–35.\nMangasarian, O. L., & Musicant, D. R. (1999). Suc-\ncessive overrelaxation for support vector machines.\nIEEE Trans. Neural Networks , 10, 1032–1037.\nOsuna, E., Freund, R., & Girosi, F. (1997). Train-\ning support vector machines: An application to face\ndetection. CVPR.\nPlatt, J. C. (1998). Fast training of support vector ma-\nchines using sequential minimal optimization. Ad-\nvances in Kernel Methods - Support Vector Learn-\ning. Cambridge, MA: MIT Press.\nShalev-Shwartz, S., Singer, Y., & Srebro, N. (2007).\nPegasos: primal estimated sub-gradient solver for\nSVM. ICML.\nSmola, A. J., Vishwanathan, S. V. N., & Le, Q. (2008).\nBundle methods for machine learning. NIPS.\nZhang, T. (2004). Solving large scale linear predic-\ntion problems using stochastic gradient descent al-\ngorithms. ICML.\n415",
  "values": {
    "Privacy": "Yes",
    "Justice": "Yes",
    "Interpretable (to users)": "Yes",
    "Collective influence": "Yes",
    "User influence": "Yes",
    "Fairness": "Yes",
    "Non-maleficence": "Yes",
    "Respect for Law and public interest": "Yes",
    "Transparent (to users)": "No",
    "Explicability": "No",
    "Critiqability": "No",
    "Deferral to humans": "No",
    "Autonomy (power to decide)": "No",
    "Respect for Persons": "No",
    "Not socially biased": "No",
    "Beneficence": "No"
  }
}