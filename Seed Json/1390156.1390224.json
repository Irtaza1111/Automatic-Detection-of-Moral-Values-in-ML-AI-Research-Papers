{
  "pdf": "1390156.1390224",
  "title": "1390156.1390224",
  "author": "Unknown",
  "paper_id": "1390156.1390224",
  "text": "Classiﬁcation using Discriminative Restricted Boltzmann Machines\nHugo Larochelle larocheh@iro.umontreal.ca\nY oshua Bengio bengioy@iro.umontreal.ca\nDept. IRO, Universit´ e de Montr´ eal C.P. 6128, Montreal, Qc, H3C 3J7, Canada\nAbstract\nRecently, many applications for Restricted\nBoltzmann Machines (RBMs) have been de-\nveloped for a large variety of learning prob-\nlems. However, RBMs are usually used as\nfeature extractors for another learning al-\ngorithm or to provide a good initialization\nfor deep feed-forward neural network clas-\nsiﬁers, and are not considered as a stand-\nalone solution to classiﬁcation problems. In\nthis paper, we argue that RBMs provide a\nself-contained framework for deriving com-\npetitive non-linear classiﬁers. We present an\nevaluation of diﬀerent learning algorithms for\nRBMs which aim at introducing a discrimi-\nnative component to RBM training and im-\nprove their performance as classiﬁers. This\napproach is simple in that RBMs are used\ndirectly to build a classiﬁer, rather than as a\nstepping stone. Finally, we demonstrate how\ndiscriminative RBMs can also be successfully\nemployed in a semi-supervised setting.\n1. Introduction\nRestricted Boltzmann Machines (RBMs) (Smolensky,\n1986) are generative models based on latent (usually\nbinary) variables to model an input distribution, and\nhave seen their applicability grow to a large variety\nof problems and settings in the past few years. From\nbinary inputs, they have been extended to model var-\nious types of input distributions (Welling et al., 2005;\nHinton et al., 2006). Conditional versions of RBMs\nhave also been developed for collaborative ﬁltering\n(Salakhutdinov et al., 2007) and to model motion cap-\nture data (Taylor et al., 2006) and video sequences\n(Sutskever & Hinton, 2007).\nRBMs have been particularly successful in classiﬁca-\ntion problems either as feature extractors for text and\nAppearing in Proceedings of the 25 th International Confer-\nence on Machine Learning , Helsinki, Finland, 2008. Copy-\nright 2008 by the author(s)/owner(s).\nimage data (Gehler et al., 2006) or as a good initial\ntraining phase for deep neural network classiﬁers (Hin-\nton, 2007). However, in both cases, the RBMs are\nmerely the ﬁrst step of another learning algorithm, ei-\nther providing a preprocessing of the data or an initial-\nization for the parameters of a neural network. When\ntrained in an unsupervised fashion, RBMs provide no\nguarantees that the features implemented by their hid-\nden layer will ultimately be useful for the supervised\ntask that needs to be solved. More practically, model\nselection can also become problematic, as we need to\nexplore jointly the space of hyper-parameters of both\nthe RBM (size of the hidden layer, learning rate, num-\nber of training iterations) and the supervised learning\nalgorithm that is fed the learned features. In partic-\nular, having two separate learning phases (feature ex-\ntraction, followed by classiﬁer training) can be prob-\nlematic in an online learning setting.\nIn this paper, we argue that RBMs can be used suc-\ncessfully as stand-alone non-linear classiﬁers along-\nside other standard classiﬁers like neural networks\nand Support Vector Machines, and not only as fea-\nture extractors. We investigate training objectives for\nRBMs that are more appropriate for training clas-\nsiﬁers than the common generative objective. We\ndescribe Discriminative Restricted Boltzmann Ma-\nchines (DRBMs), i.e. RBMs that are trained more\nspeciﬁcally to be good classiﬁcation models, and Hy-\nbrid Discriminative Restricted Boltzmann Machines\n(HDRBMs) which explore the space between discrim-\ninative and generative learning and can combine their\nadvantages. We also demonstrate that RBMs can be\nsuccessfully adapted to the common semi-supervised\nlearning setting (Chapelle et al., 2006) for classiﬁca-\ntion problems. Finally, the algorithms investigated in\nthis paper are well suited for online learning on large\ndatasets.\n2. Restricted Boltzmann Machines\nRestricted Boltzmann Machines are undirected gener-\native models that use a layer of hidden variables to\nmodel a distribution over visible variables. Though\nthey are most often trained to only model the inputs\n536\n\nClassiﬁcation using Discriminative Restricted Boltzmann Machines\nof a classiﬁcation task, they can also model the joint\ndistribution of the inputs and associated target classes\n(e.g. in the last layer of a Deep Belief Network in Hin-\nton et al. (2006)). In this section, we will focus on\nsuch joint models.\nWe assume given a training set Dtrain ={(xi, y i)},\ncomprising for the i-th example an input vector xi and\na target class yi∈{1, . . . , C }. To train a generative\nmodel on such data we consider minimization of the\nnegative log-likelihood\nLgen(Dtrain) =−\n|Dtrain|∑\ni=1\nlog p(yi, xi). (1)\nAn RBM with n hidden units is a parametric model\nof the joint distribution between a layer of hidden\nvariables (referred to as neurons or features) h =\n(h1, . . . , h n) and the observed variables made of x =\n(x1, . . . , x d) and y, that takes the form\np(y, x, h) ∝e− E(y, x, h)\nwhere\nE(y, x, h) = −hT Wx−bT x−cT h−dT ⃗ y−hT U⃗ y\nwith parameters Θ = ( W, b, c, d, U) and ⃗ y =\n(1y=i)C\ni=1 for C classes. This model is illustrated in\nFigure 2. For now, we consider for simplicity binary\ninput variables, but the model can be easily gener-\nalized to non-binary categories, integer-valued, and\ncontinuous-valued inputs (Welling et al., 2005; Hinton\net al., 2006). It is straightforward to show that\np(x|h) =\n∏\ni\np(xi|h)\np(xi = 1|h) = sigm( bi +\n∑\nj\nWjihj) (2)\np(y|h) = edy+P\nj Ujy hj\n∑\ny∗ edy∗+P\nj Ujy∗hj\n(3)\nwhere sigm is the logistic sigmoid. Equations 2 and 3\nillustrate that the hidden units are meant to capture\npredictive information about the input vector as well\nas the target class. p(h|y, x) also has a similar form:\np(h|y, x) =\n∏\nj\np(hj|y, x)\np(hj = 1|y, x) = sigm( cj + Ujy +\n∑\ni\nWjixi).\nWhen the number of hidden variables is ﬁxed, an RBM\ncan be considered a parametric model, but when it\nis allowed to vary with the data, it becomes a non-\nparametric model. In particular, Freund and Haus-\nsler (1994); Le Roux and Bengio (2008) showed that\n  0   0   0  1\ny\nx\nh\nU W\ny\nFigure 1. Restricted Boltzmann Machine modeling the\njoint distribution of inputs and target classes\nan RBM with enough hidden units can represent any\ndistribution over binary vectors, and that adding hid-\nden units guarantees that a better likelihood can be\nachieved, unless the generated distribution already\nequals the training distribution.\nIn order to minimize the negative log-likelihood (eq. 1),\nwe would like an estimator of its gradient with respect\nto the model parameters. The exact gradient, for any\nparameter θ∈Θ can be written as follows:\n∂ log p(yi, xi)\n∂θ = −E Eh|yi, xi\n[∂\n∂θ E(yi, xi, h)\n]\n+E Ey, x, h\n[∂\n∂θ E(y, x, h)\n]\n.\nThough the ﬁrst expectation is tractable, the second\none is not. Fortunately, there exists a good stochastic\napproximation of this gradient, called the contrastive\ndivergence gradient (Hinton, 2002). This approxima-\ntion replaces the expectation by a sample generated\nafter a limited number of Gibbs sampling iterations,\nwith the sampler’s initial state for the visible variables\ninitialized at the training sample ( yi, xi). Even when\nusing only one Gibbs sampling iteration, contrastive\ndivergence has been shown to produce only a small\nbias for a large speed-up in training time (Carreira-\nPerpi˜ nan & Hinton, 2005).\nOnline training of an RBM thus consists in cy-\ncling through the training examples and updating the\nRBM’s parameters according to Algorithm 1, where\nthe learning rate is controlled by λ.\nComputing p(y, x) is intractable, but it is possible\nto compute p(y|x), sample from it, or choose the\nmost probable class under this model. As shown in\nSalakhutdinov et al. (2007), for reasonable numbers of\nclasses C (over which we must sum), this conditional\ndistribution can be computed exactly and eﬃciently,\nby writing it as follows:\np(y|x) =\nedy\n∏ n\nj=1\n(\n1 + ecj +Ujy +P\ni Wji xi\n)\n∑\ny∗ edy∗ ∏ n\nj=1\n(\n1 + ecj +Ujy∗+P\ni Wji xi\n) .\n537\nClassiﬁcation using Discriminative Restricted Boltzmann Machines\nAlgorithm 1 Training update for RBM over ( y, x)\nusing Contrastive Divergence\nInput: training pair ( yi, xi) and learning rate λ\n% Notation: a←b means a is set to value b\n% a∼p means a is sampled from p\n% Positive phase\ny0←yi, x0←xi, bh0←sigm(c + W x0 + U ⃗y0)\n% Negative phase\nh0∼p(h|y0, x0), y1∼p(y|h0), x1∼p(x|h0)\nbh1←sigm(c + W x1 + U ⃗y1)\n% Update\nfor θ∈Θ do\nθ←θ−λ\n“\n∂\n∂θ E(y0, x0, bh0)−∂\n∂θ E(y1, x1, bh1)\n”\nend for\nPrecomputing the terms cj + ∑\ni Wjixi and reusing\nthem when computing ∏ n\nj=1\n(\n1 + ecj +Ujy∗+P\ni Wji xi\n)\nfor all classes y∗ permits to compute this conditional\ndistribution in time O(nd + nC).\n3. Discriminative Restricted Boltzmann\nMachines\nIn a classiﬁcation setting, one is ultimately only inter-\nested in correct classiﬁcation, not necessarily to have\na good p(x). Because our model’s p(x) can be in-\nappropriate, it can then be advantageous to optimize\ndirectly p(y|x) instead of p(y, x):\nLdisc(Dtrain) =−\n|Dtrain|∑\ni=1\nlog p(yi|xi). (4)\nWe refer to RBMs trained according to Ldisc as Dis-\ncriminative RBMs (DRBMs). Since RBMs (with\nenough hidden units) are universal approximators for\nbinary inputs, it follows also that DRBMs are uni-\nversal approximators of conditional distributions with\nbinary inputs.\nA DRBM can be trained by contrastive divergence,\nas has been done in conditional RBMs (Taylor et al.,\n2006), but since p(y|x) can be computed exactly, we\ncan compute the exact gradient:\n∂ log p(yi|xi)\n∂θ =\n∑\nj\nsigm(oyj (xi)) ∂oyj (xi)\n∂θ\n−\n∑\nj,y ∗\nsigm(oy∗j(xi))p(y∗|xi) ∂oy∗j(xi)\n∂θ\nwhere oyj (x) = cj + ∑\nk Wjk xk + Ujy . This gradient\ncan be computed eﬃciently and then used in a stochas-\ntic gradient descent optimization. This discriminative\napproach has been used previously for ﬁne-tuning the\ntop RBM of a Deep Belief Network (Hinton, 2007).\n4. Hybrid Discriminative Restricted\nBoltzmann Machines\nThe advantage brought by discriminative training usu-\nally depends on the amount of available training data.\nSmaller training sets tend to favor generative learn-\ning and bigger ones favor discriminative learning (Ng\n& Jordan, 2001). However, instead of solely rely-\ning on one or the other perspective, one can adopt a\nhybrid discriminative/generative approach simply by\ncombining the respective training criteria. Though\nthis method cannot be interpreted as a maximum like-\nlihood approach for a particular generative model as\nin Lasserre et al. (2006), it proved useful here and\nelsewhere (Bouchard & Triggs, 2004). In this paper,\nwe used the following criterion:\nLhybrid(Dtrain) =Ldisc(Dtrain) + αLgen(Dtrain) (5)\nwhere the weight α of the generative criterion can be\noptimized (e.g., based on the validation set classiﬁca-\ntion error). Here, the generative criterion can also be\nseen as a data-dependent regularizer for a DRBM. We\nwill refer to RBMs trained using the criterion of equa-\ntion 5 as Hybrid DRBMs (HDRBMs).\nTo train an HDRBM, we can use stochastic gradient\ndescent and add for each example the gradient contri-\nbution due to Ldisc with α times the stochastic gradi-\nent estimator associated with Lgen for that example.\n5. Semi-supervised Learning\nA frequent classiﬁcation setting is where there are few\nlabeled training data but many unlabeled examples of\ninputs. Semi-supervised learning algorithms (Chapelle\net al., 2006) address this situation by using the un-\nlabeled data to introduce constraints on the trained\nmodel. For example, for purely discriminative models,\nthese constraints are often imposed on the decision sur-\nface of the model. In the RBM framework, a natural\nconstraint is to ask that the model be a good gener-\native model of the unlabeled data, which corresponds\nto the following objective:\nLunsup(Dunlab) =−\n|Dunlab|∑\ni=1\nlog p(xi) (6)\nwhereDunlab ={(xi)}|Dunlab|\ni=1 contains unlabeled ex-\namples of inputs. To train on this objective, we can\nonce again use a contrastive divergence approximation\n538\nClassiﬁcation using Discriminative Restricted Boltzmann Machines\nof the log-likelihood gradient:\n∂ log p(xi)\n∂θ = −E Ey, h|xi\n[∂\n∂θ E(yi, xi, h)\n]\n+E Ey, x, h\n[∂\n∂θ E(y, x, h)\n]\nThe contrastive divergence approximation is slightly\ndiﬀerent here. The ﬁrst term can be computed in time\nO(Cn + nd), by noticing that it is equal to\nE Ey|xi\n[\nE Eh|y, xi\n[∂\n∂θ E(yi, xi, h)\n]]\n.One could either average the usual RBM gradient\n∂\n∂θ E(yi, xi, h) for each class y (weighted by p(y|xi)), or\nsample a y from p(y|xi) and only collect the gradient\nfor that value of y. In the sampling version, the online\ntraining update for this objective can be described by\nreplacing the statement y0←yi with y0∼p(y|xi) in\nAlgorithm 1. We used this version in our experiments.\nIn order to perform semi-supervised learning, we can\nweight and combine the objective of equation 6 with\nthose of equations 1, 4 or 5\nLsemi− sup(Dtrain,Dunlab) = LTYPE(Dtrain) (7)\n+βLunsup(Dunlab)\nwhere TYPE ∈ {gen, disc, hybrid }. Online training\naccording to this objective simply consists in apply-\ning the appropriate update for each training example,\nbased on whether it is labeled or not.\n6. Related Work\nAs mentioned earlier, RBMs (sometimes also referred\nto as harmoniums (Welling et al., 2005)) have already\nbeen used successfully in the past to extract useful fea-\ntures for another supervised learning algorithm. One\nof the main contributions of this paper lies in the\ndemonstration that RBMs can be used on their own\nwithout relying on another learning algorithm, and\nprovide a self-contained framework for deriving com-\npetitive classiﬁers. In addition to ensuring that the\nfeatures learned by the RBM’s hidden layer are dis-\ncriminative, this approach facilitates model selection\nsince the discriminative power of the hidden layer units\n(or features) can be tracked during learning by observ-\ning the progression of classiﬁcation error on a valida-\ntion set. It also makes it easier to tackle online learning\nproblems relatively to approaches where learning fea-\ntures (hidden representation) and learning to classify\nare done in two separate phases (Hinton et al., 2006;\nBengio et al., 2007).\nGehler et al. (2006); Xing et al. (2005) have shown\nthat the features learned by an RBM trained by ig-\nnoring the labeled targets can be useful for retriev-\ning documents or classifying images of objects. How-\never, in both these cases, the extracted features were\nlinear in the input, were not trained discriminatively\nand had to be fed to another learning algorithm which\nultimately performed classiﬁcation. McCallum et al.\n(2006) presented Multi-Conditional Learning (MCL) 1\nfor harmoniums in order to introduce a discriminative\ncomponent to harmoniums’ training, but the learned\nfeatures still had to be fed to another learning algo-\nrithm.\nRBMs can also provide a good initialization for the pa-\nrameters of neural network classiﬁers (Hinton, 2007),\nhowever model selection issues arise, for instance when\nconsidering the appropriate number of learning up-\ndates and the magnitude of learning rates of each\ntraining phase. It has also been argued that the gen-\nerative learning aspect of RBM training was a key ele-\nment to their success as good starting points for neural\nnetwork training (Bengio et al., 2007), but the extent\nto which the ﬁnal solution for the parameters of the\nneural network is inﬂuenced by generative learning is\nnot well controlled. HDRBMs can be seen as a way of\naddressing this issue.\nFinally, though semi-supervised learning was never\nreported for RBMs before, Druck et al. (2007) in-\ntroduced semi-supervised learning in hybrid genera-\ntive/discriminative models using a similar approach to\nthe one presented in section 5. However, they worked\nwith log-linear models, whereas the RBMs used here\ncan perform non-linear classiﬁcation. Log-linear mod-\nels depend much more on the discriminative quality of\nthe features that are fed as input, whereas an RBM\ncan learn useful features using their hidden variables,\nat the price of non-convex optimization.\n7. Experiments\nWe present experiments on two classiﬁcation problems:\ncharacter recognition and text classiﬁcation. In all ex-\nperiments, we performed model selection on a valida-\ntion set before testing. For the diﬀerent RBM models,\nmodel selection 2 consisted in ﬁnding good values for\n1We experimented with a version of MCL for the RBMs\nconsidered in this paper, however the results did not im-\nprove on those of HDRBMs.\n2Model selection was done with a grid-like search over\nλ (between 0.0005 and 0.1, on a log scale), n (50 to 6000),\nα for HDRBMs (0 to 0.5, on a log scale) and β for semi-\nsupervised learning (0, 0.01 or 0.1). In general, bigger val-\nues of n were found to be more appropriate with more\ngenerative learning. If no local minima was apparent, the\n539\nClassiﬁcation using Discriminative Restricted Boltzmann Machines\nthe learning rate λ, the size of the hidden layer n and\ngood weights for the diﬀerent types of learning (gener-\native and semi-supervised weights). Also, the number\nof iterations over the training set was determined using\nearly stopping according to the validation set classiﬁ-\ncation error, with a look ahead of 15 iterations.\n7.1. Character Recognition\nWe evaluated the diﬀerent RBM models on the prob-\nlem of classifying images of digits. The images were\ntaken from the MNIST dataset, where we separated\nthe original training set into training and validation\nsets of 50000 and 10000 examples and used the stan-\ndard test set of 10000 examples. The results are\ngiven in Table 1. The ordinary RBM model is trained\ngeneratively (to model ( x, y )), whereas RBM+NNet\nis an unsupervised RBM used to initialize a one-\nhidden layer supervised neural net (as in (Bengio et al.,\n2007)). We give as a comparison the results of a Gaus-\nsian kernel SVM and of a regular neural network (ran-\ndom initialization, one hidden layer, hyperbolic tan-\ngent hidden activation functions).\nFirst, we observe that a DRBM outperforms a genera-\ntive RBM. However, an HDRBM appears able to make\nthe best out of discriminative and generative learning\nand outperforms the other models.\nWe also experimented with a sparse version of the\nHDRBM model, since sparsity is known to be a good\ncharacteristic for features of images. Sparse RBMs\nwere developed by Lee et al. (2008) in the context\nof deep neural networks. To introduce sparsity in the\nhidden layer of an RBM in Lee et al. (2008), after each\niteration through the whole training set, the biases c\nin the hidden layer are set to a value that maintains\nthe average of the conditional expected value of these\nneurons to an arbitrarily small value. This procedure\ntends to make the biases negative and large. We fol-\nlow a diﬀerent approach by simply subtracting a small\nconstant δ value, considered as an hyper-parameter 3,\nfrom the biases after each update, which is more ap-\npropriate in an online setting or for large datasets.\nThis sparse version of HDRBMs outperforms all the\nother RBM models, and yields signiﬁcantly lower clas-\ngrid was extended. The biases b, c and d were initialized\nto 0 and the initial values for the elements of the weight\nmatrices U and W were each taken from uniform samples\nin\nˆ\n−m− 0. 5, m − 0. 5˜\n, where m is the maximum between the\nnumber of rows and columns of the matrix.\n3To chose δ, given the selected values for λ and α for\nthe “non sparse” HDRBM, we performed a second grid-\nsearch over δ (10− 5 and 0.1, on a log scale) and the hidden\nlayer size, testing bigger hidden layer sizes then previously\nselected.\nFigure 2. Filters learned by the HDRBM on the MNIST\ndataset. The top row shows ﬁlters that act as spatially lo-\ncalized stroke detectors, and the bottom shows ﬁlters more\nspeciﬁc to a particular shape of digit.\nTable 1. Comparison of the classiﬁcation performances on\nthe MNIST dataset. SVM results for MNIST were\ntaken from http://yann.lecun.com/exdb/mnist/. On this\ndataset, diﬀerences of 0.2% in classiﬁcation error is statis-\ntically signiﬁcant.\nModel Error\nRBM (λ = 0 . 005, n = 6000) 3.39%\nDRBM (λ = 0 . 05, n = 500) 1.81%\nRBM+NNet 1.41%\nHDRBM (α = 0 . 01, λ = 0 . 05, n = 1500 ) 1.28%\nSparse HDRBM (idem + n = 3000, δ = 10 − 4) 1.16%\nSVM 1.40%\nNNet 1.93%\nsiﬁcation error then the SVM and the standard neural\nnetwork classiﬁers. The performance achieved by the\nsparse HDRBM is particularly impressive when com-\npared to reported performances for Deep Belief Net-\nworks (1.25% in Hinton et al. (2006)) or of a deep\nneural network initialized using RBMs (around 1.2%\nin Bengio et al. (2007) and Hinton (2007)) for the\nMNIST dataset with 50000 training examples.\nThe discriminative power of the HDRBM can be better\nunderstood by looking a the rows of the weight matrix\nW, which act as ﬁlter features. Figure 2 displays some\nof these learned ﬁlters. Some of them are spatially\nlocalized stroke detectors which can possibly be active\nfor a wide variety of digit images, and others are much\nmore speciﬁc to a particular shape of digit.\n7.2. Document Classiﬁcation\nWe also evaluated the RBM models on the problem of\nclassifying documents into their corresponding news-\ngroup topic. We used a version of the 20-newsgroup\ndataset4 for which the training and test sets contain\ndocuments collected at diﬀerent times, a setting that\nis more reﬂective of a practical application. The orig-\ninal training set was divided into a smaller training\n4This dataset is available in Matlab format here:\nhttp://people.csail.mit.edu/jrennie/20Newsgroups/20news-\nbydate-matlab.tgz\n540\nClassiﬁcation using Discriminative Restricted Boltzmann Machines\nset and a validation set, with 9578 and 1691 examples\nrespectively. The test set contains 7505 examples. We\nused the 5000 most frequent words for the binary input\nfeatures. The results are given in Figure 3(a). We also\nprovide the results of a Gaussian kernel SVM 5 and of\na regular neural network for comparison.\nOnce again, HDRBM outperforms the other RBM\nmodels. However, here the generatively trained RBM\nperforms better then the DRBMs. The HDRBM also\noutperforms the SVM and neural network classiﬁers.\nIn order to get a better understanding of how the\nHDRBM solves this classiﬁcation problem, we ﬁrst\nlooked at the weights connecting each of the classes to\nthe hidden neurons. This corresponds to the columns\nU·y of the weight matrix U. Figure 3(b) shows a sim-\nilarity matrix M(U) for the weights of the diﬀerent\nnewsgroups, where M(U)y1y2 = sigm( UT\n·y1U·y2). We\nsee that the HDRBM does not use diﬀerent neurons for\ndiﬀerent newsgroups, but shares some of those neurons\nfor newsgroups that are semantically related. Another\ninteresting visualization of this characteristic is given\nin Figure 3(c), where the columns of U were projected\non their two principal components. In both cases, we\nsee that the HDRBM tends to share neurons for simi-\nlar topics, such as computer ( comp.*), science ( sci.*)\nand politics ( talk.politics.*), or secondary topics\nsuch as sports ( rec.sports.*) and other recreational\nactivities ( rec.autos and rec.motorcycles).\nTable 2 also gives the set of words used by the HDRBM\nto recognize some of the newsgroups. To obtain this\ntable we proceeded as follows: for each newsgroup y,\nwe looked at the 20 neurons with the largest weight\namong U·y, aggregated (by summing) the associated\ninput-to-hidden weight vectors, sorted the words in de-\ncreasing order of their associated aggregated weights\nand picked the ﬁrst words according to that order.\nThis procedure attempts to approximate the positive\ncontribution of the words to the conditional probabil-\nity of each newsgroup.\n7.3. Semi-supervised Learning\nWe evaluated our semi-supervised learning algorithm\nfor the HDRBM on both the digit recognition and doc-\nument classiﬁcation problems. We also experimented\nwith a version (noted MNIST-BI) of the MNIST\ndataset proposed by Larochelle et al. (2007) where\nbackground images have been added to MNIST digit\nimages. This version corresponds to a much harder\nproblem, but it will help to illustrate the advantage\nbrought by semi-supervised learning in HDRBMs. The\n5We used libSVM v2.85 to train the SVM model\nHDRBM trained on this data used truncated exponen-\ntial input units (see (Bengio et al., 2007)).\nIn this semi-supervised setting, we reduced the size\nof the labeled training set to 800 examples, and used\nsome of the remaining data to form an unlabeled\ndatasetDunlab. The validation set was also reduced\nto 200 labeled examples. Model selection 6 covered all\nthe parameters of the HDRBM as well as the unsuper-\nvised objective weight β of equation 7. For compar-\nison purposes, we also provide the performance of a\nstandard non-parametric semi-supervised learning al-\ngorithm based on function induction (Bengio et al.,\n2006b), which includes as a particular case or is very\nsimilar to other non-parametric semi-supervised learn-\ning algorithms such as Zhu et al. (2003). We provide\nresults for the use of a Gaussian kernel (NP-Gauss)\nand a data-dependent truncated Gaussian kernel (NP-\nTrunc-Gauss) used in Bengio et al. (2006b), which es-\nsentially outputs zero for pairs of inputs that are not\nnear neighbors. The experiments on the MNIST and\nMNIST-BI (with background images) datasets used\n5000 unlabeled examples and the experiment on 20-\nnewsgroup used 8778. The results are given in Table 3,\nwhere we observe that semi-supervised learning consis-\ntently improves the performance of the HDRBM.\nThe usefulness of non-parametric semi-supervised\nlearning algorithms has been demonstrated many\ntimes in the past, but usually so on problems where the\ndimensionality of the inputs is low or the data lies on\na much lower dimensional manifold. This is reﬂected\nin the result on MNIST for the non-parametric meth-\nods. However, for high dimensional data with many\nfactors of variation, these methods can quickly suﬀer\nfrom the curse of dimensionality, as argued by Bengio\net al. (2006a). This is also reﬂected in the results for\nthe MNIST-BI dataset which contains many factors of\nvariation, and for the 20-newsgroup dataset where the\ninput is very high dimensional.\nFinally, it is important to notice that semi-supervised\nlearning in HDRBMs proceeds in an online fashion and\nhence could scale to very large datasets, unlike more\nstandard non-parametric methods.\n7.4. Relationship with F eed-forward Neural\nNetworks\nThere are several similarities between discriminative\nRBMs and neural networks. In particular, the com-\nputation of p(y|x) could be implemented by a single\nlayer neural network with softplus and softmax acti-\n6β = 0 . 1 for MNIST and 20-newsgroup and β = 0 . 01\nfor MNIST-BI was found to perform best.\n541\nClassiﬁcation using Discriminative Restricted Boltzmann Machines\nModel Error\nRBM (λ = 0 . 0005, n = 1000) 24.9%\nDRBM (λ = 0 . 0005, n = 50) 27.6%\nRBM+NNet 26.8%\nHDRBM (α = 0 . 005, λ = 0 . 1, n = 1000 ) 23.8%\nSVM 32.8%\nNNet 28.2%\n(a) Classiﬁcation performances\n(b) Similarity matrix\n(c) PCA embedding\nFigure 3. Experiment on 20-newsgroup dataset. (Top left) Classiﬁcation performance for the diﬀerent models. The error\ndiﬀerences between HDRBM and other models is statistically signiﬁcant. (Bottom left) Similarity matrix of the newsgroup\nweights vectors U·y. (Right) Two dimensional PCA embedding of the newsgroup weights.\nTable 2. Most inﬂuential words in the HDRBM for predicting some of the document classes\nClass W ords Class W ords\nalt.atheism bible, atheists, benedikt, atheism, religion comp.graphics tiﬀ, ftp, window, gif, images, pixel\nmisc.forsale sell, condition, ﬂoppy, week, am, obo rec.autos cars, ford, autos, sho, toyota, roads\nsci.crypt sternlight, bontchev, nsa, escrow, hamburg talk.politics.guns ﬁrearms, handgun, ﬁrearm, gun, rkba\nTable 3. Comparison of the classiﬁcation errors in semi-\nsupervised learning setting. The errors in bold are statis-\ntically signiﬁcantly better.\nModel MNIST MNIST-BI 20-news\nHDRBM 9.73% 42.4% 40.5%\nSemi-sup HDRBM 8.04% 37.5% 31.8%\nNP-Gauss 10.60% 66.5% 85.0%\nNP-Trunc-Gauss 7.49% 61.3% 82.6%\nvation functions in its hidden and output layers re-\nspectively, with a special structure in the output and\nhidden weights where the value of the output weights is\nﬁxed and many of the hidden layer weights are shared.\nThe advantage of working in the framework of RBMs\nis that it provides a natural way to introduce gener-\native learning, which we used here to derive a semi-\nsupervised learning algorithm. As mentioned earlier, a\nform of generative learning can be introduced in stan-\ndard neural networks simply by using RBMs to ini-\ntialize the hidden layer weights. However the extent\nto which the ﬁnal solution for the parameters of the\nneural network is inﬂuenced by generative learning is\nnot well controlled. This might explain the superior\nperformance obtained by a HDRBM compared to a\nsingle hidden layer neural network initialized with an\nRBM (RBM+NNet in the tables).\n8. Conclusion and Future Work\nWe argued that RBMs can and should be used as\nstand-alone non-linear classiﬁers alongside other stan-\ndard and more popular classiﬁers, instead of merely\nbeing considered as simple feature extractors. We eval-\nuated diﬀerent training objectives that are more ap-\npropriate to train an RBM in a classiﬁcation setting.\nThese discriminative versions of RBMs integrate the\nprocess of discovering features of inputs with their use\nin classiﬁcation, without relying on a separate classi-\n542\nClassiﬁcation using Discriminative Restricted Boltzmann Machines\nﬁer. This insures that the learned features are dis-\ncriminative and facilitates model selection. We also\npresented a novel but straightforward semi-supervised\nlearning algorithm for RBMs and demonstrated its\nusefulness for complex or high dimensional data.\nFor future work, we would like to investigate the use\nof discriminative versions of RBMs in more challeng-\ning settings such as in multi-task or structured out-\nput problems. The analysis of the target weights\nfor the 20-newsgroup dataset seem to indicate that\nRBMs would be good at capturing the conditional sta-\ntistical relationship between multiple tasks or in the\ncomponents in a complex target space. Exact com-\nputation of the conditional distribution for the tar-\nget is not tractable anymore, but there exists promis-\ning techniques such as mean-ﬁeld approximations that\ncould estimate that distribution. Moreover, in the 20-\nnewsgroup experiment, we only used 5000 words in\ninput because generative training using Algorithm 1\ndoes not exploit the sparsity of the input, unlike an\nSVM or a DRBM (since in that case the sparsity of the\ninput makes the discriminative gradient sparse too).\nMotivated by this observation, we intend to explore\nways to introduce generative learning in RBMs and\nHDRBMs which would be less computationally expen-\nsive when the input vectors are large but sparse.\nAcknowledgments\nWe thank Dumitru Erhan for discussions about sparse\nRBMs and anonymous reviewers for helpful comments.\nReferences\nBengio, Y., Delalleau, O., & Le Roux, N. (2006a). The curse of\nhighly variable functions for local kernel machines. In Y. Weiss,\nB. Sch¨ olkopf and J. Platt (Eds.), Advances in neural infor-\nmation processing systems 18 , 107–114. Cambridge, MA: MIT\nPress.\nBengio, Y., Delalleau, O., & Le Roux, N. (2006b). Label propagation\nand quadratic criterion. In O. Chapelle, B. Sch¨ olkopf and A. Zien\n(Eds.), Semi-supervised learning, 193–216. MIT Press.\nBengio, Y., Lamblin, P., Popovici, D., & Larochelle, H. (2007).\nGreedy layer-wise training of deep networks. Advances in Neural\nInformation Processing Systems 19 (pp. 153–160). MIT Press.\nBouchard, G., & Triggs, B. (2004). The tradeoﬀ between genera-\ntive and discriminative classiﬁers. IASC International Sympo-\nsium on Computational Statistics (COMPSTAT) (pp. 721–728).\nPrague.\nCarreira-Perpi˜ nan, M., & Hinton, G. (2005). On contrastive diver-\ngence learning. Proceedings of the Tenth International Work-\nshop on Artiﬁcial Intelligence and Statistics, Jan 6-8, 2005,\nSavannah Hotel, Barbados (pp. 33–40). Society for Artiﬁcial\nIntelligence and Statistics.\nChapelle, O., Sch¨ olkopf, B., & Zien, A. (2006). Semi-supervised\nlearning. Cambridge, MA: MIT Press.\nDruck, G., Pal, C., Mccallum, A., & Zhu, X. (2007). Semi-\nsupervised classiﬁcation with hybrid generative/discriminative\nmethods. KDD ’07: Proceedings of the 13th ACM SIGKDD in-\nternational conference on Knowledge discovery and data min-\ning (pp. 280–289). New York, NY, USA: ACM.\nFreund, Y., & Haussler, D. (1994). Unsupervised learning of distri-\nbutions on binary vectors using two layer networks (Technical\nReport UCSC-CRL-94-25). University of California, Santa Cruz.\nGehler, P. V., Holub, A. D., & Welling, M. (2006). The rate adapting\npoisson model for information retrieval and object recognition.\nICML ’06: Proceedings of the 23rd international conference on\nMachine learning (pp. 337–344). New York, NY, USA: ACM.\nHinton, G. (2002). Training products of experts by minimizing con-\ntrastive divergence. Neural Computation , 14, 1771–1800.\nHinton, G. (2007). To recognize shapes, ﬁrst learn to generate im-\nages. In P. Cisek, T. Drew and J. Kalaska (Eds.), Computational\nneuroscience: Theoretical insights into brain function . Elsevier.\nHinton, G. E., Osindero, S., & Teh, Y. (2006). A fast learning\nalgorithm for deep belief nets. Neural Computation , 18, 1527–\n1554.\nLarochelle, H., Erhan, D., Courville, A., Bergstra, J., & Bengio, Y.\n(2007). An empirical evaluation of deep architectures on prob-\nlems with many factors of variation. Twenty-fourth Interna-\ntional Conference on Machine Learning (ICML’2007) .\nLasserre, J. A., Bishop, C. M., & Minka, T. P. (2006). Princi-\npled hybrids of generative and discriminative models. CVPR\n’06: Proceedings of the 2006 IEEE Computer Society Confer-\nence on Computer Vision and Pattern Recognition (pp. 87–94).\nWashington, DC, USA: IEEE Computer Society.\nLe Roux, N., & Bengio, Y. (2008). Representational power of re-\nstricted boltzmann machines and deep belief networks. Neural\nComputation, to appear .\nLee, H., Ekanadham, C., & Ng, A. (2008). Sparse deep belief net\nmodel for visual area v2. In J. Platt, D. Koller, Y. Singer and\nS. Roweis (Eds.), Advances in neural information processing\nsystems 20 . Cambridge, MA: MIT Press.\nMcCallum, A., Pal, C., Druck, G., & Wang, X. (2006). Multi-\nconditional learning: Generative/discriminative training for clus-\ntering and classiﬁcation. Twenty-ﬁrst National Conference on\nArtiﬁcial Intelligence (AAAI-06) . AAAI Press.\nNg, A. Y., & Jordan, M. I. (2001). On discriminative vs. generative\nclassiﬁers: A comparison of logistic regression and naive bayes.\nNIPS (pp. 841–848).\nSalakhutdinov, R., Mnih, A., & Hinton, G. (2007). Restricted boltz-\nmann machines for collaborative ﬁltering. ICML ’07: Proceed-\nings of the 24th international conference on Machine learning\n(pp. 791–798). New York, NY, USA: ACM.\nSmolensky, P. (1986). Information processing in dynamical systems:\nFoundations of harmony theory. In D. Rumelhart and J. McClel-\nland (Eds.), Parallel distributed processing , vol. 1, chapter 6,\n194–281. Cambridge: MIT Press.\nSutskever, I., & Hinton, G. (2007). Learning multilevel distributed\nrepresentations for high-dimensional sequences. Proceedings of\nthe Eleventh International Conference on Artiﬁcial Intelligence\nand Statistics, March 21-24, 2007, Porto-Rico .\nTaylor, G., Hinton, G., & Roweis, S. (2006). Modeling human mo-\ntion using binary latent variables. Advances in Neural Informa-\ntion Processing Systems 20 . MIT Press.\nWelling, M., Rosen-Zvi, M., & Hinton, G. E. (2005). Exponen-\ntial family harmoniums with an application to information re-\ntrieval. Advances in Neural Information Processing Systems .\nCambridge, MA: MIT Press.\nXing, E. P., Yan, R., & Hauptmann, A. G. (2005). Mining associated\ntext and images with dual-wing harmoniums. UAI (pp. 633–641).\nAUAI Press.\nZhu, X., Ghahramani, Z., & Laﬀerty, J. (2003). Semi-\nsupervised learning using Gaussian ﬁelds and harmonic functions.\nICML’2003.\n543",
  "values": {
    "Critiqability": "No",
    "Privacy": "No",
    "Explicability": "No",
    "Justice": "No",
    "Interpretable (to users)": "No",
    "Non-maleficence": "No",
    "Beneficence": "No",
    "Transparent (to users)": "No",
    "Respect for Persons": "No",
    "Autonomy (power to decide)": "No",
    "Deferral to humans": "No",
    "Collective influence": "No",
    "Not socially biased": "No",
    "Respect for Law and public interest": "No",
    "User influence": "No",
    "Fairness": "No"
  }
}