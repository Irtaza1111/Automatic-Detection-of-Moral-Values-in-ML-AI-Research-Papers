{
  "pdf": "1390156.1390255",
  "title": "1390156.1390255",
  "author": "Unknown",
  "paper_id": "1390156.1390255",
  "text": "Learning Diverse Rankings with Multi-Armed Bandits\nFilip Radlinski filip@cs.cornell.edu\nRobert Kleinberg rdk@cs.cornell.edu\nThorsten Joachims tj@cs.cornell.edu\nDepartment of Computer Science, Cornell University, Ithaca, NY 14853 USA\nAbstract\nAlgorithms for learning to rank Web docu-\nments usually assume a document’s relevance\nis independent of other documents. This\nleads to learned ranking functions that pro-\nduce rankings with redundant results. In\ncontrast, user studies have shown that di-\nversity at high ranks is often preferred. We\npresent two online learning algorithms that\ndirectly learn a diverse ranking of documents\nbased on users’ clicking behavior. We show\nthat these algorithms minimize abandon-\nment, or alternatively, maximize the proba-\nbility that a relevant document is found in\nthe top k positions of a ranking. Moreover,\none of our algorithms asymptotically achieves\noptimal worst-case performance even if users’\ninterests change.\n1. Introduction\nWeb search has become an essential component of the\nInternet infrastructure, and has hence attracted sig-\nniﬁcant interest from the machine learning community\n(e.g. Herbrich et al., 2000; Burges et al., 2005; Radlin-\nski & Joachims, 2005; Chu & Ghahramani, 2005; Met-\nzler & Croft, 2005; Yue et al., 2007; Taylor et al.,\n2008). The conventional approach to this learning-\nto-rank problem has been to assume the availability\nof manually labeled training data. Usually, this data\nconsists of a set of documents judged as relevant or not\nto speciﬁc queries, or of pairwise judgments compar-\ning the relative relevance of pairs of documents. These\njudgments are used to optimize a ranking function oﬀ-\nline, to a standard information retrieval metric, then\ndeploying the learned function in a live search engine.\nWe propose a new learning to rank problem formu-\nlation that diﬀers in three fundamental ways. First,\nunlike most previous methods, we learn from usage\nAppearing in Proceedings of the 25th International Confer-\nence on Machine Learning , Helsinki, Finland, 2008. Copy-\nright 2008 by the author(s)/owner(s).\ndata rather than manually labeled relevance judg-\nments. Usage data is available in much larger quan-\ntities and at much lower cost. Moreover, unlike man-\nual judgments, which need to be constantly updated\nto stay relevant, usage data naturally reﬂects current\nusers’ needs and the documents currently available.\nAlthough some researchers have transformed usage\ndata into relevance judgments, or used it to generate\nfeatures (e.g. Joachims, 2002; Radlinski & Joachims,\n2005; Agichtein et al., 2006), we go one step further\nby directly optimizing a usage-based metric.\nSecond, we propose an online learning approach for\nlearning from usage data. As training data is being\ncollected, it immediately impacts the rankings shown.\nThis means the learning problem we address is regret\nminimization, where the goal is to minimize the total\nnumber of poor rankings displayed over all time. In\nparticular, in this setting there is a natural tradeoﬀ be-\ntween exploration and exploitation: It may be valuable\nin the long run to present some rankings with unknown\ndocuments, to allow training data about these docu-\nments to be collected. In contrast, in the short run\nexploitation is typically optimal. With only few ex-\nceptions (e.g. Radlinski & Joachims, 2007), previous\nwork does not consider such an online approach.\nThird and most importantly, except for (Chen &\nKarger, 2006), previous algorithms for learning to rank\nhave considered the relevance of each document in-\ndependently of other documents. This is reﬂected in\nthe performance measures typically optimized, such\nas Precision, Recall, Mean Average Precision (MAP)\n(Baeza-Yates & Ribeiro-Neto, 1999) and Normalized\nDiscounted Cumulative Gain (NDCG) (Burges et al.,\n2006). In fact, recent work has shown that these mea-\nsures do not necessarily correlate with user satisfaction\n(Turpin & Scholer, 2006). Additionally, it intuitively\nstands to reason that presenting many slight varia-\ntions of the same relevant document in web search re-\nsults may increase the MAP or NDCG score, yet would\nbe suboptimal for users. Moreover, web queries often\nhave diﬀerent meanings for diﬀerent users (a canonical\nexample is the query jaguar) suggesting that a ranking\nwith diverse documents may be preferable.\n784\n\nLearning Diverse Rankings with Multi-Armed Bandits\nWe will show how clickthrough data can be used to\nlearn rankings maximizing the probability that any\nnew user will ﬁnd at least one relevant document high\nin the ranking.\n2. Related Work\nThe standard approach for learning to rank uses train-\ning data, in the form of judgments assessing the rele-\nvance of individual documents to a query, to learn pa-\nrameters θ for a scoring function f(q,di,θ ). Given a\nnew queryq, this function computesf(q,di,θ ) for each\ndocument di independently and ranks documents by\ndecreasing score (e.g. Herbrich et al., 2000; Joachims,\n2002; Burges et al., 2005; Chu & Ghahramani, 2005).\nThis also applies to recent algorithms that learn θ\nto maximize nonlinear performance measures such as\nMAP (Metzler & Croft, 2005; Yue et al., 2007) and\nNDCG (Burges et al., 2006; Taylor et al., 2008).\nThe theoretical model that justiﬁes ranking docu-\nments in this way is the probabilistic ranking principle\n(Robertson, 1977). It suggests that documents should\nbe ranked by their probablility of relevance to the\nquery. However, the optimality of such a ranking relies\non the assumption that there are no statistical depen-\ndencies between the probabilities of relevance among\ndocuments – an assumption that is clearly violated in\npractice. For example, if one document about jaguar\ncars is not relevant to a user who issues the query\njaguar, other car pages become less likely to be rele-\nvant. Furthermore, empirical studies have shown that\ngiven a ﬁxed query, the same document can have dif-\nferent relevance to diﬀerent users (Teevan et al., 2007).\nThis undermines the assumption that each document\nhas a single relevance score that can be provided as\ntraining data to the learning algorithm. Finally, as\nusers are usually satisﬁed with ﬁnding a small number\nof, or even just one, relevant document, the usefulness\nand relevance of a document does depend on other\ndocuments ranked higher.\nAs a result, most search engines today attempt to elim-\ninate redundant results and produce diverse rankings\nthat include documents that are potentially relevant to\nthe query for diﬀerent reasons. However, learning op-\ntimally diverse rankings using expert judgments would\nrequire document relevance to be measured for diﬀer-\nent possible meanings of a query. While the TREC\ninteractive track1 provides some documents labeled in\nthis way for a small number of queries, such document\ncollections are even more diﬃcult to create than stan-\ndard expert labeled collections.\n1http://trec.nist.gov/data/t11 interactive/t11i.html\nSeveral non-learning algorithms for obtaining a diverse\nranking of documents from a non-diverse ranking have\nbeen proposed. One common one is Maximal Marginal\nRelevance (MMR) (Carbonell & Goldstein, 1998).\nGiven a similarity (relevance) measure between docu-\nments and queriessim1(d,q ) and a similarity measure\nbetween pairs of documents sim2(di,dj), MMR iter-\natively selects documents by repeatedly ﬁnding di =\nargmaxd∈Dλsim1(d,q ) − (1 −λ) maxdj∈Ssim2(d,dj)\nwhere S is the set of documents already selected and\nλ is a tuning parameter. In this way MMR selects the\nmost relevant documents that are also diﬀerent from\nany documents already selected.\nCritically, MMR requires that the relevance function\nsim1(d,q ), and the similarity function sim2(di,dj) is\nknown. It is usual to obtain sim1 and sim2 using al-\ngorithms such as those discussed above. The goal of\nMMR is to rerank an already learned ranking (that of\nranking documents by decreasing sim1 score) to im-\nprove diversity. All previous approaches of which we\nare aware that optimize diversity similarly require a\nrelevance function to be learned prior to performing\na diversiﬁcation step (Zhu et al., 2007; Zhang et al.,\n2005; Zhai et al., 2003), with the exception of Chen\nand Karger (2006). Rather, they require that a model\nfor estimating the probability a document is relevant,\ngiven a query and other non-relevant documents, is\navailable. In contrast, we directly learn a diverse rank-\ning of documents using users’ clicking behavior.\n3. Problem Formalization\nWe address the problem of learning an optimally diver-\nsiﬁed ranking of documents D = {d1,...,d n} for one\nﬁxed query. Suppose we have a population of users,\nwhere each userui considers some subset of documents\nAi ⊂ D as relevant to the query, and the remainder of\nthe documents as non-relevant. Intuitively, users with\ndiﬀerent interpretations for the query would have dif-\nferent relevant sets, while users with similar interpre-\ntations would have similar relevant sets.\nAt time t, we interact with user ut with relevant\nset At. We present an ordered set of k documents,\nBt = (b1(t),...,b k(t)). The user considers the results\nin order, and clicks on up to one document. The prob-\nability of user ut clicking on document di (conditional\non the user not clicking on a document presented ear-\nlier in the ranking) is assumed to be pti ∈ [0, 1]. We\nrefer to the vector of probabilities (pti)i∈D as the type\nof user ut. In the simplest case, we could take pti = 1\nif di ∈ At and 0 otherwise, in which case the user\nclicks on the ﬁrst relevant document or does not click\nif no documents inBt are relevant. However, in reality\nclicks tend to be noisy although more relevant docu-\n785\nLearning Diverse Rankings with Multi-Armed Bandits\nAlgorithm 1 Ranked Explore and Commit\n1: input: Documents (d1,..,d n), parameters ϵ,δ,k .\n2: x ← ⌈2k2/ϵ2 log(2k/δ)⌉\n3: (b1,...,b k) ←k arbitrary documents.\n4: for i=1 . . . k do At every rank\n5: ∀j. pj ← 0\n6: for counter=1 . . . x do Loop x times\n7: for j=1 . . . n do over every document dj\n8: bi ←dj\n9: display {b1,...,b k} to user; record clicks\n10: if user clicked on bi then pj ←pj + 1\n11: end for\n12: end for\n13: j∗ ← argmaxjpj Commit to best document at this rank\n14: bi ←dj∗\n15: end for\nments are more likely to be clicked on. In our analysis,\nwe will take pti ∈ [0, 1].\nWe get payoﬀ 1 if the user clicks, 0 if not. The goal is\nto maximize the total payoﬀ, summing over all time.\nThis payoﬀ represents the number of users who clicked\non any result, which can be interpreted as the user\nﬁnding at least one potentially relevant document (so\nlong as pti is higher when di ∈At than when di/∈At).\nThe event that a user does not click is called aban-\ndonment since the user abandoned the search results.\nAbandonment is an important measure of user satis-\nfaction because it indicates that users were presented\nwith search results of no potential interest.\n4. Learning Algorithms\nWe now present two algorithms that directly mini-\nmize the abandonment rate. At a high level, both\nalgorithms learn a marginal utility for each document\nat each rank, displaying documents to maximize the\nprobability that a new user of the search system would\nﬁnd at least one relevant document within the top k\npositions. The algorithms diﬀer in their assumptions.\n4.1. Ranked Explore and Commit\nThe ﬁrst algorithm we present is a simple greedy strat-\negy that assumes that user interests and documents\ndo not change over time. As we will see, after T\ntime steps this algorithm achieves a payoﬀ of at least\n(1 −1/e −ϵ)OPT −O(k3n/ϵ2 ln(k/δ)) with probability\nat least 1 −δ. OPT denotes the maximal payoﬀ that\ncould be obtained if the click probabilities pti were\nknown ahead of time for all users and documents, and\n(1 − 1/e)OPT is the best obtainable polynomial time\napproximation, as will be explained in Section 5.1.\nAs described in Algorithm 1, Ranked Explore and\nAlgorithm 2 Ranked Bandits Algorithm\n1: initialize MAB1(n),..., MABk(n) Initialize MABs\n2: for t = 1 . . . T do\n3: for i = 1 . . . k do Sequentially select documents\n4: ˆbi(t) ← select-arm (MABi)\n5: if ˆbi(t) ∈ {b1(t),..,b i−1(t)} then Replace repeats\n6: bi(t) ← arbitrary unselected document\n7: else\n8: bi(t) ← ˆbi(t)\n9: end if\n10: end for\n11: display {b1(t),...,b k(t)} to user; record clicks\n12: for i = 1 . . . k do Determine feedback for MABi\n13: if user clicked bi(t) and ˆbi(t) =bi(t) then\n14: fit = 1\n15: else\n16: fit = 0\n17: end if\n18: update (MABi,arm = ˆbi(t),reward =fit)\n19: end for\n20: end for\nCommit (REC) iteratively selects documents for each\nrank. At each rank position i, every document dj is\npresented a ﬁxed number x times, and the number of\nclicks it receives during these presentations is recorded.\nAfternx presentations, the algorithm permanently as-\nsigns the document that received the most clicks to\nthe current rank, and moves on to the next rank.\n4.2. Ranked Bandits Algorithm\nRanked Explore and Commit is purely greedy, mean-\ning that after each document is selected, this deci-\nsion is never revisited. In particular, this means that\nif user interests or documents change, REC can per-\nform arbitrarily poorly. In contrast, the Ranked Ban-\ndits Algorithm (RBA) achieves a combined payoﬀ of\n(1−1/e)OPT −O(k√Tn logn) afterT time steps even\nif documents and user interests change over time.\nThis algorithm leverages standard theoretical results\nfor multi-armed bandits. Multi-armed bandits (MAB)\nare modeled on casino slot machines (sometimes called\none-armed bandits). The goal of standard MAB algo-\nrithms is to select the optimal sequence of slot ma-\nchines to play to maximize the expected total reward\ncollected. For further details, refer to (Auer et al.,\n2002a). The ranked bandits algorithm runs an MAB\ninstance MABi for each rank i. Each of the k copies of\nthe multi-armed bandit algorithm maintains a value\n(or index) for every document. When selecting the\nranking to display to users, the algorithm MAB1 is\nresponsible for choosing which document is shown at\nrank 1. Next, the algorithm MAB2 determines which\n786\nLearning Diverse Rankings with Multi-Armed Bandits\ndocument is shown at rank 2, unless the same docu-\nment was selected at the highest rank. In that case,\nthe second document is picked arbitrarily. This pro-\ncess is repeated to select all top k documents.\nNext, after a user considers up to the topk documents\nin order and clicks on one or none, we need to update\nthe indices. If the user clicks on a document actually\nselected by an MAB instance, the reward for the arm\ncorresponding to that document for the multi-armed\nbandit at that rank is 1. The reward for the arms\ncorresponding to all other selected documents is 0. In\nparticular, note that the RBA treats the bandits corre-\nsponding to each rank independently. Precise pseudo-\ncode for the algorithm is presented in Algorithm 2.\nA generalization of this algorithm, in an abstract set-\nting without the application to Information Retrieval,\nwas discovered independently by Streeter and Golovin\n(2007).\nThe actual MAB algorithm used for each MABi in-\nstance is not critical, and in fact any algorithm for the\nnon-stochastic multi-armed bandit problem will suf-\nﬁce. Our theoretical analysis only requires that:\n• The algorithm has a set S of n strategies.\n• In each periodt a payoﬀ functionft :S → [0, 1] is\ndeﬁned. This function is not revealed to the algo-\nrithm, and may depend on the algorithm’s choices\nbefore time t.\n• In each period the algorithm chooses a (random)\nelementyt ∈S based on the feedback revealed in\nprior periods.\n• The feedback revealed in period t is ft(yt).\n• The expected payoﬀs of the chosen strategies sat-\nisfy:\nT∑\nt=1\nE[ft(yt)] ≥ max\ny∈S\nT∑\nt=1\nE[ft(y)] − R(T )\nwhere R(T ) is an explicit function in o(T ) which\ndepends on the particular multi-armed bandit al-\ngorithm chosen, and the expectation is over any\nrandomness in the algorithm. We will use the\nExp3 algorithm in our analysis, where R(T ) =\nO\n(√Tn logn\n)\n(Auer et al., 2002b).\nWe will also later see that although these conditions\nare needed to bound worst-case performance, better\npractical performance may be obtained at the expense\nof worst-case performance if they are relaxed.\n5. Theoretical Analysis\nWe now present a theoretical analysis of the algorithms\npresented in Section 4. First however, we discuss the\noﬄine version of this optimization problem.\n5.1. The Oﬄine Optimization Problem\nThe problem of choosing the optimum set of k docu-\nments for a given user population is NP-hard, even if\nall the information about the user population (i.e. the\nset of relevant documents for each user) is given oﬄine\nand we restrict ourselves to pij ∈ { 0, 1}. This is be-\ncause selecting the optimal set of documents is equiva-\nlent to the maximum coverage problem: Given a posi-\ntive integerk and a collection of subsetsS1,S 2,...,S n\nof anm-element set, ﬁndk of the subsets whose union\nhas the largest possible cardinality.\nThe standard greedy algorithm for the maximum cov-\nerage problem, translated to our setting, iteratively\nchooses the document that is relevant to the most users\nfor whom a relevant document has not yet been se-\nlected. This algorithm is a (1 − 1/e)-approximation\nalgorithm for this maximization problem (Nemhauser\net al., 1978). The (1 − 1/e) factor is optimal and\nno better worst-case approximation ratio is achievable\nin polynomial time unless NP ⊆ DTIME\n(\nnlog logn)\n(Khuller et al., 1997).\n5.2. Analysis of Ranked Bandits Algorithm\nWe start by analyzing the Ranked Bandits Algorithm.\nThis algorithm works by simulating the oﬄine greedy\nalgorithm, using a separate instance of the multi-\narmed bandit algorithm for each step of the greedy\nalgorithm. Except for the sublinear regret term, the\ncombined payoﬀ is as high as possible without violat-\ning the hardness-of-approximation result stated in the\npreceding paragraph.\nTo analyze the RBA, we ﬁrst restrict ourselves to users\nwho click on any given document with probability ei-\nther 0 or 1. We refer to this restricted type of user as a\ndeterministic user ; we will relax the requirement later.\nAdditionally, this analysis applies to a worst case (and\nhence ﬁxed) sequence of users.\nFurther, it is useful to introduce some notation. For a\nset A and a sequence B = (b1,b 2,...,b k), let\nGi(A,B ) =\n{\n1 if A intersects {b1,...,b i}\n0 otherwise\ngi(A,B ) =Gi(A,B ) −Gi−1(A,B )\nRecalling that At is the set of documents relevant to\nuserut, we see thatGk(At,B ) is the payoﬀ of present-\ning B to the user ut. Let\nB∗ = argmax\nB\nT∑\nt=1\nGk(At,B ),\nOPT =\nT∑\nt=1\nGk(At,B∗).\n787\nLearning Diverse Rankings with Multi-Armed Bandits\nRecall that ( ˆb1(t),..., ˆbk(t)) is the sequence of docu-\nments chosen by the algorithms MAB1,..., MABk at\ntime t, and that ( b1(t),...,b k(t)) is the sequence of\ndocuments presented to the user. Deﬁne the feedback\nfunction fit for algorithm MABi at time t, as follows:\nfit(b) =\n{\n1 if Gi−1(At,Bt) = 0 and b ∈At\n0 otherwise .\nNote that the value offit deﬁned in the pseudocode for\nthe Ranked Bandits Algorithms is equal to fit(ˆbi(t)).\nLemma 1. For all i,\nE\n[ T∑\nt=1\ngi(At,Bt)\n]\n≥ 1\nkE\n[ T∑\nt=1\n(Gk(At,B∗) −Gi−1(At,Bt))\n]\n−R(T )\n= 1\nkOPT − 1\nkE\n[ T∑\nt=1\nGi−1(At,Bt)\n]\n−R(T ).\nProof. First, note that\ngi(At,Bt) ≥fit(ˆbi(t)). (1)\nThis is trivially true when fit(ˆbi(t)) = 0. When\nfit(ˆbi(t)) = 1, Gi−1(At,Bt) = 0 and ˆbi(t) ∈ At. This\nimplies that bi(t) = ˆbi(t) and that gi(At,Bt) = 1.\nNow using the regret bound for MABi we obtain\nT∑\nt=1\nE[fit(ˆbi(t))] ≥ max\nb\nT∑\nt=1\nE[fit(b)] −R(T )\n≥ 1\nkE\n[∑\nb∈B∗\nT∑\nt=1\nfit(b)\n]\n−R(T ). (2)\nTo complete the proof of the lemma, we will prove that\n∑\nb∈B∗\nfit(b) ≥Gk(At,B∗) −Gi−1(At,Bt). (3)\nThe lemma follows immediately by combining (1)-(3).\nObserve that the left side of (3) is a non-negative\ninteger, while the right side takes one of the values\n{−1, 0, 1}. Thus, to prove (3) it suﬃces to show that\nthe left side is greater than or equal to 1 whenever the\nright side is equal to 1. The right side equals 1 only\nwhen Gi−1(At,Bt) = 0 and At intersects B∗. In this\ncase it is clear that there exists at least one b ∈ B∗\nsuch thatfit(b) = 1, hence the left side is greater than\nor equal to 1.\nTheorem 1. The algorithm’s combined payoﬀ after T\nrounds satisﬁes:\nE\n[ T∑\nt=1\nGk(At,Bt)\n]\n≥\n(\n1 − 1\ne\n)\nOPT −kR(T ). (4)\nProof. We will prove, by induction on i, that\nOPT −E\n[ T∑\nt=1\nGi(At,Bt)\n]\n≤\n(\n1 − 1\nk\n)i\nOPT +iR(T ).\n(5)\nThe theorem follows by taking i = k and using the\ninequality\n(\n1 − 1\nk\n)k\n< 1\ne.\nIn the base casei = 0, inequality (5) is trivial. For the\ninduction step, let\nZi =OPT − E\n[ T∑\nt=1\nGi(At,Bt)\n]\n.\nWe have\nZi =Zi−1 − E\n[ T∑\nt=1\ngi(At,Bt)\n]\n, (6)\nand Lemma 1 says that\nE\n[ T∑\nt=1\ngi(At,Bt)\n]\n≥ 1\nkZi−1 −R(T ). (7)\nCombining (6) with (7), we obtain\nZi ≤\n(\n1 − 1\nk\n)\nZi−1 +R(T ).\nCombining this with the induction hypothesis proves\n(5).\nThe general case, in which user ui’s type vector\n(pij)j∈D is an arbitrary element of [0 , 1]D, can be re-\nduced via a simple transformation to the case of de-\nterministic users analyzed above. We replace user ui\nwith a random deterministic user ˆui whose type vector\nˆpi ∈ { 0, 1}D is sampled using the following rule: the\nrandom variable ˆpij has distribution\nˆpij =\n{\n1 with probability pij\n0 with probability 1 −pij,\nand these random variables are mutually independent.\nNote that the clicking behavior of user ui when pre-\nsented with a ranking B is identical to the clicking\nbehavior observed when a random user type ˆui is sam-\npled from the above distribution, and the ranking B\nis presented to this random user. Thus, if we apply\nthe speciﬁed transformation to users u1,u 2,...,u T ,\nobtaining a random sequence ˆu1, ˆu2,..., ˆuT of deter-\nministic users, this transformation changes neither the\nalgorithm’s expected payoﬀ nor that of the optimum\nranking B∗. Thus, Theorem 1 for general users can\nbe deduced by applying the same theorem to the ran-\ndom sequence ˆu1,..., ˆuT and taking the expectation of\nthe left and right sides of (4) over the random choices\ninvolved in sampling ˆu1,..., ˆuT .\n788\nLearning Diverse Rankings with Multi-Armed Bandits\nNote also that B∗ is deﬁned as the optimal subset of\nk documents, andOPT is the payoﬀ of presentingB∗,\nwithout specifying the order in which documents are\npresented. However, the Ranked Bandits Algorithm\nlearns an order for the documents in addition to iden-\ntifying a set of documents. In particular, given k′ <k ,\nRBA(k′) would receive exactly the same feedback as\nthe ﬁrst k′ instances of MABi receive when running\nRBA(k). Hence any k′ sized preﬁx of the learned rank-\ning also has the same performance bound with respect\nthe appropriate smaller set B′∗.\nFinally, it is worth noting that this analysis cannot\nbe trivially extended to non-binary payoﬀs, for exam-\nple when learning a ranking of web advertisements.\nIn particular, the greedy algorithm on which RBA is\nbased in the non-binary payoﬀ case can obtain a payoﬀ\nthat is a factor of k −ε below optimal, for any ε> 0.\n5.3. Analysis of Ranked Explore and Commit\nThe analysis of the Ranked Explore and Commit\n(REC) algorithm is analogous to that of the Ranked\nBandits algorithm, except that the equivalents of\nLemma 1 and Theorem 1 are only true with high prob-\nability after t0 = nxk time steps of exploration have\noccurred. Let B denote the ranking selected by REC.\nLemma 2. Let x = 2k2/ϵ2 log(2k/δ). Assume At is\ndrawn i.i.d. from a ﬁxed distribution of user types. For\nany i, with probability 1 −δ/k,\nE\n[ T∑\nt=t0\ngi(At,B )\n]\n≥ 1\nkE\n[ T∑\nt=t0\n(Gk(At,B∗) −Gi−1(At,B ))\n]\n− ϵ\nkT.\nProof Outline. First note that in this setting, B∗ and\nOPT are deﬁned in expectation over the At drawn.\nFor any document, by Hoeﬀding’s inequality, with\nprobability 1 −δ/2k the true payoﬀ of that document\nexplored at ranki is withinϵ/2k of the observed mean\npayoﬀ. Hence the document selected at ranki is within\nϵ/k of the payoﬀ of the best document available at\nrank i. Now, the same proof as for Lemma 1 applies,\nalthough with a diﬀerent regret R(T ).\nTheorem 2. With probability (1 −δ), the algorithm’s\ncombined payoﬀ after T rounds satisﬁes:\nE\n[ T∑\nt=1\nGk(At,B )\n]\n≥\n(\n1 − 1\ne\n)\nOPT −ϵT −nkx (8)\nProof Outline. Applying Lemma 2 for all i ∈ {1,..,k },\nwith probability (1 −kδ/k) = (1 −δ) the conclusion\nof the Lemma holds for all i.\nNext, an analogous proof as for Theorem 1 applies,\nexcept replacing R(T ) with ϵ\nkT and noting that the\nregret during the nkx exploration steps is at most 1\nfor every time step.\nIt is interesting to note that, in contrast to the Ranked\nBandits Algorithm, this algorithm can be adapted to\nthe case where clicked documents provide real valued\npayoﬀs. The only modiﬁcation necessary is that docu-\nments should always be presented by decreasing payoﬀ\nvalue. However, we do not address this extension fur-\nther due to space constraints.\n6. Evaluation\nIn this section, we evaluate the Ranked Bandits and\nRanked Explore and Commit algorithms, as well as\ntwo variants of RBA, with simulations using a user\nand document model.\nWe chose a model that produces a user population and\ndocument distribution designed to be realistic yet al-\nlow us to evaluate the performance of the presented\nalgorithms under diﬀerent levels of noise in user click-\ning behavior. Our model ﬁrst assigns each of 20 users\nto topics of interest using a Chinese Restaurant Pro-\ncess (Aldous, 1985) with parameter θ = 3. This led\nto a mean of 6.5 unique topics, with topic popularity\ndecaying according to a power law. Taking a collection\nof 50 documents, we then randomly assigned as many\ndocuments to each topic as there were users assigned\nto the topic, leading to topics with more users having\nmore relevant documents. We set each document as-\nsigned to a topic as relevant to all users assigned to\nthat topic, and all other documents as non relevant.\nThe probabilities of a user clicking on relevant and\nnon-relevant documents were set to constants pR and\npNR respectively.\nWe tested by drawing one user uniformly from the\nuser population at each time step, and presented this\nuser with the ranking selected by each algorithm, using\nk = 5. We report the average number of time steps\nwhere the user clicked on a result, and the average\nnumber of time steps where at least one of the pre-\nsented documents was relevant to the user. All num-\nbers we report are averages over 1,000 algorithm runs.\n6.1. Performance Without Click Noise\nWe start by evaluating how well the REC and RBA\nalgorithms maximize the clickthrough rate in the sim-\nplest case whenpR = 1 andpNR = 0. We also compare\ntheir performance to the clickthrough rate that the\nsame users would generate if presented with a static\nsystem that orders documents by decreasing true prob-\n789\nLearning Diverse Rankings with Multi-Armed Bandits\n 0.55\n 0.6\n 0.65\n 0.7\n 0.75\n 0.8\n 0.85\n 0.9\n 0.95\n 1\n400,000350,000300,000250,000200,000150,000100,00050,000 0\nClickthrough Rate (1-Abandonment)\nNumber of User Presentations\nBest possible performance (OPT)\nLower bound performance (1-1/e)OPT\nRanked Bandits Algorithm\nRanked Explore and Commit (x=50)\nRanked Explore and Commit (x=1000)\nPopularity-Based Static Ranking\nFigure 1. Clickthrough rate of the learned ranking as a\nfunction of the number of times the ranking was presented\nto users.\nability of relevance to the users assuming document\nrelevances are independent. Figure 1 shows that both\nREC and RBA perform well above the static baseline\nand well above the performance guarantee provided by\nthe theoretical results. This is not surprising, as the\n(1 − 1/e)OPT bound is a worst-case bound. In fact,\nwe see that REC with x = 1000 nearly matches the\nperformance of the best possible ranking after ﬁnish-\ning its initial exploration phase. We also see that the\nexploration parameter of REC plays a signiﬁcant role\nin the performance, with lower exploration leading to\nfaster convergence but slightly lower ﬁnal performance.\nNote that despite REC performing best here, the rank-\ning learned by REC is ﬁxed after the exploration steps\nhave been performed. If user interests and documents\nchange over time, the performance of REC could fall\narbitrarily. In contrast, RBA is guaranteed to remain\nnear or above the (1 − 1/e)OPT bound.\n6.2. Eﬀect of Click Noise\nIn Figure 1, the clickthrough rate and fraction of users\nwho found a relevant document in the top k positions\nis identical (since users click if and only if they are\npresented with a relevant document). In contrast,\nFigure 2 shows how the fraction of users who ﬁnd\na relevant document decays as the probability of a\nuser clicking becomes noisier. The ﬁgure presents the\nperformance lines for REC and RBA across a range\nof click probabilities, from ( pR = 1 ,pNR = 0) to\n(pR = 0.7,pNR = 0.3). We see that both algorithms\ndecay gracefully: as the clicks become noisier noisy,\nthe fraction of users presented with a relevant docu-\nments decays slowly.\n6.3. Optimizing Practical Eﬀectiveness\nDespite the theoretical results shown earlier, it would\nbe surprising if an algorithm designed for the worst\n 0.55\n 0.6\n 0.65\n 0.7\n 0.75\n 0.8\n 0.85\n 0.9\n 0.95\n 1\n400,000350,000300,000250,000200,000150,000100,00050,000 0\nFraction of Rankings with a Relevant Document\nNumber of User Presentations\nBest possible relevance performance\nRanked Bandits Algorithm\nRanked Explore and Commit (x=1000)\n1.00 / 0.00;  0.95 / 0.05\n0.90 / 0.10\n0.85 / 0.15\n0.80 / 0.20\n0.75 / 0.25\n0.70 / 0.30\nFigure 2. Eﬀect of noise in clicking behavior on the quality\nof the learned ranking.\ncase had best average case performance. Figure 3\nshows the clickthrough rate (which the algorithms op-\ntimize), and fraction of users who ﬁnd relevant doc-\numents (which is of more interest to information re-\ntrieval practitioners), for variants building on the in-\nsights of the ranked bandits idea. Speciﬁcally, two\nvariants of RBA that have the best performance we\ncould obtain in our simulation are shown. We found\nthat using a UCB1-based multi-armed bandit algo-\nrithm (Auer et al., 2002a) in place of EXP3 improves\nthe performance of RBA substantially when user inter-\nests are static. Note however, that UCB1 does not sat-\nisfy the constraints presented in Section 4.2 because it\nassumes rewards are identically distributed over time,\nan assumption violated in our setting when changes in\nthe documents presented above ranki alter the reward\ndistribution at rank i. Nevertheless, we see that this\nmodiﬁcation substantially improves the performance\nof RBA. We expect such an algorithm to perform best\nwhen few documents are prone to radical shifts in pop-\nularity.\n7. Conclusions and Extensions\nWe have presented a new formulation of the learning\nto rank problem that explicitly takes into account the\nrelevance of diﬀerent documents being interdependent.\nWe presented, analyzed and evaluated two algorithms\nand two variants for this learning setting. We have\nshown that the learning problem can be solved in a\ntheoretically sound manner, and that our algorithms\ncan be expected to perform reasonably in practice.\nWe plan to extend this work by addressing the non-\nbinary document relevance settings, and perform em-\npirical evaluations using real users and real documents.\nFurthermore, we plan to investigate how prior knowl-\nedge can be incorporated into the algorithms to im-\nprove speed of convergence. Finally, we plan to inves-\n790\nLearning Diverse Rankings with Multi-Armed Bandits\n 0.7\n 0.75\n 0.8\n 0.85\n 0.9\n 0.95\n 1\n106105104103102\nClickthrough Rate (1-Abandonment)\nNumber of User Presentations\nBest possible clickthrough rate without noise (OPT)\nRanked Bandits Algorithm\nModified-EXP3 Ranked Bandits Variant\nModified-UCB1 Ranked Bandits Variant\nRanked Explore and Commit (x=50)\nRanked Explore and Commit (x=1000)\nPopularity-Based Static Ranking\n 0.7\n 0.75\n 0.8\n 0.85\n 0.9\n 0.95\n 1\n106105104103102\nFraction of Rankings with a Relevant Document\nNumber of User Presentations\nBest possible relevance performance\nRanked Bandits Algorithm\nModified-EXP3 Ranked Bandits Variant\nModified-UCB1 Ranked Bandits Variant\nRanked Explore and Commit (x=50)\nRanked Explore and Commit (x=1000)\nPopularity-Based Static Ranking\nFigure 3. In a practical setting, it may be beneﬁcial to use a\nvariant of RBA to obtain improved performance at the cost\nof weaker theoretical guarantees. Performance is shown in\nrealistic settings pR = 0.8, pN R= 0.2.\ntigate if the bandits at diﬀerent ranks can be coupled\nto improve the rate at which RBA converges.\nAcknowledgments\nWe would like to thank the reviewers for helpful com-\nments. This work was supported by NSF Career\nAward CCF-0643934, NSF Award CCF-0729102, NSF\nCareer Award 0237381 and a gift from Google. The\nﬁrst author was supported by a Microsoft Research\nFellowship.\nReferences\nAgichtein, E., Brill, E., & Dumais, S. (2006). Improving\nweb search ranking by incorporating user behavior. In\nSIGIR (pp. 19–26).\nAldous, D. J. (1985). Exchangeability and related topics.\n´Ecole d’ ´Et´ e de Probabilit´ es de Saint-Flour XIII(pp. 1–\n198).\nAuer, P., Cesa-Bianchi, N., & Fischer, P. (2002a). Finite-\ntime analysis of the multiarmed bandit problem. Ma-\nchine Learning, 47, 235–256.\nAuer, P., Cesa-Bianchi, N., Freund, Y., & Schapire, R. E.\n(2002b). The non-stochastic multi-armed bandit prob-\nlem. SIAM Journal of Computing , 32, 48–77.\nBaeza-Yates, R., & Ribeiro-Neto, B. (1999). Modern in-\nformation retrieval. New York, NY: Addison Wesley.\nBurges, C., Shaked, T., Renshaw, E., Lazier, A., Deeds,\nM., Hamilton, N., & Hullender, G. (2005). Learning to\nrank using gradient descent. In ICML (pp. 89–96).\nBurges, C. J. C., Ragno, R., & Le, Q. V. (2006). Learning\nto rank with nonsmooth cost functions. In NIPS (pp.\n193–200). MIT Press.\nCarbonell, J., & Goldstein, J. (1998). The use of MMR,\ndiversity-based reranking for reordering documents and\nproducing summaries. In SIGIR (pp. 335–336).\nChen, H., & Karger, D. R. (2006). Less is more: Proba-\nbilistic models for retrieving fewer relevant documents.\nIn SIGIR (pp. 429–436).\nChu, W., & Ghahramani, Z. (2005). Gaussian processes\nfor ordinal regression. Journal of Machine Learning Re-\nsearch, 6, 1019–1041.\nHerbrich, R., Graepel, T., & Obermayer, K. (2000). Large\nmargin rank boundaries for ordinal regression. Advances\nin Large Margin Classiﬁers (pp. 115–132).\nJoachims, T. (2002). Optimizing search engines using click-\nthrough data. In KDD (pp. 132–142).\nKhuller, S., Moss, A., & Naor, J. (1997). The budgeted\nmaximum coverage problem. Information Processing\nLetters, 70, 39–45.\nMetzler, D., & Croft, W. B. (2005). A markov random ﬁeld\nmodel for term dependencies. In SIGIR (pp. 472–479).\nNemhauser, G. L., Wolsey, L. A., & Fisher, M. L. (1978).\nAn analysis of approximation for maximizing submodu-\nlar set functions. Mathematical Programming, 14, 265–\n294.\nRadlinski, F., & Joachims, T. (2005). Query chains: Learn-\ning to rank from implicit feedback. In KDD (pp. 239–\n248).\nRadlinski, F., & Joachims, T. (2007). Active exploration\nfor learning rankings from clickthrough data. In KDD\n(pp. 570–579).\nRobertson, S. E. (1977). The probability ranking principle\nin IR. Journal of Documentation , 33, 294–304.\nStreeter, M., & Golovin, D. (2007). An online algorithm\nfor maximizing submodular functions (Technical Report\nCMU-CS-07-171). Carnegie Mellon University.\nTaylor, M. J., Guiver, J., Robertson, S. E., & Minka, T.\n(2008). Softrank: Optimizing non-smooth ranking met-\nrics. In WSDM (pp. 77–86).\nTeevan, J., Dumais, S. T., & Horvitz, E. (2007). Charac-\nterizing the value of personalizing search. In SIGIR (pp.\n757–758).\nTurpin, A., & Scholer, F. (2006). User performance versus\nprecision measures for simple search tasks. In SIGIR\n(pp. 11–18).\nYue, Y., Finley, T., Radlinski, F., & Joachims, T. (2007). A\nsupport vector method for optimizing average precision.\nIn SIGIR (pp. 271–278).\nZhai, C., Cohen, W. W., & Laﬀerty, J. (2003). Beyond\nindependent relevance: Methods and evaluation metrics\nfor subtopic retrieval. In SIGIR (pp. 10–17).\nZhang, B., Li, H., Liu, Y., Ji, L., Xi, W., Fan, W., Chen,\nZ., & Ma, W.-Y. (2005). Improving web search results\nusing aﬃnity graph. In CIKM (pp. 504–511).\nZhu, X., Goldberg, A. B., Gael, J. V., & Andrzejewski, D.\n(2007). Improving diversity in ranking using absorbing\nrandom walks. Proceedings of NAACL HLT.\n791",
  "values": {
    "Privacy": "No",
    "Justice": "No",
    "Respect for Persons": "No",
    "Critiqability": "No",
    "Fairness": "No",
    "Not socially biased": "No",
    "Beneficence": "No",
    "Interpretable (to users)": "No",
    "Deferral to humans": "No",
    "Explicability": "No",
    "Transparent (to users)": "No",
    "Collective influence": "No",
    "Non-maleficence": "No",
    "User influence": "No",
    "Respect for Law and public interest": "No",
    "Autonomy (power to decide)": "No"
  }
}