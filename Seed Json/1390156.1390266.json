{
  "pdf": "1390156.1390266",
  "title": "1390156.1390266",
  "author": "Unknown",
  "paper_id": "1390156.1390266",
  "text": "On the Quantitative Analysis of Deep Belief Networks\nRuslan Salakhutdinov RSALAKHU @CS.TORONTO .EDU\nIain Murray MURRAY @CS.TORONTO .EDU\nDepartment of Computer Science, University of Toronto, Toronto, Ontario M5S 3G4, Canada\nAbstract\nDeep Belief Networks (DBN’s) are generative\nmodels that contain many layers of hidden vari-\nables. Efﬁcient greedy algorithms for learning\nand approximate inference have allowed these\nmodels to be applied successfully in many ap-\nplication domains. The main building block of\na DBN is a bipartite undirected graphical model\ncalled a restricted Boltzmann machine (RBM).\nDue to the presence of the partition function,\nmodel selection, complexity control, and exact\nmaximum likelihood learning in RBM’s are in-\ntractable. We show that Annealed Importance\nSampling (AIS) can be used to efﬁciently es-\ntimate the partition function of an RBM, and\nwe present a novel AIS scheme for comparing\nRBM’s with different architectures. We further\nshow how an AIS estimator, along with approx-\nimate inference, can be used to estimate a lower\nbound on the log-probability that a DBN model\nwith multiple hidden layers assigns to the test\ndata. This is, to our knowledge, the ﬁrst step\ntowards obtaining quantitative results that would\nallow us to directly assess the performance of\nDeep Belief Networks as generative models of\ndata.\n1. Introduction\nDeep Belief Networks (DBN’s), recently introduced by\nHinton et al. (2006) are probabilistic generative models that\ncontain many layers of hidden variables, in which each\nlayer captures strong high-order correlations between the\nactivities of hidden features in the layer below. The main\nbreakthrough introduced by Hinton et al. was a greedy,\nlayer-by-layer unsupervised learning algorithm that allows\nefﬁcient training of these deep, hierarchical models. The\nlearning procedure also provides an efﬁcient way of per-\nforming approximate inference, which makes the values of\nAppearing in Proceedings of the 25 th International Conference\non Machine Learning , Helsinki, Finland, 2008. Copyright 2008\nby the author(s)/owner(s).\nthe latent variables in the deepest layer easy to infer. These\ndeep generative models have been successfully applied in\nmany application domains (Hinton & Salakhutdinov, 2006;\nBengio & LeCun, 2007).\nThe main building block of a DBN is a bipartite undirected\ngraphical model called the Restricted Boltzmann Machine\n(RBM). RBM’s, and their generalizations to exponential\nfamily models, have been successfully applied in collab-\norative ﬁltering (Salakhutdinov et al., 2007), informatio n\nand image retrieval (Gehler et al., 2006), and time series\nmodeling (Taylor et al., 2006). A key feature of RBM’s\nis that inference in these models is easy. An unfortunate\nlimitation is that the probability of data under the model is\nknown only up to a computationally intractable normaliz-\ning constant, known as the partition function. A good es-\ntimate of the partition function would be extremely helpful\nfor model selection and for controlling model complexity,\nwhich are important for making RBM’s generalize well.\nThere has been extensive research on obtaining determin-\nistic approximations (Yedidia et al., 2005) or determin-\nistic upper bounds (Wainwright et al., 2005) on the log-\npartition function of arbitrary discrete Markov random\nﬁelds (MRF’s). These variational methods rely critically\non an ability to approximate the entropy of the undirected\ngraphical model. However, for densely connected MRF’s,\nsuch as RBM’s, these methods are unlikely to perform\nwell. There have also been many developments in the\nuse of Monte Carlo methods for estimating the partition\nfunction, including Annealed Importance Sampling (AIS)\n(Neal, 2001), Nested Sampling (Skilling, 2004), and many\nothers (see e.g. Neal (1993)). In this paper we show how\none such method, AIS, by taking advantage of the bipartite\nstructure of an RBM, can be used to efﬁciently estimate\nits partition function. We further show that this estimator ,\nalong with approximate inference, can be used to estimate a\nlower bound on the log-probability that a DBN model with\nmultiple hidden layers assigns to training or test data. This\nresult allows us to assess the performance of DBN’s as gen-\nerative models and to compare them to other probabilistic\nmodels, such as plain mixture models.\n872\n\nOn the Quantitative Analysis of Deep Belief Networks\n2. Restricted Boltzmann Machines\nA Restricted Boltzmann Machine is a particular type of\nMRF that has a two-layer architecture in which the visi-\nble, binary stochastic units v ∈ { 0, 1}D are connected to\nhidden binary stochastic units h ∈ {0, 1}M. The energy of\nthe state {v, h} is:\nE(v, h; θ) = −\nD∑\ni=1\nM∑\nj=1\nWijvihj −\nD∑\ni=1\nbivi −\nM∑\nj=1\najhj, (1)\nwhere θ = {W, b, a} are the model parameters: Wij repre-\nsents the symmetric interaction term between visible unit i\nand hidden unit j; bi and aj are bias terms. The probability\nthat the model assigns to a visible vector v is:\np(v; θ) = p∗(v; θ)\nZ(θ) = 1\nZ(θ)\n∑\nh\nexp (−E(v, h; θ)), (2)\nZ(θ) =\n∑\nv\n∑\nh\nexp (−E(v, h; θ)), (3)\nwhere p∗ denotes unnormalized probability, andZ(θ) is the\npartition function or normalizing constant. The conditional\ndistributions over hidden units h and visible vector v are\ngiven by logistic functions:\np(h|v) =\n∏\nj\np(hj|v), p (v|h) =\n∏\ni\np(vi|h) (4)\np(hj = 1|v) = σ(\n∑\ni\nWijvi + aj) (5)\np(vi = 1|h) = σ(\n∑\nj\nWijhj + bi), (6)\nwhere σ(x) = 1 /(1 + exp(−x)). The derivative of the log-\nlikelihood with respect to the model parameter W can be\nobtained from Eq. 2:\n∂ ln p(v)\n∂Wij\n= EP0 [vihj] − EPModel [vihj],\nwhere E P0 [·] denotes an expectation with respect to the\ndata distribution and E PModel [·] is an expectation with re-\nspect to the distribution deﬁned by the model. The ex-\npectation E PModel [·] cannot be computed analytically. In\npractice learning is done by following an approximation\nto the gradient of a different objective function, called th e\n“Contrastive Divergence” (CD) (Hinton, 2002):\n∆Wij = ϵ\n(\nEP0[vihj] − EPT [vihj]\n)\n. (7)\nThe expectation EPT [·] represents a distribution of samples\nfrom running the Gibbs sampler (Eqs. 5, 6), initialized at\nthe data, for T full steps. Setting T = ∞ recovers maxi-\nmum likelihood learning, althoughT is typically set to one.\nEven though CD learning may work well in practice, the\nproblem of model selection and complexity control still re-\nmains. Suppose we have two RBM’s with parameter values\nθA and θB. Suppose that each RBM has different num-\nber of hidden units and was trained using different learning\nrates and different numbers of CD steps. On the validation\nset, we are interested in calculating the ratio:\np(v; θA)\np(v; θB) = p∗(v; θA)\np∗(v; θB)\nZ(θB)\nZ(θA) ,\nwhich requires knowing the ratio of partition functions.\n3. Estimating Ratios of Partition Functions\nSuppose we have two distributions deﬁned on some space\nV with probability density functions: pA(v) = p∗\nA(v)/ZA\nand pB(v) = p∗\nB(v)/ZB. One way to estimate the ra-\ntio of normalizing constants is to use a simple importance\nsampling (IS) method. Suppose that pA(v) ̸= 0 whenever\npB(v) ̸= 0:\nZB\nZA\n=\n∫\np∗\nB(v)dv\nZA\n=\n∫ p∗\nB(v)\np∗\nA(v) pA(v)dv = EpA\n[ p∗\nB(v)\np∗\nA(v)\n]\n.\nAssuming we can draw independent samples from pA, the\nunbiased estimate of the ratio of partition functions can be\nobtained by using a simple Monte Carlo approximation:\nZB\nZA\n≈ 1\nM\nM∑\ni=1\np∗\nB(v(i))\np∗\nA(v(i)) ≡ 1\nM\nM∑\ni=1\nw(i) = ˆrIS, (8)\nwhere v(i) ∼ pA. If pA and pB are not close enough,\nthe estimator ˆrIS will be very poor. In high-dimensional\nspaces, the variance of ˆrIS will be very large (or possibly\ninﬁnite), unless pA is a near-perfect approximation to pB.\n3.1. Annealed Importance Sampling (AIS)\nSuppose that we can deﬁne a sequence of intermediate\nprobability distributions: p0, ..., pK, with p0 = pA and pK\n= pB, which satisfy the following conditions:\nC1 pk(v) ̸= 0 whenever pk+1(v) ̸= 0.\nC2 We must be able to easily evaluate the unnormalized\nprobability p∗\nk(v), ∀v ∈ V , k = 0, ..., K.\nC3 For each k = 0 , ..., K − 1, we must be able to draw\na sample v′ given v using a Markov chain transition\noperator Tk(v′; v) that leaves pk(v) invariant:\n∫\nTk(v′; v)pk(v)dv = pk(v′). (9)\nC4 We must be able to draw (preferably independent)\nsamples from pA.\nThe transition operators Tk(v′; v) represent the probability\ndensity of transitioning from state v to v′. Constructing a\nsuitable sequence of intermediate probability distributi ons\n873\nOn the Quantitative Analysis of Deep Belief Networks\nwill depend on the problem. One general way to deﬁne this\nsequence is to set:\npk(v) ∝ p∗\nA(v)1−βk p∗\nB(v)βk , (10)\nwith 0 = β0 < β 1 < ... < β K = 1 chosen by the user.\nOnce the sequence of intermediate distributions has been\ndeﬁned we have:\nAnnealed Importance Sampling (AIS) run:\n1. Generate v1, v2, ..., vK as follows:\n• Sample v1 from pA = p0\n• Sample v2 given v1 using T1\n• ...\n• Sample vK given vK−1 using TK−1\n2. Set\nw(i) = p∗\n1(v1)\np∗\n0(v1)\np∗\n2(v2)\np∗\n1(v2) ... p∗\nK−1(vK−1)\np∗\nK−2(vK−1)\np∗\nK(vK )\np∗\nK−1(vK)\nNote that there is no need to compute the normalizing con-\nstants of any intermediate distributions. After performin g\nM runs of AIS, the importance weights w(i) can be substi-\ntuted into Eq. 8 to obtain an estimate of the ratio of partition\nfunctions:\nZB\nZA\n≈ 1\nM\nM∑\ni=1\nw(i) = ˆrAIS. (11)\nNeal (2005) shows that for sufﬁciently large number of in-\ntermediate distributions K, the variance of ˆrAIS will be\nproportional to 1/M K. Provided K is kept large, the total\namount of computation can be split in any way between the\nnumber of intermediate distributions K and the number of\nannealing runs M without adversely affecting the accuracy\nof the estimator. If samples drawn from pA are indepen-\ndent, the number of AIS runs can be used to control the\nvariance in the estimate of ˆrAIS:\nVar(ˆrAIS) = 1\nM Var(w(i)) ≈ ˆs2\nM = ˆσ2, (12)\nwhere ˆs2 is estimated simply from the sample variance of\nthe importance weights.\n3.2. Ratios of Partition Functions of two RBM’s\nSuppose we have two RBM’s with parameter values θA =\n{W A, bA, aA} and θB = {W B, bB, aB} that deﬁne prob-\nability distributions pA and pB over V ∈ { 0, 1}D. Each\nRBM can have a different number of hidden units hA ∈\n{0, 1}MA and hB ∈ { 0, 1}MB. The generic AIS interme-\ndiate distributions (Eq. 10) would be harder to sample from\nthan an RBM. Instead we introduce the following sequence\nof distributions for k = 0, ..., K:\npk(v) = p∗\nk(v)\nZk\n= 1\nZk\n∑\nh\nexp (−Ek(v, h)), (13)\nwhere the energy function is given by:\nEk(v, h) = (1 − βk)E(v, hA; θA) + βkE(v, hB; θB), (14)\nwith 0 = β0 < β 1 < ... < β K = 1 . For i = 0 , we have\nβ0 = 0 and so p0 = pA. Similarly, for i = K, we have\npK = pB. For the intermediate values of k, we will have\nsome interpolation between pA and pB.\nLet us now deﬁne a Markov chain transition operator\nTk(v′; v) that leaves pk(v) invariant. Using Eqs. 13, 14,\nit is straightforward to derive a block Gibbs sampler. The\nconditional distributions are given by logistic functions:\np(hA\nj = 1|v) = σ\n(\n(1 − βk)(\n∑\ni\nW A\nij vi + aA\nj )\n)\n(15)\np(hB\nj = 1|v) = σ\n(\nβk(\n∑\ni\nW B\nij vi + aB\nj )\n)\n(16)\np(v′\ni = 1|h) = σ\n(\n(1 − βk)(\n∑\nj\nW A\nij hA\nj + bA\ni )\n+ βk(\n∑\nj\nW B\nij hB\nj + bB\ni )\n)\n. (17)\nGiven v, Eqs. 15, 16 are used to stochastically activate hid-\nden units hA and hB. Eq. 17 is then used to draw a new\nsample v′ as shown in Fig. 1 (left panel). Due to the special\nstructure of RBM’s, the cost of summing out h is linear in\nthe number of hidden units. We can therefore easily evalu-\nate:\np∗\nk(v) =\n∑\nhA,hB\ne(1−βk)E(v,hA;θA)+βkE(v,hB;θB )\n= e(1−βk)\n∑\ni bA\ni vi\nMA∏\nj=1\n(1 + e(1−βk)(\n∑\ni W A\nij vi+aA\nj ))\n× eβk\n∑\ni bB\ni vi\nMB∏\nj=1\n(1 + eβk(\n∑\ni W B\nij vi+aB\nj )).\nWe will assume that the parameter values of each RBM\nsatisfy |θ| < ∞, in which case p(v) > 0 for all v ∈ V .\nThis will ensure that condition C1 of the AIS procedure is\nalways satisﬁed. We have already shown that conditions\nC2 and C3 are satisﬁed. For condition C4, we can run\na blocked Gibbs sampler (Eqs. 5, 6) to generate samples\nfrom pA. These sample points will not be independent, but\nthe AIS estimator will still converge to the correct value,\nprovided our Markov chain is ergodic (Neal, 2001). How-\never, assessing the accuracy of this estimator can be difﬁ-\ncult, as it depends on both the variance of the importance\nweights and on autocorrelations in the Gibbs sampler.\n3.3. Estimating Partition Functions of RBM’s\nThe partition function of an RBM can be found by ﬁnding\nthe ratio to the normalizer for θA = {0, bA, aA}, an RBM\n874\nOn the Quantitative Analysis of Deep Belief Networks\nW\nW\nv v’\nh h\nModel BModel A\nβ (1−   )Wββ(1−   )W β B\nB\nA B\nk k k\nA\nA k\nP(v|h ,W )Q(h |v,W )\nP(h ,h |W )\nh\nW\nv\nh\nv\nhW\nh\nh\n111 1\n1 2 2\nRBM1\n1\n1\n22\n1\n2\nRBM\nFigure 1. Left: The Gibbs transition operator Tk(v′; v) leaves pk(v) invariant when estimating the ratio of partition functionsZB/ZA.\nMiddle: Recursive greedy learning consists of learning a stack of RBMs. Right: Two-layer DBN as a generative model.\nwith a zero weight matrix. From Eq. 3, we know:\nZA = 2 MA\n∏\ni\n(1 + ebi). (18)\nMoreover,\npA(v) =\n∏\ni\npA(vi) =\n∏\ni\n1/(1 + e−bi),\nso we can draw exact independent samples from this “base-\nrate” RBM. AIS in this case allows us to obtain an unbi-\nased estimate of the partition function ZB. This approach\nclosely resembles simulated annealing, since the interme-\ndiate distributions of Eq. 13 take form:\npk(v) = exp((1−βk)vT bA)\nZk\n∑\nhB\nexp(−βkE(v, hB; θB)).\nWe gradually change βk (or inverse temperature) from 0\nto 1, annealing from a simple “base-rate” model to the ﬁnal\ncomplex model. The importance weights w(i) ensure that\nAIS produces correct estimates.\n4. Deep Belief Networks (DBN’s)\nIn this section we brieﬂy review a greedy learning algo-\nrithm for training Deep Belief Networks. We then show\nhow to obtain an estimate of the lower bound on the log-\nprobability that the DBN assigns to the data.\n4.1. Greedy Learning of DBN’s\nConsider learning a DBN with two layers of hidden fea-\ntures as shown in Fig. 1 (right panel). The greedy strategy\ndeveloped by Hinton et al. (2006) uses a stack of RBM’s\n(Fig. 1, middle panel). We ﬁrst train the bottom RBM with\nparameters W 1, as described in section 2.\nA key observation is that the RBM’s joint distribution\np(v, h1|W 1) is identical to that of a DBN with second-\nlayer weights tied toW 2 = W 1⊤\n. We now consider untying\nand reﬁning W 2, improving the ﬁt to the training data.\nFor any approximating distribution Q(h1|v), the DBN’s\nlog-likelihood has the following variational lower bound:\nln p(v|W 1, W 2) ≥\n∑\nh1\nQ(h1|v)\n[\nln p(h1|W 2) +\nln p(v|h1, W 1)\n]\n+ H(Q(h1|v)), (19)\nwhere H(·) is the entropy functional. We set Q(h1|v) =\np(h1|v, W 1) deﬁned by the RBM (Eq. 5). Initially, when\nW 2 = W 1⊤\n, Q is the DBN’s true factorial posterior over\nh1, and the bound is tight. Therefore, any increase in the\nbound will lead to an increase in the true likelihood of the\nmodel. Maximizing the bound of Eq. 19 with frozen W 1 is\nequivalent to maximizing:\n∑\nh1\nQ(h1|v) ln p(h1|W 2). (20)\nThis is equivalent to training the second layer RBM with\nvectors drawn from Q(h1|v) as data.\nThis scheme can be extended by training a third RBM on\nh2 vectors drawn from the second RBM. If we initialize\nW 3 = W 2⊤\n, we are guaranteed to improve the lower bound\non the log-likelihood, though the log-likelihood itself ca n\nfall (Hinton et al., 2006). Repeating this greedy, layer-by -\nlayer training several times results in a deep, hierarchica l\nmodel.\nRecursive Greedy Learning Procedure for the DBN.\n1. Fit parameters W 1 of a 1-layer RBM to data.\n2. Freeze the parameter vector W 1 and use samples from\np(h1|v, W 1) as the data for training the next layer of\nbinary features with an RBM.\n3. Proceed recursively for as many layers as desired.\nIn practice, when adding a new layer l, we typically do not\ninitialize W l = W l−1⊤\n, so the number of hidden units of\nthe new RBM does not need to be the same as the number\nof the visible units of the lower-level RBM.\n4.2. Estimating Lower Bounds for DBN’s\nConsider the same DBN model with two layers of hidden\nfeatures shown in Fig. 1. The model’s joint distribution is:\np(v, h1, h2) = p(v|h1) p(h2, h1), (21)\nwhere p(v|h1) is deﬁned by Eq. 6), and p(h1, h2) is the\njoint distribution deﬁned by the second layer RBM. Note\nthat p(v|h1) is normalized.\n875\nOn the Quantitative Analysis of Deep Belief Networks\nBy explicitly summing out h2, we can easily evaluate an\nunnormalized probability p∗(v,h1) =Zp(v, h1). Using the\napproximating factorial distribution Q, which we get as a\nbyproduct of the greedy learning procedure, and the varia-\ntional lower bound of Eq. 19, we obtain:\nln\n∑\nh1\np(v, h1) ≥\n∑\nh1\nQ(h1|v) ln p∗(v, h1)\n− ln Z + H(Q(h1|v)) = B(v). (22)\nThe entropy term H(·) can be computed analytically, since\nQ is factorial. The partition function Z is estimated by run-\nning AIS on the top-level RBM. And the expectation term\ncan be estimated by a simple Monte Carlo approximation:\n∑\nh1\nQ(h1|v) ln p∗(v, h1) ≈ 1\nM\nM∑\ni=1\nln p∗(v, h1(i)), (23)\nwhere h1(i) ∼ Q(h1|v). The variance of this Monte Carlo\nestimator will be proportional to 1/M provided the vari-\nance of ln p∗(v, h1(i)) is ﬁnite. In general, we will be in-\nterested in calculating the lower bound averaged over the\ntest set containing Nt samples, so\n1\nNt\nNt∑\nn=1\nB(vn) ≈ 1\nNt\nNt∑\nn=1\n[ 1\nM\nM∑\ni=1\nln p∗(vn, h1(i)) +\nH(Q(h1|vn))\n]\n− ln ˆZ = ˆrB − ln ˆZ = ˆrBound. (24)\nIn this case the variance of the estimator induced by the\nMonte Carlo approximation will asymptotically scale as\n1/(NtM ). We will show in the experimental results sec-\ntion that the value of M can be small provided Nt is large.\nThe error of the overall estimator ˆrBound in Eq. 24 will be\nmostly dominated by the error in the estimate of ln Z. In\nour experiments, we obtained unbiased estimates of ˆZ and\nits standard deviation ˆσ using Eqs. 11, 12. We report ln ˆZ\nand ln ( ˆZ ± ˆσ).\nEstimating this lower bound for Deep Belief Networks with\nmore layers is now straightforward. Consider a DBN with\nL hidden layers. The model’s joint distribution and its ap-\nproximate posterior distribution Q are given by:\np(v, h1, ..., hL) = p(v|h1)...p(hL−2|hL−1)p(hL−1, hL)\nQ(h1, ..., hL|v) = Q(h1|v)Q(h2|h1)...Q(hL|hL−1).\nThe bound can now be obtained by using Eq. 22. Note\nthat most of the computation resources will be spent on\nestimating the partition function Z of the top level RBM.\n5. Experimental Results\nIn our experiments we used the MNIST digit dataset, which\ncontains 60,000 training and 10,000 test images of ten\nhandwritten digits (0 to 9), with 28×28 pixels. The dataset\nwas binarized: each pixel value was stochastically set to 1\nin proportion to its pixel intensity. Samples from the train-\ning set are shown in Fig. 2 (top left panel). Annealed im-\nportance sampling requires the βk that deﬁne a sequence\nof intermediate distributions. In all of our experiments this\nsequence was chosen by quickly running a few preliminary\nexperiments and picking the spacing of βk so as to mini-\nmize the log variance of the ﬁnal importance weights. The\nbiases bA of a base-rate model (see Eq. 18) were set by\nmaximum likelihood, then smoothed to ensure that p(v) >\n0, ∀ v ∈ V . Code that can be used to reproduce experimen-\ntal results is available at www.cs.toronto.edu/∼rsalakhu.\n5.1. Estimating partition functions of RBM’s\nIn our ﬁrst experiment we trained three RBM’s on the\nMNIST digits. The ﬁrst two RBM’s had 25 hidden units\nand were learned using CD (section 2) with T =1 and T =3\nrespectively. We call these models CD1(25) and CD3(25).\nThe third RBM had 20 hidden units and was learned using\nCD with T =1. For all three models we can calculate the ex-\nact value of the partition function simply by summing out\nthe 784 visible units for each conﬁguration of the hiddens.\nFor all three models we used 500βk spaced uniformly from\n0 to 0.5, 4,000 βk spaced uniformly from 0.5 to 0.9, and\n10,000 βk spaced uniformly from 0.9 to 1.0, with a total of\n14,500 intermediate distributions.\nTable 1 shows that for all three models, using only 10 AIS\nruns, we were able to obtain good estimates of partition\nfunctions in just 20 seconds on a Pentium Xeon 3.00GHz\nmachine. For model CD1(25), however, the variance of\nthe estimator was high, even with 100 AIS runs. However,\nﬁgure 3 (top row) reveals that as the number of annealing\nruns is increased, AIS can almost exactly recover the true\nvalue of the partition function across all three models.\nWe also estimated the ratio of normalizing constants of\ntwo RBM’s that have different numbers of hidden units:\nCD1(20) and CD1(25). This estimator could be used to\ndo complexity control. In detail, using 100 AIS runs with\nuniform spacing of 10,000 βk, we obtained ln ˆrAIS =\nln (ZCD1(20)/ZCD1(25)) = −24.49 with an error estimate\nln (ˆrAIS ± 3ˆσ) = ( −24.19, −24.93). Each sample from\nCD1(25) was generated by starting a Markov chain at the\nprevious sample and running it for 10,000 steps. Com-\npared to the true value of −24.18, this result suggests that\nour estimates may have a small systematic error due to the\nMarkov chain failing to visit some modes.\nOur second experiment consisted of training two more re-\nalistic models: CD1(500) and CD3(500). We used exactly\nthe same spacing ofβk as before and exactly the same base-\nrate model. Results are shown in table 1 (bottom row). For\neach model we were able to get what appears to be a rather\n876\nOn the Quantitative Analysis of Deep Belief Networks\nTraining samples MoB (100) Base-rate β = 0 β = 0.5 β = 0.95 β = 1.0\n\u0017 \u0014The course of AIS run for model CD25(500)\nCD1(500) CD3(500) CD25(500) DBN-CD1 DBN-CD3 DBN-CD25\nFigure 2. Top row:First two panels show random samples from the training set and a mixture of Bernoullis model with 100 components.\nThe last 4 panels display the course of 16 AIS runs for CD25(500) model by starting from a simple base-rate model and annealing to the\nﬁnal complex model. Bottom row: Random samples generated from three RBM’s and corresponding three DBN’s models.\nTable 1. Results of estimating partition functions of RBM’s along wi th the estimates of the average training and test log-probab ilities.\nFor all models we used 14,500 intermediate distributions.\nAIS True Estimates Time Avg. Test log-prob. Avg. Train log-prob.\nRuns ln Z ln ˆZ ln ( ˆZ ± ˆσ) ln ( ˆZ ± 3ˆσ) (mins) true estimate true estimate\n100 CD1(25) 255.41 256.52 255.00, 257.10 0 .0000, 257.73 3.3 −151.57 −152.68 −152.35 −153.46\nCD3(25) 307.47 307.63 307.44, 307.79 306 .91, 308.05 3.3 −143.03 −143.20 −143.94 −144.11\nCD1(20) 279.59 279.57 279.43, 279.68 279 .12, 279.87 3.1 −164.52 −164.50 −164.89 −164.87\n100 CD1(500) — 350.15 350.04, 350.25 349 .77, 350.42 10.4 — −125.53 — −122.86\nCD3(500) — 280.09 279.99, 280.17 279 .76, 280.33 10.4 — −105.50 — −102.81\nCD25(500) — 451.28 451.19, 451.37 450 .97, 451.52 10.4 — −86.34 — −83.10\naccurate estimate of Z. Of course, we are relying on an em-\npirical estimate of AIS’s accuracy, which could potentially\nbe misleading. Nonetheless, Fig. 3 (bottom row) shows that\nas we increase the number of annealing runs, the value of\nthe estimator does not oscillate drastically.\nWhile performing these tests, we observed that contrastive\ndivergence learning with T =3 results in considerably better\ngenerative model than CD learning with T =1: the differ-\nence of 20 nats is striking! Clearly, the widely used prac-\ntice of CD learning with T =1 is a rather poor “substitute”\nfor maximum likelihood learning. Inspired by this result,\nwe trained a model by starting with T =1, and gradually\nincreasing T to 25 during the course of CD training, as\nsuggested by (Carreira-Perpinan & Hinton, 2005). We call\nthis model CD25(500). Training this model was computa-\ntionally much more demanding. However, the estimate of\nthe average test log-probability for this model was about\n−86, which is 39 and 19 nats better than the CD1(500) and\nCD3(500) models respectively. Fig. 2 (bottom row) shows\nsamples generated from all three models by randomly ini-\ntializing binary states of the visible units and running alter-\nnating Gibbs for 100,000 steps. Certainly, samples gener-\nated by CD25(500) look much more like the real handwrit-\nten digits, than either CD1(500) or CD3(500).\nWe also obtained an estimate of the log ratio of two parti-\ntion functions ˆrAIS = ln ZCD25(500)/ZCD3(500) = 169.96,\nusing 10,000 βk and 100 annealing runs. The estimates of\nthe individual log-partition functions were ln ˆZCD25(500) =\n451.28 and ln ˆZCD3(500) = 280.09, in which case the log\nratio is 451.28−280.09 = 171.19. This is in agreement (to\nwithin three standard deviations) with the direct estimateof\nthe ratio, ˆrAIS = 169.96.\nFor a simple comparison we also trained several mixture of\nBernoullis models (see Fig. 2, top left panel) with 10, 100,\nand 500 components. The corresponding average test log-\nprobabilities were −168.95, −142.63, and −137.64. The\ndata generated from the mixture model looks better than\nCD3(500), although our quantitive results reveal this is due\nto over-ﬁtting. The RBM’s make much better predictions.\n5.2. Estimating lower bounds for DBN’s\nWe greedily trained three DBN models with two hidden\nlayers. The ﬁrst model, called DBN-CD1, was greedily\n877\nOn the Quantitative Analysis of Deep Belief Networks\n 10   100  500 1000 10000252\n253\n254\n255\n256\n257\n258\n259\nNumber of AIS runs\nlog Z\n \n \nLarge Variance\n20 sec\n3.3 min\n17 min\n33 min\n5.5 hrs\nEstimated logZ\nTrue logZ\n 10   100  500 1000 10000304\n305\n306\n307\n308\n309\n310\nNumber of AIS runs\nlog Z\n \n \nEstimated logZ\nTrue logZ\n 10   100  500 1000 10000276\n277\n278\n279\n280\n281\n282\nNumber of AIS runs\nlog Z\n \n \nEstimated logZ\nTrue logZ\nCD1(25) CD3(25) CD1(20)\n 10   100  500 1000 10000347\n348\n349\n350\n351\n352\n353\nNumber of AIS runs\nlog Z\nLarge variance\n1.1 min\n10.4 min\n52 min 1.8 hrs 17.4 hrs\n 10   100  500 1000 10000277\n278\n279\n280\n281\n282\n283\nNumber of AIS runs\nlog Z\n 10   100  500 1000 10000448\n449\n450\n451\n452\n453\nNumber of AIS runs\nlog Z\nCD1(500) CD3(500) CD25(500)\nFigure 3. Estimates of the log-partition functions ln ˆZ as we increase the number of annealing runs. The error bars show ln ( ˆZ ± 3ˆσ).\nlearned by freezing the parameter vector of the CD1(500)\nmodel and learning the 2nd layer RBM with 2000 hidden\nunits using CD with T =1. Similarly, the other two models,\nDBN-CD3 and DBN-CD25, added 2000 hidden units on\ntop of CD3(500) and CD25(500), using CD with T =3 and\nT =25 respectively. Training the DBN’s took roughly three\ntimes longer than the RBM’s.\nTable 2 shows the results. We used 15,000 intermediate\ndistributions and 500 annealing runs to estimate the parti-\ntion function of the 2nd layer RBM. This took 2.3 hours.\nFurther sampling was required for the simple Monte Carlo\napproximation of Eq. 23. We used M=5 samples from\nthe approximating distribution Q(h|v) for each data vec-\ntor v. Setting M=100 did not make much difference. Ta-\nble 2 also reports the empirical error in the estimate of the\nlower bound ˆrBound. From Eq. 24, we have Var(ˆrBound) =\nVar(ˆrB) + Var(ln ˆZ), both of which are shown in table 2.\nNote that models DBN-CD1 and DBN-CD3 signiﬁcantly\noutperform their single layer counterparts: CD1(500) and\nCD3(500). Adding a second layer for those two models im-\nproves model performance by at least 25 and 7 nats. This\ncorresponds to a dramatic improvement in the quality of\nsamples generated from the models (Fig. 2, bottom row).\nObserve that greedy learning of DBN’s does not appear to\nsuffer severely from overﬁtting. For single layer models,\nthe difference between the estimates of training and test\nlog-probabilities was about 3 nats. For DBN’s, the corre-\nsponding difference in the estimates of the lower bounds\nwas about 4 nats, even though adding a second layer intro-\nduced over twice as many (or one million) new parameters.\nTable 2. Results of estimating lower bounds ˆrBound (Eq. 24) on\nthe average training and test log-probabilities for DBN’s. On av-\nerage, the total error of the estimator is about ± 2 nats.\nAvg. AIS error\nbound Error ˆrB ln ( ˆZ ± 3ˆσ)\nModel log-prob ±3 std − ln ˆZ\nTest DBN-CD1 −100.64 ±0.77 −1.43, +0.57\nDBN-CD3 −98.29 ±0.75 −0.91, +0.31\nDBN-CD25 −86.22 ±0.67 −0.84, +0.65\nTrain DBN-CD1 −97.67 ±0.30 −1.43, +0.57\nDBN-CD3 −94.86 ±0.29 −0.91, +0.31\nDBN-CD25 −82.47 ±0.25 −0.84, +0.65\nThe result of our experiments for DBN-CD25, however,\nwas very different. For this model, on the test data we ob-\ntained ˆrBound = −86.22. This is comparable to the esti-\nmate of −86.34 for the average test log-probability of the\nCD25(500) model. Clearly, we cannot conﬁdently assert\nthat DBN-CD25 is a better generative model compared to\nthe carefully trained single layer RBM. This peculiar result\nalso supports previous claims that if the ﬁrst level RBM al-\nready models data well, adding extra layers will not help\n(LeRoux & Bengio, 2008; Hinton et al., 2006). As an ad-\nditional test, instead of randomly initializing parameters of\nthe 2nd layer RBM, we initialized it by using the same pa-\nrameters as the 1st layer RBM but with hidden and visible\nunits switched (see Fig. 1). This initialization ensures th at\nthe distribution over the visible units v deﬁned by the two-\nlayer DBN is exactly the same as the distribution over v\ndeﬁned by the 1st layer RBM. Therefore, after learning\nparameters of the 2nd layer RBM, the lower bound on the\ntraining data log-likelihood can only improve. After care-\n878\nOn the Quantitative Analysis of Deep Belief Networks\nfully training the second level RBM, our estimate of the\nlower bound on the test log-probability was only −85.97.\nOnce again, we cannot conﬁdently claim that adding an ex-\ntra layer in this case yields better generalization.\n6. Discussions\nThe original paper of Hinton et al. (2006) showed that for\nDBN’s, each additional layer increases a lower bound (see\nEq. 19) on the log-probability of the training data, pro-\nvided the number of hidden units per layer does not de-\ncrease. However, assessing generalization performance of\nthese generative models is quite difﬁcult, since it require s\nenumeration over an exponential number of terms. In this\npaper we developed an annealed importance sampling pro-\ncedure that takes advantage of the bipartite structure of the\nRBM. This can provide a good estimate of the partition\nfunction in a reasonable amount of computer time. Further-\nmore, we showed that this estimator, along with approx-\nimate inference, can be used to obtain an estimate of the\nlower bound on the log-probability of the test data, thus al-\nlowing us to obtain some quantitative evaluation of the gen-\neralization performance of these deep hierarchical models.\nThere are some disadvantages to using AIS. There is a\nneed to specify the βk that deﬁne a sequence of interme-\ndiate distributions. The number and the spacing of βk will\nbe problem dependent and will affect the variance of the\nestimator. We also have to rely on the empirical estimate of\nAIS accuracy, which could potentially be very misleading\n(Neal, 2001; Neal, 2005). Even though AIS provides an\nunbiased estimator of Z, it occasionally gives large overes-\ntimates and usually gives small underestimates, so in prac-\ntice, it is more likely to underestimate of the true value of\nthe partition function, which will result in an overestimat e\nof the log-probability. But these drawbacks should not re-\nsult in disfavoring the use of AIS for RBM’s and DBN’s:\nit is much better to have a slightly unreliable estimate than\nno estimate at all, or an extremely indirect estimate, such\nas discriminative performance (Hinton et al., 2006).\nWe ﬁnd AIS and other stochastic methods attractive as they\ncan just as easily be applied to undirected graphical models\nthat generalize RBM’s and DBN’s to exponential family\ndistributions. This will allow future application to mod-\nels of real-valued data, such as image patches (Osindero &\nHinton, 2008), or count data (Gehler et al., 2006).\nAnother alternative would be to employ deterministic ap-\nproximations (Yedidia et al., 2005) or deterministic upper\nbounds (Wainwright et al., 2005) on the log-partition func-\ntion. However, for densely connected MRF’s, we would\nnot expect these methods to work well. Indeed, preliminary\nresults suggest that these methods provide quite inaccurate\nestimates of (or very loose upper bounds on) the partition\nfunction, even for small RBM’s whentrained on real data.\nAcknowledgments\nWe thank Geoffrey Hinton and Radford Neal for many\nhelpful suggestions. This research was supported by\nNSERC and CFI. Iain Murray is supported by the govern-\nment of Canada.\nReferences\nBengio, Y ., & LeCun, Y . (2007). Scaling learning algorithmsto-\nwards AI. Large-Scale Kernel Machines. MIT Press.\nCarreira-Perpinan, M., & Hinton, G. (2005). On contrastive di-\nvergence learning. 10th Int. Workshop on Artiﬁcial Intelligence\nand Statistics (AISTATS’2005).\nGehler, P., Holub, A., & Welling, M. (2006). The Rate Adapt-\ning Poisson (RAP) model for information retrieval and objec t\nrecognition. Proceedings of the 23rd International Conference\non Machine Learning.\nHinton, & Salakhutdinov (2006). Reducing the dimensionality of\ndata with neural networks. Science, 313, 504 – 507.\nHinton, G. E. (2002). Training products of experts by minimizing\ncontrastive divergence. Neural Computation, 14, 1711–1800.\nHinton, G. E., Osindero, S., & Teh, Y . W. (2006). A fast learning\nalgorithm for deep belief nets. Neural Computation, 18, 1527–\n1554.\nLeRoux, N., & Bengio, Y . (2008). Representational power of\nrestricted Boltzmann machines and deep belief networks. To\nappear in Neural Computation.\nNeal, R. M. (1993). Probabilistic inference using Markov chain\nMonte Carlo methods (Technical Report CRG-TR-93-1). De-\npartment of Computer Science, University of Toronto.\nNeal, R. M. (2001). Annealed importance sampling. Statistics\nand Computing, 11, 125–139.\nNeal, R. M. (2005). Estimating ratios of normalizing constants\nusing linked importance sampling (Technical Report 0511).\nDepartment of Statistics, University of Toronto.\nOsindero, S., & Hinton, G. (2008). Modeling image patches with\na directed hierarchy of Markov random ﬁelds. NIPS 20. Cam-\nbridge, MA: MIT Press.\nSalakhutdinov, R., Mnih, A., & Hinton, G. (2007). Restricte d\nBoltzmann machines for collaborative ﬁltering. Proceedings\nof the Twenty-fourth International Conference (ICML 2004).\nSkilling, J. (2004). Nested sampling. Bayesian inference and\nmaximum entropy methods in science and engineering, AIP\nConference Proceeedings, 735, 395–405.\nTaylor, G. W., Hinton, G. E., & Roweis, S. T. (2006). Model-\ning human motion using binary latent variables. Advances in\nNeural Information Processing Systems. MIT Press.\nWainwright, M. J., Jaakkola, T., & Willsky, A. S. (2005). A\nnew class of upper bounds on the log partition function. IEEE\nTransactions on Information Theory, 51, 2313–2335.\nYedidia, J. S., Freeman, W. T., & Weiss, Y . (2005). Construct -\ning free-energy approximations and generalized belief pro pa-\ngation algorithms. IEEE Transactions on Information Theory,\n51, 2282–2312.\n879",
  "values": {
    "Respect for Law and public interest": "Yes",
    "Interpretable (to users)": "Yes",
    "Transparent (to users)": "Yes",
    "Autonomy (power to decide)": "Yes",
    "Respect for Persons": "Yes",
    "Deferral to humans": "Yes",
    "Fairness": "Yes",
    "Privacy": "Yes",
    "Collective influence": "Yes",
    "Beneficence": "Yes",
    "User influence": "Yes",
    "Justice": "Yes",
    "Not socially biased": "Yes",
    "Critiqability": "Yes",
    "Explicability": "Yes",
    "Non-maleficence": "Yes"
  }
}