{
  "pdf": "1390156.1390267",
  "title": "1390156.1390267",
  "author": "Unknown",
  "paper_id": "1390156.1390267",
  "text": "Bayesian Probabilistic Matrix Factorization\nusing Markov Chain Monte Carlo\nRuslan Salakhutdinov rsalakhu@cs.toronto.edu\nAndriy Mnih amnih@cs.toronto.edu\nDepartment of Computer Science, University of Toronto, Tor onto, Ontario M5S 3G4, Canada\nAbstract\nLow-rank matrix approximation methods\nprovide one of the simplest and most eﬀective\napproaches to collaborative ﬁltering. Such\nmodels are usually ﬁtted to data by ﬁnding\na MAP estimate of the model parameters, a\nprocedure that can be performed eﬃciently\neven on very large datasets. However, un-\nless the regularization parameters are tuned\ncarefully, this approach is prone to overﬁt-\nting because it ﬁnds a single point estimate\nof the parameters. In this paper we present a\nfully Bayesian treatment of the Probabilistic\nMatrix Factorization (PMF) model in which\nmodel capacity is controlled automatically by\nintegrating over all model parameters and\nhyperparameters. We show that Bayesian\nPMF models can be eﬃciently trained us-\ning Markov chain Monte Carlo methods by\napplying them to the Netﬂix dataset, which\nconsists of over 100 million movie ratings.\nThe resulting models achieve signiﬁcantly\nhigher prediction accuracy than PMF models\ntrained using MAP estimation.\n1. Introduction\nFactor-based models have been used extensively in the\ndomain of collaborative ﬁltering for modelling user\npreferences. The idea behind such models is that pref-\nerences of a user are determined by a small number of\nunobserved factors. In a linear factor model, a user’s\nrating of an item is modelled by the inner product of\nan item factor vector and a user factor vector. This\nmeans that the N × M preference matrix of ratings\nthat N users assign to M movies is modeled by the\nproduct of an D × N user coeﬃcient matrix U and a\nD× M factor matrix V (Rennie & Srebro, 2005; Srebro\nAppearing in Proceedings of the 25 th International Confer-\nence on Machine Learning , Helsinki, Finland, 2008. Copy-\nright 2008 by the author(s)/owner(s).\n& Jaakkola, 2003). Training such a model amounts to\nﬁnding the best rank- D approximation to the observed\nN × M target matrix R under the given loss function.\nA variety of probabilistic factor-based models have\nbeen proposed (Hofmann, 1999; Marlin, 2004; Marlin\n& Zemel, 2004; Salakhutdinov & Mnih, 2008). In these\nmodels factor variables are assumed to be marginally\nindependent while rating variables are assumed to be\nconditionally independent given the factor variables.\nThe main drawback of such models is that inferring\nthe posterior distribution over the factors given the\nratings is intractable. Many of the existing methods\nresort to performing MAP estimation of the model pa-\nrameters. Training such models amounts to maximiz-\ning the log-posterior over model parameters and can\nbe done very eﬃciently even on very large datasets.\nIn practice, we are usually interested in predicting rat-\nings for new user/movie pairs rather than in estimat-\ning model parameters. This view suggests taking a\nBayesian approach to the problem which involves in-\ntegrating out the model parameters. In this paper, we\ndescribe a fully Bayesian treatment of the Probabilis-\ntic Matrix Factorization (PMF) model which has been\nrecently applied to collaborative ﬁltering (Salakhutdi-\nnov & Mnih, 2008). The distinguishing feature of our\nwork is the use of Markov chain Monte Carlo (MCMC)\nmethods for approximate inference in this model. In\npractice, MCMC methods are rarely used on large-\nscale problems because they are perceived to be very\nslow by practitioners. In this paper we show that\nMCMC can be successfully applied to the large, sparse,\nand very imbalanced Netﬂix dataset, containing over\n100 million user/movie ratings. We also show that it\nsigniﬁcantly increases the model’s predictive accuracy,\nespecially for the infrequent users, compared to the\nstandard PMF models trained using MAP with regu-\nlarization parameters that have been carefully tuned\non the validation set.\nPrevious applications of Bayesian matrix factorization\nmethods to collaborative ﬁltering (Lim & Teh, 2007;\nRaiko et al., 2007) have used variational approxima-\n880\n\nBayesian Probabilistic Matrix Factorization using MCMC\ntions for performing inference. These methods at-\ntempt to approximate the true posterior distribution\nby a simpler, factorized distribution under which the\nuser factor vectors are independent of the movie factor\nvectors. The consequence of this assumption is that\nthat the variational posterior distributions over the\nfactor vectors is a product of two multivariate Gaus-\nsians: one for the viewer factor vectors and one for\nthe movie factor vectors. This assumption of indepen-\ndence between the viewer and movie factors seems un-\nreasonable, and, as our experiments demonstrate, the\ndistributions over factors in such models turn out to\nbe non-Gaussian. This conclusion is supported by the\nfact that the Bayesian PMF models outperform their\nMAP trained counterparts by a much larger margin\nthan the variationally trained models do.\n2. Probabilistic Matrix Factorization\nProbabilistic Matrix Factorization (PMF) is a proba-\nbilistic linear model with Gaussian observation noise\n(see Fig. 1, left panel). Suppose we have N users\nand M movies. Let Rij be the rating value of user i\nfor movie j, Ui and Vj represent D-dimensional user-\nspeciﬁc and movie-speciﬁc latent feature vectors re-\nspectively. The conditional distribution over the ob-\nserved ratings R ∈ RN ×M (the likelihood term) and\nthe prior distributions over U ∈ RD×N and V ∈\nRD×M are given by:\np(R|U, V, α) =\nN∏\ni=1\nM∏\nj=1\n[\nN (Rij|U T\ni Vj, α−1)\n]Iij\n(1)\np(U |αU ) =\nN∏\ni=1\nN (Ui|0, α−1\nU I) (2)\np(V |αV ) =\nM∏\nj=1\nN (Vj |0, α−1\nV I), (3)\nwhere N (x|µ, α−1) denotes the Gaussian distribution\nwith mean µ and precision α, and Iij is the indicator\nvariable that is equal to 1 if user i rated movie j and\nequal to 0 otherwise.\nLearning in this model is performed by maximizing\nthe log-posterior over the movie and user features with\nﬁxed hyperparameters (i.e. the observation noise vari-\nance and prior variances):\nln p(U, V |R, α, αV , αU ) = ln p(R|U, V, α) +\n+ ln p(U |αU ) + ln p(V |αV ) + C,\nwhere C is a constant that does not depend on the pa-\nrameters. Maximizing this posterior distribution with\nrespect to U and V is equivalent to minimizing the\nsum-of-squares error function with quadratic regular-\nization terms:\nE = 1\n2\nN∑\ni=1\nM∑\nj=1\nIij\n(\nRij − U T\ni Vj\n) 2\n+ λU\n2\nN∑\ni=1\n∥ Ui ∥2\nFro + λV\n2\nM∑\nj=1\n∥ Vj ∥2\nFro, (4)\nwhere λU = αU /α, λV = αV /α, and ∥ · ∥ 2\nFro denotes\nthe Frobenius norm. A local minimum of the objective\nfunction given by Eq. 4 can be found by performing\ngradient descent in U and V .\nThe main drawback of this training procedure is the\nneed for manual complexity control that is essential\nto making the model generalize well, particularly on\nsparse and imbalanced datasets. One way to control\nthe model complexity is to search for appropriate val-\nues of regularization parameters λU and λV deﬁned\nabove. We could, for example, consider a set of reason-\nable parameter values, train a model for each setting\nof the parameters, and choose the model that performs\nbest on the validation set. This approach however is\ncomputationally very expensive, since it requires train-\ning a multitude of models instead of training a single\none.\nAlternatively, we could introduce priors for the hy-\nperparameters and maximize the log-posterior of the\nmodel over both parameters and hyperparameters ,\nwhich allows model complexity to be controlled auto-\nmatically based on the training data (Nowlan & Hin-\nton, 1992; Salakhutdinov & Mnih, 2008). Though this\napproach has been shown to work in practice it is not\nwell-grounded theoretically, and it is not diﬃcult to\nconstruct a simple example for which such joint opti-\nmization would not produce the desired results.\nIn the next section we describe a fully Bayesian treat-\nment of the PMF model with model parameters and\nhyperparameters integrated out using MCMC meth-\nods, which provides fully automatic complexity con-\ntrol.\n3. Bayesian Probabilistic Matrix\nFactorization\n3.1. The Model\nThe graphical model representing Bayesian PMF is\nshown in Fig. 1 (right panel). As in PMF, the likeli-\nhood of the observed ratings is given by Eq. 1. The\nprior distributions over the user and movie feature vec-\n881\nBayesian Probabilistic Matrix Factorization using MCMC\nUVj i\nRij\nj=1,...,M i=1,...,N\nV U\nα\nα α\nj\nRij\nj=1,...,M i=1,...,N\nVµ µ Ui\nΛU\nµU\n0ν , W0\nµ0V0\nVΛ\n, W00ν\nα\nFigure 1. The left panel shows the graphical model for Probabilistic M atrix Factorization (PMF). The right panel shows\nthe graphical model for Bayesian PMF.\ntors are assumed to be Gaussian:\np(U |µU , ΛU ) =\nN∏\ni=1\nN (Ui|µU , Λ−1\nU ), (5)\np(V |µV , ΛV ) =\nM∏\ni=1\nN (Vi|µV , Λ−1\nV ). (6)\nWe further place Gaussian-Wishart priors on the user\nand movie hyperparameters Θ U = {µU , ΛU } and\nΘV = {µV , ΛV }:\np(ΘU |Θ0) = p(µU |ΛU )p(ΛU )\n= N (µU |µ0, (β0ΛU )−1)W(ΛU |W0, ν0), (7)\np(ΘV |Θ0) = p(µV |ΛV )p(ΛV )\n= N (µV |µ0, (β0ΛV )−1)W(ΛV |W0, ν0). (8)\nHere W is the Wishart distribution with ν0 degrees of\nfreedom and a D × D scale matrix W0:\nW(Λ|W0, ν0) = 1\nC |Λ|(ν0−D−1)/2 exp (− 1\n2 Tr(W −1\n0 Λ)),\nwhere C is the normalizing constant. For convenience\nwe also deﬁne Θ 0 = {µ0, ν0, W0}. In our experiments\nwe also set ν0 = D and W0 to the identity matrix\nfor both user and movie hyperparameters and choose\nµ0 = 0 by symmetry.\n3.2. Predictions\nThe predictive distribution of the rating value R∗\nij for\nuser i and query movie j is obtained by marginalizing\nover model parameters and hyperparameters:\np(R∗\nij|R, Θ0) =\n∫ ∫\np(R∗\nij|Ui, Vj)p(U, V |R, ΘU , ΘV )\np(ΘU , ΘV |Θ0)d{U, V }d{ΘU , ΘV }. (9)\nSince exact evaluation of this predictive distribution\nis analytically intractable due to the complexity of the\nposterior we need to resort to approximate inference.\nOne choice would be to use variational methods (Hin-\nton & van Camp, 1993; Jordan et al., 1999) that pro-\nvide deterministic approximation schemes for posteri-\nors. In particular, we could approximate the true pos-\nterior p(U, V, ΘU , ΘV |R) by a distribution that factors,\nwith each factor having a speciﬁc parametric form such\nas a Gaussian distribution. This approximate poste-\nrior would allow us to approximate the integrals in\nEq. 9. Variational methods have become the method-\nology of choice, since they typically scale well to large\napplications. However, they can produce inaccurate\nresults because they tend to involve overly simple ap-\nproximations to the posterior.\nMCMC-based methods (Neal, 1993), on the other\nhand, use the Monte Carlo approximation to the pre-\ndictive distribution of Eq. 9 given by:\np(R∗\nij|R, Θ0) ≈ 1\nK\nK∑\nk=1\np(R∗\nij|U (k)\ni , V (k)\nj ). (10)\nThe samples {U (k)\ni , V (k)\nj } are generated by running\na Markov chain whose stationary distribution is the\nposterior distribution over the model parameters and\nhyperparameters {U, V, ΘU , ΘV }. The advantage of\n882\nBayesian Probabilistic Matrix Factorization using MCMC\nthe Monte Carlo-based methods is that asymptoti-\ncally they produce exact results. In practice, how-\never, MCMC methods are usually perceived to be so\ncomputationally demanding that their use is limited\nto small-scale problems.\n3.3. Inference\nOne of the simplest MCMC algorithms is the Gibbs\nsampling algorithm, which cycles through the latent\nvariables, sampling each one from its distribution con-\nditional on the current values of all other variables.\nGibbs sampling is typically used when these condi-\ntional distributions can be sampled from easily.\nDue to the use of conjugate priors for the parame-\nters and hyperparameters in the Bayesian PMF model,\nthe conditional distributions derived from the poste-\nrior distribution are easy to sample from. In particu-\nlar, the conditional distribution over the user feature\nvector Ui, conditioned on the movie features, observed\nuser rating matrix R, and the values of the hyperpa-\nrameters is Gaussian:\np(Ui|R, V, ΘU , α) = N\n(\nUi|µ∗\ni ,\n[\nΛ∗\ni\n]−1)\n(11)\n∼\nM∏\nj=1\n[\nN (Rij|U T\ni Vj, α−1)\n]Iij\np(Ui|µU , ΛU ),\nwhere\nΛ∗\ni = Λ U + α\nM∑\nj=1\n[\nVj V T\nj\n]Iij\n(12)\nµ∗\ni = [Λ ∗\ni ]−1\n(\nα\nM∑\nj=1\n[\nVjRij\n]Iij\n+ ΛU µU\n)\n. (13)\nNote that the conditional distribution over the user\nlatent feature matrix U factorizes into the product of\nconditional distributions over the individual user fea-\nture vectors:\np(U |R, V, ΘU ) =\nN∏\ni=1\np(Ui|R, V, ΘU ).\nTherefore we can easily speed up the sampler by sam-\npling from these conditional distributions in parallel.\nThe speedup could be substantial, particularly when\nthe number of users is large.\nThe conditional distribution over the user hyperpa-\nrameters conditioned on the user feature matrix U is\ngiven by the Gaussian-Wishart distribution:\np(µU , ΛU |U, Θ0) =\nN (µU |µ∗\n0, (β∗\n0 ΛU )−1)W(ΛU |W ∗\n0 , ν∗\n0 ), (14)\nwhere\nµ∗\n0 = β0µ0 + N ¯U\nβ0 + N , β ∗\n0 = β0 + N, ν ∗\n0 = ν0 + N,\n[\nW ∗\n0\n]−1\n= W −1\n0 + N ¯S + β0N\nβ0 + N (µ0 − ¯U)(µ0 − ¯U)T\n¯U = 1\nN\nN∑\ni=1\nUi ¯S = 1\nN\nN∑\ni=1\nUiU T\ni .\nThe conditional distributions over the movie feature\nvectors and the movie hyperparameters have exactly\nthe same form. The Gibbs sampling algorithm then\ntakes the following form:\nGibbs sampling for Bayesian PMF\n1. Initialize model parameters {U 1, V 1}\n2. For t=1,...,T\n• Sample the hyperparameters\n(Eq. 14):\nΘt\nU ∼ p(ΘU |U t, Θ0)\nΘt\nV ∼ p(ΘV |V t, Θ0)\n• For each i = 1 , ..., N sample user features in\nparallel (Eq. 11):\nU t+1\ni ∼ p(Ui|R, V t, Θt\nU )\n• For each i = 1, ..., M sample movie features in\nparallel:\nV t+1\ni ∼ p(Vi|R, U t+1, Θt\nV )\n4. Experimental Results\n4.1. Description of the dataset\nThe data, collected by Netﬂix, represent the distribu-\ntion of all ratings Netﬂix obtained between October,\n1998 and December, 2005. The training data set con-\nsists of 100,480,507 ratings from 480,189 randomly-\nchosen, anonymous users on 17,770 movie titles. As\npart of the training data, Netﬂix also provides valida-\ntion data, containing 1,408,395 ratings. In addition,\nNetﬂix also provides a test set containing 2,817,131\nuser/movie pairs with the ratings withheld. The pairs\nwere selected from the most recent ratings from a sub-\nset of the users in the training data set. Performance\nis assessed by submitting predicted ratings to Netﬂix\nwhich then posts the root mean squared error (RMSE)\non an unknown half of the test set. As a baseline, Net-\nﬂix provided the test score of its own system trained\non the same data, which is 0.9514.\n883\nBayesian Probabilistic Matrix Factorization using MCMC\n0 10 20 30 40 50 60\n0.9\n0.91\n0.92\n0.93\n0.94\n0.95\n0.96\n0.97\nEpochs\nRMSE\nPMF\nBayesian PMF\nNetflix \nBaseline Score\nSVD\nLogistic PMF\n 4   8   16  32  64  128  256  5120.89\n0.895\n0.9\n0.905\n0.91\n0.915\n0.92\nNumber of Samples\nRMSE\n30−D\n60−D\n5.7 hrs. 23 hrs. 90 hrs.\n11.7 hrs.\n47 hrs. 188 hrs.\nBayesian PMF\nFigure 2. Left panel: Performance of SVD, PMF, logistic PMF, and Bayes ian PMF using 30D feature vectors, on the\nNetﬂix validation data. The y-axis displays RMSE (root mean squared error), and the x-axis shows the number of epochs,\nor passes, through the entire training set. Right panel: RMS E for the Bayesian PMF models on the validation set as a\nfunction of the number of samples generated. The two curves a re for the models with 30D and 60D feature vectors.\n4.2. T raining PMF models\nFor comparison, we have trained a variety of linear\nPMF models using MAP, choosing their regularization\nparameters using the validation set. In addition to lin-\near PMF models, we also trained logistic PMF mod-\nels, in which we pass the dot product between user-\nand movie-speciﬁc feature vectors through the logistic\nfunction σ(x) = 1 /(1 + exp(− x)) to bound the range\nof predictions:\np(R|U, V, α) =\nN∏\ni=1\nM∏\nj=1\n[\nN (Rij|σ(U T\ni Vj), α−1)\n]Iij\n. (15)\nThe ratings 1 , ..., 5 are mapped to the interval [0 , 1]\nusing the function t(x) = ( x − 1)/4, so that the range\nof valid rating values matches the range of predictions\nour model can make. Logistic PMF models can some-\ntimes provide slightly better results than their linear\ncounterparts.\nTo speed up training, instead of performing full batch\nlearning, we subdivided the Netﬂix data into mini-\nbatches of size 100,000 (user/movie/rating triples) and\nupdated the feature vectors after each mini-batch. We\nused a learning rate of 0.005 and a momentum of 0.9\nfor training the linear as well as logistic PMF models.\n4.3. T raining Bayesian PMF models\nWe initialized the Gibbs sampler by setting the model\nparameters U and V to their MAP estimates obtained\nby training a linear PMF model. We also set µ0 =\n0, ν0 = D, and W0 to the identity matrix, for both\nuser and movie hyperpriors. The observation noise\nprecision α was ﬁxed at 2. The predictive distribution\nwas computed using Eq. 10 by running the Gibbs\nsampler with samples {U (k)\ni , V (k)\nj } collected after each\nfull Gibbs step.\n4.4. Results\nIn our ﬁrst experiment, we compared a Bayesian PMF\nmodel to an SVD model, a linear PMF model, and a\nlogistic PMF model, all using 30D feature vectors. The\nSVD model was trained to minimize the sum-squared\ndistance to the observed entries of the target matrix,\nwith no regularization applied to the feature vectors.\nNote that this model can be seen as a PMF model\ntrained using maximum likelihood (ML). For the PMF\nmodels, the regularization parameters λU and λV were\nset to 0 .002. Predictive performance of these models\non the validation set is shown in Fig. 2 (left panel).\nThe mean of the predictive distribution of the Bayesian\nPMF model achieves an RMSE of 0 .8994, compared to\nan RMSE of 0 .9174 of a moderately regularized linear\nPMF model, an improvement of over 1.7%.\nThe logistic PMF model does slightly outperform its\nlinear counterpart, achieving an RMSE of 0.9097.\nHowever, its performance is still considerably worse\nthan that of the Bayesian PMF model. A simple\nSVD achieves an RMSE of about 0.9280 and after\nabout 10 epochs begins to overﬁt heavily. This ex-\nperiment clearly demonstrates that SVD and MAP-\ntrained PMF models can overﬁt and that the pre-\ndictive accuracy can be improved by integrating out\nmodel parameters and hyperparameters.\n884\nBayesian Probabilistic Matrix Factorization using MCMC\n−20 −10 0 10 20\n−20\n−10\n0\n10\n20\nDimension1\nDimension3\nUser A (4 ratings)\n−20 −10 0 10 200\n5\n10\n15\n20\n25\n30\n35\nDimension3\nFrequency Count\n−20 −10 0 10 200\n5\n10\n15\n20\n25\n30\n35\nDimension1\nFrequency Count\n−20 −10 0 10 20\n−20\n−10\n0\n10\n20\nDimension5\nDimension1\nUser C (319 ratings)\n−20 −10 0 10 200\n5\n10\n15\n20\n25\n30\n35\nDimension1\nFrequency Count\n−20 −10 0 10 200\n5\n10\n15\n20\n25\nDimension5\nFrequency Count\n−0.4 −0.2 0 0.2 0.4\n−0.4\n−0.2\n0\n0.2\n0.4\nDimension1\nDimension2\nMovie X (5 ratings)\n−1 −0.5 0 0.5 10\n20\n40\n60\n80\n100\n120\nDimension2\nFrequency Count\n−1 −0.5 0 0.5 10\n20\n40\n60\n80\n100\nDimension1\nFrequency Count\n−0.4 −0.2 0 0.2 0.4\n−0.4\n−0.2\n0\n0.2\n0.4\nDimension1\nDimension2\nMovie Y (142 ratings)\n−0.2 −0.1 0 0.1 0.20\n10\n20\n30\n40\n50\n60\n70\nDimension2\nFrequency Count\n−0.2 −0.1 0 0.1 0.20\n10\n20\n30\n40\n50\n60\nDimension1\nFrequency Count\nFigure 3. Samples from the posterior over the user and movie feature ve ctors generated by each step of the Gibbs\nsampler. The two dimensions with the highest variance are sh own for two users and two movies. The ﬁrst 800 samples\nwere discarded as “burn-in”.\nD Valid. RMSE % Test RMSE %\nPMF BPMF Inc. PMF BPMF Inc.\n30 0.9154 0.8994 1.74 0.9188 0.9029 1.73\n40 0.9135 0.8968 1.83 0.9170 0.9002 1.83\n60 0.9150 0.8954 2.14 0.9185 0.8989 2.13\n150 0.9178 0.8931 2.69 0.9211 0.8965 2.67\n300 0.9231 0.8920 3.37 0.9265 0.8954 3.36\nTable 1. Performance of Bayesian PMF (BPMF) and lin-\near PMF on Netﬂix validation and test sets.\nWe than trained larger PMF models with D = 40 and\nD = 60. Capacity control for such models becomes a\nrather challenging task. For example, a PMF model\nwith D = 60 has approximately 30 million parameters.\nSearching for appropriate values of the regularization\ncoeﬃcients becomes a very computationally expensive\ntask. Table 1 further shows that for the 60-dimensional\nfeature vectors, Bayesian PMF outperforms its MAP\ncounterpart by over 2%. We should also point out\nthat even the simplest possible Bayesian extension of\nthe PMF model, where Gamma priors are placed over\nthe precision hyperparameters αU and αV (see Fig. 1,\nleft panel), signiﬁcantly outperforms the MAP-trained\nPMF models, even though it does not perform as well\nas the Bayesian PMF models.\nIt is interesting to observe that as the feature di-\nmensionality grows, the performance accuracy for the\nMAP-trained PMF models does not improve, and con-\ntrolling overﬁtting becomes a critical issue. The pre-\ndictive accuracy of the Bayesian PMF models, how-\never, steadily improves as the model complexity grows.\nInspired by this result, we experimented with Bayesian\nPMF models with D = 150 and D = 300 feature\nvectors. Note that these models have about 75 and\n150 million parameters, and running the Gibbs sam-\npler becomes computationally much more expensive.\nNonetheless, the validation set RMSEs for the two\nmodels were 0 .8931 and 0 .8920. Table 1 shows that\nthese models not only signiﬁcantly outperform their\nMAP counterparts but also outperform Bayesian PMF\nmodels that have fewer parameters. These results\nclearly show that the Bayesian approach does not re-\nquire limiting the complexity of the model based on the\nnumber of the training samples. In practice, however,\nwe will be limited by the available computer resources.\nFor completeness, we also report the performance re-\nsults on the Netﬂix test set . These numbers were ob-\n885\nBayesian Probabilistic Matrix Factorization using MCMC\n A   B   C   D  \n1\n1.5\n2\n2.5\n3\n3.5\n4\n4.5\n5\nPredicted Ratings\nUsers\n   1−5      6−10      −20      −40      −80      −160     −320     −640   >641   \n0.8\n0.9\n1\n1.1\n1.2\nNumber of Observed Ratings\nRMSE\nLogistic \nPMF\nBayesian \nPMF\nMovie Average\nFigure 4. Left panel: Box plot of predictions, obtained after each ful l Gibbs step, for 4 users on a randomly chosen test\nmovies. Users A,B,C, and D have 4, 23, 319 and 660 ratings resp ectively. Right panel: Performance of Bayesian PMF,\nlogistic PMF, and the movie average algorithm that always pr edicts the average rating of each movie. The users were\ngrouped by the number of observed ratings in the training dat a. The linear PMF model performed slightly worse than\nthe logistic PMF model.\ntained by submitting the predicted ratings to Netﬂix\nwho then provided us with the test score on an un-\nknown half of the test set. The test scores are slightly\nworse than the validation scores, but the relative be-\nhavior across all models remains the same.\nTo diagnose the convergence of the Gibbs sampler, we\nmonitored the behaviour of the Frobenius norms of\nthe model parameters and hyperparameters: U , V , Λ,\nand µ. Typically, after a few hundred samples these\nquantities stabilize. Fig. 2 (right panel) shows the\nRMSE error on the Netﬂix validation set for Bayesian\nPMF models as the number of samples increases. Af-\nter obtaining a few hundred samples 1 the predictive\naccuracy does not signiﬁcantly improve. Note that\nthe initial predictive accuracy is already high because\nthe Markov chain is initialized using the MAP values\nof the model parameters.\nFor the Bayesian PMF model with D = 30 we also col-\nlected samples over the user and movie feature vectors\ngenerated by each full step of the Gibbs sampler. The\nﬁrst 800 samples were discarded as “burn-in”. Figure\n3 shows these samples for two users and two movies\nprojected onto the two dimensions of the highest vari-\nance. Users A and C were chosen by randomly picking\namong rare users who have fewer than 10 ratings and\nmore frequent users who have more than 100 ratings\nin the training set. Movies X and Y were chosen in the\nsame way. Note that the empirical distribution of the\n1We store the model parameters after each full Gibbs\nstep as a sample. The fact that these samples are not\nindependent does not matter for making predictions.\nsamples from the posterior appear to be non-Gaussian.\nUsing these samples from the posterior we also looked\nat the uncertainty of the predictions of four users on\nrandomly chosen test movies. Figure 4 (left panel)\nshows results for users A,B,C, and D who have 4, 23,\n319 and 660 ratings respectively. Note that there is\nmuch more uncertainty about the prediction of user\nA than about the prediction of user D, whose feature\nvector is well-determined. Figure 4 (right panel) shows\nthat the Bayesian PMF model considerably outper-\nforms the logistic PMF model on users with few rat-\nings. As the number of ratings increases, both the\nlogistic PMF and the Bayesian PMF exhibit similar\nperformance.\nThe advantage of Bayesian PMF models is that by av-\neraging over all settings of parameters that are com-\npatible with the data as well as the prior they deal with\nuncertainty more eﬀectively than the non-Bayesian\nPMF models, which commit to a single most probable\nsetting.\nSince the main concern when applying Bayesian meth-\nods to large datasets is their running time, we provide\nthe times for our simple Matlab Bayesian PMF im-\nplementation. One full Gibbs step on a single core of\na recent Pentium Xeon 3.00GHz machine for models\nwith D = 10, 30, 60, 300 takes 6.6, 12.9 , 31.6, and 220\nminutes respectively. Note that the most expensive as-\npect of training Bayesian PMF models is the inversion\nof a D × D matrix per feature vector 2 (see Eq. 13),\n2In our implementation, we solve a system of D equa-\n886\nBayesian Probabilistic Matrix Factorization using MCMC\nwhich is an O(D3) operation.\n5. Conclusions\nWe have presented a fully Bayesian treatment of Prob-\nabilistic Matrix Factorization by placing hyperpriors\nover the hyperparameters and using MCMC meth-\nods to perform approximate inference. We have also\ndemonstrated that Bayesian PMF models can be suc-\ncessfully applied to a large dataset containing over\n100 million movie ratings, and achieve signiﬁcantly\nhigher predictive accuracy compared to the MAP-\ntrained PMF models with carefully tuned regulariza-\ntion parameters. An additional advantage of using a\nBayesian model is that it provides a predictive dis-\ntribution instead of just a single number, allowing the\nconﬁdence in the prediction to be quantiﬁed and taken\ninto account when making recommendations using the\nmodel.\nUsing MCMC instead of variational methods for ap-\nproximate inference in Bayesian matrix factorization\nmodels leads to much larger improvements over the\nMAP trained models, which suggests that the assump-\ntions made by the variational methods about the struc-\nture of the posterior are not entirely reasonable. This\nconclusion is conﬁrmed by inspecting the empirical dis-\ntribution of the samples from the posterior, which ap-\npears to be signiﬁcantly non-Gaussian.\nA major problem of MCMC methods is that it is hard\nto determine when the Markov chain has converged to\nthe desired distribution. In practice, we have to rely on\nrules of thumb to diagnose convergence, which means\nthat there is a risk of using samples from a distribu-\ntion that diﬀers from the true posterior distribution,\npotentially leading to suboptimal predictions. Our re-\nsults show that this problem is not a suﬃcient reason\nto reject MCMC methods.\nFor our models, the number of samples from the\nposterior that can be generated within a reasonable\namount of time will typically be constrained by the\navailable computer resources. However, as mentioned\nabove, sampling the feature vectors for multiple users\nor movies in parallel provides an easy way to greatly\nspeed up the process of generating samples using mul-\ntiple cores.\nAcknowledgments\nWe thank Geoﬀrey Hinton for many helpful discus-\nsions. This research was supported by NSERC.\ntions instead of inverting a matrix. The computational cost\nof this operation is still O(D3).\nReferences\nHinton, G. E., & van Camp, D. (1993). Keeping the\nneural networks simple by minimizing the descrip-\ntion length of the weights. COLT (pp. 5–13).\nHofmann, T. (1999). Probabilistic latent semantic\nanalysis. Proceedings of the 15th Conference on Un-\ncertainty in AI (pp. 289–296). San Fransisco, Cali-\nfornia: Morgan Kaufmann.\nJordan, M. I., Ghahramani, Z., Jaakkola, T. S., &\nSaul, L. K. (1999). An introduction to variational\nmethods for graphical models. Machine Learning ,\n37, 183.\nLim, Y. J., & Teh, Y. W. (2007). Variational Bayesian\napproach to movie rating prediction. Proceedings of\nKDD Cup and Workshop .\nMarlin, B. (2004). Modeling user rating proﬁles for\ncollaborative ﬁltering. In S. Thrun, L. Saul and\nB. Sch¨ olkopf (Eds.),Advances in neural information\nprocessing systems 16 . Cambridge, MA: MIT Press.\nMarlin, B., & Zemel, R. S. (2004). The multiple mul-\ntiplicative factor model for collaborative ﬁltering.\nMachine Learning, Proceedings of the Twenty-ﬁrst\nInternational Conference (ICML 2004), Banﬀ, Al-\nberta, Canada. ACM.\nNeal, R. M. (1993). Probabilistic inference using\nMarkov chain Monte Carlo methods (Technical Re-\nport CRG-TR-93-1). Department of Computer Sci-\nence, University of Toronto.\nNowlan, S. J., & Hinton, G. E. (1992). Simplify-\ning neural networks by soft weight-sharing. Neural\nComputation, 4, 473–493.\nRaiko, T., Ilin, A., & Karhunen, J. (2007). Princi-\npal component analysis for large scale problems with\nlots of missing values. ECML (pp. 691–698).\nRennie, J. D. M., & Srebro, N. (2005). Fast max-\nimum margin matrix factorization for collabora-\ntive prediction. Machine Learning, Proceedings of\nthe Twenty-Second International Conference (ICML\n2005), Bonn, Germany (pp. 713–719). ACM.\nSalakhutdinov, R., & Mnih, A. (2008). Probabilistic\nmatrix factorization. Advances in Neural Informa-\ntion Processing Systems 20 . Cambridge, MA: MIT\nPress.\nSrebro, N., & Jaakkola, T. (2003). Weighted low-rank\napproximations. Machine Learning, Proceedings\nof the Twentieth International Conference (ICML\n2003), Washington, DC, USA (pp. 720–727). AAAI\nPress.\n887",
  "values": {
    "Deferral to humans": "No",
    "Interpretable (to users)": "No",
    "Transparent (to users)": "No",
    "Beneficence": "No",
    "Non-maleficence": "No",
    "Explicability": "No",
    "Privacy": "No",
    "User influence": "No",
    "Collective influence": "No",
    "Respect for Persons": "No",
    "Respect for Law and public interest": "No",
    "Justice": "No",
    "Fairness": "No",
    "Not socially biased": "No",
    "Autonomy (power to decide)": "No",
    "Critiqability": "No"
  }
}