{
  "pdf": "1553374.1553380",
  "title": "Curriculum learning",
  "author": "Yoshua Bengio, J&#233;r&#244;me Louradour, Ronan Collobert, Jason Weston",
  "paper_id": "1553374.1553380",
  "text": "Curriculum Learning\nYoshua Bengio1 Yoshua.Bengio@umontreal.ca\nJ´ erˆ ome Louradour1,2 jeromelouradour@gmail.com\nRonan Collobert 3 ronan@collobert.com\nJason Weston3 jasonw@nec-labs.com\n(1) U. Montreal, P.O. Box 6128, Montreal, Canada (2) A2iA SA, 40bis F abert, Paris, France\n(3) NEC Laboratories America, 4 Independence W ay, Princeton, NJ, USA\nAbstract\nHumans and animals learn much better when\nthe examples are not randomly presented but\norganized in a meaningful order which illus-\ntrates gradually more concepts, and gradu-\nally more complex ones. Here, we formal-\nize such training strategies in the context\nof machine learning, and call them “curricu-\nlum learning”. In the context of recent re-\nsearch studying the diﬃculty of training in\nthe presence of non-convex training criteria\n(for deep deterministic and stochastic neu-\nral networks), we explore curriculum learn-\ning in various set-ups. The experiments show\nthat signiﬁcant improvements in generaliza-\ntion can be achieved. We hypothesize that\ncurriculum learning has both an eﬀect on the\nspeed of convergence of the training process\nto a minimum and, in the case of non-convex\ncriteria, on the quality of the local minima\nobtained: curriculum learning can be seen\nas a particular form of continuation method\n(a general strategy for global optimization of\nnon-convex functions).\n1. Introduction\nHumans need about two decades to be trained as\nfully functional adults of our society. That training\nis highly organized, based on an education system and\na curriculum which introduces diﬀerent concepts at\ndiﬀerent times, exploiting previously learned concepts\nto ease the learning of new abstractions. By choos-\ning which examples to present and in which order to\npresent them to the learning system, one can guide\nAppearing in Proceedings of the 26 th International Confer-\nence on Machine Learning, Montreal, Canada, 2009. Copy-\nright 2009 by the author(s)/owner(s).\ntraining and remarkably increase the speed at which\nlearning can occur. This idea is routinely exploited in\nanimal training where it is called shaping (Skinner,\n1958; Peterson, 2004; Krueger & Dayan, 2009).\nPrevious research (Elman, 1993; Rohde & Plaut, 1999;\nKrueger & Dayan, 2009) at the intersection of cogni-\ntive science and machine learning has raised the follow-\ning question: can machine learning algorithms beneﬁt\nfrom a similar training strategy? The idea of training a\nlearning machine with a curriculum can be traced back\nat least to Elman (1993). The basic idea is to start\nsmall, learn easier aspects of the task or easier sub-\ntasks, and then gradually increase the diﬃculty level.\nThe experimental results, based on learning a simple\ngrammar with a recurrent network (Elman, 1993), sug-\ngested that successful learning of grammatical struc-\nture depends, not on innate knowledge of grammar,\nbut on starting with a limited architecture that is at\nﬁrst quite restricted in complexity, but then expands\nits resources gradually as it learns. Such conclusions\nare important for developmental psychology, because\nthey illustrate the adaptive value of starting, as hu-\nman infants do, with a simpler initial state, and then\nbuilding on that to develop more and more sophis-\nticated representations of structure. Elman (1993)\nmakes the statement that this strategy could make\nit possible for humans to learn what might otherwise\nprove to be unlearnable. However, these conclusions\nhave been seriously questioned in Rohde and Plaut\n(1999). The question of guiding learning of a recurrent\nneural network for learning a simple language and in-\ncreasing its capacity along the way was recently revis-\nited from the cognitive perspective (Krueger & Dayan,\n2009), providing evidence for faster convergence using\na shaping procedure. Similar ideas were also explored\nin robotics (Sanger, 1994), by gradually making the\nlearning task more diﬃcult.\nWe want to clarify when and why a curriculum or\n41\n\nCurriculum Learning\n“starting small” strategy can beneﬁt machine learning\nalgorithms. We contribute to this question by show-\ning several cases - involving vision and language tasks -\nin which very simple multi-stage curriculum strategies\ngive rise to improved generalization and faster con-\nvergence. We also contribute to this question with the\nintroduction of a hypothesis which may help to explain\nsome of the advantages of a curriculum strategy. This\nhypothesis is essentially that a well chosen curriculum\nstrategy can act as a continuation method (Allgower &\nGeorg, 1980), i.e., can help to ﬁnd better local minima\nof a non-convex training criterion. In addition, the ex-\nperiments reported here suggest that (like other strate-\ngies recently proposed to train deep deterministic or\nstochastic neural networks) the curriculum strategies\nappear on the surface to operate like a regularizer, i.e.,\ntheir beneﬁcial eﬀect is most pronounced on the test\nset. Furthermore, experiments on convex criteria also\nshow that a curriculum strategy can speed the conver-\ngence of training towards the global minimum.\n2. On the diﬃcult optimization\nproblem of training deep neural\nnetworks\nTo test the hypothesis that a curriculum strategy could\nhelp to ﬁnd better local minima of a highly non-convex\ncriterion, we turn our attention to training of deep ar-\nchitectures, which have been shown to involve good\nsolutions in local minima that are almost impossible\nto ﬁnd by random initialization (Erhan et al., 2009).\nDeep learning methods attempt to learn feature hi-\nerarchies. Features at higher levels are formed by\nthe composition of lower level features. Automati-\ncally learning multiple levels of abstraction may al-\nlow a system to induce complex functions mapping\nthe input to the output directly from data, without\ndepending heavily on human-crafted features. A the-\noretical motivation for deep architectures comes from\ncomplexity theory: some functions can be represented\ncompactly with an architecture of depth k, but re-\nquire an exponential size architecture when the depth\nis restricted to be less than k (H˚ astad & Goldmann,\n1991; Bengio, 2009). However, training deep archi-\ntectures involves a potentially intractable non-convex\noptimization problem (Bengio, 2009), which compli-\ncates their analysis. There were no good algorithms\nfor training fully-connected deep architectures before\n2006, when Hinton et al. (2006) introduced a learn-\ning algorithm that greedily trains one layer at a time.\nIt exploits an unsupervised generative learning algo-\nrithm for each layer: a Restricted Boltzmann Machine\n(RBM) (Freund & Haussler, 1994). It is conceivable\nthat by training each layer one after the other, one\nﬁrst learns the simpler concepts (represented in the\nﬁrst layer), then slightly more abstract concepts (rep-\nresented in the second layer), etc. Shortly after, strate-\ngies for building deep architectures from related vari-\nants were proposed (Ranzato et al., 2007; Bengio et al.,\n2007). These works showed the advantage of deep ar-\nchitectures over shallow ones and of the unsupervised\npre-training strategy in a variety of settings. Deep ar-\nchitectures have been applied with success not only in\nclassiﬁcation tasks (Ranzato et al., 2007; Bengio et al.,\n2007; Larochelle et al., 2007; Ranzato et al., 2008; Vin-\ncent et al., 2008), but also in regression (Salakhutdi-\nnov & Hinton, 2008), dimensionality reduction (Hin-\nton & Salakhutdinov, 2006; Salakhutdinov & Hinton,\n2007), natural language processing (Collobert & We-\nston, 2008; Weston et al., 2008), and collaborative ﬁl-\ntering (Salakhutdinov et al., 2007).\nNonetheless, training deep architectures is a diﬃcult\nproblem. Erhan et al. (2009) and Larochelle et al.\n(2007) studied this question experimentally to clarify\nwhy deeper networks can sometimes generalize much\nbetter and why some strategies such as unsupervised\npre-training can make this possible. Erhan et al.\n(2009) found that unsupervised pre-training makes it\npossible to start the supervised optimization in a re-\ngion of parameter space corresponding to solutions\nthat were not much better in terms of ﬁnal training er-\nror but substantially better in terms of test error. This\nsuggested a dual eﬀect of unsupervised pre-training,\nboth in terms of helping optimization (starting in bet-\nter basins of attraction of the descent procedure in\nparameter space) and as a kind of regularizer.\nThe experiments presented here suggest that pre-\ntraining with a curriculum strategy might act similarly\nto unsupervised pre-training, acting both as a way to\nﬁnd better local minima and as a regularizer. They\nalso suggest that they help to reach faster convergence\nto a minimum of the training criterion.\n3. A curriculum as a continuation\nmethod\nContinuation methods (Allgower & Georg, 1980) are\noptimization strategies for dealing with minimizing\nnon-convex criteria. Although these global optimiza-\ntion methods provide no guarantee that the global\noptimum will be obtained, they have been particu-\nlarly useful in computational chemistry to ﬁnd approx-\nimate solutions of diﬃcult optimization problems in-\nvolving the conﬁgurations of molecules (Coleman &\nWu, 1994; Wu, 1997). The basic idea is to ﬁrst opti-\nmize a smoothed objective and gradually consider less\nsmoothing, with the intuition that a smooth version\n42\nCurriculum Learning\nof the problem reveals the global picture. One deﬁnes\na single-parameter family of cost functions Cλ(θ) such\nthat C0 can be optimized easily (maybe convex in θ),\nwhileC1 is the criterion that we actually wish to mini-\nmize. One ﬁrst minimizesC0(θ) and then gradually in-\ncreasesλ while keepingθ at a local minimum ofCλ(θ).\nTypically C0 is a highly smoothed version of C1, so\nthat θ gradually moves into the basin of attraction of\na dominant (if not global) minimum of C1. Applying\na continuation method to the problem of minimizing\na training criterion involves a sequence of training cri-\nteria, starting from one that is easier to optimize, and\nending with the training criterion of interest.\nAt an abstract level, a curriculum can also be seen\nas a sequence of training criteria. Each training crite-\nrion in the sequence is associated with a diﬀerent set of\nweights on the training examples, or more generally, on\na reweighting of the training distribution. Initially, the\nweights favor “easier” examples, or examples illustrat-\ning the simplest concepts, that can be learned most\neasily. The next training criterion involves a slight\nchange in the weighting of examples that increases the\nprobability of sampling slightly more diﬃcult exam-\nples. At the end of the sequence, the reweighting of\nthe examples is uniform and we train on the target\ntraining set or the target training distribution.\nOne way to formalize this idea is the following. Let z\nbe a random variable representing an example for the\nlearner (possibly an ( x,y ) pair for supervised learn-\ning). Let P (z) be the target training distribution from\nwhich the learner should ultimately learn a function of\ninterest. Let 0 ≤Wλ(z)≤ 1 be the weight applied to\nexample z at step λ in the curriculum sequence, with\n0≤ λ≤ 1, and W1(z) = 1. The corresponding train-\ning distribution at step λ is\nQλ(z)∝Wλ(z)P (z) ∀z (1)\nsuch that\n∫\nQλ(z)dz = 1. Then we have\nQ1(z) =P (z) ∀z. (2)\nConsider a monotonically increasing sequence ofλ val-\nues, starting from λ = 0 and ending at λ = 1.\nDeﬁnition We call the corresponding sequence of dis-\ntributions Qλ (following eqns 1 and 2) a curriculum\nif the entropy of these distributions increases\nH(Qλ)<H (Qλ+ϵ) ∀ϵ> 0 (3)\nand Wλ(z) is monotonically increasing in λ, i.e.,\nWλ+ϵ(z)≥Wλ(z) ∀z,∀ϵ> 0. (4)\nTo illustrate this deﬁnition, consider the simple set-\nting where Qλ is concentrated on a ﬁnite set of ex-\namples, and increasing λ means adding new examples\nto that set: the support of Qλ increases with λ, and\nthe sequence of training distributions corresponds to\na sequence of embedded training sets, starting with a\nsmall set of easy examples and ending with the target\ntraining set. We want the entropy to increase so as\nto increase the diversity of training examples, and we\nwant the weights of particular examples to increase as\nthey get “added” into the training set.\nIn the experiments below the sequence of training sets\nis always discrete. In fact the curriculum strategy\nworked in some of our experiments with a sequence\nof just two steps: ﬁrst a set of easy examples, and\nthen the target training set. At the other extreme,\nif training proceeds in a stochastic manner by sam-\npling training examples from a distribution, then one\ncould imagine a continuous sequence of sampling dis-\ntributions which gradually gives more weightWλ(z) to\nthe more diﬃcult examples, until all examples have an\nequal weight of 1.\nUp to now we have not deﬁned what “easy examples”\nmeant, or equivalently, how to sort examples into a\nsequence that illustrates the simpler concepts ﬁrst. In\nthe following experiments we explore a few simple ways\nto deﬁne a curriculum, but clearly a lot more work is\nneeded to explore diﬀerent curriculum strategies, some\nof which may be very speciﬁc to particular tasks.\n4. Toy Experiments with a Convex\nCriterion\n4.1. Cleaner Examples May Yield Better\nGeneralization Faster\nOne simple way in which easy examples could help is\nby being less “noisy”, as shown theoretically (Der´ enyi\net al., 1994) in the case of a Teacher-Learner pair\nof Perceptrons. In the supervised classiﬁcation set-\nting, an example is considered noisy if it falls on the\nincorrect side of the decision surface of the Bayes\nclassiﬁer. Noisy examples can slow down conver-\ngence, as illustrated with the following toy experiment.\nTwo-dimensional inputs are generated from a diﬀer-\nent Gaussian for each one of the two classes. We de-\nﬁne class targets y = 1 and y =−1 respectively. The\nGaussian mean for classy is at (y/\n√\n2,y/\n√\n2) and both\nGaussians have standard deviation 1. Starting from\nrandom initial parameters (50 times), we train a linear\nSVM with 50 training examples. Let w be the weight\nvector of the Bayes classiﬁer. We ﬁnd that training\nonly with “easy” examples (for which yw′x> 0) gives\nrise to lower generalization error: 16.3% error vs 17.1%\nerror (average over 50 runs), and the diﬀerence is sta-\ntistically signiﬁcant.\n43\nCurriculum Learning\nIn principle one could argue that diﬃcult examples\ncan be more informative than easy examples. Here\nthe diﬃcult examples are probably not useful because\nthey confuse the learner rather than help it establish\nthe right location of the decision surface. This exper-\niment does not involve a curriculum strategy yet, but\nit may help to understand why easier examples could\nbe useful, by avoiding to confuse the learner.\n4.2. Introducing Gradually More Diﬃcult\nExamples Speeds-up Online Training\nWe train a Perceptron from artiﬁcially generated data\nwhere the target is y = sign(w′xrelevant) andw is sam-\npled from a Normal(0,1). The training pairs are (x,y )\nwith x = (xrelevant,x irrelevant), i.e., some of the inputs\nare irrelevant, not predictive of the target class. Rel-\nevant inputs are sampled from a Uniform(0,1) distri-\nbution. Irrelevant inputs can either be set to 0 or to\na Uniform(0,1). The number of irrelevant inputs that\nis set to 0 varies randomly (uniformly) from example\nto example, and can be used to sort examples from\nthe easiest (with all irrelevant inputs zeroed out) to\nthe most diﬃcult (with none of the irrelevant inputs\nzeroed out). Another way to sort examples is by the\nmargin yw′x, with easiest examples corresponding to\nlarger values. The learning rate is 1 (it does not matter\nsince there is no margin and the classiﬁer output does\nnot depend on the magnitude of w′x but only on its\nsign). Initial weights are sampled from a Normal(0,1).\nWe train the Perceptron with 200 examples (i.e., 200\nPerceptron updates) and measure generalization error\nat the end. Figure 1 shows average estimated gen-\neralization error measured at the end of training and\naveraged across 500 repetitions from diﬀerent initial\nconditions and diﬀerent random sampling of training\nexamples. We compare a no curriculum setting (ran-\ndom ordering), with a curriculum setting in which\nexamples are ordered by easiness, starting with the\neasiest examples, and two easiness criteria (number of\nnoisy irrelevant inputs, margin yw′x). All error rate\ndiﬀerences between the curriculum strategy and the\nno-curriculum are statistically signiﬁcant (diﬀerences\nof more than .01 were all statistically signiﬁcant at 5%\nunder a t-test).\n5. Experiments on shape recognition\nThe task of interest here is to classify geometri-\ncal shapes into 3 classes (rectangle, ellipse, trian-\ngle), where the input is a 32 ×32 grey-scale image.\nAs shown in Figure 2, two diﬀerent datasets were\ngenerated: whereas GeomShapes data consist in im-\nages of rectangles, ellipses and triangles, BasicShapes\ndata only include special cases of the above: squares,\nFigure 1. Average error rate of Perceptron, with or with-\nout the curriculum. Top: the number of nonzero irrelevant\ninputs determines easiness. Bottom: the margin yw′x de-\ntermines easiness.\ncircles and equilateral triangles. The diﬀerence be-\ntween BasicShapes data and GeomShapes data is that\nBasicShapes images exhibit less variability in shape.\nOther degrees of variability which are present in both\nsets are the following: object position, size, orienta-\ntion, and also the grey levels of the foreground and\nbackground. Besides, some geometrical constraints are\nalso added so as to ensure that any shape object ﬁts\nentirely within the image, and a minimum size and\nminimum contrast (diﬀerence in grey levels) between\nforeground and background is imposed.\nNote that the above “easy distribution” occupying a\nvery small volume in input space compared to the tar-\nget distribution does not contradict condition 4. In-\ndeed, the non-zero weights (on easy examples) can ini-\ntially be very small, so that their ﬁnal weight in the\ntarget distribution can be very small.\nFigure 2. Sample inputs from BasicShapes (top) and\nGeomShapes (bottom). Images are shown here with a\nhigher resolution than the actual dataset (32x32 pixels).\nThe experiments were carried out on a multi-layer neu-\nral network with 3 hidden layers, trained by stochas-\n44\nCurriculum Learning\ntic gradient descent on the negative conditional log-\nlikelihood, i.e., a task which is known to involve a dif-\nﬁcult non-convex optimization problem (Erhan et al.,\n2009). An epoch is a stochastic gradient descent pass\nthrough a training set of 10 000 examples. The cur-\nriculum consists in a 2-step schedule:\n1. Perform gradient descent on the BasicShapes\ntraining set, until “switch epoch” is reached.\n2. Then perform gradient descent on the GeomShapes\ntraining set.\nGeneralization error is always evaluted on the\nGeomShapes test set. The baseline corresponds to\ntraining the network only on the GeomShapes train-\ning set (for the same number of training epochs),\nand corresponds to “switch epoch”=0. In our ex-\nperiments, there is a total of 10 000 examples in\nboth training sets, and 5 000 examples for valida-\ntion, 5 000 for testing. All datasets are available at\nwww.iro.umontreal.ca/∼lisa/ptwiki/BabyAIShapesDatasets\nThe hyper-parameters are the following: learning rate\nof stochastic gradient descent and number of hidden\nunits. The selection of hyper-parameters is simpli-\nﬁed using the following heuristic: all hyper-parameters\nwere chosen so as to have the best baseline perfor-\nmance on the GeomShapes validation set without cur-\nriculum. These hyper-parameter values are then used\nfor the curriculum experiments.\nFigure (3) shows the distribution of test errors over\n20 diﬀerent random seeds, for diﬀerent values of the\n“switch epoch”: 0 (the baseline with no curriculum)\nand the powers of 2 until 128. After switching to the\ntarget training distribution, training continues either\nuntil 256 epochs or until validation set error reaches\na minimum (early stopping). The ﬁgure shows the\ndistribution of test error (after early stopping) as a\nfunction of the “switch epoch”. Clearly, the best gen-\neralization is obtained by doing a 2-stage curriculum\nwhere the ﬁrst half of the total allowed training time\n(of 256 epochs) is spent on the easier examples rather\nthan on the target examples.\nOne potential issue with this experiment is that the\ncurriculum-trained model overall saw more examples\nthan the no-curriculum examples, although in the sec-\nond part of training (with the target distribution) both\ntypes of models converge (in the sense of early stop-\nping) to a local minimum with respect to the error on\nthe target training distribution, suggesting that diﬀer-\nent local minima are obtained. Note also that the easy\nexamples have less variability than the hard examples\n(only a subset of the shape variations are shown, e.g.\nonly squares instead of all kinds of rectangles). To\n|\n0 2 4 8 16 32 64 128\n0.15 0.16 0.17 0.18 0.19 0.20 0.21\nswitch epoch\nbest validation classification error\nFigure 3. Box plot of test classiﬁcation error distribution\nas a function of the “switch epoch”, with a 3-hidden-\nlayers neural network trained by stochastic gradient de-\nscent. Each box corresponds to 20 seeds for initializing the\nparameters. The horizontal line inside the box represents\nthe median (50th percentile), the borders of the box the\n25th and the 75th percentile and the ends of the bars the\n5th and 95th percentiles.\neliminate the explanation that better results are ob-\ntained with the curriculum because of seeing more ex-\namples, we trained a no-curriculum model with the\nunion of the BasicShapes and GeomShapes training sets,\nwith a ﬁnal test error still signiﬁcantly worse than\nwith the curriculum (with errors similar to “switch\nepoch”=16). We also veriﬁed that training only with\nBasicShapes yielded poor results.\n6. Experiments on language modeling\nWe are interested here in training a language model ,\npredicting the best word which can follow a given con-\ntext of words in a correct English sentence. Follow-\ning Collobert and Weston (2008) we only try to com-\npute a score for the next word that will have a large\nrank compared to the scores of other words, and we\ncompute the score with the architecture of Figure 4.\nWhereas other language models prior to Collobert and\nWeston (2008) optimized the log-likelihood of the next\nword, the ranking approach does not require com-\nputing the score over all the vocabulary words dur-\ning training, as shown below. Instead it is enough to\nsample a negative example. In Collobert and Weston\n(2008), the main objective is to learn an embedding\nfor the words as a side-eﬀect of learning to compute\nthis score. The authors showed how to use these em-\nbeddings in several language modeling tasks, in a form\nof multi-task learning, yielding improved results.\nGiven any ﬁxed size window of text s, we consider a\nlanguage model f(s) which produces a score for these\nwindows of text. We want the score of a correct win-\ndow of text s to be larger, with a margin of 1, than any\nother word sequence sw where the last word has been\nreplaced by another word w of the vocabulary. This\ncorresponds to minimizing the expected value of the\n45\nCurriculum Learning\nInput Window\nthe cat sat on the\nword to score\ns(1) s(2) s(3) s(4) s(5)\ntext\nindices\nLookup Table\nLTw\nTanh\nLinear\nLinear\n50\n250 (concatenation)\nScore\n100\ncontext\nFigure 4. Architecture of the deep neural network comput-\ning the score of the next word given the previous ones.\nfollowing ranking loss over sequences s sampled from\na datasetS of valid English text windows:\nCs =\n∑\nw∈D\n1\n|D|Cs,w =\n∑\nw∈D\n1\n|D| max(0, 1−f(s)+f(sw))\n(5)\nwhere D is the considered word vocabulary and S\nis the set of training word sequences. Note that a\nstochastic sample of the gradient with respect to Cs\ncan be obtained by sampling a counter-example word\nw uniformly from D. For each word sequence s we\nthen compute f(s) and f(sw) and the gradient of\nmax(0, 1−f(s) +f(sw)) with respect to parameters.\n6.1. Architecture\nThe architecture of our language model (Figure 4)\nfollows the work introduced by Bengio et al. (2001)\nand Schwenk and Gauvain (2002), and closely resem-\nbles the one used in Collobert and Weston (2008).\nEach word i ∈ Dis embedded into a d-dimensional\nspace using a look-up table LTW (·): LTW (i) = Wi,\nwhere W ∈ Rd×|D| is a matrix of parameters to\nbe learnt, Wi ∈ Rd is the ith column of W and\nd is the embedding dimension hyper-parameter. In\nthe ﬁrst layer an input window {s1, s2, ... sn} of n\nwords inD is thus transformed into a series of vectors\n{Ws1, Ws2, ... Wsn} by applying the look-up table to\neach of its words.\nThe feature vectors obtained by the look-up table layer\nare then concatenated and fed to a classical linear\nlayer. A non-linearity (like tanh(·)) follows and the\nscore of the language model is ﬁnally obtained after\napplying another linear layer with one output.\nThe cost (5) is minimized using stochastic gradient\ndescent, by iteratively sampling pairs (s, w) composed\nof a window of text s from the training set S and a\nrandom wordw, and performing a step in the direction\nof the gradient ofCs,w with respect to the parameters,\nincluding the matrix of embeddings W .\nFigure 5. Ranking language model trained with vs without\ncurriculum on Wikipedia. “Error” is log of the rank of the\nnext word (within 20k-word vocabulary). In its ﬁrst pass\nthrough Wikipedia, the curriculum-trained model skips ex-\namples with words outside of 5k most frequent words (down\nto 270 million from 631 million), then skips examples out-\nside 10k most frequent words (doing 370 million updates),\netc. The drop in rank occurs when the vocabulary size\nis increased, as the curriculum-trained model quickly gets\nbetter on the new words.\n6.2. Experiments\nWe chose the training set S as all possible win-\ndows of text of size n = 5 from Wikipedia\n(http://en.wikipedia.org), obtaining 631 million\nwindows processed as in Collobert and Weston (2008).\nWe chose as a curriculum strategy to grow the vocabu-\nlary size: the ﬁrst pass over Wikipedia was performed\nusing the 5, 000 most frequent words in the vocabu-\nlary, which was then increased by 5, 000 words at each\nsubsequent pass through Wikipedia. At each pass, any\nwindow of text containing a word not in the consid-\nered vocabulary was discarded. The training set is\nthus increased after each pass through Wikipedia. We\ncompare against no curriculum, where the network\nis trained using the ﬁnal desired vocabulary size of\n20, 000. The evaluation criterion was the average of\nthe log of the rank of the last word in each test win-\ndow, taken in a test set of 10, 000 windows of text not\nseen during the training, with words from the most\n20, 000 frequent ones (i.e. from the target distribu-\ntion). We chose the word embedding dimension to be\nd = 50, and the number of hidden units as 100.\nIn Figure 5, we observe that the log rank on the target\n46\nCurriculum Learning\ndistribution with the curriculum strategy crosses the\nerror of the no-curriculum strategy after about 1 bil-\nlion updates, shortly after switching to the target vo-\ncabulary size of 20,000 words, and the diﬀerence keeps\nincreasing afterwards. The ﬁnal test set average log-\nranks are 2.78 and 2.83 respectively, and the diﬀerence\nis statistically signiﬁcant.\n7. Discussion and Future Work\nWe started with the following question left from previ-\nous cognitive science research (Elman, 1993; Rohde &\nPlaut, 1999): can machine learning algorithms ben-\neﬁt from a curriculum strategy? Our experimental\nresults in many diﬀerent settings bring evidence to-\nwards a positive answer to that question. It is plausi-\nble that some curriculum strategies work better than\nothers, that some are actually useless for some tasks\n(as in Rohde and Plaut (1999)), and that better re-\nsults could be obtained on our data sets with more\nappropriate curriculum strategies. After all, the art of\nteaching is diﬃcult and humans do not agree among\nthemselves about the order in which concepts should\nbe introduced to pupils.\nFrom the machine learning point of view, once the\nsuccess of some curriculum strategies has been estab-\nlished, the important questions are: why? and how?\nThis is important to help us devise better curriculum\nstrategies and automate that process to some extent.\nWe proposed a number of hypotheses to explain the\npotential advantages of a curriculum strategy:\n• faster training in the online setting (i.e. faster\nboth from an optimization and statistical point of\nview) because the learner wastes less time with\nnoisy or harder to predict examples (when it is\nnot ready to incorporate them),\n• guiding training towards better regions in param-\neter space, i.e. into basins of attraction (local\nminima) of the descent procedure associated with\nbetter generalization: a curriculum can be seen as\na particular continuation method.\nFaster convergence with a curriculum was already ob-\nserved in (Krueger & Dayan, 2009). However, unlike\nin our experiments where capacity is ﬁxed throughout\nthe curriculum, they found that compared to using\nno curriculum, worse results were obtained with ﬁxed\nneural resources. The reasons for these diﬀerences re-\nmain to be clariﬁed. In both cases, though, an appro-\npriate curriculum strategy acts to help the training\nprocess (faster convergence to better solutions), and\nwe even ﬁnd that it regularizes, giving rise to lower\ngeneralization error for the same training error. This\nis like in the case of unsupervised pre-training (Erhan\net al., 2009), and again it remains to be clariﬁed why\none would expect improved generalization, for both\ncurriculum and unsupervised pre-training procedures.\nThe way we have deﬁned curriculum strategies leaves\na lot to be deﬁned by the teacher. It would be nice\nto understand general principles that make some cur-\nriculum strategies work better than others, and this\nclearly should be the subject of future work on curricu-\nlum learning. In particular, to reap the advantages of\na curriculum strategy while minimizing the amount of\nhuman (teacher) eﬀort involved, it is natural to con-\nsider a form of active selection of examples similar to\nwhat humans (and in particular children) do. At any\npoint during the “education” of a learner, some exam-\nples can be considered “too easy” (not helping much\nto improve the current model), while some examples\ncan be considered “too diﬃcult” (no small change in\nthe model would allow to capture these examples). It\nwould be advantageous for a learner to focus on “in-\nteresting” examples, which would be standing near the\nfrontier of the learner’s knowledge and abilities, nei-\nther too easy nor too hard. Such an approach could be\nused to at least automate the pace at which a learner\nwould move along a predeﬁned curriculum. In the ex-\nperiments we performed, that pace was ﬁxed arbitrar-\nily. This kind of strategy is clearly connected to active\nlearning (Cohn et al., 1995), but with a view that is\ndiﬀerent from the standard one: instead of focusing on\nthe examples near the decision surface to quickly infer\nits location, we think of the set of examples that the\nlearner succeeds to capture and gradually expand that\nset by preferentially adding examples near its border.\nCurriculum learning is related to boosting algorithms,\nin that diﬃcult examples are gradually emphasized.\nHowever, a curriculum starts with a focus on the eas-\nier examples, rather than a uniform distribution over\nthe training set. Furthermore, from the point of view\nof the boosted weighted sum of weak learners, there is\nno change in the training criterion: the change is only\nfrom the point of view of the next weak learner. As\nfar as the boosted sum is concerned, we are following a\nfunctional gradient on the same training criterion (the\nsum of exponentiated margins). Curriculum strategies\nare also connected to transfer (or multi-task) learning\nand lifelong learning (Thrun, 1996). Curriculum learn-\ning strategies can be seen as a special form of trans-\nfer learning where the initial tasks are used to guide\nthe learner so that it will perform better on the ﬁnal\ntask. Whereas the traditional motivation for multi-\ntask learning is to improve generalization by sharing\nacross tasks, curriculum learning adds the notion of\nguiding the optimization process, either to converge\nfaster, or more importantly, to guide the learner to-\nwards better local minima.\n47\nCurriculum Learning\nAcknowledgements: The authors thank NSERC,\nCIFAR, and MITACS for support.\nReferences\nAllgower, E. L., & Georg, K. (1980).Numerical contin-\nuation methods. An introduction . Springer-Verlag.\nBengio, Y. (2009). Learning deep architectures for AI.\nFoundations & Trends in Mach. Learn. , to appear.\nBengio, Y., Ducharme, R., & Vincent, P. (2001). A\nneural probabilistic language model. Adv. Neural\nInf. Proc. Sys. 13 (pp. 932–938).\nBengio, Y., Lamblin, P., Popovici, D., & Larochelle, H.\n(2007). Greedy layer-wise training of deep networks.\nAdv. Neural Inf. Proc. Sys. 19 (pp. 153–160).\nCohn, D., Ghahramani, Z., & Jordan, M. (1995). Ac-\ntive learning with statistical models. Adv. Neural\nInf. Proc. Sys. 7 (pp. 705–712).\nColeman, T., & Wu, Z. (1994). Parallel continuation-\nbased global optimization for molecular conforma-\ntion and protein folding (Technical Report). Cornell\nUniversity, Dept. of Computer Science.\nCollobert, R., & Weston, J. (2008). A uniﬁed archi-\ntecture for natural language processing: Deep neural\nnetworks with multitask learning. Int. Conf. Mach.\nLearn. 2008 (pp. 160–167).\nDer´ enyi, I., Geszti, T., & Gy¨ orgyi, G. (1994). Gener-\nalization in the programed teaching of a perceptron.\nPhysical Review E, 50, 3192–3200.\nElman, J. L. (1993). Learning and development in\nneural networks: The importance of starting small.\nCognition, 48, 781–799.\nErhan, D., Manzagol, P.-A., Bengio, Y., Bengio, S.,\n& Vincent, P. (2009). The diﬃculty of training\ndeep architectures and the eﬀect of unsupervised\npre-training. AI & Stat.’2009 .\nFreund, Y., & Haussler, D. (1994). Unsupervised\nlearning of distributions on binary vectors using two\nlayer networks (Technical Report UCSC-CRL-94-\n25). University of California, Santa Cruz.\nH˚ astad, J., & Goldmann, M. (1991). On the power of\nsmall-depth threshold circuits. Computational Com-\nplexity, 1, 113–129.\nHinton, G. E., Osindero, S., & Teh, Y.-W. (2006). A\nfast learning algorithm for deep belief nets. Neural\nComputation, 18, 1527–1554.\nHinton, G. E., & Salakhutdinov, R. (2006). Reduc-\ning the dimensionality of data with neural networks.\nScience, 313, 504–507.\nKrueger, K. A., & Dayan, P. (2009). Flexible shaping:\nhow learning in small steps helps. Cognition, 110,\n380–394.\nLarochelle, H., Erhan, D., Courville, A., Bergstra, J.,\n& Bengio, Y. (2007). An empirical evaluation of\ndeep architectures on problems with many factors\nof variation. Int. Conf. Mach. Learn. (pp. 473–480).\nPeterson, G. B. (2004). A day of great illumination:\nB. F. Skinner’s discovery of shaping. Journal of the\nExperimental Analysis of Behavior, 82, 317–328.\nRanzato, M., Boureau, Y., & LeCun, Y. (2008). Sparse\nfeature learning for deep belief networks. Adv. Neu-\nral Inf. Proc. Sys. 20 (pp. 1185–1192).\nRanzato, M., Poultney, C., Chopra, S., & LeCun, Y.\n(2007). Eﬃcient learning of sparse representations\nwith an energy-based model. Adv. Neural Inf. Proc.\nSys. 19 (pp. 1137–1144).\nRohde, D., & Plaut, D. (1999). Language acquisition\nin the absence of explicit negative evidence: How\nimportant is starting small? Cognition, 72, 67–109.\nSalakhutdinov, R., & Hinton, G. (2007). Learning a\nnonlinear embedding by preserving class neighbour-\nhood structure. AI & Stat.’2007.\nSalakhutdinov, R., & Hinton, G. (2008). Using Deep\nBelief Nets to learn covariance kernels for Gaussian\nprocesses. Adv. Neural Inf. Proc. Sys. 20 (pp. 1249–\n1256).\nSalakhutdinov, R., Mnih, A., & Hinton, G. (2007). Re-\nstricted Boltzmann machines for collaborative ﬁlter-\ning. Int. Conf. Mach. Learn. 2007 (pp. 791–798).\nSanger, T. D. (1994). Neural network learning con-\ntrol of robot manipulators using gradually increas-\ning task diﬃculty. IEEE Trans. on Robotics and\nAutomation, 10.\nSchwenk, H., & Gauvain, J.-L. (2002). Connection-\nist language modeling for large vocabulary continu-\nous speech recognition. International Conference on\nAcoustics, Speech and Signal Processing (pp. 765–\n768). Orlando, Florida.\nSkinner, B. F. (1958). Reinforcement today. American\nPsychologist, 13, 94–99.\nThrun, S. (1996). Explanation-based neural network\nlearning: A lifelong learning approach. Boston, MA:\nKluwer Academic Publishers.\nVincent, P., Larochelle, H., Bengio, Y., & Manzagol,\nP.-A. (2008). Extracting and composing robust fea-\ntures with denoising autoencoders. Int. Conf. Mach.\nLearn. (pp. 1096–1103).\nWeston, J., Ratle, F., & Collobert, R. (2008). Deep\nlearning via semi-supervised embedding. Int. Conf.\nMach. Learn. 2008 (pp. 1168–1175).\nWu, Z. (1997). Global continuation for distance geom-\netry problems. SIAM Journal of Optimization, 7,\n814–836.\n48",
  "values": {
    "Justice": "Yes",
    "Privacy": "Yes",
    "Fairness": "Yes",
    "User influence": "Yes",
    "Collective influence": "Yes",
    "Interpretable (to users)": "Yes",
    "Respect for Law and public interest": "Yes",
    "Transparent (to users)": "Yes",
    "Deferral to humans": "Yes",
    "Autonomy (power to decide)": "Yes",
    "Non-maleficence": "Yes",
    "Critiqability": "Yes",
    "Explicability": "Yes",
    "Beneficence": "Yes",
    "Respect for Persons": "Yes",
    "Not socially biased": "Yes"
  }
}