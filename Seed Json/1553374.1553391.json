{
  "pdf": "1553374.1553391",
  "title": "Multi-view clustering via canonical correlation analysis",
  "author": "Kamalika Chaudhuri, Sham M. Kakade, Karen Livescu, Karthik Sridharan",
  "paper_id": "1553374.1553391",
  "text": "Multi-View Clustering via Canonical Correlation Analysis\nKamalika Chaudhuri kamalika@soe.ucsd.edu\nITA, UC San Diego, 9500 Gilman Drive, La Jolla, CA\nSham M. Kakade sham@tti-c.org\nKaren Livescu klivescu@tti-c.org\nKarthik Sridharan karthik@tti-c.org\nToyota Technological Institute at Chicago, 6045 S. Kenwood Ave., Chicago, IL\nAbstract\nClustering data in high dimensions is be-\nlieved to be a hard problem in general. A\nnumber of eﬃcient clustering algorithms de-\nveloped in recent years address this prob-\nlem by projecting the data into a lower-\ndimensional subspace, e.g. via Principal\nComponents Analysis (PCA) or random pro-\njections, before clustering. Here, we consider\nconstructing such projections using multiple\nviews of the data, via Canonical Correlation\nAnalysis (CCA).\nUnder the assumption that the views are un-\ncorrelated given the cluster label, we show\nthat the separation conditions required for\nthe algorithm to be successful are signiﬁ-\ncantly weaker than prior results in the lit-\nerature. We provide results for mixtures\nof Gaussians and mixtures of log concave\ndistributions. We also provide empirical\nsupport from audio-visual speaker clustering\n(where we desire the clusters to correspond to\nspeaker ID) and from hierarchical Wikipedia\ndocument clustering (where one view is the\nwords in the document and the other is the\nlink structure).\n1. Introduction\nThe multi-view approach to learning is one in which\nwe have ‘views’ of the data (sometimes in a rather\nabstract sense) and the goal is to use the relation-\nship between these views to alleviate the diﬃculty of a\nlearning problem of interest (Blum & Mitchell, 1998;\nKakade & Foster, 2007; Ando & Zhang, 2007). In this\nAppearing in Proceedings of the 26 th International Confer-\nence on Machine Learning, Montreal, Canada, 2009. Copy-\nright 2009 by the author(s)/owner(s).\nwork, we explore how having two ‘views’ makes the\nclustering problem signiﬁcantly more tractable.\nMuch recent work has been done on understanding\nunder what conditions we can learn a mixture model.\nThe basic problem is as follows: We are given indepen-\ndent samples from a mixture of k distributions, and\nour task is to either: 1) infer properties of the under-\nlying mixture model (e.g. the mixing weights, means,\netc.) or 2) classify a random sample according to which\ndistribution in the mixture it was generated from.\nUnder no restrictions on the underlying mixture, this\nproblem is considered to be hard. However, in many\napplications, we are only interested in clustering the\ndata when the component distributions are “well sep-\narated”. In fact, the focus of recent clustering al-\ngorithms (Dasgupta, 1999; Vempala & Wang, 2002;\nAchlioptas & McSherry, 2005; Brubaker & Vempala,\n2008) is on eﬃciently learning with as little separa-\ntion as possible. Typically, the separation conditions\nare such that when given a random sample from the\nmixture model, the Bayes optimal classiﬁer is able to\nreliably recover which cluster generated that point.\nThis work makes a natural multi-view assumption:\nthat the views are (conditionally) uncorrelated, con-\nditioned on which mixture component generated the\nviews. There are many natural applications for which\nthis assumption applies. For example, we can con-\nsider multi-modal views, with one view being a video\nstream and the other an audio stream, of a speaker —\nhere, conditioned on the speaker identity and maybe\nthe phoneme (both of which could label the generat-\ning cluster), the views may be uncorrelated. A second\nexample is the words and link structure in a document\nfrom a corpus such as Wikipedia – here, conditioned\non the category of each document, the words in it and\nits link structure may be uncorrelated. In this paper,\nwe provide experiments for both settings.\n129\n\nMulti-View Clustering via Canonical Correlation Analysis\nUnder this multi-view assumption, we provide a sim-\nple and eﬃcient subspace learning method, based\non Canonical Correlation Analysis (CCA). This algo-\nrithm is aﬃne invariant and is able to learn with some\nof the weakest separation conditions to date. The in-\ntuitive reason for this is that under our multi-view\nassumption, we are able to (approximately) ﬁnd the\nlow-dimensional subspace spanned by the means of\nthe component distributions. This subspace is impor-\ntant, because, when projected onto this subspace, the\nmeans of the distributions are well-separated, yet the\ntypical distance between points from the same distri-\nbution is smaller than in the original space. The num-\nber of samples we require to cluster correctly scales\nas O(d), where d is the ambient dimension. Finally,\nwe show through experiments that CCA-based algo-\nrithms consistently provide better performance than\nstandard PCA-based clustering methods when applied\nto datasets in the two quite diﬀerent domains of audio-\nvisual speaker clustering and hierarchical Wikipedia\ndocument clustering by category.\nOur work adds to the growing body of results which\nshow how the multi-view framework can alleviate the\ndiﬃculty of learning problems.\nRelated W ork. Most provably eﬃcient clustering\nalgorithms ﬁrst project the data down to some low-\ndimensional space and then cluster the data in this\nlower dimensional space (an algorithm such as sin-\ngle linkage usually suﬃces here). Typically, these al-\ngorithms also work under a separation requirement,\nwhich is measured by the minimum distance between\nthe means of any two mixture components.\nOne of the ﬁrst provably eﬃcient algorithms for learn-\ning mixture models is due to (Dasgupta, 1999), who\nlearns a mixture of spherical Gaussians by randomly\nprojecting the mixture onto a low-dimensional sub-\nspace. (Vempala & Wang, 2002) provide an algorithm\nwith an improved separation requirement that learns\na mixture of k spherical Gaussians, by projecting the\nmixture down to thek-dimensional subspace of highest\nvariance. (Kannan et al., 2005; Achlioptas & McSh-\nerry, 2005) extend this result to mixtures of general\nGaussians; however, they require a separation propor-\ntional to the maximum directional standard deviation\nof any mixture component. (Chaudhuri & Rao, 2008)\nuse a canonical correlations-based algorithm to learn\nmixtures of axis-aligned Gaussians with a separation\nproportional toσ∗, the maximum directional standard\ndeviation in the subspace containing the means of the\ndistributions. Their algorithm requires a coordinate-\nindependence property, and an additional “spreading”\ncondition. None of these algorithms are aﬃne invari-\nant.\nFinally, (Brubaker & Vempala, 2008) provide an aﬃne-\ninvariant algorithm for learning mixtures of general\nGaussians, so long as the mixture has a suitably low\nFisher coeﬃcient when in isotropic position. However,\ntheir separation involves a large polynomial depen-\ndence on 1\nwmin\n.\nThe two results most closely related to ours are the\nwork of (Vempala & Wang, 2002) and (Chaudhuri &\nRao, 2008). (Vempala & Wang, 2002) show that it is\nsuﬃcient to ﬁnd the subspace spanned by the means\nof the distributions in the mixture for eﬀective clus-\ntering. Like our algorithm, (Chaudhuri & Rao, 2008)\nuse a projection onto the top k− 1 singular value de-\ncomposition subspace of the canonical correlations ma-\ntrix. They also require a spreading condition, which is\nrelated to our requirement on the rank. We borrow\ntechniques from both of these papers.\n(Blaschko & Lampert, 2008) propose a similar algo-\nrithm for multi-view clustering, in which data is pro-\njected onto the top directions obtained by kernel CCA\nacross the views. They show empirically that for clus-\ntering images using the associated text as a second\nview (where the target clustering is a human-deﬁned\ncategory), CCA-based clustering methods out-perform\nPCA-based algorithms.\nThis W ork. Our input is data on a ﬁxed set of ob-\njects from two views, where View j is assumed to be\ngenerated by a mixture of k Gaussians (Dj\n1,...,D j\nk),\nforj = 1, 2. To generate a sample, a source i is picked\nwith probability wi, and x(1) and x(2) in Views 1 and\n2 are drawn from distributions D1\ni and D2\ni . Following\nprior theoretical work, our goal is to show that our al-\ngorithm recovers the correct clustering, provided the\ninput mixture obeys certain conditons.\nWe impose two requirements on these mixtures. First,\nwe require that conditioned on the source, the two\nviews are uncorrelated. Notice that this is a weaker\nrestriction than the condition that given source i, the\nsamples from D1\ni and D2\ni are drawn independently.\nMoreover, this condition allows the distributions in the\nmixture within each view to be completely general, so\nlong as they are uncorrelated across views. Although\nwe do not prove this, our algorithm seems robust to\nsmall deviations from this assumption.\nSecond, we require the rank of the CCA matrix across\nthe views to be at least k− 1, when each view is in\nisotropic position, and the k− 1-th singular value of\nthis matrix to be at least λmin. This condition ensures\nthat there is suﬃcient correlation between the views.\nIf the ﬁrst two conditions hold, then we can recover\nthe subspace containing the means in both views.\n130\nMulti-View Clustering via Canonical Correlation Analysis\nIn addition, for mixtures of Gaussians, if in at least\none view, say View 1, we have that for every pair of\ndistributions i and j in the mixture,\n||µ1\ni−µ1\nj||>Cσ∗k1/4√\nlog(n/δ)\nfor some constant C, then our algorithm can also de-\ntermine which component each sample came from.\nHere µ1\ni is the mean of the i-th component in View\n1 and σ∗ is the maximum directional standard devi-\nation in the subspace containing the means in View\n1. Moreover, the number of samples required to learn\nthis mixture grows (almost) linearly with d.\nThis separation condition is considerably weaker than\nprevious results in that σ∗ only depends on the direc-\ntional variance in the subspace spanned by the means,\nwhich can be considerably lower than the maximum di-\nrectional variance over all directions. The only other\nalgorithm which provides aﬃne-invariant guarantees\nis due to (Brubaker & Vempala, 2008) — the implied\nseparation in their work is rather large and grows with\ndecreasing wmin, the minimum mixing weight. To get\nour improved sample complexity bounds, we use a re-\nsult due to (Rudelson & Vershynin, 2007) which may\nbe of independent interest.\nWe stress that our improved results are really due to\nthe multi-view condition. Had we simply combined the\ndata from both views, and applied previous algorithms\non the combined data, we could not have obtained our\nguarantees. We also emphasize that for our algorithm\nto cluster successfully, it is suﬃcient for the distribu-\ntions in the mixture to obey the separation condition\nin one view, so long as the multi-view and rank condi-\ntions are obeyed.\nFinally, we study through experiments the perfor-\nmance of CCA-based algorithms on data sets from two\ndiﬀerent domains. First, we experiment with audio-\nvisual speaker clustering, in which the two views are\naudio and face images of a speaker, and the target\ncluster variable is the speaker. Our experiments show\nthat CCA-based algorithms perform better than PCA-\nbased algorithms on audio data and just as well on\nimage data, and are more robust to occlusions of the\nimages. For our second experiment, we cluster docu-\nments in Wikipedia. The two views are the words and\nthe link structure in a document, and the target cluster\nis the category. Our experiments show that a CCA-\nbased hierarchical clustering algorithm out-performs\nPCA-based hierarchical clustering for this data.\n2. The Setting\nWe assume that our data is generated by a mixture\nof k distributions. In particular, we assume that we\nobtain samples x = (x(1),x (2)), where x(1) and x(2)\nare the two views, which live in the vector spaces V1\nof dimension d1 andV2 of dimension d2, respectively.\nWe letd =d1+d2. Let µj\ni , fori = 1,...,k andj = 1, 2,\nbe the mean of distribution i in view j, and let wi be\nthe mixing weight for distribution i.\nFor simplicity, we assume that the data have mean 0.\nWe denote the covariance matrix of the data as:\nΣ = E[xx⊤], Σ11 = E[x(1)(x(1))⊤]\nΣ22 = E[x(2)(x(2))⊤], Σ12 = E[x(1)(x(2))⊤]\nHence, we have: Σ =\n[Σ11\nΣ21\nΣ12 Σ22\n]\n(1)\nThe multi-view assumption we work with is as follows:\nAssumption 1 (Multi-View Condition) We assume\nthat conditioned on the source distributionl in the mix-\nture (wherel =i is picked with probabilitywi), the two\nviews are uncorrelated. More precisely, we assume that\nfor all i∈ [k],\nE[x(1)(x(2))⊤|l =i] = E[x(1)|l =i]E[(x(2))⊤|l =i]\nThis assumption implies that: Σ 12 =∑\niwiµ1\ni· (µ2\ni )T .\nTo see this, observe that\nE[x(1)(x(2))⊤] =\n∑\ni\nEDi[x(1)(x(2))⊤] Pr[Di]\n=\n∑\ni\nwiEDi[x(1)]· EDi[(x(2))⊤]\n=\n∑\ni\nwiµ1\ni· (µ2\ni )T (2)\nAs the distributions are in isotropic position, we ob-\nserve that ∑\niwiµ1\ni = ∑\niwiµ2\ni = 0. Therefore, the\nabove equation shows that the rank of Σ 12 is at most\nk− 1. We now assume that it has rank precisely k− 1.\nAssumption 2 (Non-Degeneracy Condition) We as-\nsume that Σ12 has rank k− 1 and that the minimal\nnon-zero singular value of Σ12 is λmin > 0 (where we\nare working in a coordinate system where Σ11 and Σ22\nare identity matrices).\nFor clarity of exposition, we also work in an isotropic\ncoordinate system in each view. Speciﬁcally, the ex-\npected covariance matrix of the data, in each view, is\nthe identity matrix, i.e. Σ 11 =Id1, Σ22 =Id2.\nAs our analysis shows, our algorithm is robust to er-\nrors, so we assume that data is whitened as a pre-\nprocessing step.\nOne way to view the Non-Degeneracy Assumption is\nin terms of correlation coeﬃcients. Recall that for two\n131\nMulti-View Clustering via Canonical Correlation Analysis\ndirections u∈V 1 and v∈V 2, the correlation coeﬃ-\ncient is deﬁned as:\nρ(u,v ) = E[(u·x(1))(v·x(2))]√\nE[(u·x(1))2]E[(v·x(2))2]\n.\nAn alternative deﬁnition of λmin is the min-\nimal non-zero correlation coeﬃcient, λmin =\nminu,v:ρ(u,v)̸=0ρ(u,v ). Note 1 ≥λmin > 0.\nWe use ˆΣ11 and ˆΣ22 to denote the sample covariance\nmatrices in views 1 and 2 respectively. We use ˆΣ12 to\ndenote the sample covariance matrix combined across\nviews 1 and 2. We assume these are obtained through\nempirical averages from i.i.d. samples from the under-\nlying distribution.\n3. The Clustering Algorithm\nThe following lemma provides the intuition for our al-\ngorithm.\nLemma 1 Under Assumption 2, if U,D,V is the\n‘thin’ SVD of Σ12 (where the thin SVD removes all\nzero entries from the diagonal), then the subspace\nspanned by the means in view 1 is precisely the col-\numn span of U (and we have the analogous statement\nfor view 2).\nThe lemma is a consequence of Equation 2 and the\nrank assumption. Since samples from a mixture are\nwell-separated in the space containing the means of the\ndistributions, the lemma suggests the following strat-\negy: use CCA to (approximately) project the data\ndown to the subspace spanned by the means to get\nan easier clustering problem, and then apply standard\nclustering algorithms in this space.\nOur clustering algorithm, based on the above idea, is\nstated below. We can show that this algorithm clusters\ncorrectly with high probability, when the data in at\nleast one of the views obeys a separation condition, in\naddition to our assumptions.\nThe input to the algorithm is a set of samples S, and\na number k, and the output is a clustering of these\nsamples intok clusters. For this algorithm, we assume\nthat the data obeys the separation condition in View\n1; an analogous algorithm can be applied when the\ndata obeys the separation condition in View 2 as well.\nAlgorithm 1.\n1. Randomly partition S into two subsets A and B\nof equal size.\n2. Let ˆΣ12(A) (ˆΣ12(B) resp.) denote the empirical\ncovariance matrix between views 1 and 2, com-\nputed from the sample set A (B resp.). Com-\npute the top k− 1 left singular vectors of ˆΣ12(A)\n(ˆΣ12(B) resp.), and project the samples in B (A\nresp.) on the subspace spanned by these vectors.\n3. Apply single linkage clustering (Dunn & Everitt,\n2004) (for mixtures of log-concave distributions),\nor the algorithm in Section 3.5 of (Arora & Kan-\nnan, 2005) (for mixtures of Gaussians) on the pro-\njected examples in View 1.\nWe note that in Step 3, we apply either single linkage\nor the algorithm of (Arora & Kannan, 2005); this al-\nlows us to show theoretically that if the distributions\nin the mixture are of a certain type, and given the\nright separation conditions, the clusters can be recov-\nered correctly. In practice, however, these algorithms\ndo not perform as well due to lack of robustness, and\none would use an algorithm such as k-means or EM to\ncluster in this low-dimensional subspace. In particular,\na variant of the EM algorithm has been shown (Das-\ngupta & Schulman, 2000) to cluster correctly mixtures\nof Gaussians, under certain conditions.\nMoreover, in Step 1, we divide the data set into two\nhalves to ensure independence between Steps 2 and 3\nfor our analysis; in practice, however, these steps can\nbe executed on the same sample set.\nMain Results. Our main theorem is as follows.\nTheorem 1 (Gaussians) Suppose the source distri-\nbution is a mixture of Gaussians, and suppose As-\nsumptions 1 and 2 hold. Let σ∗ be the maximum di-\nrectional standard deviation of any distribution in the\nsubspace spanned by{µ1\ni}k\ni=1. If, for each pair i and j\nand for a ﬁxed constant C,\n||µ1\ni−µ1\nj||≥ Cσ∗k1/4\n√\nlog(kn\nδ )\nthen, with probability 1−δ, Algorithm 1 correctly clas-\nsiﬁes the examples if the number of examples used is\nc· d\n(σ∗)2λ2\nminw2\nmin\nlog2( d\nσ∗λminwmin\n) log2(1/δ)\nfor some constant c.\nHere we assume that a separation condition holds in\nView 1, but a similar theorem also applies to View 2.\nAn analogous theorem can also be shown for mixtures\nof log-concave distributions.\nTheorem 2 (Log-concave Distributions)\nSuppose the source distribution is a mixture of\n132\nMulti-View Clustering via Canonical Correlation Analysis\nlog-concave distributions, and suppose Assumptions\n1 and 2 hold. Let σ∗ be the maximum directional\nstandard deviation of any distribution in the subspace\nspanned by{µ1\ni}k\ni=1. If, for each pair i and j and for\na ﬁxed constant C,\n||µ1\ni−µ1\nj||≥ Cσ∗√\nk log(kn\nδ )\nthen, with probability 1−δ, Algorithm 1 correctly clas-\nsiﬁes the examples if the number of examples used is\nc· d\n(σ∗)2λ2\nminw2\nmin\nlog3( d\nσ∗λminwmin\n) log2(1/δ)\nfor some constant c.\nThe proof follows from the proof of Theorem 1, along\nwith standard results on log-concave probability dis-\ntributions – see (Kannan et al., 2005; Achlioptas &\nMcSherry, 2005). We do not provide a proof here due\nto space constraints.\n4. Analyzing Our Algorithm\nIn this section, we prove our main theorems.\nNotation. In the sequel, we assume that we are given\nsamples from a mixture which obeys Assumptions 2\nand 1. We use the notation S1 (resp. S2) to denote\nthe subspace containing the centers of the distributions\nin the mixture in View 1 (resp. View 2), and notation\nS′1 (resp. S′2) to denote the orthogonal complement to\nthe subspace containing the centers of the distributions\nin the mixture in View 1 (resp. View 2).\nFor any matrixA, we use||A|| to denote the L2 norm\nor maximum singular value of A.\nProofs. Now, we are ready to prove our main the-\norem. First, we show the following two lemmas,\nwhich demonstrate properties of the expected cross-\ncorrelational matrix across the views. Their proofs\nare immediate from Assumptions 2 and 1.\nLemma 2 Letv1 and v2 be any vectors in S1 and S2\nrespectively. Then,|(v1)T Σ12v2|>λ min.\nLemma 3 Let v1 (resp. v2) be any vector in S′1\n(resp. S′2). Then, for any u1 ∈ V1 and u2 ∈ V2,\n(v1)T Σ12u2 = (u1)T Σ12v2 = 0.\nNext, we show that given suﬃciently many samples,\nthe subspace spanned by the top k− 1 singular vec-\ntors of ˆΣ12 still approximates the subspace containing\nthe means of the distributions comprising the mixture.\nFinally, we use this fact, along with some results in\n(Arora & Kannan, 2005) to prove Theorem 1. Our\nmain lemma of this section is the following.\nLemma 4 (Projection Subspace Lemma) Let v1\n(resp. v2) be any vector in S1 (resp. S2). If the num-\nber of samples n>c d\nτ 2λ2\nminwmin\nlog2( d\nτλ minwmin\n) log2( 1\nδ )\nfor some constant c, then, with probability 1−δ, the\nlength of the projection of v1 (resp. v2 ) in the sub-\nspace spanned by the top k− 1 left (resp. right) sin-\ngular vectors of ˆΣ12 is at least\n√\n1−τ 2||v1|| (resp.√\n1−τ 2||v2||).\nThe main tool in the proof of Lemma 4 is the follow-\ning lemma, which uses a result due to (Rudelson &\nVershynin, 2007).\nLemma 5 (Sample Complexity Lemma) If the\nnumber of samples\nn>c · d\nϵ2wmin\nlog2( d\nϵwmin\n) log2(1\nδ )\nfor some constantc, then, with probability at least 1−δ,\n||ˆΣ12− Σ12||≤ ϵ.\nA consequence of Lemmas 5, 2 and 3 is the following.\nLemma 6 Let n > C d\nϵ2wmin\nlog2( d\nϵwmin\n) log2( 1\nδ ), for\nsome constant C. Then, with probability 1−δ, the\ntop k− 1 singular values of ˆΣ12 have value at least\nλmin−ϵ. The remaining min(d1,d 2)−k + 1 singular\nvalues of ˆΣ12 have value at most ϵ.\nThe proof follows by a combination of Lemmas 2,3, 5.\nProof:(Of Lemma 5) To prove this lemma, we apply\nLemma 7. Observe the block representation of Σ in\nEquation 1. Moreover, with Σ 11 and Σ22 in isotropic\nposition, we have that the L2 norm of Σ 12 is at most\n1. Using the triangle inequality, we can write:\n||ˆΣ12−Σ12||≤ 1\n2(||ˆΣ−Σ||+||ˆΣ11−Σ11||+||ˆΣ22−Σ22||)\n(where we applied the triangle inequality to the 2 × 2\nblock matrix with oﬀ-diagonal entries ˆΣ12− Σ12 and\nwith 0 diagonal entries). We now apply Lemma 7 three\ntimes, on ˆΣ11− Σ11, ˆΣ22− Σ22, and a scaled version\nof ˆΣ− Σ. The ﬁrst two applications follow directly.\nFor the third application, we observe that Lemma 7\nis rotation invariant, and that scaling each covariance\nvalue by some factor s scales the norm of the matrix\nby at most s. We claim that we can apply Lemma\n7 on ˆΣ− Σ with s = 4. Since the covariance of any\ntwo random variables is at most the product of their\nstandard deviations, and since Σ 11 and Σ 22 are Id1\nand Id2 respectively, the maximum singular value of\nΣ12 is at most 1; so the maximum singular value of Σ\nis at most 4. Our claim follows. The lemma follows by\nplugging in n as a function of ϵ, d and wmin □\n133\nMulti-View Clustering via Canonical Correlation Analysis\nLemma 7 Let X be a set of n points generated by\na mixture of k Gaussians over Rd, scaled such that\nE[x·xT ] = Id. If M is the sample covariance matrix\nofX, then, for n large enough, with probability at least\n1−δ,\n||M− E[M]||≤ C·\n√\nd logn log( 2n\nδ ) log(1/δ)\n√wminn\nwhereC is a ﬁxed constant, and wmin is the minimum\nmixing weight of any Gaussian in the mixture.\nProof: To prove this lemma, we use a concentration\nresult on the L2-norms of matrices due to (Rudelson\n& Vershynin, 2007). We observe that each vectorxi in\nthe scaled space is generated by a Gaussian with some\nmean µ and maximum directional variance σ2. As\nthe total variance of the mixture along any direction\nis at most 1, wmin(µ2 +σ2)≤ 1. Therefore, for all\nsamples xi, with probability at least 1 −δ/2,||xi||≤\n||µ|| +σ\n√\nd log( 2n\nδ ).\nWe condition on the fact that the event ||xi|| ≤\n||µ|| +σ\n√\nd log( 2n\nδ ) happens for all i = 1,...,n . The\nprobability of this event is at least 1 −δ/2.\nConditioned on this event, the distributions of the vec-\ntorsxi are independent. Therefore, we can apply The-\norem 3.1 in (Rudelson & Vershynin, 2007) on these\nconditional distributions, to conclude that:\nPr[||M− E[M]||>t] ≤ 2e−cnt2/Λ2 logn\nwherec is a constant, and Λ is an upper bound on the\nnorm of any vector||xi||. The lemma follows by plug-\nging in t =\n√\nΛ2 log(4/δ) logn\ncn , and Λ≤\n2\n√\nd log(2n/δ)√wmin\n. □\nProof: (Of Lemma 4) For the sake of contradiction,\nsuppose there exists a vector v1∈ S1 such that the\nprojection ofv1 on the topk−1 left singular vectors of\nˆΣ12 is equal to\n√\n1− ˜τ 2||v1||, where ˜τ >τ. Then, there\nexists some unit vectoru1 inV1 in the orthogonal com-\nplement of the space spanned by the top k− 1 left sin-\ngular vectors of ˆΣ12 such that the projection of v1 on\nu1 is equal to ˜τ||v1||. This vector u1 can be written as:\nu1 = ˜τv 1 +(1− ˜τ 2)1/2y1, wherey1 is in the orthogonal\ncomplement of S1. From Lemma 2, there exists some\nvector u2 in S2, such that (v 1)⊤Σ12u2≥ λmin; from\nLemma 3, for this vector u2, (u 1)⊤Σ12u2 ≥ ˜τλ min.\nIf n > c d\n˜τ 2λ2\nminwmin\nlog2( d\n˜τλ minwmin\n) log2( 1\nδ ), then, from\nLemma 6, (u1)TˆΣ12u2≥ ˜τ\n2λmin.\nNow, since u1 is in the orthogonal complement of\nthe subspace spanned by the top k− 1 left singu-\nlar vectors of ˆΣ12, for any vector y2 in the subspace\nspanned by the top k− 1 right singular vectors of ˆΣ12,\n(u1)⊤ˆΣ12y2 = 0. This means that there exists a vector\nz2∈V 2, the orthogonal complement of the subspace\nspanned by the top k− 1 right singular vectors of ˆΣ12\nsuch that (u 1)TˆΣ12z2≥ ˜τ\n2λmin. This implies that the\nk-th singular value of ˆΣ12 is at least ˜τ\n2λmin. However,\nfrom Lemma 6, all but the top k− 1 singular values of\nˆΣ12 are at most τ\n3λmin, which is a contradiction. □\nProof:(Of Theorem 1) From Lemma 4, if n >\nCd\nτ 2λ2\nminwmin\nlog2( d\nτλ minwmin\n) log2( 1\nδ ), then, with proba-\nbility at least 1 −δ, the projection of any vector v\nin S1 or S2 onto the subspace returned by Step 2 of\nAlgorithm 1 has length at least\n√\n1−τ 2||v||. There-\nfore, the maximum directional variance of any Di in\nthis subspace is at most (1 −τ 2)(σ∗)2 +τ 2σ2, where\nσ2 is the maximum directional variance of any Di.\nWhen τ ≤ σ∗\nσ , this is at most 2( σ∗)2. From the\nisotropic condition, σ ≤ 1√wmin\n. Therefore, when\nn > Cd\n(σ∗)2λ2\nminw2\nmin\nlog2( d\nσ∗λminwmin\n) log2( 1\nδ ), the maxi-\nmum directional variance of any Di in the mixture in\nthe space output by Step 2 is at most 2( σ∗)2.\nSinceA andB are random partitions of the sample set\nS, the subspace produced by the action of Step 2 of\nAlgorithm 1 on the setA is independent ofB, and vice\nversa. Therefore, when projected onto the top k− 1\nSVD subspace of ˆΣ12(A), the samples from B are dis-\ntributed as a mixture of (k−1)-dimensional Gaussians.\nThe theorem follows from the previous paragraph, and\nTheorem 1 of (Arora & Kannan, 2005). □\n5. Experiments\n5.1. Audio-visual speaker clustering\nIn the ﬁrst set of experiments, we consider cluster-\ning either audio or face images of speakers. We use\n41 speakers from the VidTIMIT database (Sanderson,\n2008), speaking 10 sentences (about 20 seconds) each,\nrecorded at 25 frames per second in a studio environ-\nment with no signiﬁcant lighting or pose variation.\nThe audio features are standard 12-dimensional mel\ncepstra (Davis & Mermelstein, 1980) and their deriva-\ntives and double derivatives computed every 10ms over\na 20ms window, and ﬁnally concatenated over a win-\ndow of 440ms centered on the current frame, for a total\nof 1584 dimensions. The video features are pixels of\nthe face region extracted from each image (2394 di-\nmensions). We consider the target cluster variable to\nbe the speaker. We use either CCA or PCA to project\nthe data to a lower dimensionality N. In the case of\nCCA, we initially project to an intermediate dimen-\nsionality M using PCA to reduce the eﬀects of spuri-\nous correlations. For the results reported here, typical\nvalues (selected using a held-out set) are N = 40 and\n134\nMulti-View Clustering via Canonical Correlation Analysis\nPCA CCA\nImages 1.1 1.4\nAudio 35.3 12.5\nImages + occlusion 6.1 1.4\nAudio + occlusion 35.3 12.5\nImages + translation 3.4 3.4\nAudio + translation 35.3 13.4\nTable 1. Conditional perplexities of the speaker given the\ncluster, using PCA or CCA bases. “+ occlusion” and “+\ntranslation” indicate that the images are corrupted with\nocclusion/translation; the audio is unchanged, however.\nM = 100 for images and 1000 for audio. For CCA, we\nrandomize the vectors of one view in each sentence, to\nreduce correlations between the views due to other la-\ntent variables such as the current phoneme. We then\ncluster either view using k-means into 82 clusters (2\nper speaker). To alleviate the problem of local min-\nima found by k-means, each clustering consists of 5\nruns of k-means, and the one with the lowest score is\ntaken as the ﬁnal clustering.\nSimilarly to (Blaschko & Lampert, 2008), we measure\nclustering performance using the conditional entropy\nof the speakers given the clusterc,H(s|c). We report\nthe results in terms of conditional perplexity, 2 H(s|c),\nwhich is the mean number of speakers corresponding\nto each cluster. Table 1 shows results on the raw data,\nas well as with synthetic occlusions and translations\nof the image data. Considering the clean visual envi-\nronment, we expect PCA to do very well on the image\ndata. Indeed, PCA provides an almost perfect clus-\ntering of the raw images and CCA does not improve\nit. However, CCA far outperforms PCA when cluster-\ning the more challenging audio view. When synthetic\nocclusions or translations are applied to the images,\nthe performance of PCA-based clustering is greatly de-\ngraded. CCA is unaﬀected in the case of occlusion; in\nthe case of translation, CCA-based image clustering\nis degraded similarly to PCA, but audio clustering is\nalmost unaﬀected. In other words, even when the im-\nage data are degraded, CCA is able to recover a good\nclustering in at least one of the views. 1 For a more\ndetailed look at the clustering behavior, Figures 1(a-d)\nshow the distributions of clusters for each speaker.\n1The audio task is unusually challenging, as each fea-\nture vector corresponds to only a few phonemes. A typ-\nical speaker classiﬁcation setting uses entire sentences. If\nwe force the cluster identity to be constant over each sen-\ntence (the most frequent cluster label in the sentence), per-\nformance improves greatly; e.g., in the “audio+occlusion”\ncase, the perplexity improves to 8.5 (PCA) and 2.1 (CCA).\n5.2. Clustering Wikipedia articles\nNext we consider the task of clustering Wikipedia ar-\nticles, based on either their text or their incoming and\noutgoing links. The link structure L is represented as\na concatenation of “to”and “from” link incidence vec-\ntors, where each element L(i) is the number of times\nthe current article links to/from article i. The article\ntext is represented as a bag-of-words feature vector,\ni.e. the raw count of each word in the article. A lex-\nicon of about 8 million words and a list of about 12\nmillion articles were used to construct the two feature\nvectors. Since the dimensionality of the feature vec-\ntors is very high (over 20 million for the link view), we\nuse random projection to reduce the dimensionality to\na computationally manageable level.\nWe present clustering experiments on a subset of\nWikipedia consisting of 128,327 articles. We use either\nPCA or CCA to reduce the feature vectors to the ﬁnal\ndimensionality, followed by clustering. In these experi-\nments, we use a hierarchical clustering procedure, as a\nﬂat clustering is poor with either PCA or CCA (CCA\nstill usually outperforms PCA, however). In the hier-\narchical procedure, all points are initially considered\nto be in a single cluster. Next, we iteratively pick the\nlargest cluster, reduce the dimensionality using PCA\nor CCA on the points in this cluster, and use k-means\nto break the cluster into smaller sub-clusters (for some\nﬁxed k), until we reach the total desired number of\nclusters. The intuition for this is that diﬀerent clus-\nters may have diﬀerent natural subspaces.\nAs before, we evaluate the clustering using the condi-\ntional perplexity of the article category a (as given by\nWikipedia) given the cluster c, 2H(a|c). For each arti-\ncle we use the ﬁrst category listed in the article. The\n128,327 articles include roughly 15,000 categories, of\nwhich we use the 500 most frequent ones, which cover\n73,145 articles. While the clustering is performed on\nall 128,327 articles, the reported entropies are for the\n73,145 articles. Each sub-clustering consists of 10 runs\nof k-means, and the one with the lowest k-means score\nis taken as the ﬁnal cluster assignment.\nFigure 1(e) shows the conditional perplexity versus the\nnumber of clusters for PCA and CCA based hierarchi-\ncal clustering. For any number of clusters, CCA pro-\nduces better clusterings, i.e. ones with lower perplex-\nity. In addition, the tree structures of the PCA/CCA-\nbased clusterings are qualitatively diﬀerent. With\nPCA based clustering, most points are assigned to a\nfew large clusters, with the remaining clusters being\nvery small. CCA-based hierarchical clustering pro-\nduces more balanced clusters. To see this, in Fig-\nure 1(f) we show the perplexity of the cluster distribu-\n135\nMulti-View Clustering via Canonical Correlation Analysis\ncluster\nspeaker\n(a) AV: Audio, PCA basis\n20 40 60 80\n5\n10\n15\n20\n25\n30\n35\n40\n(c) AV: Images + occlusion, PCA basis\ncluster\nspeaker\n20 40 60 80\n5\n10\n15\n20\n25\n30\n35\n40\n0 20 40 60 80 100 12020\n40\n60\n80\n100\n120\n140\n160\nnumber of clusters\nperplexity\n(e) Wikipedia: Category perplexity\n \n \nhierarchical CCA\nhierarchical PCA\n(b) AV: Audio, CCA basis\ncluster\nspeaker\n20 40 60 80\n5\n10\n15\n20\n25\n30\n35\n40\n(d) AV: Images + occlusion, CCA basis\ncluster\nspeaker\n20 40 60 80\n5\n10\n15\n20\n25\n30\n35\n40\n0 20 40 60 80 100 1200\n20\n40\n60\n80\n100\n120\nnumber of clusters\n2Entropy\n(f) Wikipedia: Cluster perplexity\n \n \nbalanced clustering\nhierarchical CCA\nhierarchical PCA\nFigure 1. (a-d) Distributions of cluster assignments per speaker in audio-visual experiments. The color of each cell ( s, c)\ncorresponds to the empirical probability p(c|s) (darker = higher). (e-f) Wikipedia experiments: (e) Conditional perplexity\nof article category given cluster (2 H(a|c)). (f) Perplexity of the cluster distribution (2 H(c))\ntion versus number of clusters. For about 25 or more\nclusters, the CCA-based clustering has higher perplex-\nity, indicating a more uniform distribution of clusters.\nReferences\nAchlioptas, D., & McSherry, F. (2005). On spec-\ntral learning of mixtures of distributions. Conf. on\nLearning Thy (pp. 458–469).\nAndo, R. K., & Zhang, T. (2007). Two-view feature\ngeneration model for semi-supervised learning. Int.\nConf. on Machine Learning (pp. 25–32).\nArora, S., & Kannan, R. (2005). Learning mixtures\nof separated nonspherical Gaussians. Ann. Applied\nProb., 15, 69–92.\nBlaschko, M. B., & Lampert, C. H. (2008). Correla-\ntional spectral clustering. Conf. on Comp. Vision\nand Pattern Recognition.\nBlum, A., & Mitchell, T. (1998). Combining la-\nbeled and unlabeled data with co-training. Conf.\non Learning Thy. (pp. 92–100).\nBrubaker, S. C., & Vempala, S. (2008). Isotropic PCA\nand aﬃne-invariant clustering.Found. of Comp. Sci.\n(pp. 551–560).\nChaudhuri, K., & Rao, S. (2008). Learning mixtures of\ndistributions using correlations and independence.\nConf. On Learning Thy. (pp. 9–20).\nDasgupta, S. (1999). Learning mixtures of Gaussians.\nFound. of Comp. Sci. (pp. 634–644).\nDasgupta, S., & Schulman, L. (2000). A two-round\nvariant of EM for Gaussian mixtures. Uncertainty\nin Art. Int. (pp. 152–159).\nDavis, S. B., & Mermelstein, P. (1980). Comparison\nof parametric representations for monosyllabic word\nrecognition in continuously spoken sentences. IEEE\nTrans. Acoustics, Speech, and Signal Proc.,28, 357–\n366.\nDunn, G., & Everitt, B. (2004). An introduction to\nmath. taxonomy. Dover Books.\nKakade, S. M., & Foster, D. P. (2007). Multi-view\nregression via canonical correlation analysis. Conf.\nLearning Thy (pp. 82–96).\nKannan, R., Salmasian, H., & Vempala, S. (2005). The\nspectral method for general mixture models. Conf.\non Learning Thy (pp. 444–457).\nRudelson, M., & Vershynin, R. (2007). Sampling\nfrom large matrices: An approach through geomet-\nric functional analysis. Jour. of ACM.\nSanderson, C. (2008). Biometric person recognition:\nFace, speech and fusion. VDM-Verlag.\nVempala, V., & Wang, G. (2002). A spectral algo-\nrithm for learning mixtures of distributions. Found.\nof Comp. Sci. (pp. 113–123).\n136",
  "values": {
    "Critiqability": "No",
    "Explicability": "No",
    "Not socially biased": "No",
    "Beneficence": "No",
    "Respect for Persons": "No",
    "Collective influence": "No",
    "User influence": "No",
    "Fairness": "No",
    "Privacy": "No",
    "Justice": "No",
    "Respect for Law and public interest": "No",
    "Interpretable (to users)": "No",
    "Non-maleficence": "No",
    "Transparent (to users)": "No",
    "Autonomy (power to decide)": "No",
    "Deferral to humans": "No"
  }
}