{
  "pdf": "1553374.1553429",
  "title": "Learning with structured sparsity",
  "author": "Junzhou Huang, Tong Zhang, Dimitris Metaxas",
  "paper_id": "1553374.1553429",
  "text": "Learning with Structured Sparsity\nJunzhou Huang J ZHUANG @CS.RUTGERS .EDU\nDepartment of Computer Science, Rutgers University, 110 Frelinghuysen Road, Piscataway, NJ 08854, USA\nTong Zhang TZHANG @STAT.RUTGERS .EDU\nDepartment of Statistics, Rutgers University, 110 Frelinghuysen Road, Piscataway, NJ 08854, USA\nDimitris Metaxas DNM @CS.RUTGERS .EDU\nDepartment of Computer Science, Rutgers University, 110 Frelinghuysen Road, Piscataway, NJ 08854, USA\nAbstract\nThis paper investigates a new learning formula-\ntion called structured sparsity, which is a natu-\nral extension of the standard sparsity concept in\nstatistical learning and compressive sensing. By\nallowing arbitrary structures on the feature set,\nthis concept generalizes the group sparsity idea.\nA general theory is developed for learning with\nstructured sparsity, based on the notion of coding\ncomplexity associated with the structure. More-\nover, a structured greedy algorithm is proposed\nto efﬁciently solve the structured sparsity prob-\nlem. Experiments demonstrate the advantage of\nstructured sparsity over standard sparsity.\n1. Introduction\nWe are interested in the sparse learning problem under the\nﬁxed design condition. Consider a ﬁxed set of p basis vec-\ntors {x1, . . . ,xp} where xj ∈ Rn for each j. Here, n\nis the sample size. Denote by X the n × p data matrix,\nwith column j of X being xj. Given a random observation\ny = [ y1, . . . ,yn] ∈ Rn that depends on an underlying co-\nefﬁcient vector ¯β ∈ Rp, we are interested in the problem\nof estimating ¯β under the assumption that the target coefﬁ-\ncient ¯β is sparse. Throughout the paper, we consider ﬁxed\ndesign only. That is, we assume X is ﬁxed, and random-\nization is with respect to the noise in the observation y.\nWe consider the situation thatEy can be approximated by a\nsparse linear combination of the basis vectors: Ey ≈ X ¯β,\nwhere we assume that ¯β is sparse. Deﬁne the support\nof a vector β ∈ Rp as supp(β) = {j : ¯βj ̸= 0 } and\nAppearing in Pr oceedings of the 26 th International Conference\non Machine Learning, Montreal, Canada, 2009. Copyright 2009\nby the author(s)/owner(s).\n∥β∥0 = |supp(β)|. A natural method for sparse learning is\nL0 regularization for desired sparsity s:\nˆβL0 = arg min\nβ∈Rp\nˆQ(β) subject to ∥β∥0 ≤ s,\nFor simplicity, we only consider the least squares loss\nˆQ(β) = ∥Xβ − y∥2\n2 in this paper. Since this optimiza-\ntion problem is generally NP-hard, in practice, one often\nconsiders approximate solutions. A standard approach is\nconvex relaxation ofL0 regularization to L1 regularization,\noften referred to as Lasso (Tibshirani, 1996). Another com-\nmonly used approach is greedy algorithms, such as the or-\nthogonal matching pursuit (OMP) (Tropp & Gilbert, 2007).\nIn practical applications, one often knows a structure on\nthe coefﬁcient vector ¯β in addition to sparsity. For ex-\nample, in group sparsity (Yuan & Lin, 2006; Bach, 2008;\nStojnic et al., 2008; Huang & Zhang, 2009), one assumes\nthat variables in the same group tend to be zero or nonzero\nsimultaneously. However, the groups are assumed to be\nstatic and ﬁxed a priori. Moreover, algorithms such as\ngroup Lasso do not correctly handle overlapping groups\n(in that overlapping components are over-counted); that is,\na given coefﬁcient should not belong to different groups.\nThis requirement is too rigid for many practical applica-\ntions. To address this issue, a method called composite ab-\nsolute penalty (CAP) is proposed in (Zhao et al., ) which\ncan handle overlapping groups. Unfortunately, no theory\nis established to demonstrate the effectiveness of the ap-\nproach. Other structures have also been explored in the\nliterature. For example, so-called tonal and transient struc-\ntures were considered for sparse decomposition of audio\nsignals in (Daudet, 2004), but again without any theory.\nGrimm et al. (Grimm et al., 2007) investigated positive\npolynomials with structured sparsity from an optimization\nperspective. The theoretical result there did not address\nthe effectiveness of such methods in comparison to stan-\ndard sparsity. The closest work to ours is a recent pa-\nper (Baraniuk et al., 2008) which was pointed out to us\n417\n\nLearning with Structured Sparsity\nby an anonymous reviewer. In that paper, a speciﬁc case\nof\nstructured sparsity, referred to as model based sparsity,\nwas considered. It is important to note that some theoreti-\ncal results were obtained there to show the effectiveness of\ntheir method in compressive sensing. However, their set-\nting is more restrictive than the structured sparsity frame-\nwork which we shall establish here.\nThe purpose of this paper is to present a framework for\nstructured sparsity, and to study the more general estima-\ntion problem under this framework. If meaningful struc-\ntures exist, we show that one can take advantage of such\nstructures to improve the standard sparse learning.\n2. Structured Sparsity\nIn structured sparsity, not all sparse patterns are equally\nlikely. For example, in group sparsity, coefﬁcients within\nthe same group are more likely to be zeros or nonzeros si-\nmultaneously. This means that if a sparse coefﬁcient’s sup-\nport set is consistent with the underlying group structure,\nthen it is more likely to occur, and hence incurs a smaller\npenalty in learning. One contribution of this work is to for-\nmulate how to deﬁne structure on top of sparsity, and how\nto penalize each sparsity pattern.\nIn order to formalize the idea, we denote byI = {1, . . . , p}\nthe index set of the coefﬁcients. We assign a cost cl(F ) to\nany sparse subset F ⊂ { 1, . . . , p}. In structured sparsity,\ncl(F ) is an upper bound of the coding length ofF (number\nof bits needed to represent F by a computer program) in a\npre-chosen preﬁx coding scheme. It is a well-known fact\nin information theory that mathematically, the existence of\nsuch a coding scheme is equivalent to ∑\nF ⊂I 2−cl(F ) ≤ 1.\nFrom the Bayesian statistics point of view, 2−cl(F ) can be\nregarded as a lower bound of the probability of F . The\nprobability model of structured sparse learning is thus: ﬁrst\ngenerate the sparsity pattern F according to probability\n2−cl(F ); then generate the coefﬁcients in F .\nDeﬁnition 2.1 A cost function cl(F ) deﬁned on subsets of\nI is called a coding length (in base-2) if\n∑\nF ⊂I,F ̸=∅\n2−cl(F ) ≤ 1.\nWe give ∅ a coding length 0. The corresponding structured\nsparse coding complexity of F is deﬁned as\nc(F ) = |F | + cl(F ).\nA coding length cl(F ) is sub-additive if cl(F ∪ F ′) ≤\ncl(F ) + cl( F ′), and a coding complexity c(F ) is sub-\nadditive if c(F ∪ F ′) ≤ c(F ) + c(F ′).\nClearly if cl(F ) is sub-additive, then the corresponding\ncoding complexity c(F ) is also sub-additive. Based on the\nstructured coding complexity of subsets of I, we can now\ndeﬁne the structured coding complexity of a sparse coefﬁ-\ncient vector ¯β ∈ Rp.\nDeﬁnition 2.2 Giving a coding complexityc(F ), the struc-\ntured sparse coding complexity of a coefﬁcient vector ¯β ∈\nRp is\nc( ¯β) = min{c( F ) : supp( ¯β) ⊂ F }.\nLater in the paper, we will show that if a coefﬁcient vec-\ntor ¯β has a small coding complexity c( ¯β), then ¯β can be\neffectively learned, with good in-sample prediction perfor-\nmance (in statistical learning) and reconstruction perfor-\nmance (in compressive sensing). In order to see why the\ndeﬁnition requires adding |F | to cl(F ), we consider the\ngenerative model for structured sparsity mentioned earlier.\nIn this model, the number of bits to encode a sparse coef-\nﬁcient vector is the sum of the number of bits to encode F\n(which is cl(F )) and the number of bits to encode nonzero\ncoefﬁcients in F (this requires O(|F |) bits up to a ﬁxed\nprecision). Therefore the total number of bits required is\ncl(F ) + O(|F |). This information theoretical result trans-\nlates into a statistical estimation result: without additional\nregularization, the learning complexity for least squares re-\ngression within any ﬁxed support set F is O(|F |). By\nadding the model selection complexity cl(F ) for each sup-\nport set F , we obtain an overall statistical estimation com-\nplexity of O(cl(F ) + |F |). While the idea of using coding\nbased penalization resembles minimum description length\n(MDL), the actual penalty we obtain for structured spar-\nsity problems is different from the standard MDL penalty\nfor model selection. This difference is important, and thus\nin order to prevent confusion, we avoid using MDL in our\nterminology.\n3. General Coding Scheme\nWe introduce a general coding scheme calledblock coding.\nThe basic idea of block coding is to deﬁne a coding scheme\non a small number of base blocks (a block is a subset ofI),\nand then deﬁne a coding scheme on all subsets of I using\nthese base blocks.\nConsider a subset B ⊂ 2I. That is, each element (a block)\nof B is a subset of I. We call B a block set if I = ∪B∈BB\nand all single element sets {j} belong to B (j ∈ I ). Note\nthat B may contain additional non single-element blocks.\nThe requirement of B containing all single element sets is\nfor convenience, as it implies that every subset F ⊂ I can\nbe expressed as the union of blocks in B.\nLet cl0 be a code length on B:\n∑\nB∈B\n2−cl0(B) ≤ 1,\n418\nLearning with Structured Sparsity\nwe deﬁne cl( B) = cl 0(B) + 1 for B ∈ B . It not difﬁcult\nto show that the following cost function on F ⊂ I is a\ncode-length\ncl(F ) = min\n\n\n\nb∑\nj=1\ncl(Bj) : F =\nb⋃\nj=1\nBj (Bj ∈ B )\n\n\n .\nThis is a coding length because\n∑\nF ⊂I,F ̸=∅\n2−cl(F ) ≤\n∑\nb≥1\n∑\n{Bℓ}∈Bb\n2− ∑k\nℓ=1 cl(Bℓ)\n≤\n∑\nb≥1\nb∏\nℓ=1\n∑\nBℓ∈B\n2−cl(Bℓ) ≤\n∑\nb≥1\n2−b = 1.\nIt is obvious that block coding is sub-additive.\nThe main purpose of introducing block coding is to de-\nsign computational efﬁcient algorithms based on the block\nstructure. In particular, we consider a structured greedy al-\ngorithm that can take advantage of block structures. In the\nstructured greedy algorithm, instead of searching over all\nsubsets of I up to a ﬁxed coding complexity s (exponen-\ntial in s number of such subsets), we greedily add blocks\nfrom B one at a time. Each search problem over B can\nbe efﬁciently performed because B is supposed to contain\nonly a computationally manageable number of base blocks.\nTherefore the algorithm is computationally efﬁcient. Con-\ncrete structured sparse coding examples described below\ncan be efﬁciently approximated by block coding.\nStandard sparsity\nA simple coding scheme is to code each subset F ⊂ I of\ncardinality k using k log2(2p) bits, which corresponds to\nblock coding with B consisted only of single element sets,\nand each base block has a coding length log2 p. This corre-\nsponds to the complexity for the standard sparse learning.\nGroup sparsity\nThe concept of group sparsity has been appeared in vari-\nous recent work, such as the group Lasso in (Yuan & Lin,\n2006). Consider a partition of I = ∪m\nj=1Gj to m dis-\njoint groups. Let BG contain the m groups Gj, and B1\ncontain p single element blocks. The strong group spar-\nsity coding scheme is to give each element in B1 a code-\nlength cl0 of ∞, and each element in BG a code-length\ncl0 of log2 m. Then the block coding scheme with blocks\nB = BG ∪ B 1 leads to group sparsity, which only looks\nfor signals consisted of the groups. The resulting coding\nlength is: cl(B) = g log2(2m) if B can be represented as\nthe union of g disjoint groups Gj; and cl(B) = ∞ oth-\nerwise. Note that if the signal can be expressed as the\nunion of g groups, and each group size isk0, then the group\ncoding length g log2(2m) can be signiﬁcantly smaller than\nthe standard sparsity coding length of gk0 log2(p). As we\nshall see later, the smaller coding complexity implies bet-\nter learning behavior, which is essentially the advantage of\nusing group sparse structure.\nGraph sparsity\nWe consider a generalization of the group sparsity idea that\nemploys a directed graph structure G on I. Each element\nof I is a node of G but G may contain additional nodes.\nFor simplicity, we assumeG contains a starting node not in\nI. At each node v ∈ G, we deﬁne coding length clv(S) on\nthe neighborhood Nv of v (that contains the empty set), as\nwell as any other single node u ∈ G with clv(u), such that∑\nS⊂Nv\n2−clv(S) + ∑\nu∈G 2−clv(u) ≤ 1. To encode F ⊂\nG, we start with the active set containing only the starting\nnode, and ﬁnish when the set becomes empty. At each node\nv before termination, we may either pick a subset S ⊂ Nv,\nwith coding length clv(S), or a node in u ∈ G, with coding\nlength clv(u), and then put the selection into the active set.\nWe then remove v from the active set (once v is removed,\nit does not return to the active set anymore). This process\nis continued until the active set becomes empty.\nThe wavelet coefﬁcients of a signal are well known to have\na tree-graph structure, which has been widely used for com-\npressing natural images and is a special case of graph spar-\nsity. Each wavelet coefﬁcient of the signal is connected to\nits parent coefﬁcient and its child coefﬁcients. The wavelet\ncoefﬁcients of 1D signals have a binary tree connected\ngraph structure while the wavelet coefﬁcients of 2D images\nhave a quad-tree connected graph structure.\nAs a concrete example, we consider image processing\nproblem, where each image is a rectangle of pixels (nodes);\neach pixel is corrected to four adjacent pixels, which forms\nthe underlying graph structure. At each pixel, the number\nof subsets in its neighborhood is 24 = 16 (including the\nempty set), with a coding length clv(S) = 5 each; we also\nencode all other pixels in the image with random jumping,\neach with a coding length 1 + log 2 p. Using this scheme,\nwe can encode each connected region F by no more than\nlog2 p+5|F | bits by growing the region from a single point\nin the region. Therefore if F is composed of g connected\nregions, then the coding length isg log2 p+5|F |, which can\nbe signiﬁcantly better than standard sparse coding length of\n|F | log2 p. This example shows that the general graph cod-\ning scheme presented here favors connected regions (that\nis, nodes that are grouped together with respect to the graph\nstructure). This scheme can be efﬁciently approximated\nwith block coding as follows: we consider relatively small\nsized base blocks consisted of nodes that are close together\nwith respect to the graph structure, and then use the induced\nblock coding scheme to approximate the graph coding.\n419\nLearning with Structured Sparsity\n4. Algorithms for Structured Sparsity\nThe\nfollowing algorithm is a natural extension of L0 regu-\nlarization to structured sparsity problems. It penalizes the\ncoding complexity instead of the cardinality (sparsity) of\nthe feature set.\nˆβconstr = arg min\nβ∈Rp\nˆQ(β) subject to c(β) ≤ s. (1)\nThe optimization of (1) is generally hard. There are two\ncommon approaches to alleviate this problem. One is con-\nvex relaxation (L1 regularization to replace L0 regulariza-\ntion for standard sparsity); the other is forward greedy algo-\nrithm. We do not know any extensions ofL1 regularization\nlike convex relaxation that can handle general structured\nsparsity formulations. However, one can extend greedy al-\ngorithm by using a block structure. We call the resulting\nprocedure structured greedy algorithm (see Algorithm 1),\nwhich approximately solves (1).\nAlgorithm 1 Structured Greedy Algorithm (StructOMP)\n1: Input: (X , y), B ⊂ 2I, s > 0\n2: Output: F (k) and β(k)\n3: let F (0) = ∅ and β(0) = 0\n4: for all K = 1, ... do\n5: select B(k) ∈ B to maximize progress (∗)\n6: let F (k) = B(k) ∪ F (k−1)\n7: let β(k) = arg minβ∈Rp ˆQ(β)\nsubject to supp(β) ⊂ F (k)\n8: if (c(β(k)) > s) break\n9: end for\nIn Algorithm 1, we are given a set of blocks B that con-\ntains subsets of I. Instead of searching all subsets F ⊂ I\nup to a certain complexity |F | + c(F ), which is computa-\ntionally infeasible, we search only the blocks restricted to\nB. It is assumed that searching over B is computationally\nmanageable. At each step (∗), we try to ﬁnd a block fromB\nto maximize progress. It is thus necessary to deﬁne a quan-\ntity that measures progress. Our idea is to approximately\nmaximize the gain ratio:\nλ(k) =\nˆQ(β(k−1)) − ˆQ(β(k))\nc(β(k)) − c (βk−1) ,\nwhich measures the reduction of objective function per unit\nincrease of coding complexity. This greedy criterion is\na natural generalization of the standard greedy algorithm,\nand essential in our analysis. For least squares regression,\nwe can approximate λ(k) using the following deﬁnition\nφ(B) = ∥PB−F (k−1)(Xβ (k−1) − y)∥2\n2\nc(B ∪ F (k− 1)) − c(F (k−1)) , (2)\nwhere PF = XF (X ⊤\nF XF )−1X ⊤\nF is the projection matrix\nto the subspaces generated by columns of XF . We then\nselect B(k) so that\nφ(B(k)) ≥ γ max\nB∈B\nφ(B),\nwhere γ ∈ (0, 1] is a ﬁxed approximation ratio that speci-\nﬁes the quality of approximate optimization.\n5. Theory of Structured Sparsity\nDue to the space limitation, the proofs of the theorems are\ndetailed in (Huang et al., 2009).\n5.1. Assumptions\nWe assume sub-Gaussian noise as follows.\nAssumption 5.1 Assume that {yi}i=1,...,n are indepen-\ndent (but not necessarily identically distributed) sub-\nGaussians: there exists a constant σ ≥ 0 such that ∀i and\n∀t ∈ R, Eyi et(yi−Eyi) ≤ eσ2t2/2.\nWe also need to generalize sparse eigenvalue condition,\nused in the modern sparsity analysis. It is related to (and\nweaker than) the RIP (restricted isometry property) as-\nsumption (Candes & Tao, 2005) in the compressive sensing\nliterature. This deﬁnition takes advantage of coding com-\nplexity, and can be also considered as (a weaker version of)\nstructured RIP. We introduce a deﬁnition.\nDeﬁnition 5.1 For all F ⊂ {1, . . . , p}, deﬁne\nρ−(F ) = inf\n{ 1\nn ∥ Xβ ∥2\n2/∥β∥2\n2 : supp(β) ⊂ F\n}\n,\nρ+(F ) = sup\n{ 1\nn ∥ Xβ ∥2\n2/∥β∥2\n2 : supp(β) ⊂ F\n}\n.\nMoreover, for alls > 0, deﬁne\nρ−(s) = inf {ρ−(F ) : F ⊂ I , c(F ) ≤ s},\nρ+(s) = sup{ρ+(F ) : F ⊂ I , c(F ) ≤ s}.\nIn the theoretical analysis, we need to assume that ρ−(s)\nis not too small for some s that is larger than the sig-\nnal complexity. Since we only consider eigenvalues for\nsubmatrices with small cost c( ¯β), the sparse eigenvalue\nρ−(s) can be signiﬁcantly larger than the corresponding\nratio for standard sparsity (which will consider all sub-\nsets of {1, . . . , p} up to size s). For example, for random\nprojections used in compressive sensing applications, the\ncoding length c(supp( ¯β)) is O(k ln p) in standard spar-\nsity, but can be as low as c(supp( ¯β)) = O(k) in struc-\ntured sparsity (if we can guesssupp( ¯β) approximately cor-\nrectly. Therefore instead of requiring n = O(k ln p) sam-\nples, we requires only O(k + cl(supp(¯β))). The difference\ncan be signiﬁcant when p is large and the coding length\ncl(supp( ¯β)) ≪ k ln p.\n420\nLearning with Structured Sparsity\nThe theorem implies that the structured RIP condition is\nsatisﬁed\nwith sample size n = O((k/k0) ln(p/k0)) in\ngroup sparsity rather than n = O(k ln(p)) in standard spar-\nsity.\nTheorem 5.1 (Structured-RIP) Suppose that elements in\nX are iid standard Gaussian random variables N (0, 1).\nFor any t > 0 and δ ∈ (0, 1), let\nn ≥ 8\nδ2 [ln 3 + t + s ln(1 + 8/δ)].\nThen with probability at least 1 − e−t, the random matrix\nX ∈ Rn×p satisﬁes the following structured-RIP inequal-\nity for all vector ¯β ∈ Rp with coding complexity no more\nthan s:\n(1 − δ)∥ ¯β∥2 ≤ 1√n ∥ X ¯β∥2 ≤ (1 + δ)∥ ¯β∥2.\n5.2. Coding complexity regularization\nTheorem 5.2 Suppose that Assumption 5.1 is valid. Con-\nsider any ﬁxed target ¯β ∈ Rp. Then with probability ex-\nceeding 1 − η, for all λ ≥ 0, ϵ ≥ 0, ˆβ ∈ Rp such that:\nˆQ( ˆβ) ≤ ˆQ( ¯β) + ϵ, we have\n∥X ˆβ − Ey∥2 ≤ ∥X ¯β − Ey∥2 + σ\n√\n2 ln(6/η) + 2Γ,\nΓ = (7 .4σ2c( ˆβ) + 2.4σ2 ln(6/η) + ϵ)1/2.\nMoreover, if the coding schemec(·) is sub-additive, then\nnρ−(c( ˆβ) + c( ¯β))∥ ˆβ − ¯β∥2\n2 ≤ 10∥X ¯β − Ey∥2\n2 + ∆,\n∆ = 37σ 2c( ˆβ) + 29σ2 ln(6/η) + 2.5ϵ.\nThis theorem immediately implies the following result for\n(1): ∀ ¯β such that c( ¯β) ≤ s,\n1\n√n ∥ X ˆβconstr − Ey∥2 ≤ 1√n ∥ X ¯β − Ey∥2 + Λ,\nΛ = σ√n\n√\n2 ln(6/η) + 2σ√n (7.4 s + 4.7 ln(6/η))1/2,\n∥ ˆβconstr − ¯β∥2\n2 ≤ 1\nρ−(s + c ( ¯β))n\n[\n10∥X ¯β − Ey∥2\n2 + Π\n]\n,\nΠ = 37 σ2s + 29σ2 ln(6/η).\nIn compressive sensing applications, we take σ = 0, and\nwe are interested in recovering ¯β from random projections.\nFor simplicity, we let X ¯β = Ey = y, and our result\nshows that the constrained coding complexity penalization\nmethod achieves exact reconstruction ˆβconstr = ¯β as long\nas ρ−(2c( ¯β)) > 0 (by setting s = c( ¯β)). According to\nTheorem 5.1, this is possible when the number of random\nprojections (sample size) reaches n = O(2c( ¯β)). This is\na generalization of corresponding results in compressive\nsensing (Candes & Tao, 2005). As we have pointed out\nearlier, this number can be signiﬁcantly smaller than the\nstandard sparsity requirement of n = O(∥ ¯β∥0 ln p), when\nthe structure imposed is meaningful.\n5.3. Structured greedy algorithm\nDeﬁnition 5.2 Given B ⊂ 2I, deﬁne\nρ0(B) = max\nB∈B\nρ+(B), c 0(B) = max\nB∈B\nc(B)\nand\nc( ¯β, B) = min\nb∑\nj=1\nc( ¯Bj), supp( ¯β) ⊂\nb⋃\nj=1\n¯Bj ( ¯Bj ∈ B ).\nThe following theorem shows that if c( ¯β, B) is small, then\none can use the structured greedy algorithm to ﬁnd a coef-\nﬁcient vector β(k) that is competitive to ¯β, and the coding\ncomplexity c(β(k)) is not much worse than that of c( ¯β, B).\nThis implies that if the original coding complexityc( ¯β) can\nbe approximated by block complexity c( ¯β, B), then we can\napproximately solve (1).\nTheorem 5.3 Suppose the coding scheme is sub-additive.\nConsider ¯β and ϵ such that ϵ ∈ (0, ∥y∥2\n2 − ∥X ¯β − y∥2\n2] and\ns ≥ ρ0(B)c( ¯β, B)\nγρ−(s + c ( ¯β)) ln ∥y∥2\n2 − ∥X ¯β − y∥2\n2\nϵ .\nThen\nat the stopping time k, we have\nˆQ(β(k)) ≤ ˆQ( ¯β) + ϵ.\nBy Theorem 5.2, the result in Theorem 5.3 implies that\n∥Xβ (k) − Ey∥2 ≤ ∥X ¯β − Ey∥2 + σ\n√\n2 ln(6/η) + Λ,\nΛ = 2σ (7.4(s + c0(B)) + 4.7 ln(6/η) + ϵ/σ2)1/2,\n∥β(k) − ¯β∥2\n2 ≤\n[\n10∥X ¯β − Ey∥2\n2 + Π\n]\nρ−(s + c0(B) + c( ¯β))n ,\nΠ = 37 σ2(s + c0(B)) + 29σ2 ln(6/η) + 2.5ϵ.\nThe result shows that in order to approximate a sig-\nnal ¯β up to ϵ, one needs to use coding complexity\nO(ln(1/ϵ))c( ¯β, B). If B contains small blocks and their\nsub-blocks with equal coding length, and the coding\nscheme is block coding generated by B, then c( ¯β, B) =\nc( ¯β). In this case we need O(s ln(1/ϵ)) to approximate a\nsignal with coding complexity s.\nIn order to get rid of the O(ln(1/ϵ)) factor, backward\ngreedy strategies can be employed, as shown in various re-\ncent work such as (Zhang, 2008). For simplicity, we will\n421\nLearning with Structured Sparsity\nnot analyze such strategies in this paper. However, in the\nfollo\nwing, we present an additional convergence result for\nstructured greedy algorithm that can be applied to weakly\nsparse p-compressible signals common in practice. It is\nshown that the ln(1/ϵ) can be removed for such weakly\nsparse signals. More precisely, we introduce the following\nconcept of weakly sparse compressible target that gener-\nalizes the corresponding concept of compressible signal in\nstandard sparsity from the compressive sensing literature\n(Donoho, 2006).\nDeﬁnition 5.3 The target Ey is (a, q)-compressible with\nrespect to block B if there exist constants a, q > 0 such\nthat for each s > 0, ∃ ¯β(s) such that c( ¯β(s), B) ≤ s and\n1\nn ∥ X ¯β(s) − Ey∥2\n2 ≤ as−q.\nTheorem 5.4 Suppose that the target is (a, q)-\ncompressible with respect to B. Then with probability\n1 − η, at the stopping time k, we have\nˆQ(β(k)) ≤ ˆQ( ¯β(s′)) + 2na/s′q + 2σ2[ln(2/η) + 1],\nwhere\ns′ ≤ s γ\n(10 + 3q)ρ0(B) min\nu ≤s′\nρ−(s + c( ¯β(u))).\nThis result shows that we can approximate a compressible\nsignal of complexity s′ with complexity s = O(qs′) us-\ning greedy algorithm. This means the greedy algorithm\nobtains optimal rate for weakly-sparse compressible sig-\nnals. The sample complexity suffers only a constant fac-\ntor O(q). Combine this result with Theorem 5.2, and take\nunion bound, we have with probability 1 − 2η, at stopping\ntime k:\n1\n√n ∥ Xβ (k) − Ey∥2 ≤\n√ a\ns′ q + σ\n√\n2 ln(6/η)\nn + 2σ\n√\nΛ,\nΛ\n= 7.4(s + c0(B)) + 6.7 ln(6/η)\nn + 2\na\nσ2s′ q ,\n∥β(k) − ¯β∥2\n2 ≤ 1\nρ−(s + s′ + c0(B))\n[ 15\na\ns′ q + Π\nn\n]\n,\nΠ\n= 37σ2(s + c0(B)) + 34σ2 ln(6/η).\nGiven a ﬁxed n, we can obtain a convergence result by\nchoosing s (and thus s′) to optimize the right hand side.\nThe resulting rate is optimal for the special case of stan-\ndard sparsity, which implies that the bound has the optimal\nform for structured q-compressible targets. In particular, in\ncompressive sensing applications where σ = 0, we obtain\nwhen samples size reaches n = O(qs′), the reconstruction\nperformance is\n∥ ¯β(k) − ¯β∥2\n2 = O(a/s′q),\nwhich matches that of the constrained coding complexity\nregularization method in (1) up to a constant O(q).\n6. Experiments\nThe purpose of these experiments is to demonstrate the ad-\nvantage of structured sparsity over standard sparsity. We\ncompare the proposed StructOMP to OMP and Lasso,\nwhich are standard algorithms to achieve sparsity but with-\nout considering structure. In our experiments, we use\nLasso-modiﬁed least angle regression (LAS/Lasso) as the\nsolver of Lasso (Bradley Efron & Tibshirani, 2004). In\norder to quantitatively compare performance of different\nalgorithms, we use recovery error, deﬁned as the relative\ndifference in 2-norm between the estimated sparse coef-\nﬁcient vector ˆβest and the ground-truth sparse coefﬁcient\n¯β: ∥ ˆβest − ¯β∥2/∥ ¯β∥2. Our experiments focus on graph\nsparsity that is more general than group sparsity. In fact,\nconnected regions may be regarded as dynamic groups that\nare not pre-deﬁned. For this reason, we do not compare to\ngroup-Lasso which requires pre-deﬁned groups.\n100\r 200\r 300\r 400\r 500\r\n-2\r\n0\r\n2\r\n(a) Original Signal\r\n100\r 200\r 300\r 400\r 500\r\n-2\r\n0\r\n2\r\n(c) Lasso\r\n100\r 200\r 300\r 400\r 500\r\n-2\r\n0\r\n2\r\n(b) OMP\r\n100\r 200\r 300\r 400\r 500\r\n-2\r\n0\r\n2\r\n(d) StructOMP\r\nFigure 1. Recovery results of 1D signal with graph-structured\nsparsity. (a) original data; (b) recovered results with OMP (er-\nror is 0.9921); (c) recovered results with Lasso (error is 0.6660);\n(d) recovered results with StructOMP (error is 0.0993).\n6.1. 1D Signals with Line-Structured Sparsity\nIn the ﬁrst experiment, we randomly generate a 1D struc-\ntured sparse signal with values±1, where p = 512, k = 32\nand g = 2. The support set of these signals is composed of\ng connected regions. Here, each element of the sparse coef-\nﬁcient is connected to two of its adjacent elements, which\nforms the underlying graph structure. The graph sparsity\nconcept introduced earlier is used to compute the coding\nlength of sparsity patterns in StructOMP. The projection\nmatrix X is generated by creating an n × p matrix with\ni.i.d. draws from a standard Gaussian distribution N (0, 1).\nFor simplicity, the rows of X are normalized to unit mag-\nnitude. Zero-mean Gaussian noise with standard deviation\nσ = 0. 01 is added to the measurements. Figure 1 shows\none generated signal and its recovered results by different\nalgorithms when n = 4 k = 128 . To study how the sam-\nple size n effects the recovery performance, we change the\nsample size and record the recovery results by different al-\ngorithms. Figure 2(a) shows the recovery performance of\nthe three algorithms, averaged over 100 random runs for\neach sample size. As expected, StructOMP is better than\nthe OMP and Lasso and can achieve better recovery per-\nformance for structured sparsity signals with less samples.\n422\nLearning with Structured Sparsity\n2 3 4 5 6 7 8\n0\n0.2\n0.4\n0.6\n0.8\n1\n1.2\n1.4\n1.6Recovery Error\nSample Size Ratio ( n / k )\n \n \nOMP\nLasso\nStructOMP\n(a)\n2 3 4 5 6 7 8\n0\n0.2\n0.4\n0.6\n0.8\n1\n1.2\n1.4\n1.6Recovery Error\nSample Size Ratio (n/k)\n \n \nOMP\nLasso\nStructOMP\n(b)\nFigure 2. Recovery error vs. Sample size ratio (n/k): a) 1D sig-\nnals; (b) 2D gray images\n6.2. 2D Images with Graph-structured Sparsity\nTo demonstrate the structure sparsity concept on 2D im-\nages, we randomly generate a 2D structured sparsity im-\nage by putting four letters in random locations, where\np = H ∗ W = 48 ∗ 48, k = 160 and g = 4 . The sup-\nport set of these signals is thus composed of g connected\nregions. Here, each pixel of the 2D gray image is con-\nnected to four of its adjacent pixels, which forms the un-\nderlying graph structure. The graph sparsity coding scheme\ndiscussed earlier is applied to calculate coding length of a\nsparsity pattern. Figure 3 shows one example of 2D gray\nimages and the recovered results by different algorithms\nwhen m = 4k = 640 . We also record the recovery re-\nsults by different algorithms with increasing sample sizes.\nFigure 2(b) shows the recovery performance of the three al-\ngorithms, averaged over 100 random runs for each sample\nsize. The recovery results of StructOMP are always better\nthan those of OMP. Comparing to Lasso, however, the dif-\nference is not always clear cut. This result is reasonable,\nconsidering that this artiﬁcial signal is strongly sparse, and\nour theory says that OMP works best for weakly sparse\nsignals. For strongly sparse signals, recovery bounds for\nLasso are known to be better than that of OMP. However,\nas shown in the next two examples, real data are often not\nstrongly sparse, and StructOMP can signiﬁcantly outper-\nform Lasso. We shall mention that a few recent works have\nshown that the backward greedy strategies can be added\nto further improve the forward greedy methods and obtain\nsimilarly results as those of L1 regularization based meth-\nods (Needell & Tropp, 2008)(Zhang, 2008). It will be a\nfuture work to include such modiﬁcations into StructOMP.\n(a)\n (b)\n (c)\n (d)\nFigure 3. Recovery results of a 2D gray image: (a) original gray\nimage, (b) recovered image with OMP (error is 0.9012), (c) recov-\nered image with Lasso (error is 0.4556) and (d) recovered image\nwith StructOMP (error is 0.1528)\n6.3. 2D Images with Tree-structured Sparsity\nIt is well known that the 2D natural images are sparse in\na wavelet basis. Their wavelet coefﬁcients have a hierar-\nchical tree structure (Mallat, ). Figure 4(a) shows a widely\nused example image with size 64 × 64: cameraman. Each\n2D wavelet coefﬁcient of this image is connected to its par-\nent coefﬁcient and child coefﬁcients, which forms the un-\nderlying hierarchical tree structure (which is a special case\nof graph sparsity). In our experiment, we choose Haar-\nwavelet to obtain its tree-structured sparsity wavelet coef-\nﬁcients. The projection matrix X and noises are generated\nwith the same method as that for 1D structured sparsity sig-\nnals. OMP, Lasso and StructOMP are used to recover the\nwavelet coefﬁcients from the random projection samples\nrespectively. Then, the inverse wavelet transform is used to\nreconstruct the images with these recovered wavelet coef-\nﬁcients. Our task is to compare the recovery performance\nof the StructOMP to those of OMP and Lasso. Figure 4\nshows one example of the recovered results by different al-\ngorithms. It shows that StructOMP obtains the best recov-\nered result. Figure 5(a) shows the recovery performance\nof the three algorithms, averaged over 100 random runs for\neach sample size. The StructOMP algorithm is better than\nboth Lasso and OMP in this case. The difference of this\nexample from the previous example is that real image data\nare only weakly sparse, for which even the standard OMP\n(without structured sparsity) bound obtained in this paper\nmatches that of Lasso. It is thus consistent with our theory\nthat StructOMP should outperform unstructured Lasso in\nthis case.\n(a)\n (b)\n (c)\n (d)\nFigure 4. Recovery results with sample size n = 2048: (a) the\nbackground subtracted image, (b) recovered image with OMP (er-\nror is 0.21986), (c) recovered image with Lasso (error is 0.1670)\nand (d) recovered image with StructOMP (error is 0.0375)\n1200 1400 1600 1800 2000 2200 2400 2600 2800\n0\n0.05\n0.1\n0.15\n0.2\n0.25\n0.3\n0.35Recovery Error\nSample Size\n \n \nOMP\nLasso\nStructOMP\n(a)\n500 1000 1500 2000 2500\n0\n0.2\n0.4\n0.6\n0.8\n1\n1.2\n1.4\n1.6Recovery Error\nSample Size\n \n \nOMP\nLasso\nStructOMP\n(b)\nFigure 5. Recovery error vs. Sample size: a) 2D image with tree-\nstructured sparsity in wavelet basis; (b) background subtracted\nimages with structured sparsity\n423\nLearning with Structured Sparsity\n6.4. Background Subtracted Images\nBackground\nsubtracted images are typical structure spar-\nsity data in static video surveillance applications. They\ngenerally correspond to the foreground objects of in-\nterest. These images are not only spatially sparse\nbut also inclined to cluster into groups, which cor-\nrespond to different foreground objects. In this\nexperiment, the testing video is downloaded from\nhttp://homepages.inf.ed.ac.uk/rbf/CA VIARDATA1/. One\nsample image frame is shown in Figure 6(a). Each pixel of\nthe 2D background subtracted image is connected to four of\nits adjacent pixels, forming the underlying graph structure.\nWe randomly choose 100 background subtracted images as\ntest images. The recovery performance is recorded as a\nfunction of increasing sample sizes. Figure 6 and Figure\n5(b) demonstrate that StructOMP signiﬁcantly outperforms\nOMP and Lasso in recovery performance on video data.\n(a)\n (b)\n (c)\n (d)\nFigure 6. Recovery results with sample size n = 900: (a) the\nbackground subtracted image, (b) recovered image with OMP (er-\nror is 1.1833), (c) recovered image with Lasso (error is 0.7075)\nand (d) recovered image with StructOMP (error is 0.1203)\n7. Conclusion\nThis paper develops a theory for structured sparsity where\nprior knowledge allows us to prefer certain sparsity patterns\nto others. A general framework is established based on a\ncoding scheme, which includes the group sparsity idea as\na special case. The proposed structured greedy algorithm\nis the ﬁrst efﬁcient algorithm to handle the general struc-\ntured sparsity learning. Experimental results demonstrate\nthat signiﬁcant improvements can be obtained on some real\nproblems that have natural structures, and the results are\nconsistent with our theory. Future work include additional\ncomputationally efﬁcient methods such as convex relax-\nation methods and backward greedy strategies.\nReferences\nBach, F. R. (2008). Consistency of the group lasso and\nmultiple kernel learning. Journal of Machine Learning\nResearch, 9, 1179–1225.\nBaraniuk, R., Cevher, V ., Duarte, M., & Hegde, C. (2008).\nModel based compressive sensing. preprint.\nBradley Efron, Trevor Hastie, I. J., & Tibshirani, R. (2004).\nLeast angle regression. Annals of Statistics, 32, 407–\n499.\nCandes, E. J., & Tao, T. (2005). Decoding by linear pro-\ngramming. IEEE Trans. on Information Theory, 51,\n4203–4215.\nDaudet, L. (2004). Sparse and structured decomposition\nof audio signals in overcomplete spaces. International\nConference on Digital Audio Effects (pp. 1–5).\nDonoho, D. (2006). Compressed sensing. IEEE Transac-\ntions on Information Theory, 52, 1289–1306.\nGrimm, D., Netzer, T., & Schweighofer, M. (2007). A note\non the representation of positive polynomials with struc-\ntured sparsity. Arch. Math., 89, 399–403.\nHuang, J., & Zhang, T. (2009). The beneﬁt of group spar-\nsity (Technical Report). Rutgers University.\nHuang, J., Zhang, T., & Metaxas, D. (2009).Learning with\nstructured sparsity (Technical Report). Rutgers Univer-\nsity. available from http://arxiv.org/abs/0903.3002.\nMallat, S. A Wavelet Tour of Signal Processing. Academic\nPress.\nNeedell, D., & Tropp, J. (2008). Cosamp: Iterative signal\nrecovery from incomplete and inaccurate samples. Ap-\nplied and Computational Harmonic Analysis. Accepted.\nStojnic, M., Parvaresh, F., & Hassibi, B. (2008). On the\nreconstruction of block-sparse signals with an optimal\nnumber of measurements. Preprint.\nTibshirani, R. (1996). Regression shrinkage and selection\nvia the lasso. Journal of the Royal Statistical Society,58,\n267–288.\nTropp, J., & Gilbert, A. (2007). Signal recovery from\nrandom measurements via orthogonal matching pursuit.\nIEEE Transactions on Information Theory, 53, 4655–\n4666.\nYuan, M., & Lin, Y . (2006). Model selection and estima-\ntion in regression with grouped variables.Journal of The\nRoyal Statistical Society Series B, 68, 49–67.\nZhang, T. (2008). Adaptive forward-backward greedy al-\ngorithm for learning sparse representations.Proceedings\nof Neural Information Processing Systems (pp. 1–8).\nZhao, P., Rocha, G., & Yu, B. Grouped and hierarchical\nmodel selection through composite absolute penalties.\nThe Annals of Statistics. to appear.\n424",
  "values": {
    "Critiqability": "Yes",
    "Not socially biased": "Yes",
    "Explicability": "Yes",
    "Interpretable (to users)": "Yes",
    "Beneficence": "Yes",
    "Privacy": "Yes",
    "Respect for Persons": "Yes",
    "Non-maleficence": "Yes",
    "Transparent (to users)": "Yes",
    "Justice": "Yes",
    "Autonomy (power to decide)": "Yes",
    "Respect for Law and public interest": "Yes",
    "Deferral to humans": "Yes",
    "User influence": "Yes",
    "Collective influence": "Yes",
    "Fairness": "Yes"
  }
}