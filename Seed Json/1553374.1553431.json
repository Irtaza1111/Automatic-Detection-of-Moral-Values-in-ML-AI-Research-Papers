{
  "pdf": "1553374.1553431",
  "title": "Group lasso with overlap and graph lasso",
  "author": "Laurent Jacob, Guillaume Obozinski, Jean-Philippe Vert",
  "paper_id": "1553374.1553431",
  "text": "Group Lasso with Overlap and Graph Lasso\nLaurent Jacob LA URENT .JACOB @MINES -PARISTECH .FR\nMines ParisTech – CBIO, INSERM U900, Institut Curie, 26 rue d’Ulm, Paris cedex 05, F-75248 France\nGuillaume Obozinski1 GOBO @STAT.BERKELEY .EDU\nINRIA – Willow Project-Team, Ecole Normale Supérieure, 45 rue d’Ulm, Paris cedex 05, F-75230 France\nJean-Philippe Vert JEAN -PHILIPPE .VERT @MINES -PARISTECH .FR\nMines ParisTech – CBIO, INSERM U900, Institut Curie, 26 rue d’Ulm, Paris cedex 05, F-75248 France\nAbstract\nWe propose a new penalty function which, when\nused as regularization for empirical risk mini-\nmization procedures, leads to sparse estimators.\nThe support of the sparse vector is typically a\nunion of potentially overlapping groups of co-\nvariates deﬁned a priori, or a set of covariates\nwhich tend to be connected to each other when\na graph of covariates is given. We study theo-\nretical properties of the estimator, and illustrate\nits behavior on simulated and breast cancer gene\nexpression data.\n1. Introduction\nEstimation of sparse linear models by the minimization of\nan empirical error penalized by a regularization term is\na very popular and successful approach in statistics and\nmachine learning. Controlling the trade-off between data\nﬁtting and regularization, one can obtain estimators with\ngood statistical properties, even in very large dimension.\nMoreover, sparse classiﬁers lend themselves particularly\nwell to interpretation, which is often of primary importance\nin many applications such as biology or social sciences. A\npopular example is the penalization of a ℓ 2 criterion by the\nℓ 1 norm of the estimator, known aslasso (Tibshirani, 1996)\nor basis pursuit (Chen et al., 1998). Interestingly, the lasso\nis able to recover the exact support of a sparse model from\ndata generated by this model if the covariates are not too\ncorrelated (Zhao & Yu, 2006; Wainwright, 2006).\n1This work was undertaken while Guillaume Obozinski was\nafﬁliated with UC Berkeley, Department of Statistics.\nAppearing in Pr oceedings of the 26 th International Conference\non Machine Learning, Montreal, Canada, 2009. Copyright 2009\nby the author(s)/owner(s).\nWhile the ℓ 1 norm penalty leads to sparse models, it does\nnot contain any prior information about, e.g., possible\ngroups of covariates that one may wish to see selected\njointly. Several authors have recently proposed new penal-\nties to enforce the estimation of models with speciﬁc spar-\nsity patterns. For example, when the covariates are parti-\ntioned into groups, the group lasso leads to the selection\nof groups of covariates (Yuan & Lin, 2006). The group\nlasso penalty for a model, also called ℓ 1/ℓ 2 penalty, is the\nsum (i.e., ℓ 1 norm) of the ℓ 2 norms of the restrictions of the\nmodel to the different groups of covariates. It recovers the\nsupport of a model if the support is a union of groups and\nif covariates of different groups are not too correlated. It\ncan be generalized to an inﬁnite-dimensional setting (Bach,\n2008). Other variants of the group lasso include joint selec-\ntion of covariates for multi-task learning (Obozinski et al.,\n2009) and penalties to enforce hierarchical selection of co-\nvariates, e.g., when one has a hierarchy over the covariates\nand wants to select covariates only if their ancestors in the\nhierarchy are also selected (Zhao et al., 2009; Bach, 2009).\nIn this paper we are interested in a more general situation.\nWe assume that either (i) groups of covariates are given,\npotentially with overlap between the groups, and we wish\nto estimate a model whose support is a union of groups, or\n(ii) that a graph with covariates as vertices is given, and we\nwish to estimate a model whose support contains covari-\nates which tend to be connected to each others on the graph.\nAlthough quite general, this framework is motivated in par-\nticular by applications in bioinformatics, when we have to\nsolve classiﬁcation or regression problems with few sam-\nples in high dimension, such as predicting the class of a\ntumour from gene expression measurements with microar-\nrays, and simultaneously select a few genes to establish a\npredictive signature (Roth, 2002). Selecting a few genes\nthat either belong to the same functional groups, where\nthe groups are given a priori and may overlap, or tend to\nbe connected to each other in a given biological network,\n433\n\nGroup Lasso with Overlap and Graph Lasso\ncould then lead to increased interpretability of the signature\nand\npotential better performances (Rapaport et al., 2007).\nTo reach this goal, we propose and study a new penalty\nwhich generalizes theℓ 1/ℓ 2 norm to overlapping groups for\nthe ﬁrst case, and propose to cast the problem of selecting\nconnected covariates in a graph as the problem of selecting\na union of overlapping groups, with adequate deﬁnition of\ngroups, for the second case. We mention various properties\nof this penalty, and provide conditions for the consistency\nof support estimation in the regression setting. Finally, we\nreport promising results on both simulated and real data.\n2. Problem and notations\nFor any vectorw ∈ Rp, ∥w∥ denotes the Euclidean norm of\nw, and supp (w) ⊂ [1, p] denotes the support of w, i.e., the\nset of covariates i ∈ [1, p] such that wi ̸= 0. A group of co-\nvariates is a subsetg ⊂ [1, p] . The set of all possible groups\nis therefore P([1, p]) , the power set of [1, p] . Throughout\nthe paper, G ⊂ P ([1, p]) denotes a set of groups, usually\nﬁxed in advance for each application. We say that two\ngroups overlap if they have at least one covariate in com-\nmon. For any vector w ∈ Rp, and any group g ∈ G , we\ndenote wg ∈ Rp the vector whose entries are the same as\nw for the covariates in g, and are 0 for other other covari-\nates. However, we use a different convention for elements\nof VG ⊂ Rp×G the set of|G| -tuples of vectorsv = (vg)g∈G ,\nwhere each vg is this time a separate vector in Rp, which\nsatisﬁes supp (vg) ⊂ g for each g ∈ G . For any differen-\ntiable function f : Rp → R, we denote by ∇f (w) ∈ Rp\nthe gradient of f at w ∈ Rp and by ∇gf (w) ∈ Rg the\npartial gradient of f with respect to to the covariates in g.\n3. Group lasso with overlapping groups\nWhen the groups in G do not overlap, the group lasso\npenalty (Yuan & Lin, 2006) is deﬁned as:\n∀w ∈ Rp , ΩG\ngroup (w) =\n∑\ng∈G\n∥wg∥ . (1)\nWhen the groups in G form a partition of the set of covari-\nates, then ΩG\ngroup (w) is a norm whose balls have singulari-\nties when some wg are equal to zero. Minimizing a smooth\nconvex risk functional over such a ball often leads to a so-\nlution that lies on a singularity, i.e., to a vector w such that\nwg = 0 for some of the g in G.\nWhen some of the groups in G overlap, the penalty (1)\nis still a norm (if all covariates are in at least one group)\nwhose ball has singularities when some wg are equal to\nzero. Indeed, for a vector w, if we denote by G0 ⊂ G the\nset of groups such that wg = 0, then\nsupp (w) ⊂\n(⋃\ng∈G0\ng\n)c\n.\nFigure 1. Balls for ΩG\ngroup (·) (left) and ΩG\noverlap (·) (right) for the\ngroups G = {{1, 2}, {2, 3}} where w2 is represented as the ver-\ntical coordinate.\nWe see that this penalty induces the estimation of sparse\nvectors, whose support in typically the complement of\na union of groups. Although this may be relevant for\nsome applications, with appropriately designed families of\ngroups — as considered by (Jenatton et al., 2009) — , we\nare interested in this paper in penalties which induce the\nopposite effect: that the support of w be a union of groups.\nFor that purpose, we propose instead the following penalty:\nΩG\noverlap (w) = inf\nv∈VG ,P\ng∈G vg=w\n∑\ng∈G\n∥vg∥ . (2)\nWhen the groups do not overlap and form a partition of\n[1, p ], there exists a unique decomposition of w ∈ Rp as\nw = ∑\ng∈G vg with supp (vg) ⊂ g, namely, vg = wg for\nall g ∈ G . In that case, both penalties (1) and (2) are the\nsame. If some groups overlap, then we show below that this\npenalty induces the selection of w that can be decomposed\nas w = ∑\ng∈G vg where some vg are equal to 0. If we\ndenote by G1 ⊂ G the set of groups g with vg ̸= 0, then we\nimmediately get w = ∑\ng∈G1\nvg, and therefore:\nsupp (w) ⊂ ⋃\ng∈G1\ng .\nIn other words, the penalty (2) leads to sparse solutions\nwhose support is typically a union of groups, matching\nthe setting of applications that motivate this work. In the\nrest of this paper, we therefore investigate in more details\nΩG\noverlap (. ), both theoretically and empirically.\nFigure 1 shows the ball for both norms in R3 with groups\nG = {{1, 2}, {2, 3}}. The pillow shaped ball of ΩG\ngroup (≤)\nhas four singularities corresponding to cases where either\nonly w1 or only w3 is non-zero. By contrast, ΩG\noverlap (≤)\nhas two circular sets of singularities corresponding to cases\nwhere (w1, w 2) only or (w2, w 3) only is non zero.\n4. Some properties of ΩG\noverlap (.)\nWe ﬁrst analyze the decomposition of a vector w ∈ Rp as∑\ng∈G vg induced by (2). For that purpose, let V(w) ⊂ V G\n434\nGroup Lasso with Overlap and Graph Lasso\nbe the set of |G | -tuples of vectors v = (v g)g∈G which reach\nthe minimum in (2), i.e., which satisfy\nw = ∑\ng∈G vg and ΩG\noverlap (w) = ∑\ng∈G ∥vg∥ .\nThe optimization problem (2) deﬁningΩG\noverlap (w) is a con-\nvex problem and its objective is coercive, so that the set of\nsolutions V(w) is non-empty and convex. Moreover,\nLemma 1. w ↦→ ΩG\noverlap (w) is a norm.\nProof. Positive homogeneity and positive deﬁniteness hold\ntrivially. We show the triangular inequality. Consider\nw, w ′ ∈ Rp; let (vg)g∈ G and (v′\ng)g∈ G be respectively op-\ntimal decompositions of w and w′ so that ΩG\noverlap (w) =∑\ng ∥vg∥ and ΩG\noverlap (w′) = ∑\ng ∥v′\ng∥. Since (vg + v′\ng)g∈G\nis a (a priori non-optimal) decomposition of w + w′, we\nclearly have ΩG\noverlap (w + w′) ≤ ∑\ng∈G ∥vg + v′\ng∥ ≤∑\ng(∥vg∥ + ∥v′\ng∥) = Ω G\noverlap (w) + ΩG\noverlap (w′).\nUsing the conic dual of (2), we give another formulation of\nthe\nnorm ΩG\noverlap (. ) yelding some important properties.\nLemma 2. 1. It holds that:\nΩG\noverlap (w) = sup α∈Rp:∀g∈G ,∥αg∥≤1 α ⊤ w . (3)\n2. A vector α ∈ Rp is a solution of (3) if and only if there\nexists v = (vg)g∈G ∈ V(w) such that:\n∀g ∈ G , if vg ̸= 0, α g = vg\n∥vg∥ else ∥ α g∥ ≤ 1 (4)\n3. Conversely, a G-tuple of vectors v = (v g)g∈G ∈ V G\nsuch that w = ∑\ng vg is a solution to (2) if and only if\nthere exists a vector α ∈ Rp such that (4) holds.\nProof. Let us introduce slack variables t = ( tg)g∈G ∈ RG\nand rewrite the optimization problem (2) as follows:\nmin\nt∈RG ,v∈VG\n∑\ng∈G\ntg s.t.\n∑\ng∈G\nvg = w and ∀g ∈ G , ∥vg∥ ≤ tg .\nWe can form a Lagrangian for this problem with the dual\nvariables α ∈ Rp for the constraint ∑\ng∈G vg = w, and\n(β, γ ) ∈ VG × RG with ∥β g∥ ≤ γ g for the conic constraints\n∥vg∥ ≤ tg, and get:\nL =\n∑\ng∈G\ntg + α ⊤\n(\nw −\n∑\ng∈G\nvg\n)\n−\n∑\ng∈G\n(\nβ ⊤\ng vg + γ gtg\n)\n.\nThe minimum of L with respect to the primal variables t\nand v is non trivial only if γ g = 1 and α g = −β g for any\ng ∈ G. Therefore, we get the dual function:\nmin\nt,v\nL =\n{\nα ⊤ w if γ g = 1 and α g = −β g for all g ∈ G ,\n−∞ otherwise.\nBy strong duality (since,e.g., Slater’s condition is fulﬁlled),\nthe optimal value ΩG\noverlap (w) of the primal is equal to the\nmaximum of the dual problem. Maximizing this dual func-\ntion over γ g = 1, ∥β g∥ ≤ γ g and α g = −β g is equivalent\nto maximizing α ⊤ w over the vectors α ∈ Rp such that\n∥α g∥ ≤ 1 for all g ∈ G , which proves (3). To prove the\nsecond point, we note that the variables (t, v, α, β, γ ) are\nprimal/dual optimal for this convex optimization problem\nif and only if the Karush-Kuhn-Tucker (KKT) conditions\nare satisﬁed, i.e., if and only if, for all g ∈ G:\n\n\n\n\n\n\n\nsupp (vg) = g, ∥vg∥ ≤ tg and w = ∑\ng∈G vg\nsupp (β g) = g, ∥β g∥ ≤ γ g\nα g = −β g and γ g = 1\nβ ⊤\ng vg + γ gtg = 0\nEliminating β and γ with the stationarity conditions, all\nconditions are fulﬁlled if and only if w = ∑\ng∈G vg and for\nall g ∈ G , (i) either vg = 0 and ∥α g∥ ≤ 1, (ii) or vg ̸= 0\nand α g = vg/∥v g∥. If a pair (α, v) fulﬁlls these conditions,\nthen we obtain a primal/dual solution by taking tg = ∥vg∥,\nβ g = −α g and γ g = 1. This proves points 2 and 3.\nDenote by G1 the group-support of w, i.e., the set of groups\nbelonging to the support of at least one optimal decompo-\nsition of w: G1 = {g ∈ G | ∃v = (v g)g ∈ V(w), v g ̸= 0}\nand J1 the corresponding set of variables J1 = ∪g∈G1 g.\nLemma 3. Let α be an optimum in the formulation (3) of\nthe ΩG\noverlap (≤)norm, then α J1 is uniquely deﬁned.\nProof. Consider any solution v = (v g)g∈G of (2). Let α be\nany optimal solution of (3). Since(v, α ) form a primal/dual\npair, they must satisfy the KKT conditions. In particular,\nfor all g such that vg ̸= 0, α g is deﬁned uniquely by α g =\nvg\n∥vg∥. Since this is true for all solutions v ∈ V(w), α J1 is\nuniquely deﬁned.\nCorollary 1. For any v, v′∈ V(w) and for any g ∈ G,\n∥vg∥ ×\nv′\ng\n\n = 0 or ∃γg ≥ 0 s.t. v′\ng = γv g . (5)\nProof. If vg ̸= 0 and v′\ng ̸= 0, let α be solution of (3), by the\nprevious lemma α g is unique and α g = vg\n∥vg∥ =\nv′\ng\n∥v′\ng∥.\n5. Using ΩG\nov erlap (.) as a penalty\nWe now consider a learning scenario where we use\nΩG\noverlap (w) as a regularization term to the minimization of\nan objective functionR(w), typically an empirical risk. We\nassume that R(w) is convex and differentiable in w, and\nconsider the optimization problem:\nminw∈Rp R(w) + λΩ G\noverlap (w) , (6)\n435\nGroup Lasso with Overlap and Graph Lasso\nwhere λ > 0 is a regularization parameter. We ﬁrst de-\nrive optimality conditions for any solution of (6). For that\npurpose, let us denote AG(w) the set of vectors α ∈ Rp\nsolution of (3).\nLemma 4. A vector w ∈ Rp is a solution of (6) if and only\nif −∇R (w)/λ ∈ AG(w).\nProof. The proof follows from the same Lagrangian based\nderivation as for Lemma 2, adding only the loss term.\nRemark 1. By point 2 of Lemma 2, an equivalent formula-\ntion is the following: a vector w ∈ Rp is a solution of (6) if\nand only if it can be decomposed as w = ∑\ng∈G vg where,\nfor any g ∈ G, vg ∈ Rp, supp (vg) = g, and if vg = 0 then\n∥∇gR(w)∥ ≤ λ, and ∇gR(w) = −λv g/∥v g∥ otherwise.\n6. Consistency\nBefore we present a consistency result on ΩG\noverlap (.), we\nwill need the following lemma.\nLemma 5. Assume that for all w′ in a small neighbor-\nhood U of w, w′admits a unique decomposition (v′\ng)g∈G\nof minimal norm supported by the same set of groups G1\nas w. Writing η g = ∥vg∥, there exists a neighborhood U0\nof wJ1 in R|J 1| and a neighborhood U ′\n0 of (α J1 , η G1 ) in\nR|J 1|×|G 1| such that there exists a unique continuous func-\ntion φ : wJ1 ↦→ (α J1(w), η G1 (w)) from U0 to U ′\n0.\nProof. The dual problem (3) is equivalent to the saddle-\npoint problem minα maxη L′(α, η, w ) s.t. η g ∈ R+ with\nlagrangian L′(α, η, w ) = −α ⊤ w + ∑\ng∈G\nηg\n2 (∥α g∥2 − 1)\nand\nKKT conditions:\n\n\n\n\n∀g ∈ G , ∥α g∥2 ≤ 1, (primal feas.)\n∀g ∈ G , η g ≥ 0, (dual feas.)\n∀i ∈ [1, p] , −w i +\n(∑\ng∋i η g\n)\nα i = 0, (stationarity)\n∀g ∈ G , η g(∥α g∥2 − 1) = 0 , (comp.slack.)\nBy stationarity, (vg)g∈G deﬁned by vg = η gα g is a decom-\nposition of w; it is optimal because it satisﬁes property 3 of\nlemma 2; ﬁnally we have η g = ∥vg∥ consistently with our\ndeﬁnition of η g(w). For any w with the same set of support-\ning groups G1, we have ∥α g(w)∥ = 1 for all g ∈ G 1 and\nη g = 0 for all g ∈ G\\G 1. For all wJ1 with group-support no\nsmaller than G1, the corresponding pair (α J1 (w), η G1 (w))\nis therefore a solution of the set of non-linear equations:\n{\n∀i ∈ J1, −w i +\n(∑\ng∋i η g\n)\nα i = 0\n∀g ∈ G1, ∥α g∥2 − 1 = 0\n(7)\nIn other words consider the function\nF : R|J 1|×| J1|×|G 1| → R|J 1|×|G 1|\n(wJ1 , α J1 , η G1) ↦→\n((\n−w i +\n[∑\ng∋i η g\n]\nα i\n)\ni∈J1\n(∥α g∥2 − 1)g∈G1\n)\n,\nthen (7) is equivalent to F (wJ1 , α J1 , η G1 ) = 0 . We use the\nimplicit function theorem for non-differentiable function of\n(Kumagai, 1980). The theorem states that for a continuous\nfunction F : R|J 1| × R|J 1|×|G 1| → R|J 1|×|G 1| such that\nF (w0, (α 0, η 0)) = 0 , if there exist open neighborhoods\nU ⊂ R|J 1| and U ′⊂ R|J 1|×|G 1| of w0 and (α 0, η 0) respec-\ntively, such that, for all w ∈ U, F (w, ≤) :U ′→ R|J 1|×|G 1|\nis locally one-to-one then there exist open neighborhoods\nU0 ⊂ R|J 1| and U ′\n0 ⊂ R|J 1|×|G 1| of w0 and (α 0, η 0), such\nthat, for all w ∈ U0, the equation F (w, (α, η )) = 0 has a\nunique solution (α, η ) = φ( w) ∈ U ′\n0, where φ is a con-\ntinuous function from U0 into U ′\n0. By continuity of the\naddition, the product and the Euclidean norm, the above\ndeﬁned F is continuous. For each w ﬁxed, F (w, ≤)is bijec-\ntive, because of the assumption of the existence of a unique\ndecomposition in a neighborhood of w. Applying the theo-\nrem of (Kumagai, 1980) then yields the desired result.\nWe are now ready to prove the consistency of ΩG\nov erlap (. ).\nConsider the linear regression model Y = X ¯w + ǫ , where\nX ∈ Rn×p is a design matrix, Y ∈ Rp is the response\nvector and ǫ ∈ Rp is a vector of i.i.d. random variables with\nmean 0 and ﬁnite variance. We denote the true regression\nfunction by ¯w. We assume that\n1. (H1) Σ := 1\nn X ⊤ X ≻ 0\n2.\n(H2) There exists a neighborhood of ¯w in which (2)\nhas a unique solution.\nIf G1 is the set of group supporting the unique solution of\n(2), we denote G2\n∆\n= G\\G 1 and J2\n∆\n= [1 , p ]\\J 1. For con-\nvenience, for any group of covariates g we note Xg the\nn × | g | design matrix restricted to the predictors in g, and\nfor any two groups g, g ′we note Σgg ′ = X ⊤\ng Xg′. We can\nthen provide a condition under which minimizing the least-\nsquare error penalized by ΩG\noverlap (w) leads to an estimator\nwith the correct support. Consider the two conditions:\n∀g ∈ G2, ∥ΣgJ1Σ−1\nJ1J1α J1( ¯w)∥ ≤ 1 (C1)\n∀g ∈ G2, ∥ΣgJ1Σ−1\nJ1J1α J1( ¯w)∥ < 1 (C2)\nLemma 6. With assumptions (H1-2), for λ n → 0 and\nλ nn1/2 → ∞, conditions (C1) and (C2) are respectively\nnecessary and sufﬁcient for the solution of (6) to estimate\nconsistently the group-support of ¯w.\nProof. We follow the line of proof of (Bach, 2008) but\nconsider a ﬁxed design for simplicity of notations. Let\nus ﬁrst consider the subproblem of estimating a vector\nonly on the support of ¯w by using only the groups in\nJ1 in the penalty, i.e., consider w1 ∈ RJ1 a solution of\n436\nGroup Lasso with Overlap and Graph Lasso\nminwJ1 ∈RJ1\n1\n2n ∥Y − XJ1 wJ1∥2 + λ nΩG1\nov erlap (wJ1) . By\nstandard arguments, we can prove that w1 converges in\nEuclidean norm to ¯w restricted to J1 as n tends to in-\nﬁnity (Fu & Knight, 2000). In the rest of the proof we\nshow how to construct a vector w ∈ Rp from w1 which\nunder condition (C2) is with high probability a solution\nto (6). By adding null components to w1, we obtain a vec-\ntor w ∈ Rp whose support is also J1, and u = w − ¯w\ntherefore satisﬁes supp (u) ⊂ J1. A direct computation\nof the gradient of the risk R(w) = ∥Y − Xw ∥2 gives\n∇R (w) = Σu − W , where W = 1\nn Xǫ . From this\nwe deduce that u = Σ −1\nJ1J1 (∇ J1R(w) + WJ1), and since\n∇ J1 R(w) = −λ nα J1(w) we have :\n∇ J2 R(w) = Σ J2J1Σ−1\nJ1J1 (WJ1 − λ nα J1 (w)) − WJ2 .\nTo show that w is a feasible solution to (6) it is enough to\nshow that ∀g ∈ G 2, ∥∇gR(w)∥ ≤ λ n. Moreover, since\nthe noise has bounded variance, ΣJ2J1Σ−1\nJ1J1WJ1 − WJ2 =\nX ⊤\nJ2\n[ 1\nn XJ1Σ−1\nJ1J1X ⊤\nJ1 − I\n]\nǫ is √\nn-consistent and\n1\nλn\n∥∇gR(w)∥ ≤ ∥ ΣgJ1 Σ−1\nJ1J1 α J1(w)∥ + Op(λ −1\nn n−1/ 2).\nBy Lemma 5, we have that α J1 is a continuous function\nof w in a neighborhood of ¯w so that wJ1\nP\n→ ¯wJ1 im-\nplies α J1(w)\nP\n→ α J1 ( ¯w). Since we chose λ n such that\nλ −1\nn n−1/ 2 → 0, we have\n1\nλn\n∥∇gR(w)∥ ≤ ∥ ΣgJ1 Σ−1\nJ1J1α J1( ¯w)∥ + op(1).\nHence the result for the sufﬁcient condition. Symmetri-\ncally, for the necessary condition we have\n1\nλn\n∥∇gR(w)∥ ≥ ∥ ΣgJ1 Σ−1\nJ1J1α J1( ¯w)∥ − op(1).\n7. Graph lasso\nWe\nnow consider the situation where we have a simple\nundirected graph (I, E ), where the set of verticesI = [1, k ]\nis the set of covariates and E ⊂ I × I is a set of edges\nthat connect covariates. We suppose that we wish to es-\ntimate a sparse model such that selected covariates tend\nto be connected to each other, i.e., form a limited num-\nber of connected components on the graph. An obvious\napproach is to consider the prior ΩG\noverlap (. ) where G is\na set that generates by union the connected components.\nFor example, we may consider for G the set of edges,\ncliques, or small linear subgraphs. As an example, con-\nsidering all edges, i.e., G = E leads to Ωgraph(w) =\nminv∈VE\n∑\ne∈E ∥ve∥ s.t. ∑\ne∈E ve = w, supp (ve) = e .\nAlternatively, we will consider in the experiments the set\nof all linear subgraphs of length k ≥ 1. Although we have\nno formal statement on how to chose k, it intuitively con-\ntrols the size of the groups of connected variables which\nare selected, and should therefore be typically chosen to\nbe slightly smaller than the size of the minimal connected\ncomponent expected in the support of the model.\n8. Implementation\nA simple way to implement empirical risk minimization\nusing ΩG\noverlap (. ) as the regularizer is to explicitly dupli-\ncate the variables in the design matrix, i.e., to replace\nX ∈ Rn×p by ˜X ∈ Rn× P|g | deﬁned by the concatena-\ntion of copies of the design matrix restricted each to a\ncertain group g, i.e., ˜X = [ Xg1, X g2 , ..., X g|G|], where\nG = {g 1, . . . , g G}. To see this, denote ˜vg = ( vgi)i∈g and\n˜v = (˜ v⊤\ng1, . . . , ˜v⊤\ng|G|)⊤ , and consider that, for an empiri-\ncal risk of the form R(w) = ˜R(Xw ), we can eliminate w\nfrom (6) to get R(w) = ˜R(X(∑\ng vg)) = ˜R( ˜X ˜v) and thus\nfor the full objective : ˜R( ˜X ˜v) + λ ∑\ng ∥˜vg∥. That way the\nvector ˜v ∈ R\nP|g | can be directly estimated from ˜X with a\nclassical group lasso for non-overlapping groups. We im-\nplemented the approach of (Meier et al., 2008) to estimate\nthe group lasso in the expanded space. Note that (Roth\n& Fischer, 2008) provides a faster algorithm for the group\nLasso. When there are many groups with important over-\nlap however, an alternative implementation without explicit\ndata duplication, e.g., with a variational formulation simi-\nlar to the one of (Rakotomamonjy et al., 2008) might be\nmore scalable.\n9. Experiments\n9.1. Synthetic data: given overlapping groups\nTo assess the performance of our method when overlap-\nping groups are given as a priori, we simulated data with\np = 82 variables, covered by 10 groups of 10 variables\nwith 2 variables of overlap between two successive groups:\n{1, . . . , 10}, {9, . . . , 18}, . . . , {73, . . . , 82}. We chose the\nsupport of w to be the union of groups4 and 5 and sampled\nboth the support weights and the offset from i.i.d. Gaussian\nvariables. Note that in this setting, the support can be ex-\npressed as a union of groups, but not as the complement of a\nunion. Therefore, ΩG\noverlap (.) can recover the right support,\nwhereas by construction ΩG\ngroup (≤)using the same groups\nwould be unable to recover it.\nThe model is learned from n data points (xi, y i), with yi =\nw⊤ xi + ε, ε ∼ N (0, σ 2), σ = | E(Xw + b)| . Using an ℓ 2\nloss R(w) = ∥Y − Xw − b∥2, we learn models from 50\nsuch training sets. On Figure 2, for each variable (on the\nvertical axis), we plot its frequency of selection in levels of\ngray as a function of the regularization parameter λ, both\nfor the lasso penalty and ΩG\noverlap (.).\n437\nGroup Lasso with Overlap and Graph Lasso\nlog2(λ\n)\n20\n40\n60\n80\nlog2(λ\n)\n20\n40\n60\n80\nlog2(λ\n)\n20\n40\n60\n80\nlog2(λ\n)\n20\n40\n60\n80\nFigure 2. Frequency of selection of each variable with the lasso\n(left) and ΩG\noverlap (.) (right) for n = 50 (top) and 100 (bottom).\nFor any choice of λ the lasso frequently misses some vari-\nables from the support, while ΩG\noverlap (. ) never misses any\nvariable from the support for a large part of the regulariza-\ntion path. Besides, we observed that over the replicates, the\nlasso never selected the exact correct pattern for n < 100.\nFor n = 100 , the right pattern was selected with low fre-\nquency on a small part of the regularization path.ΩG\noverlap (. )\non the other hand selected it up to 92% of the times for\nn = 50 and more than 99% on more than one third of the\npath for n = 100. We tried the same experiment for various\nn and as long as n was too small for the lasso to recover the\nright support, the group regularization always helped.1 1.5 20\n2\n4\n6\n8\n10\nlog10(n)\nRMSE\n \n \noverlapping\nlasso\nFigure 3. Root mean squared error of overlapped group lasso and\nlasso as a function of the number of training points.\nFigure 3 shows the root mean squared error of both meth-\nods for various n. For both methods, the full regulariza-\ntion path is computed and tested on three replicates of n\ntraining and 100 testing points. The best average parame-\nter is selected and used to train and test a model on a fourth\nreplicate. On a large range of n, ΩG\noverlap (.), not only helps\nto recover the right pattern, improves the regression per-\nformance. A possible explanation is that if several vari-\nables from the support are correlated in the design matrix\nX, the lasso selects one and is less robust than ΩG\noverlap (. )\nwhich uses all the variables. Note that when enough train-\ning points become available (last point on Figure 3), Fig-\nure 2 shows that the selected model is generally better but\nstill not correct whereas ΩG\noverlap (. ) selects the right model,\neven if it does not give much lower error anymore.\n9.2. Synthetic data: given linear graph structure\nWe now consider that the prior given on the variables is\na graph structure and that we are interested by solutions\nwhich are connected components on this graph. As a ﬁrst\nsimple illustration, we consider a chain. We use w ∈ Rp,\np = 100 , supp (w) = [20 , 40]. The nodes of the graph are\nthe variables wi, the edges are all the pairs (wi, w i+1), i =\n1, . . . , n . The model’s weights, offset and the 50 training\nexamples (x, y ) are drawn using the same protocol as in\nthe previous experiment. We take for the groups all the\nsub-chains of length k. We present the results for various\nchoices of k and compare to the lasso (k = 1).\nlog2(λ\n)\n20\n40\n60\n80\n100\nlog2(λ\n)\n20\n40\n60\n80\n100\nlog2(λ\n)\n20\n40\n60\n80\n100\nlog2(λ\n)\n20\n40\n60\n80\n100\nFigure 4. Variable selection frequency with ΩG\noverlap (.) using the\nchains of length k (left) as groups, for k = 1, 2, 4, 8.\nFigure 4 shows the frequency of each variable selection\nover 20 replications. Here again, using a group prior helps\nthe pattern recovery. We also observe as expected that the\nchoice of k plays a role in the improvement.\n9.3. Synthetic data: given non-linear graph structure\nHere we consider the same setting as in the linear case,\nexcept that instead of a chain we are given a grid structure\non the variables. Each node is connected to the 4 nodes\nabove, below, left and right. The support is a 20-variable\nregion in the center of the grid, x-axis 4 to 7, y-axis 4 to 8.\nAs groups, we use all the 4-cycles, which is a natural prior\ngiven the graph topology and the expected pattern.\n438\nGroup Lasso with Overlap and Graph Lasso\nFigure 5 shows the variable selection frequency of each\nvariable\nfor both methods at a ﬁxedλ (chosen in both cases\nto give the best behavior). ΩG\noverlap (. ) seems to generally\ngive better selection performances than lasso.\nBesides, we observed that on each run, variables incor-\nrectly selected where always unions of groups whereas the\nlasso selected disconnected variables on the graph. We\nmade the same observation for the linear graph case. This is\nan expected property of our method, and implies that even\nif variables which are not in the model are selected, they en-\nter the model as large connected components, whereas the\nfalse positive of the lasso are more randomly distributed on\nthe graph, often as isolated variables. This is an interesting\nproperty for real applications because it may then be easier\nto discard manually a few large connected components of\nfalse positives, than many isolated variables (assuming of\ncourse that the right variables are selected as well).\n2 4 6 8 10\n2\n4\n6\n8\n10\n2 4 6 8 10\n2\n4\n6\n8\n10\nFigure 5. Grid view of the variable selection frequencies with the\ngraph setting. Left: lasso, right: ΩG\noverlap (.) using 4-cycles as\ngroups. n = 30 training points, λ is arbitrarily ﬁxed.\n9.4. Breast cancer data: pathway analysis\nAn important motivation for our method is the possibility\nto perform gene selection from microarray data using priors\nwhich are overlapping groups. For example, one may want\nto analyse microarrays in terms of biologically meaning-\nful gene sets. In most such analysis, genes discriminating\nthe classes (e.g. tumors leading to metastasis versus non-\nmetastasis) are selected in a ﬁrst step, then enrichment anal-\nysis is performed by looking for gene sets in which selected\ngenes are overrepresented (Subramanian et al., 2005). Sev-\neral organizations of the genes into gene sets are available\nin various databases. We use the canonical pathways from\nMSigDB (Subramanian et al., 2005) containing 639 groups\nof genes, 637 of which involve genes from our study.\nWe use the breast cancer dataset compiled by (Van de Vi-\njver et al., 2002), which consists of gene expression data\nfor 8, 141 genes in 295 breast cancer tumors (78 metastatic\nand 217 non-metastatic). We restrict the analysis to the\n3510 genes which are in at least one pathway. Since the\ndataset is very unbalanced, we balance it by using 3 repli-\ncates of each metastasis patient (keeping all duplicates in\nthe same fold during cross-validation).\nTable 1. Classiﬁcation error, number and proportion of pathways\nselected by the ℓ 1 and ΩG\noverlap (.) on the 3 folds.\nMETHOD ℓ 1 ΩG\nO VERLAP (.)\nERROR 0 .38 ± 0.04 0.36 ± 0.03\n♯ PATH. 148, 58, 183 6, 5, 78\nPROP. PATH. 0.32,0.14,0.41 0.01, 0.01,0.17\nWe estimate by 3-fold cross validation the accuracy of a\nlogistic regression with ℓ 1 and ΩG\noverlap (. ) penalties, using\nthe pathways as groups. As a pre-processing, we keep the\n300 genes most correlated with the output (on each training\nset). λ is selected by cross validation on each training set.\nTable 1 shows the results of both methods. Using\nΩG\noverlap (. ) instead of the ℓ 1 penalty leads to a slight\nimprovement in the prediction performances, and much\nsparser solutions at the pathway level, which makes the se-\nlected model easier to interpret.\n9.5. Breast cancer data: graph analysis\nAnother important application in microarray data analysis\nis the search for potential drug targets. In order to iden-\ntify genes which are related to a disease, one would like\nto ﬁnd groups of genes forming connected components on\na graph carrying biological information such as regulation,\ninvolvement in the same chain of metabolic reactions, or\nprotein-protein interaction. Similarly to what is done in\npathway analysis, (Chuang et al., 2007) built a network by\ncompiling several biological networks and performed such\ngraph analysis by identifying discriminant subnetworks in\none step and using these subnetworks to learn a classiﬁer\nin a separate step. We use this network and the approach\ndescribed in section 7, taking all the edges on the network\nas the groups, on the breast cancer dataset. Here again,\nwe restrict the data to the 7910 genes which are present\nin the network, and use the same correlation-based pre-\nprocessing as for the pathway analysis.\nTable 2 shows the results of the logistic regression with\nℓ 1 and ΩG\noverlap (. ). Here again, both methods give similar\nperformances, with a slight advantage for ΩG\noverlap (.). On\nthe other hand, while the ℓ 1 mostly selects disconnected\nvariables on the graph, ΩG\noverlap (. ) tends to select variables\nwhich are grouped into larger connected components on the\ngraph. This would make the interpretation and the search\nfor new drug targets easier.\n10. Discussion\nWe have presented a generalization of the group lasso\npenalty, which leads to sparse models with sparsity pat-\n439\nGroup Lasso with Overlap and Graph Lasso\nTable 2. Classiﬁcation error and average size of the connected\ncomponents selected by the ℓ 1 and ΩG\noverlap (.) on the 3 folds.\nMETHOD ℓ 1 ΩG\nO VERLAP (.)\nERROR 0 .39 ± 0.04 0.36 ± 0.01\nAV. SIZE C .C. 1.1,1, 1.0 1.3, 1.4,1.2\nterns that are unions of pre-deﬁned groups of covariates,\nor\n, given a graph of covariates, groups of connected covari-\nates in the graph. We obtained promising results on both\nsimulated and real data.\nFrom a theoretical point of view, we gave both sufﬁcient\nand necessary conditions for the correct recovery of the\nsame union of groups as in the decomposition induced by\nΩG\noverlap (≤)on the true optimal parameter vector. It still re-\nmains to characterize when the latter decomposition has the\nsmallest number of groups. The situation where several de-\ncompositions exist should be analyzed. Also, the construc-\ntion of an adaptive version of the Group Lasso with over-\nlap that could possibly generalize the scheme proposed by\n(Bach, 2008) would be of interest.\nFrom a practical point of view, although algorithms for the\nstandard group Lasso can be used to implement ΩG\noverlap (≤),\nmore dedicated and scalable algorithms could be designed\nfor cases with large overlaps.\nFuture work should compare more systematically\nΩG\noverlap (≤)and ΩG\ngroup (≤)empirically and theoretically.\nAcknowledgments\nThis work was supported by ANR grant ANR-07-BLAN-\n0311 and the France-Berkeley fund. The authors thank Bin\nYu and Michael Jordan for useful discussions.\nReferences\nBach, F. (2008). Consistency of the group lasso and multi-\nple kernel learning. J. Mach. Learn. Res., 9, 1179–1225.\nBach, F. (2009). Exploring large feature spaces with hier-\narchical multiple kernel learning. Adv. Neural. Inform.\nProcess Syst., 105–112.\nChen, S. S., Donoho, D. L., & Saunders, M. (1998).\nAtomic decomposition by basis pursuit. SIAM J. Sci.\nComput., 20, 33–61.\nChuang, H.-Y ., Lee, E., Liu, Y .-T., Lee, D., & Ideker, T.\n(2007). Network-based classiﬁcation of breast cancer\nmetastasis. Mol. Syst. Biol., 3, 140.\nFu, W., & Knight, K. (2000). Asymptotics for Lasso-type\nestimators. Ann. Stat., 28, 1356–1378.\nJenatton, R., Audibert, J.-Y ., & Bach, F. (2009). Struc-\ntured Variable Selection with Sparsity-Inducing Norms.\nINRIA - Ecole Normale Supérieure de Paris.\nKumagai, S. (1980). An implicit function theorem: Com-\nment. J. Optim. Theor. Appl., 31, 285–288.\nMeier, L., van de Geer, S., & Bühlmann, P. (2008). The\ngroup lasso for logistic regression. J. Roy. Stat. Soc. B,\n70, 53–71.\nObozinski, G., Taskar, B., & Jordan, M. (2009). Joint co-\nvariate selection and joint subspace selection for multi-\nple classiﬁcation problems. Stat. Comput.. To appear.\nRakotomamonjy, A., Bach, F., Canu, S., & Grandvalet, Y .\n(2008). SimpleMKL. J. Mach. Learn. Res., 9, 2491–\n2521.\nRapaport, F., Zynoviev, A., Dutreix, M., Barillot, E., &\nVert, J.-P. (2007). Classiﬁcation of microarray data using\ngene networks. BMC Bioinformatics, 8, 35.\nRoth, V . (2002). The generalized lasso: a wrapper approach\nto gene selection for microarray data. Proc. Conference\non Automated Deduction 14, 252–255.\nRoth, V ., & Fischer, B. (2008). The group-lasso for gen-\neralized linear models: uniqueness of solutions and efﬁ-\ncient algorithms. Int. Conf. Mach. Learn., 848–855.\nSubramanian, A., et al., (2005). Gene set enrichment\nanalysis: a knowledge-based approach for interpreting\ngenome-wide expression proﬁles. Proc. Natl. Acad. Sci.\nUSA, 102, 15545–15550.\nTibshirani, R. (1996). Regression shrinkage and selection\nvia the lasso. J. Royal. Statist. Soc. B., 58, 267–288.\nVan de Vijver, M. J., et al., (2002). A gene-expression sig-\nnature as a predictor of survival in breast cancer.N. Engl.\nJ. Med., 347, 1999–2009.\nWainwright, M. J. (2006). Sharp thresholds for high-\ndimensional and noisy recovery of sparsity (Technical\nReport 709). UC Berkeley, Department of Statistics.\nYuan, M., & Lin, Y . (2006). Model selection and estimation\nin regression with grouped variables. J. R. Stat. Soc. Ser.\nB, 68, 49–67.\nZhao, P., Rocha, G., & Yu, B. (2009). Grouped and hi-\nerarchical model selection through composite absolute\npenalties. Ann. Stat. To appear.\nZhao, P., & Yu, B. (2006). On model selection consistency\nof lasso. J. Mach. Learn. Res., 7, 2541–2563.\n440",
  "values": {
    "Interpretable (to users)": "Yes",
    "Critiqability": "Yes",
    "Explicability": "Yes",
    "User influence": "Yes",
    "Transparent (to users)": "Yes",
    "Collective influence": "Yes",
    "Fairness": "Yes",
    "Justice": "Yes",
    "Not socially biased": "Yes",
    "Respect for Persons": "Yes",
    "Privacy": "Yes",
    "Non-maleficence": "Yes",
    "Respect for Law and public interest": "Yes",
    "Deferral to humans": "Yes",
    "Beneficence": "Yes",
    "Autonomy (power to decide)": "Yes"
  }
}