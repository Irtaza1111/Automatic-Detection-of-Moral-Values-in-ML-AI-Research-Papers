{
  "pdf": "1553374.1553453",
  "title": "Convolutional deep belief networks for scalable unsupervised learning of hierarchical representations",
  "author": "Honglak Lee, Roger Grosse, Rajesh Ranganath, Andrew Y. Ng",
  "paper_id": "1553374.1553453",
  "text": "Convolutional Deep Belief Networks\nfor Scalable Unsupervised Learning of Hierarchical Representations\nHonglak Lee hllee@cs.stanford.edu\nRoger Grosse rgrosse@cs.stanford.edu\nRajesh Ranganath rajeshr@cs.stanford.edu\nAndrew Y. Ng ang@cs.stanford.edu\nComputer Science Department, Stanford University, Stanford, CA 94305, USA\nAbstract\nThere has been much interest in unsuper-\nvised learning of hierarchical generative mod-\nels such as deep belief networks. Scaling\nsuch models to full-sized, high-dimensional\nimages remains a diﬃcult problem. To ad-\ndress this problem, we present the convolu-\ntional deep belief network, a hierarchical gen-\nerative model which scales to realistic image\nsizes. This model is translation-invariant and\nsupports eﬃcient bottom-up and top-down\nprobabilistic inference. Key to our approach\nis probabilistic max-pooling, a novel technique\nwhich shrinks the representations of higher\nlayers in a probabilistically sound way. Our\nexperiments show that the algorithm learns\nuseful high-level visual features, such as ob-\nject parts, from unlabeled images of objects\nand natural scenes. We demonstrate excel-\nlent performance on several visual recogni-\ntion tasks and show that our model can per-\nform hierarchical (bottom-up and top-down)\ninference over full-sized images.\n1. Introduction\nThe visual world can be described at many levels: pixel\nintensities, edges, object parts, objects, and beyond.\nThe prospect of learning hierarchical models which\nsimultaneously represent multiple levels has recently\ngenerated much interest. Ideally, such “deep” repre-\nsentations would learn hierarchies of feature detectors,\nand further be able to combine top-down and bottom-\nup processing of an image. For instance, lower layers\ncould support object detection by spotting low-level\nfeatures indicative of object parts. Conversely, infor-\nmation about objects in the higher layers could resolve\nAppearing in Proceedings of the 26th International Confer-\nence on Machine Learning, Montreal, Canada, 2009. Copy-\nright 2009 by the author(s)/owner(s).\nlower-level ambiguities in the image or infer the loca-\ntions of hidden object parts.\nDeep architectures consist of feature detector units ar-\nranged in layers. Lower layers detect simple features\nand feed into higher layers, which in turn detect more\ncomplex features. There have been several approaches\nto learning deep networks (LeCun et al., 1989; Bengio\net al., 2006; Ranzato et al., 2006; Hinton et al., 2006).\nIn particular, the deep belief network (DBN) (Hinton\net al., 2006) is a multilayer generative model where\neach layer encodes statistical dependencies among the\nunits in the layer below it; it is trained to (approxi-\nmately) maximize the likelihood of its training data.\nDBNs have been successfully used to learn high-level\nstructure in a wide variety of domains, including hand-\nwritten digits (Hinton et al., 2006) and human motion\ncapture data (Taylor et al., 2007). We build upon the\nDBN in this paper because we are interested in learn-\ning a generative model of images which can be trained\nin a purely unsupervised manner.\nWhile DBNs have been successful in controlled do-\nmains, scaling them to realistic-sized (e.g., 200x200\npixel) images remains challenging for two reasons.\nFirst, images are high-dimensional, so the algorithms\nmust scale gracefully and be computationally tractable\neven when applied to large images. Second, objects\ncan appear at arbitrary locations in images; thus it\nis desirable that representations be invariant at least\nto local translations of the input. We address these\nissues by incorporating translation invariance. Like\nLeCun et al. (1989) and Grosse et al. (2007), we\nlearn feature detectors which are shared among all lo-\ncations in an image, because features which capture\nuseful information in one part of an image can pick up\nthe same information elsewhere. Thus, our model can\nrepresent large images using only a small number of\nfeature detectors.\nThis paper presents the convolutional deep belief net-\nwork, a hierarchical generative model that scales to\nfull-sized images. Another key to our approach is\n609\n\nConvolutional Deep Belief Networks for Scalable Unsupervised Learning of Hierarchical Representations\nprobabilistic max-pooling, a novel technique that allows\nhigher-layer units to cover larger areas of the input in a\nprobabilistically sound way. To the best of our knowl-\nedge, ours is the ﬁrst translation invariant hierarchical\ngenerative model which supports both top-down and\nbottom-up probabilistic inference and scales to real-\nistic image sizes. The ﬁrst, second, and third layers\nof our network learn edge detectors, object parts, and\nobjects respectively. We show that these representa-\ntions achieve excellent performance on several visual\nrecognition tasks and allow “hidden” object parts to\nbe inferred from high-level object information.\n2. Preliminaries\n2.1. Restricted Boltzmann machines\nThe restricted Boltzmann machine (RBM) is a two-\nlayer, bipartite, undirected graphical model with a set\nof binary hidden units h, a set of (binary or real-\nvalued) visible units v, and symmetric connections be-\ntween these two layers represented by a weight matrix\nW . The probabilistic semantics for an RBM is deﬁned\nby its energy function as follows:\nP (v, h) = 1\nZ exp(−E(v, h)),\nwhere Z is the partition function. If the visible units\nare binary-valued, we deﬁne the energy function as:\nE(v, h) =−\n∑\ni,j\nviWijhj−\n∑\nj\nbjhj−\n∑\ni\ncivi,\nwherebj are hidden unit biases and ci are visible unit\nbiases. If the visible units are real-valued, we can de-\nﬁne the energy function as:\nE(v, h) = 1\n2\n∑\ni\nv2\ni−\n∑\ni,j\nviWijhj−\n∑\nj\nbjhj−\n∑\ni\ncivi.\nFrom the energy function, it is clear that the hid-\nden units are conditionally independent of one another\ngiven the visible layer, and vice versa. In particular,\nthe units of a binary layer (conditioned on the other\nlayer) are independent Bernoulli random variables. If\nthe visible layer is real-valued, the visible units (condi-\ntioned on the hidden layer) are Gaussian with diagonal\ncovariance. Therefore, we can perform eﬃcient block\nGibbs sampling by alternately sampling each layer’s\nunits (in parallel) given the other layer. We will often\nrefer to a unit’s expected value as its activation.\nIn principle, the RBM parameters can be optimized\nby performing stochastic gradient ascent on the log-\nlikelihood of training data. Unfortunately, computing\nthe exact gradient of the log-likelihood is intractable.\nInstead, one typically uses the contrastive divergence\napproximation (Hinton, 2002), which has been shown\nto work well in practice.\n2.2. Deep belief networks\nThe RBM by itself is limited in what it can represent.\nIts real power emerges when RBMs are stacked to form\na deep belief network, a generative model consisting of\nmany layers. In a DBN, each layer comprises a set of\nbinary or real-valued units. Two adjacent layers have a\nfull set of connections between them, but no two units\nin the same layer are connected. Hinton et al. (2006)\nproposed an eﬃcient algorithm for training deep belief\nnetworks, by greedily training each layer (from low-\nest to highest) as an RBM using the previous layer’s\nactivations as inputs. This procedure works well in\npractice.\n3. Algorithms\nRBMs and DBNs both ignore the 2-D structure of im-\nages, so weights that detect a given feature must be\nlearned separately for every location. This redundancy\nmakes it diﬃcult to scale these models to full images.\n(However, see also (Raina et al., 2009).) In this sec-\ntion, we introduce our model, the convolutional DBN,\nwhose weights are shared among all locations in an im-\nage. This model scales well because inference can be\ndone eﬃciently using convolution.\n3.1. Notation\nFor notational convenience, we will make several sim-\nplifying assumptions. First, we assume that all inputs\nto the algorithm are NV ×NV images, even though\nthere is no requirement that the inputs be square,\nequally sized, or even two-dimensional. We also as-\nsume that all units are binary-valued, while noting\nthat it is straightforward to extend the formulation to\nthe real-valued visible units (see Section 2.1). We use\n∗ to denote convolution1, and• to denote element-wise\nproduct followed by summation, i.e., A•B = trATB.\nWe place a tilde above an array ( ˜A) to denote ﬂipping\nthe array horizontally and vertically.\n3.2. Convolutional RBM\nFirst, we introduce the convolutional RBM (CRBM).\nIntuitively, the CRBM is similar to the RBM, but\nthe weights between the hidden and visible layers are\nshared among all locations in an image. The basic\nCRBM consists of two layers: an input layer V and a\nhidden layerH (corresponding to the lower two layers\nin Figure 1). The input layer consists of an NV×NV\narray of binary units. The hidden layer consists of K\n“groups”, where each group is an NH×NH array of\nbinary units, resulting in NH\n2K hidden units. Each\nof the K groups is associated with a NW×NW ﬁlter\n1The convolution of anm×m array with ann×n array\nmay result in an (m +n− 1)× (m +n− 1) array or an\n(m−n + 1)× (m−n + 1) array. Rather than invent a\ncumbersome notation to distinguish these cases, we let it\nbe determined by context.\n610\nConvolutional Deep Belief Networks for Scalable Unsupervised Learning of Hierarchical Representations\n(NW ≜ NV−NH + 1); the ﬁlter weights are shared\nacross all the hidden units within the group. In addi-\ntion, each hidden group has a bias bk and all visible\nunits share a single bias c.\nWe deﬁne the energy function E(v, h) as:\nP (v, h) = 1\nZ exp(−E(v, h))\nE(v, h) = −\nK∑\nk=1\nNH∑\ni,j=1\nNW∑\nr,s=1\nhk\nijWk\nrsvi+r−1,j+s−1\n−\nK∑\nk=1\nbk\nNH∑\ni,j=1\nhk\nij−c\nNV∑\ni,j=1\nvij. (1)\nUsing the operators deﬁned previously,\nE(v, h) =−\nK∑\nk=1\nhk• ( ˜Wk∗v)−\nK∑\nk=1\nbk\n∑\ni,j\nhk\ni,j−c\n∑\ni,j\nvij.\nAs with standard RBMs (Section 2.1), we can perform\nblock Gibbs sampling using the following conditional\ndistributions:\nP (hk\nij = 1|v) = σ(( ˜Wk∗v)ij +bk)\nP (vij = 1|h) = σ((\n∑\nk\nWk∗hk)ij +c),\nwhereσ is the sigmoid function. Gibbs sampling forms\nthe basis of our inference and learning algorithms.\n3.3. Probabilistic max-pooling\nIn order to learn high-level representations, we stack\nCRBMs into a multilayer architecture analogous to\nDBNs. This architecture is based on a novel opera-\ntion that we call probabilistic max-pooling.\nIn general, higher-level feature detectors need informa-\ntion from progressively larger input regions. Existing\ntranslation-invariant representations, such as convolu-\ntional networks, often involve two kinds of layers in\nalternation: “detection” layers, whose responses are\ncomputed by convolving a feature detector with the\nprevious layer, and “pooling” layers, which shrink the\nrepresentation of the detection layers by a constant\nfactor. More speciﬁcally, each unit in a pooling layer\ncomputes the maximum activation of the units in a\nsmall region of the detection layer. Shrinking the rep-\nresentation with max-pooling allows higher-layer rep-\nresentations to be invariant to small translations of the\ninput and reduces the computational burden.\nMax-pooling was intended only for feed-forward archi-\ntectures. In contrast, we are interested in a generative\nmodel of images which supports both top-down and\nbottom-up inference. Therefore, we designed our gen-\nerative model so that inference involves max-pooling-\nlike behavior.\nv\nWk\nhk\ni,j\npk\nα\nV  (visible layer)\nHk (detection layer)\nPk (pooling layer)\nNH\nNV\nC\nNW\nNP\nFigure 1. Convolutional RBM with probabilistic max-\npooling. For simplicity, only group k of the detection layer\nand the pooing layer are shown. The basic CRBM corre-\nsponds to a simpliﬁed structure with only visible layer and\ndetection (hidden) layer. See text for details.\nTo simplify the notation, we consider a model with a\nvisible layerV , a detection layerH, and a pooling layer\nP , as shown in Figure 1. The detection and pooling\nlayers both have K groups of units, and each group\nof the pooling layer has NP×NP binary units. For\neach k∈{1,...,K }, the pooling layer Pk shrinks the\nrepresentation of the detection layer Hk by a factor\nof C along each dimension, where C is a small in-\nteger such as 2 or 3. I.e., the detection layer Hk is\npartitioned into blocks of size C×C, and each block\nα is connected to exactly one binary unit pk\nα in the\npooling layer (i.e., NP =NH/C). Formally, we deﬁne\nBα ≜{(i,j ) :hij belongs to the block α.}.\nThe detection units in the block Bα and the pooling\nunit pα are connected in a single potential which en-\nforces the following constraints: at most one of the\ndetection units may be on, and the pooling unit is on\nif and only if a detection unit is on. Equivalently, we\ncan consider theseC 2+1 units as a single random vari-\nable which may take on one of C 2 + 1 possible values:\none value for each of the detection units being on, and\none value indicating that all units are oﬀ.\nWe formally deﬁne the energy function of this simpli-\nﬁed probabilistic max-pooling-CRBM as follows:\nE(v, h) = −\nX\nk\nX\ni,j\n“\nhk\ni,j( ˜Wk∗v)i,j +bkhk\ni,j\n”\n−c\nX\ni,j\nvi,j\nsubj. to\nX\n(i,j)∈Bα\nhk\ni,j≤ 1, ∀k,α.\nWe now discuss sampling the detection layer H and\nthe pooling layer P given the visible layerV . Group k\nreceives the following bottom-up signal from layer V :\nI(hk\nij) ≜bk + ( ˜Wk∗v)ij. (2)\nNow, we sample each block independently as a multi-\nnomial function of its inputs. Suppose hk\ni,j is a hid-\nden unit contained in block α (i.e., (i,j )∈ Bα), the\n611\nConvolutional Deep Belief Networks for Scalable Unsupervised Learning of Hierarchical Representations\nincrease in energy caused by turning on unit hk\ni,j is\n−I(hk\ni,j), and the conditional probability is given by:\nP (hk\ni,j = 1|v) = exp(I(hk\ni,j))\n1 + ∑\n(i′,j′)∈Bα\nexp(I(hk\ni′,j′))\nP (pk\nα = 0|v) = 1\n1 + ∑\n(i′,j′)∈Bα\nexp(I(hk\ni′,j′)).\nSampling the visible layer V given the hidden layer\nH can be performed in the same way as described in\nSection 3.2.\n3.4. Training via sparsity regularization\nOur model is overcomplete in that the size of the rep-\nresentation is much larger than the size of the inputs.\nIn fact, since the ﬁrst hidden layer of the network con-\ntains K groups of units, each roughly the size of the\nimage, it is overcomplete roughly by a factor of K. In\ngeneral, overcomplete models run the risk of learning\ntrivial solutions, such as feature detectors represent-\ning single pixels. One common solution is to force the\nrepresentation to be “sparse,” in that only a tiny frac-\ntion of the units should be active in relation to a given\nstimulus (Olshausen & Field, 1996; Lee et al., 2008).\nIn our approach, like Lee et al. (2008), we regularize\nthe objective function (data log-likelihood) to encour-\nage each of the hidden units to have a mean activation\nclose to some small constant ρ. For computing the\ngradient of sparsity regularization term, we followed\nLee et al. (2008)’s method.\n3.5. Convolutional deep belief network\nFinally, we are ready to deﬁne the convolutional deep\nbelief network (CDBN), our hierarchical generative\nmodel for full-sized images. Analogously to DBNs, this\narchitecture consists of several max-pooling-CRBMs\nstacked on top of one another. The network deﬁnes an\nenergy function by summing together the energy func-\ntions for all of the individual pairs of layers. Training\nis accomplished with the same greedy, layer-wise pro-\ncedure described in Section 2.2: once a given layer is\ntrained, its weights are frozen, and its activations are\nused as input to the next layer.\n3.6. Hierarchical probabilistic inference\nOnce the parameters have all been learned, we com-\npute the network’s representation of an image by sam-\npling from the joint distribution over all of the hidden\nlayers conditioned on the input image. To sample from\nthis distribution, we use block Gibbs sampling, where\nthe units of each layer are sampled in parallel (see Sec-\ntions 2.1 & 3.3).\nTo illustrate the algorithm, we describe a case with one\nvisible layerV , a detection layerH, a pooling layer P ,\nand another, subsequently-higher detection layer H′.\nSuppose H′ has K′ groups of nodes, and there is a\nset of shared weights Γ = {Γ1,1,..., ΓK,K ′\n}, where\nΓk,ℓ is a weight matrix connecting pooling unit Pk to\ndetection unit H′ℓ. The deﬁnition can be extended to\ndeeper networks in a straightforward way.\nNote that an energy function for this sub-network con-\nsists of two kinds of potentials: unary terms for each\nof the groups in the detection layers, and interaction\nterms between V and H and between P and H′:\nE(v, h, p, h′) = −\n∑\nk\nv• (Wk∗hk)−\n∑\nk\nbk\n∑\nij\nhk\nij\n−\n∑\nk,ℓ\npk• (Γkℓ∗h′ℓ)−\n∑\nℓ\nb′\nℓ\n∑\nij\nh′ℓ\nij\nTo sample the detection layer H and pooling layer P ,\nnote that the detection layerHk receives the following\nbottom-up signal from layer V :\nI(hk\nij) ≜bk + ( ˜Wk∗v)ij, (3)\nand the pooling layer Pk receives the following top-\ndown signal from layer H′:\nI(pk\nα) ≜\n∑\nℓ\n(Γkℓ∗h′ℓ)α. (4)\nNow, we sample each of the blocks independently as a\nmultinomial function of their inputs, as in Section 3.3.\nIf (i,j )∈Bα, the conditional probability is given by:\nP (hk\ni,j = 1|v, h′) = exp(I(hk\ni,j) +I(pk\nα))\n1 +P\n(i′,j′)∈Bα exp(I(hk\ni′,j′) +I(pkα))\nP (pk\nα = 0|v, h′) = 1\n1 +P\n(i′,j′)∈Bα exp(I(hk\ni′,j′) +I(pkα)).\nAs an alternative to block Gibbs sampling, mean-ﬁeld\ncan be used to approximate the posterior distribution.2\n3.7. Discussion\nOur model used undirected connections between lay-\ners. This contrasts with Hinton et al. (2006), which\nused undirected connections between the top two lay-\ners, and top-down directed connections for the layers\nbelow. Hinton et al. (2006) proposed approximat-\ning the posterior distribution using a single bottom-up\npass. This feed-forward approach often can eﬀectively\nestimate the posterior when the image contains no oc-\nclusions or ambiguities, but the higher layers cannot\nhelp resolve ambiguities in the lower layers. Although\nGibbs sampling may more accurately estimate the pos-\nterior in this network, applying block Gibbs sampling\nwould be diﬃcult because the nodes in a given layer\n2In all our experiments except for Section 4.5, we used\nthe mean-ﬁeld approximation to estimate the hidden layer\nactivations given the input images. We found that ﬁve\nmean-ﬁeld iterations suﬃced.\n612\nConvolutional Deep Belief Networks for Scalable Unsupervised Learning of Hierarchical Representations\nare not conditionally independent of one another given\nthe layers above and below. In contrast, our treatment\nusing undirected edges enables combining bottom-up\nand top-down information more eﬃciently, as shown\nin Section 4.5.\nIn our approach, probabilistic max-pooling helps to\naddress scalability by shrinking the higher layers;\nweight-sharing (convolutions) further speeds up the\nalgorithm. For example, inference in a three-layer\nnetwork (with 200x200 input images) using weight-\nsharing but without max-pooling was about 10 times\nslower. Without weight-sharing, it was more than 100\ntimes slower.\nIn work that was contemporary to and done indepen-\ndently of ours, Desjardins and Bengio (2008) also ap-\nplied convolutional weight-sharing to RBMs and ex-\nperimented on small image patches. Our work, how-\never, develops more sophisticated elements such as\nprobabilistic max-pooling to make the algorithm more\nscalable.\n4. Experimental results\n4.1. Learning hierarchical representations\nfrom natural images\nWe ﬁrst tested our model’s ability to learn hierarchi-\ncal representations of natural images. Speciﬁcally, we\ntrained a CDBN with two hidden layers from the Ky-\noto natural image dataset. 3 The ﬁrst layer consisted\nof 24 groups (or “bases”) 4 of 10x10 pixel ﬁlters, while\nthe second layer consisted of 100 bases, each one 10x10\nas well.5 As shown in Figure 2 (top), the learned ﬁrst\nlayer bases are oriented, localized edge ﬁlters; this re-\nsult is consistent with much prior work (Olshausen &\nField, 1996; Bell & Sejnowski, 1997; Ranzato et al.,\n2006). We note that the sparsity regularization dur-\ning training was necessary for learning these oriented\nedge ﬁlters; when this term was removed, the algo-\nrithm failed to learn oriented edges.\nThe learned second layer bases are shown in Fig-\nure 2 (bottom), and many of them empirically re-\nsponded selectively to contours, corners, angles, and\nsurface boundaries in the images. This result is qual-\nitatively consistent with previous work (Ito & Ko-\nmatsu, 2004; Lee et al., 2008).\n4.2. Self-taught learning for object recognition\nRaina et al. (2007) showed that large unlabeled data\ncan help in supervised learning tasks, even when the\n3http://www.cnbc.cmu.edu/cplab/data_kyoto.html\n4We will call one hidden group’s weights a “basis.”\n5Since the images were real-valued, we used Gaussian\nvisible units for the ﬁrst-layer CRBM. The pooling ratio C\nfor each layer was 2, so the second-layer bases cover roughly\ntwice as large an area as the ﬁrst-layer ones.\nFigure 2. The ﬁrst layer bases (top) and the second layer\nbases (bottom) learned from natural images. Each second\nlayer basis (ﬁlter) was visualized as a weighted linear com-\nbination of the ﬁrst layer bases.\nunlabeled data do not share the same class labels, or\nthe same generative distribution, as the labeled data.\nThis framework, where generic unlabeled data improve\nperformance on a supervised learning task, is known\nas self-taught learning. In their experiments, they used\nsparse coding to train a single-layer representation,\nand then used the learned representation to construct\nfeatures for supervised learning tasks.\nWe used a similar procedure to evaluate our two-layer\nCDBN, described in Section 4.1, on the Caltech-101\nobject classiﬁcation task. 6 The results are shown in\nTable 1. First, we observe that combining the ﬁrst\nand second layers signiﬁcantly improves the classiﬁca-\ntion accuracy relative to the ﬁrst layer alone. Overall,\nwe achieve 57.7% test accuracy using 15 training im-\nages per class, and 65.4% test accuracy using 30 train-\ning images per class. Our result is competitive with\nstate-of-the-art results using highly-specialized single\nfeatures, such as SIFT, geometric blur, and shape-\ncontext (Lazebnik et al., 2006; Berg et al., 2005; Zhang\net al., 2006). 7 Recall that the CDBN was trained en-\n6Details: Given an image from the Caltech-101\ndataset (Fei-Fei et al., 2004), we scaled the image so that\nits longer side was 150 pixels, and computed the activations\nof the ﬁrst and second (pooling) layers of our CDBN. We\nrepeated this procedure after reducing the input image by\nhalf and concatenated all the activations to construct fea-\ntures. We used an SVM with a spatial pyramid matching\nkernel for classiﬁcation, and the parameters of the SVM\nwere cross-validated. We randomly selected 15/30 training\nset and 15/30 test set images respectively, and normal-\nized the result such that classiﬁcation accuracy for each\nclass was equally weighted (following the standard proto-\ncol). We report results averaged over 10 random trials.\n7Varma and Ray (2007) reported better performance\nthan ours (87.82% for 15 training images/class), but they\ncombined many state-of-the-art features (or kernels) to im-\nprove the performance. In another approach, Yu et al.\n(2009) used kernel regularization using a (previously pub-\nlished) state-of-the-art kernel matrix to improve the per-\n613\nConvolutional Deep Belief Networks for Scalable Unsupervised Learning of Hierarchical Representations\nTable 1. Classiﬁcation accuracy for the Caltech-101 data\nTraining Size 15 30\nCDBN (ﬁrst layer) 53.2±1.2% 60.5±1.1%\nCDBN (ﬁrst+second layers) 57.7±1.5% 65.4±0.5%\nRaina et al. (2007) 46.6% -\nRanzato et al. (2007) - 54.0%\nMutch and Lowe (2006) 51.0% 56.0%\nLazebnik et al. (2006) 54.0% 64.6%\nZhang et al. (2006) 59.0±0.56% 66.2±0.5%\ntirely from natural scenes, which are completely un-\nrelated to the classiﬁcation task. Hence, the strong\nperformance of these features implies that our CDBN\nlearned a highly general representation of images.\n4.3. Handwritten digit classiﬁcation\nWe further evaluated the performance of our model\non the MNIST handwritten digit classiﬁcation task,\na widely-used benchmark for testing hierarchical rep-\nresentations. We trained 40 ﬁrst layer bases from\nMNIST digits, each 12x12 pixels, and 40 second layer\nbases, each 6x6. The pooling ratio C was 2 for both\nlayers. The ﬁrst layer bases learned “strokes” that\ncomprise the digits, and the second layer bases learned\nbigger digit-parts that combine the strokes. We con-\nstructed feature vectors by concatenating the ﬁrst and\nsecond (pooling) layer activations, and used an SVM\nfor classiﬁcation using these features. For each labeled\ntraining set size, we report the test error averaged over\n10 randomly chosen training sets, as shown in Table 2.\nFor the full training set, we obtained 0.8% test error.\nOur result is comparable to the state-of-the-art (Ran-\nzato et al., 2007; Weston et al., 2008). 8\n4.4. Unsupervised learning of object parts\nWe now show that our algorithm can learn hierarchi-\ncal object-part representations in an unsupervised set-\nting. Building on the ﬁrst layer representation learned\nfrom natural images, we trained two additional CDBN\nlayers using unlabeled images from single Caltech-101\ncategories.9 As shown in Figure 3, the second layer\nlearned features corresponding to object parts, even\nthough the algorithm was not given any labels speci-\nfying the locations of either the objects or their parts.\nThe third layer learned to combine the second layer’s\npart representations into more complex, higher-level\nfeatures. Our model successfully learned hierarchi-\ncal object-part representations of most of the other\nCaltech-101 categories as well. We note that some of\nformance of their convolutional neural network model.\n8We note that Hinton and Salakhutdinov (2006)’s\nmethod is non-convolutional.\n9The images were unlabeled in that the position of the\nobject is unspeciﬁed. Training was on up to 100 images,\nand testing was on diﬀerent images than the training set.\nThe pooling ratio for the ﬁrst layer was set as 3. The\nsecond layer contained 40 bases, each 10x10, and the third\nlayer contained 24 bases, each 14x14. The pooling ratio in\nboth cases was 2.\nthese categories (such as elephants and chairs) have\nfairly high intra-class appearance variation, due to de-\nformable shapes or diﬀerent viewpoints. Despite this,\nour model still learns hierarchical, part-based repre-\nsentations fairly robustly.\nHigher layers in the CDBN learn features which are\nnot only higher level, but also more speciﬁc to particu-\nlar object categories. We now quantitatively measure\nthe speciﬁcity of each layer by determining how in-\ndicative each individual feature is of object categories.\n(This contrasts with most work in object classiﬁca-\ntion, which focuses on the informativeness of the en-\ntire feature set, rather than individual features.) More\nspeciﬁcally, we consider three CDBNs trained on faces,\nmotorbikes, and cars, respectively. For each CDBN,\nwe test the informativeness of individual features from\neach layer for distinguishing among these three cate-\ngories. For each feature,10 we computed area under the\nprecision-recall curve (larger means more speciﬁc). 11\nAs shown in Figure 4, the higher-level representations\nare more selective for the speciﬁc object class.\nWe further tested if the CDBN can learn hierarchi-\ncal object-part representations when trained on im-\nages from several object categories (rather than just\none). We trained the second and third layer represen-\ntations using unlabeled images randomly selected from\nfour object categories (cars, faces, motorbikes, and air-\nplanes). As shown in Figure 3 (far right), the second\nlayer learns class-speciﬁc as well as shared parts, and\nthe third layer learns more object-speciﬁc representa-\ntions. (The training examples were unlabeled, so in a\nsense, this means the third layer implicitly clusters the\nimages by object category.) As before, we quantita-\ntively measured the speciﬁcity of each layer’s individ-\nual features to object categories. Because the train-\ning was completely unsupervised, whereas the AUC-\nPR statistic requires knowing which speciﬁc object or\nobject parts the learned bases should represent, we\ninstead computed conditional entropy. 12 Informally\nspeaking, conditional entropy measures the entropy of\n10For a given image, we computed the layerwise activa-\ntions using our algorithm, partitioned the activation into\nLxL regions for each group, and computed the q% highest\nquantile activation for each region and each group. If the\nq% highest quantile activation in region i isγ, we then de-\nﬁne a Bernoulli random variable Xi,L,q with probability γ\nof being 1. To measure the informativeness between a fea-\nture and the class label, we computed the mutual informa-\ntion between Xi,L,q and the class label. Results reported\nare using (L,q ) values that maximized the average mutual\ninformation (averaging over i).\n11For each feature, by comparing its values over pos-\nitive examples and negative examples, we obtained the\nprecision-recall curve for each classiﬁcation problem.\n12We computed the quantile features γ for each layer\nas previously described, and measured conditional entropy\nH(class|γ >0.95).\n614\nConvolutional Deep Belief Networks for Scalable Unsupervised Learning of Hierarchical Representations\nTable 2. Test error for MNIST dataset\nLabeled training samples 1,000 2,000 3,000 5,000 60,000\nCDBN 2.62±0.12% 2.13±0.10% 1.91±0.09% 1.59±0.11% 0.82%\nRanzato et al. (2007) 3.21% 2.53% - 1.52% 0.64%\nHinton and Salakhutdinov (2006) - - - - 1.20%\nWeston et al. (2008) 2.73% - 1.83% - 1.50%\nfaces\ncars\nelephants\nchairs\nfaces, cars, airplanes, motorbikes\nFigure 3. Columns 1-4: the second layer bases (top) and the third layer bases (bottom) learned from speciﬁc object\ncategories. Column 5: the second layer bases (top) and the third layer bases (bottom) learned from a mixture of four\nobject categories (faces, cars, airplanes, motorbikes).\n0.2 0.4 0.6 0.8 10\n0.2\n0.4\n0.6\nArea under the PR curve (AUC)\nFaces\n \n \nfirst layer\nsecond layer\nthird layer\n0.2 0.4 0.6 0.8 10\n0.2\n0.4\n0.6\nArea under the PR curve (AUC)\nMotorbikes\n \n \nfirst layer\nsecond layer\nthird layer\n0.2 0.4 0.6 0.8 10\n0.2\n0.4\n0.6\nArea under the PR curve (AUC)\nCars\n \n \nfirst layer\nsecond layer\nthird layer\nFeatures Faces Motorbikes Cars\nFirst layer 0.39±0.17 0.44±0.21 0.43±0.19\nSecond layer 0.86±0.13 0.69±0.22 0.72±0.23\nThird layer 0.95±0.03 0.81±0.13 0.87±0.15\nFigure 4. (top) Histogram of the area under the precision-\nrecall curve (AUC-PR) for three classiﬁcation problems\nusing class-speciﬁc object-part representations. (bottom)\nAverage AUC-PR for each classiﬁcation problem.\n0 0.5 1 1.5 20\n0.2\n0.4\n0.6\n0.8\n1\nConditional entropy\n \n \nfirst layer\nsecond layer\nthird layer\nFigure 5. Histogram of conditional entropy for the repre-\nsentation learned from the mixture of four object classes.\nthe posterior over class labels when a feature is ac-\ntive. Since lower conditional entropy corresponds to a\nmore peaked posterior, it indicates greater speciﬁcity.\nAs shown in Figure 5, the higher-layer features have\nprogressively less conditional entropy, suggesting that\nthey activate more selectively to speciﬁc object classes.\n4.5. Hierarchical probabilistic inference\nLee and Mumford (2003) proposed that the human vi-\nsual cortex can conceptually be modeled as performing\n“hierarchical Bayesian inference.” For example, if you\nobserve a face image with its left half in dark illumina-\nFigure 6. Hierarchical probabilistic inference. For each col-\numn: (top) input image. (middle) reconstruction from the\nsecond layer units after single bottom-up pass, by project-\ning the second layer activations into the image space. (bot-\ntom) reconstruction from the second layer units after 20\niterations of block Gibbs sampling.\ntion, you can still recognize the face and further infer\nthe darkened parts by combining the image with your\nprior knowledge of faces. In this experiment, we show\nthat our model can tractably perform such (approxi-\nmate) hierarchical probabilistic inference in full-sized\nimages. More speciﬁcally, we tested the network’s abil-\nity to infer the locations of hidden object parts.\nTo generate the examples for evaluation, we used\nCaltech-101 face images (distinct from the ones the\nnetwork was trained on). For each image, we simu-\nlated an occlusion by zeroing out the left half of the\nimage. We then sampled from the joint posterior over\nall of the hidden layers by performing Gibbs sampling.\nFigure 6 shows a visualization of these samples. To en-\nsure that the ﬁlling-in required top-down information,\nwe compare with a “control” condition where only a\nsingle upward pass was performed.\nIn the control (upward-pass only) condition, since\nthere is no evidence from the ﬁrst layer, the second\nlayer does not respond much to the left side. How-\n615\nConvolutional Deep Belief Networks for Scalable Unsupervised Learning of Hierarchical Representations\never, with full Gibbs sampling, the bottom-up inputs\ncombine with the context provided by the third layer\nwhich has detected the object. This combined evi-\ndence signiﬁcantly improves the second layer represen-\ntation. Selected examples are shown in Figure 6.\n5. Conclusion\nWe presented the convolutional deep belief network, a\nscalable generative model for learning hierarchical rep-\nresentations from unlabeled images, and showed that\nour model performs well in a variety of visual recog-\nnition tasks. We believe our approach holds promise\nas a scalable algorithm for learning hierarchical repre-\nsentations from high-dimensional, complex data.\nAcknowledgment\nWe give warm thanks to Daniel Oblinger and Rajat\nRaina for helpful discussions. This work was sup-\nported by the DARPA transfer learning program under\ncontract number FA8750-05-2-0249.\nReferences\nBell, A. J., & Sejnowski, T. J. (1997). The ‘indepen-\ndent components’ of natural scenes are edge ﬁlters.\nVision Research, 37, 3327–3338.\nBengio, Y., Lamblin, P., Popovici, D., & Larochelle, H.\n(2006). Greedy layer-wise training of deep networks.\nAdv. in Neural Information Processing Systems .\nBerg, A. C., Berg, T. L., & Malik, J. (2005). Shape\nmatching and object recognition using low distor-\ntion correspondence. IEEE Conference on Com-\nputer Vision and Pattern Recognition (pp. 26–33).\nDesjardins, G., & Bengio, Y. (2008). Empirical eval-\nuation of convolutional RBMs for vision (Technical\nReport).\nFei-Fei, L., Fergus, R., & Perona, P. (2004). Learning\ngenerative visual models from few training exam-\nples: an incremental Bayesian approach tested on\n101 object categories. CVPR Workshop on Gen.-\nModel Based Vision.\nGrosse, R., Raina, R., Kwong, H., & Ng, A. (2007).\nShift-invariant sparse coding for audio classiﬁcation.\nProceedings of the Conference on Uncertainty in AI.\nHinton, G. E. (2002). Training products of experts by\nminimizing contrastive divergence. Neural Compu-\ntation, 14, 1771–1800.\nHinton, G. E., Osindero, S., & Teh, Y.-W. (2006). A\nfast learning algorithm for deep belief nets. Neural\nComputation, 18, 1527–1554.\nHinton, G. E., & Salakhutdinov, R. (2006). Reduc-\ning the dimensionality of data with neural networks.\nScience, 313, 504–507.\nIto, M., & Komatsu, H. (2004). Representation of\nangles embedded within contour stimuli in area V2\nof macaque monkeys. J. Neurosci., 24, 3313–3324.\nLazebnik, S., Schmid, C., & Ponce, J. (2006). Beyond\nbags of features: Spatial pyramid matching for rec-\nognizing natural scene categories. IEEE Conference\non Computer Vision and Pattern Recognition .\nLeCun, Y., Boser, B., Denker, J. S., Henderson, D.,\nHoward, R. E., Hubbard, W., & Jackel, L. D. (1989).\nBackpropagation applied to handwritten zip code\nrecognition. Neural Computation, 1, 541–551.\nLee, H., Ekanadham, C., & Ng, A. Y. (2008). Sparse\ndeep belief network model for visual area V2. Ad-\nvances in Neural Information Processing Systems .\nLee, T. S., & Mumford, D. (2003). Hierarchical\nbayesian inference in the visual cortex. Journal of\nthe Optical Society of America A, 20, 1434–1448.\nMutch, J., & Lowe, D. G. (2006). Multiclass object\nrecognition with sparse, localized features. IEEE\nConf. on Computer Vision and Pattern Recognition.\nOlshausen, B. A., & Field, D. J. (1996). Emergence\nof simple-cell receptive ﬁeld properties by learning\na sparse code for natural images. Nature, 381, 607–\n609.\nRaina, R., Battle, A., Lee, H., Packer, B., & Ng, A. Y.\n(2007). Self-taught learning: Transfer learning from\nunlabeled data. International Conference on Ma-\nchine Learning (pp. 759–766).\nRaina, R., Madhavan, A., & Ng, A. Y. (2009). Large-\nscale deep unsupervised learning using graphics pro-\ncessors. International Conf. on Machine Learning.\nRanzato, M., Huang, F.-J., Boureau, Y.-L., & LeCun,\nY. (2007). Unsupervised learning of invariant fea-\nture hierarchies with applications to object recog-\nnition. IEEE Conference on Computer Vision and\nPattern Recognition.\nRanzato, M., Poultney, C., Chopra, S., & LeCun, Y.\n(2006). Eﬃcient learning of sparse representations\nwith an energy-based model. Advances in Neural\nInformation Processing Systems (pp. 1137–1144).\nTaylor, G., Hinton, G. E., & Roweis, S. (2007). Mod-\neling human motion using binary latent variables.\nAdv. in Neural Information Processing Systems .\nVarma, M., & Ray, D. (2007). Learning the discrimina-\ntive power-invariance trade-oﬀ. International Con-\nference on Computer Vision.\nWeston, J., Ratle, F., & Collobert, R. (2008). Deep\nlearning via semi-supervised embedding. Interna-\ntional Conference on Machine Learning.\nYu, K., Xu, W., & Gong, Y. (2009). Deep learn-\ning with kernel regularization for visual recognition.\nAdv. Neural Information Processing Systems.\nZhang, H., Berg, A. C., Maire, M., & Malik, J. (2006).\nSVM-KNN: Discriminative nearest neighbor classiﬁ-\ncation for visual category recognition. IEEE Confer-\nence on Computer Vision and Pattern Recognition .\n616",
  "values": {
    "Transparent (to users)": "Yes",
    "Interpretable (to users)": "Yes",
    "Respect for Law and public interest": "Yes",
    "Not socially biased": "Yes",
    "Privacy": "Yes",
    "Respect for Persons": "Yes",
    "Autonomy (power to decide)": "Yes",
    "User influence": "Yes",
    "Non-maleficence": "Yes",
    "Justice": "Yes",
    "Beneficence": "Yes",
    "Fairness": "Yes",
    "Explicability": "Yes",
    "Collective influence": "Yes",
    "Critiqability": "Yes",
    "Deferral to humans": "Yes"
  }
}