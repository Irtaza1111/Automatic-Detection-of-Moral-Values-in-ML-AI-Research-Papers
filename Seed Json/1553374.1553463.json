{
  "pdf": "1553374.1553463",
  "title": "Online dictionary learning for sparse coding",
  "author": "Julien Mairal, Francis Bach, Jean Ponce, Guillermo Sapiro",
  "paper_id": "1553374.1553463",
  "text": "Online Dictionary Learning for Sparse Coding\nJulien Mairal JU LIEN .MAIRAL @INRIA .FR\nFrancis Bach FRANCIS .BACH @INRIA .FR\nINRIA,1 45 rue d’Ulm 75005 Paris, France\nJean Ponce JEAN .PONCE @ENS .FR\nEcole Normale Sup ´erieure,1 45 rue d’Ulm 75005 Paris, France\nGuillermo Sapiro GUILLE @UMN .EDU\nUniversity of Minnesota - Department of Electrical and Computer Engineering, 200 Union Street SE, Minneapolis, USA\nAbstract\nSparse coding—that is, modelling data vectors as\nsparse linear combinations of basis elements—is\nwidely used in machine learning, neuroscience,\nsignal processing, and statistics. This paper fo-\ncuses on learning the basis set, also called dic-\ntionary, to adapt it to speciﬁc data, an approach\nthat has recently proven to be very effective for\nsignal reconstruction and classiﬁcation in the au-\ndio and image processing domains. This paper\nproposes a new online optimization algorithm\nfor dictionary learning, based on stochastic ap-\nproximations, which scales up gracefully to large\ndatasets with millions of training samples. A\nproof of convergence is presented, along with\nexperiments with natural images demonstrating\nthat it leads to faster performance and better dic-\ntionaries than classical batch algorithms for both\nsmall and large datasets.\n1. Introduction\nThe linear decomposition of a signal using a few atoms of\na learned dictionary instead of a predeﬁned one—based on\nwavelets (Mallat, 1999) for example—has recently led to\nstate-of-the-art results for numerous low-level image pro-\ncessing tasks such as denoising (Elad & Aharon, 2006)\nas well as higher-level tasks such as classiﬁcation (Raina\net al., 2007; Mairal et al., 2009), showing that sparse\nlearned models are well adapted to natural signals. Un-\n1WI LLOW Project, Laboratoire d’Informatique de l’Ecole\nNormale Sup´erieure, ENS/INRIA/CNRS UMR 8548.\nAppearing in Pr oceedings of the 26 th International Conference\non Machine Learning, Montreal, Canada, 2009. Copyright 2009\nby the author(s)/owner(s).\nlike decompositions based on principal component analy-\nsis and its variants, these models do not impose that the\nbasis vectors be orthogonal, allowing more ﬂexibility to\nadapt the representation to the data. While learning the\ndictionary has proven to be critical to achieve (or improve\nupon) state-of-the-art results, effectively solving the cor-\nresponding optimization problem is a signiﬁcant compu-\ntational challenge, particularly in the context of the large-\nscale datasets involved in image processing tasks, that may\ninclude millions of training samples. Addressing this chal-\nlenge is the topic of this paper.\nConcretely, consider a signal x in Rm. We say that it ad-\nmits a sparse approximation over a dictionary D in Rm×k,\nwith k columns referred to as atoms, when one can ﬁnd a\nlinear combination of a “few” atoms from D that is “close”\nto the signal x. Experiments have shown that modelling a\nsignal with such a sparse decomposition (sparse coding) is\nvery effective in many signal processing applications (Chen\net al., 1999). For natural images, predeﬁned dictionaries\nbased on various types of wavelets (Mallat, 1999) have\nbeen used for this task. However, learning the dictionary\ninstead of using off-the-shelf bases has been shown to dra-\nmatically improve signal reconstruction (Elad & Aharon,\n2006). Although some of the learned dictionary elements\nmay sometimes “look like” wavelets (or Gabor ﬁlters), they\nare tuned to the input images or signals, leading to much\nbetter results in practice.\nMost recent algorithms for dictionary learning (Olshausen\n& Field, 1997; Aharon et al., 2006; Lee et al., 2007)\nare second-order iterative batch procedures, accessing the\nwhole training set at each iteration in order to minimize a\ncost function under some constraints. Although they have\nshown experimentally to be much faster than ﬁrst-order\ngradient descent methods (Lee et al., 2007), they cannot\neffectively handle very large training sets (Bottou & Bous-\nquet, 2008), or dynamic training data changing over time,\n689\n\nOnline Dictionary Learning for Sparse Coding\nsuch as video sequences. To address these issues, we pro-\npose\nan online approach that processes one element (or a\nsmall subset) of the training set at a time. This is particu-\nlarly important in the context of image and video process-\ning (Protter & Elad, 2009), where it is common to learn\ndictionaries adapted to small patches, with training data\nthat may include several millions of these patches (roughly\none per pixel and per frame). In this setting, online tech-\nniques based on stochastic approximations are an attractive\nalternative to batch methods (Bottou, 1998). For example,\nﬁrst-order stochastic gradient descent with projections on\nthe constraint set is sometimes used for dictionary learn-\ning (see Aharon and Elad (2008) for instance). We show\nin this paper that it is possible to go further and exploit the\nspeciﬁc structure of sparse coding in the design of an opti-\nmization procedure dedicated to the problem of dictionary\nlearning, with low memory consumption and lower compu-\ntational cost than classical second-order batch algorithms\nand without the need of explicit learning rate tuning. As\ndemonstrated by our experiments, the algorithm scales up\ngracefully to large datasets with millions of training sam-\nples, and it is usually faster than more standard methods.\n1.1. Contributions\nThis paper makes three main contributions.\n•We cast in Section 2 the dictionary learning problem as\nthe optimization of a smooth nonconvex objective function\nover a convex set, minimizing the (desired) expected cost\nwhen the training set size goes to inﬁnity.\n•We propose in Section 3 an iterative online algorithm that\nsolves this problem by efﬁciently minimizing at each step a\nquadratic surrogate function of the empirical cost over the\nset of constraints. This method is shown in Section 4 to\nconverge with probability one to a stationary point of the\ncost function.\n•As shown experimentally in Section 5, our algorithm is\nsigniﬁcantly faster than previous approaches to dictionary\nlearning on both small and large datasets of natural im-\nages. To demonstrate that it is adapted to difﬁcult, large-\nscale image-processing tasks, we learn a dictionary on a\n12-Megapixel photograph and use it for inpainting.\n2. Problem Statement\nClassical dictionary learning techniques (Olshausen &\nField, 1997; Aharon et al., 2006; Lee et al., 2007) consider\na ﬁnite training set of signals X = [ x1, . . . , xn] in Rm×n\nand optimize the empirical cost function\nfn(D)\n△\n= 1\nn\nn∑\ni=1\nl(xi, D ), (1)\nwhere D in Rm×k is the dictionary, each column represent-\ning a basis vector, and l is a loss function such that l(x, D)\nshould be small if D is “good” at representing the signal x.\nThe number of samples n is usually large, whereas the sig-\nnal dimension m is relatively small, for example, m = 100\nfor 10×10 image patches, and n≥100, 000 for typical\nimage processing applications. In general, we also have\nk≪n (e.g., k = 200 for n = 100 , 000), and each signal\nonly uses a few elements of D in its representation. Note\nthat, in this setting, overcomplete dictionaries with k > m\nare allowed. As others (see (Lee et al., 2007) for example),\nwe deﬁne l(x, D) as the optimal value of the ℓ 1-sparse cod-\ning problem:\nl(x, D)\n△\n= min\nα∈Rk\n1\n2||x−D α||2\n2 + λ||α||1, (2)\nwhere λ is a regularization parameter. 2 This problem is\nalso known as basis pursuit (Chen et al., 1999), or the\nLasso (Tibshirani, 1996). It is well known that the ℓ 1\npenalty yields a sparse solution for α, but there is no an-\nalytic link between the value of λ and the corresponding\neffective sparsity||α||0. To prevent D from being arbitrar-\nily large (which would lead to arbitrarily small values of\nα), it is common to constrain its columns (dj)k\nj=1 to have\nan ℓ 2 norm less than or equal to one. We will call C the\nconvex set of matrices verifying this constraint:\nC\n△\n={D∈Rm×k s.t.∀j= 1, . . . , k, dT\nj dj≤1}.(3)\nNote that the problem of minimizing the empirical cost\nfn(D) is not convex with respect to D. It can be rewrit-\nten as a joint optimization problem with respect to the dic-\ntionary D and the coefﬁcients α = [ α1, . . . , αn] of the\nsparse decomposition, which is not jointly convex, but con-\nvex with respect to each of the two variables D and α when\nthe other one is ﬁxed:\nmin\nD∈C,α∈R k×n\n1\nn\nn∑\ni=1\n( 1\n2||xi−D αi||2\n2 + λ||αi||1\n)\n. (4)\nA natural approach to solving this problem is to alter-\nnate between the two variables, minimizing over one while\nkeeping the other one ﬁxed, as proposed by Lee et al.\n(2007) (see also Aharon et al. (2006), who use ℓ 0 rather\nthan ℓ 1 penalties, for related approaches). 3 Since the\ncomputation of α dominates the cost of each iteration, a\nsecond-order optimization technique can be used in this\ncase to accurately estimate D at each step when α is ﬁxed.\nAs pointed out by Bottou and Bousquet (2008), however,\none is usually not interested in a perfect minimization of\n2The ℓ p nor m of a vector x in Rm is deﬁned, for p ≥ 1, by\n||x||p\n△\n= ( Pm\ni=1 |x[i]|p)1/p. Following tradition, we denote by\n||x||0 the number of nonzero elements of the vector x. This “ℓ 0”\nsparsity measure is not a true norm.\n3In our setting, as in (Lee et al., 2007), we use the convex ℓ 1\nnorm, that has empirically proven to be better behaved in general\nthan the ℓ 0 pseudo-norm for dictionary learning.\n690\nOnline Dictionary Learning for Sparse Coding\nthe empirical cost fn(D ), but in the minimization of the\nexpected cost\nf (D)\n△\n= Ex[l(x, D)] = lim\nn→∞\nfn(D) a.s., (5)\nwhere the expectation (which is assumed ﬁnite) is taken rel-\native to the (unknown) probability distribution p(x) of the\ndata.4 In particular, given a ﬁnite training set, one should\nnot spend too much effort on accurately minimizing the\nempirical cost, since it is only an approximation of the ex-\npected cost.\nBottou and Bousquet (2008) have further shown both the-\noretically and experimentally that stochastic gradient algo-\nrithms, whose rate of convergence is not good in conven-\ntional optimization terms, may in fact in certain settings be\nthe fastest in reaching a solution with low expected cost.\nWith large training sets, classical batch optimization tech-\nniques may indeed become impractical in terms of speed or\nmemory requirements.\nIn the case of dictionary learning, classical projected ﬁrst-\norder stochastic gradient descent (as used by Aharon and\nElad (2008) for instance) consists of a sequence of updates\nof D:\nDt = Π C\n[\nDt−1−ρ\nt∇Dl(xt, Dt−1)\n]\n, (6)\nwher\ne ρ is the gradient step, ΠC is the orthogonal projec-\ntor onC, and the training set x1, x2, . . . are i.i.d. samples\nof the (unknown) distribution p(x). As shown in Section\n5, we have observed that this method can be competitive\ncompared to batch methods with large training sets, when\na good learning rate ρ is selected.\nThe dictionary learning method we present in the next\nsection falls into the class of online algorithms based\non stochastic approximations, processing one sample at a\ntime, but exploits the speciﬁc structure of the problem to\nefﬁciently solve it. Contrary to classical ﬁrst-order stochas-\ntic gradient descent, it does not require explicit learning\nrate tuning and minimizes a sequentially quadratic local ap-\nproximations of the expected cost.\n3. Online Dictionary Learning\nWe present in this section the basic components of our on-\nline algorithm for dictionary learning (Sections 3.1–3.3), as\nwell as two minor variants which speed up our implemen-\ntation (Section 3.4).\n3.1. Algorithm Outline\nOur algorithm is summarized in Algorithm 1. Assuming\nthe training set composed of i.i.d. samples of a distribu-\n4We use “a.s.” (almost sure) to denote convergence with prob-\nability one.\nAlgorithm 1 Online dictionary learning.\nRequire: x∈Rm∼p(x) (r andom variable and an algo-\nrithm to draw i.i.d samples of p), λ ∈R (regularization\nparameter), D0∈Rm×k (initial dictionary), T (num-\nber of iterations).\n1: A0←0, B0←0 (reset the “past” information).\n2: for t = 1 to T do\n3: Draw xt from p(x).\n4: Sparse coding: compute using LARS\nαt\n△\n= arg min\nα∈Rk\n1\n2||xt−Dt− 1α||2\n2 + λ||α||1. (8)\n5: At←At−1 + αtαT\nt .\n6: Bt←Bt−1 + xtαT\nt .\n7: Compute Dt using Algorithm 2, with Dt−1 as warm\nrestart, so that\nDt\n△\n= arg min\nD∈C\n1\nt\nt∑\ni=1\n1\n2||xi−D αi||2\n2 + λ||αi||1,\n= arg min\nD∈C\n1\nt\n( 1\n2 Tr(DT D At)−Tr(DT Bt)\n)\n.\n(9)\n8: end for\n9: Return DT (learned dictionary).\ntion p(x), its inner loop draws one element xt at a time,\nas in stochastic gradient descent, and alternates classical\nsparse coding steps for computing the decomposition αt of\nxt over the dictionary Dt−1 obtained at the previous itera-\ntion, with dictionary update steps where the new dictionary\nDt is computed by minimizing overC the function\nˆft(D)\n△\n= 1\nt\nt∑\ni=1\n1\n2||xi−D αi||2\n2 + λ||αi||1, (7)\nwhere the vectors αi are computed during the previous\nsteps of the algorithm. The motivation behind our approach\nis twofold:\n•The quadratic function ˆft aggregates the past informa-\ntion computed during the previous steps of the algorithm,\nnamely the vectors αi, and it is easy to show that it up-\nperbounds the empirical cost ft(Dt) from Eq. (1). One\nkey aspect of the convergence analysis will be to show that\nˆft(Dt) and ft(Dt) converges almost surely to the same\nlimit and thus ˆft acts as a surrogate for ft.\n•Since ˆft is close to ˆft−1, Dt can be obtained efﬁciently\nusing Dt−1 as warm restart.\n3.2. Sparse Coding\nThe sparse coding problem of Eq. (2) with ﬁxed dictio-\nnary is an ℓ 1-regularized linear least-squares problem. A\n691\nOnline Dictionary Learning for Sparse Coding\nAlgorithm 2 Dictionar y Update.\nRequire: D = [d1, . . . , dk]∈Rm×k (input dictionary),\nA = [a1, . . . , ak]∈Rk×k = ∑t\ni=1 αiαT\ni ,\nB = [b1, . . . , bk]∈Rm×k = ∑t\ni=1 xiαT\ni .\n1: repeat\n2: for j = 1 to k do\n3: Update the j-th column to optimize for (9):\nuj←1\nAjj\n(bj−Daj) + dj.\ndj← 1\nmax(||uj||2, 1) uj.\n(10)\n4: end\nfor\n5: until convergence\n6: Return D (updated dictionary).\nnumber of recent methods for solving this type of prob-\nlems\nare based on coordinate descent with soft threshold-\ning (Fu, 1998; Friedman et al., 2007). When the columns\nof the dictionary have low correlation, these simple meth-\nods have proven to be very efﬁcient. However, the columns\nof learned dictionaries are in general highly correlated, and\nwe have empirically observed that a Cholesky-based im-\nplementation of the LARS-Lasso algorithm, an homotopy\nmethod (Osborne et al., 2000; Efron et al., 2004) that pro-\nvides the whole regularization path—that is, the solutions\nfor all possible values of λ , can be as fast as approaches\nbased on soft thresholding, while providing the solution\nwith a higher accuracy.\n3.3. Dictionary Update\nOur algorithm for updating the dictionary uses block-\ncoordinate descent with warm restarts, and one of its main\nadvantages is that it is parameter-free and does not require\nany learning rate tuning, which can be difﬁcult in a con-\nstrained optimization setting. Concretely, Algorithm 2 se-\nquentially updates each column of D. Using some simple\nalgebra, it is easy to show that Eq. (10) gives the solution\nof the dictionary update (9) with respect to the j-th column\ndj, while keeping the other ones ﬁxed under the constraint\ndT\nj dj ≤1. Since this convex optimization problem ad-\nmits separable constraints in the updated blocks (columns),\nconvergence to a global optimum is guaranteed (Bertsekas,\n1999). In practice, since the vectors αi are sparse, the coef-\nﬁcients of the matrix A are in general concentrated on the\ndiagonal, which makes the block-coordinate descent more\nefﬁcient.5 Since our algorithm uses the value of Dt−1 as a\n5Note that this assumption does not exactly hold: To be more\nexact, if a group of columns in D are highly correlated, the co-\nefﬁcients of the matrix A can concentrate on the corresponding\nprincipal submatrices of A.\nwarm restart for computing Dt, a single iteration has em-\npirically been found to be enough. Other approaches have\nbeen proposed to update D, for instance, Lee et al. (2007)\nsuggest using a Newton method on the dual of Eq. (9), but\nthis requires inverting a k×k matrix at each Newton itera-\ntion, which is impractical for an online algorithm.\n3.4. Optimizing the Algorithm\nWe have presented so far the basic building blocks of our\nalgorithm. This section discusses simple improvements\nthat signiﬁcantly enhance its performance.\nHandling Fixed-Size Datasets. In practice, although it\nmay be very large, the size of the training set is often ﬁ-\nnite (of course this may not be the case, when the data\nconsists of a video stream that must be treated on the ﬂy\nfor example). In this situation, the same data points may\nbe examined several times, and it is very common in on-\nline algorithms to simulate an i.i.d. sampling of p(x) by\ncycling over a randomly permuted training set (Bottou &\nBousquet, 2008). This method works experimentally well\nin our setting but, when the training set is small enough,\nit is possible to further speed up convergence: In Algo-\nrithm 1, the matrices At and Bt carry all the information\nfrom the past coefﬁcients α1, . . . , αt. Suppose that at time\nt0, a signal x is drawn and the vector αt0 is computed. If\nthe same signal x is drawn again at time t > t 0, one would\nlike to remove the “old” information concerning x from At\nand Bt—that is, write At←At−1 + αtαT\nt −αt0 αT\nt0 for\ninstance. When dealing with large training sets, it is im-\npossible to store all the past coefﬁcients αt0 , but it is still\npossible to partially exploit the same idea, by carrying in\nAt and Bt the information from the current and previous\nepochs (cycles through the data) only.\nMini-Batch Extension. In practice, we can improve the\nconvergence speed of our algorithm by drawing η > 1\nsignals at each iteration instead of a single one, which is\na classical heuristic in stochastic gradient descent algo-\nrithms. Let us denote xt,1 , . . . , xt,η the signals drawn at\niteration t. We can then replace the lines 5 and 6 of Algo-\nrithm 1 by\n{ At ←β At−1 + ∑η\ni=1 αt,i αT\nt,i ,\nBt ←β Bt−1 + ∑η\ni=1 xαT\nt,i , (11)\nwhere β is chosen so that β = θ +1−η\nθ +1 , where θ = tη if\nt < η and η 2 + t−η if t≥η , which is compatible with our\nconvergence analysis.\nPurging the Dictionary from Unused Atoms. Every dic-\ntionary learning technique sometimes encounters situations\nwhere some of the dictionary atoms are never (or very sel-\ndom) used, which happens typically with a very bad intial-\nization. A common practice is to replace them during the\n692\nOnline Dictionary Learning for Sparse Coding\noptimization by elements of the training set, which solves\nin\npractice this problem in most cases.\n4. Convergence Analysis\nAlthough our algorithm is relatively simple, its stochas-\ntic nature and the non-convexity of the objective function\nmake the proof of its convergence to a stationary point\nsomewhat involved. The main tools used in our proofs\nare the convergence of empirical processes (V an der V aart,\n1998) and, following Bottou (1998), the convergence of\nquasi-martingales (Fisk, 1965). Our analysis is limited to\nthe basic version of the algorithm, although it can in prin-\nciple be carried over to the optimized version discussed\nin Section 3.4. Because of space limitations, we will re-\nstrict ourselves to the presentation of our main results and\na sketch of their proofs, which will be presented in de-\ntails elsewhere, and ﬁrst the (reasonable) assumptions un-\nder which our analysis holds.\n4.1. Assumptions\n(A) The data admits a bounded probability density p\nwith compact support K. Assuming a compact support\nfor the data is natural in audio, image, and video process-\ning applications, where it is imposed by the data acquisition\nprocess.\n(B) The quadratic surrogate functions ˆft are strictly\nconvex with lower-bounded Hessians. We assume that\nthe smallest eigenvalue of the semi-deﬁnite positive ma-\ntrix 1\nt At deﬁned in Algorithm 1 is greater than or equal\nto a non-zero constant κ 1 (making At invertible and ˆft\nstrictly convex with Hessian lower-bounded). This hypoth-\nesis is in practice veriﬁed experimentally after a few iter-\nations of the algorithm when the initial dictionary is rea-\nsonable, consisting for example of a few elements from the\ntraining set, or any one of the “off-the-shelf” dictionaries,\nsuch as DCT (bases of cosines products) or wavelets. Note\nthat it is easy to enforce this assumption by adding a term\nκ 1\n2||D ||2\nF to the objective function, which is equivalent in\npractice to replacing the positive semi-deﬁnite matrix 1\nt At\nby 1\nt At + κ 1I. W e have omitted for simplicity this penal-\nization in our analysis.\n(C) A sufﬁcient uniqueness condition of the sparse cod-\ning solution is veriﬁed: Given some x∈K, where K is\nthe support of p, and D∈C, let us denote by Λ the set of\nindices j such that|dT\nj (x−Dα⋆ )|= λ , where α⋆ is the\nsolution of Eq. (2). We assume that there exists κ 2 > 0\nsuch that, for all x in K and all dictionaries D in the subset\nS ofC considered by our algorithm, the smallest eigen-\nvalue of DT\nΛDΛ is greater than or equal to κ 2. This matrix\nis thus invertible and classical results (Fuchs, 2005) ensure\nthe uniqueness of the sparse coding solution. It is of course\neasy to build a dictionary D for which this assumption fails.\nHowever, having DT\nΛDΛ invertible is a common assump-\ntion in linear regression and in methods such as the LARS\nalgorithm aimed at solving Eq. (2) (Efron et al., 2004). It\nis also possible to enforce this condition using an elastic\nnet penalization (Zou & Hastie, 2005), replacing ||α||1 by\n||α||1 + κ 2\n2||α||2\n2 and thus improving the numerical stabil-\nity of homotopy algorithms such as LARS. Again, we have\nomitted this penalization for simplicity.\n4.2. Main Results and Proof Sketches\nGiven assumptions (A) to (C), let us now show that our\nalgorithm converges to a stationary point of the objective\nfunction.\nProposition 1 (convergence of f (Dt) and of the sur-\nrogate function). Let ˆft denote the surrogate function\ndeﬁned in Eq. (7). Under assumptions (A) to (C):\n•ˆft(Dt) converges a.s.;\n•f (Dt)−ˆft(Dt) converges a.s. to 0; and\n•f (Dt) converges a.s.\nProof sktech: The ﬁrst step in the proof is to show that\nDt−Dt−1 = O\n( 1\nt\n)\nwhich, although it does not ensure\nthe convergence of Dt, ensures the convergence of the se-\nries ∑∞\nt=1||Dt−Dt−1||2\nF , a classical condition in gradi-\nent descent convergence proofs (Bertsekas, 1999). In turn,\nthis reduces to showing that Dt minimizes a parametrized\nquadratic function over C with parameters 1\nt At and 1\nt Bt,\nthen\nshowing that the solution is uniformly Lipschitz with\nrespect to these parameters, borrowing some ideas from\nperturbation theory (Bonnans & Shapiro, 1998). At this\npoint, and following Bottou (1998), proving the conver-\ngence of the sequence ˆft(Dt) amounts to showing that the\nstochastic positive process\nut\n△\n= ˆft(Dt)≥0, (12)\nis a quasi-martingale. To do so, denoting by Ft the ﬁltra-\ntion of the past information, a theorem by Fisk (1965) states\nthat if the positive sum ∑∞\nt=1 E[max(E[ut+1−ut|Ft], 0)]\nconverges, then ut is a quasi-martingale which converges\nwith probability one. Using some results on empirical pro-\ncesses (V an der V aart, 1998, Chap. 19.2, Donsker The-\norem), we obtain a bound that ensures the convergence\nof this series. It follows from the convergence of ut that\nft(Dt)−ˆft(Dt) converges to zero with probability one.\nThen, a classical theorem from perturbation theory (Bon-\nnans & Shapiro, 1998, Theorem 4.1) shows that l(x, D)\nis C 1. This, allows us to use a last result on empirical\nprocesses ensuring that f (Dt)−ˆft(Dt) converges almost\nsurely to 0. Therefore f (Dt) converges as well with prob-\nability one.\n693\nOnline Dictionary Learning for Sparse Coding\nProposition 2 (convergence to a stationary point). Un-\nder\nassumptions (A) to (C), Dt is asymptotically close to\nthe set of stationary points of the dictionary learning prob-\nlem with probability one.\nProof sktech: The ﬁrst step in the proof is to show using\nclassical analysis tools that, given assumptions (A) to (C),\nf is C 1 with a Lipschitz gradient. Considering ˜A and ˜B\ntwo accumulation points of 1\nt At and 1\nt Bt respecti vely, we\ncan deﬁne the corresponding surrogate function ˆf∞ such\nthat for all D inC, ˆf∞(D) = 1\n2 Tr(DT D ˜A)−T r(DT ˜B),\nand its optimum D∞ onC. The next step consists of show-\ning that∇ˆf∞(D∞) =∇f(D∞) and that−∇f(D∞) is in\nthe normal cone of the set C—that is, D∞ is a stationary\npoint of the dictionary learning problem (Borwein &\nLewis, 2006).\n5. Experimental Validation\nIn\nthis section, we present experiments on natural images\nto demonstrate the efﬁciency of our method.\n5.1. Performance evaluation\nFor our experiments, we have randomly selected 1. 25×106\npatches from images in the Berkeley segmentation dataset,\nwhich is a standard image database; 106 of these are kept\nfor training, and the rest for testing. We used these patches\nto create three datasets A, B, and C with increasing patch\nand dictionary sizes representing various typical settings in\nimage processing applications:\nData Signal size m Nb k of atoms Type\nA 8×8 = 64 256 b&w\nB 12×12×3 = 432 512 color\nC 16×16 = 256 1024 b&w\nWe have normalized the patches to have unit ℓ 2-nor m and\nused the regularization parameter λ = 1. 2/√m in all of\nour\nexperiments. The 1/√m term is a classical normaliza-\ntion\nfactor (Bickel et al., 2007), and the constant 1. 2 has\nbeen experimentally shown to yield reasonable sparsities\n(about 10 nonzero coefﬁcients) in these experiments. We\nhave implemented the proposed algorithm in C++ with a\nMatlab interface. All the results presented in this section\nuse the mini-batch reﬁnement from Section 3.4 since this\nhas shown empirically to improve speed by a factor of 10\nor more. This requires to tune the parameter η , the number\nof signals drawn at each iteration. Trying different powers\nof 2 for this variable has shown that η = 256 was a good\nchoice (lowest objective function values on the training set\n— empirically, this setting also yields the lowest values on\nthe test set), but values of 128 and and 512 have given very\nsimilar performances.\nOur implementation can be used in both the online setting\nit is intended for, and in a regular batch mode where it\nuses the entire dataset at each iteration (corresponding to\nthe mini-batch version with η = n). We have also imple-\nmented a ﬁrst-order stochastic gradient descent algorithm\nthat shares most of its code with our algorithm, except\nfor the dictionary update step. This setting allows us to\ndraw meaningful comparisons between our algorithm and\nits batch and stochastic gradient alternatives, which would\nhave been difﬁcult otherwise. For example, comparing our\nalgorithm to the Matlab implementation of the batch ap-\nproach from (Lee et al., 2007) developed by its authors\nwould have been unfair since our C++ program has a built-\nin speed advantage. Although our implementation is multi-\nthreaded, our experiments have been run for simplicity on a\nsingle-CPU, single-core 2.4Ghz machine. To measure and\ncompare the performances of the three tested methods, we\nhave plotted the value of the objective function on the test\nset, acting as a surrogate of the expected cost, as a function\nof the corresponding training time.\nOnline vs Batch. Figure 1 (top) compares the online and\nbatch settings of our implementation. The full training set\nconsists of 106 samples. The online version of our algo-\nrithm draws samples from the entire set, and we have run\nits batch version on the full dataset as well as subsets of size\n104 and 105 (see ﬁgure). The online setting systematically\noutperforms its batch counterpart for every training set size\nand desired precision. We use a logarithmic scale for the\ncomputation time, which shows that in many situations, the\ndifference in performance can be dramatic. Similar experi-\nments have given similar results on smaller datasets.\nComparison with Stochastic Gradient Descent. Our ex-\nperiments have shown that obtaining good performance\nwith stochastic gradient descent requires using both the\nmini-batch heuristic and carefully choosing the learning\nrate ρ. To give the fairest comparison possible, we have\nthus optimized these parameters, sampling η values among\npowers of 2 (as before) and ρ values among powers of 10.\nThe combination of values ρ = 10 4, η = 512 gives the\nbest results on the training and test data for stochastic gra-\ndient descent. Figure 1 (bottom) compares our method with\nstochastic gradient descent for different ρ values around\n104 and a ﬁxed value of η = 512 . We observe that the\nlarger the value of ρ is, the better the eventual value of the\nobjective function is after many iterations, but the longer it\nwill take to achieve a good precision. Although our method\nperforms better at such high-precision settings for dataset\nC, it appears that, in general, for a desired precision and a\nparticular dataset, it is possible to tune the stochastic gra-\ndient descent algorithm to achieve a performance similar\nto that of our algorithm. Note that both stochastic gradi-\nent descent and our method only start decreasing the ob-\njective function value after a few iterations. Slightly better\nresults could be obtained by using smaller gradient steps\n694\nOnline Dictionary Learning for Sparse Coding\n10\n−1\n10\n0\n10\n1\n10\n2\n10\n3\n10\n4\n0.1455\n0.146\n0.1465\n0.147\n0.1475\n0.148\n0.1485\n0.149\n0.1495\nEvaluation set A\ntime (in seconds)\nObjective function (test set)\n \n \nOur method\nBatch n=104\nBatch n=105\nBatch n=106\n10\n−1\n10\n0\n10\n1\n10\n2\n10\n3\n10\n4\n0.107\n0.108\n0.109\n0.11\n0.111\n0.112\nEvaluation set B\ntime (in seconds)\nObjective function (test set)\n \n \nOur method\nBatch n=104\nBatch n=105\nBatch n=106\n10\n−1\n10\n0\n10\n1\n10\n2\n10\n3\n10\n4\n0.083\n0.084\n0.085\n0.086\n0.087\nEvaluation set C\ntime (in seconds)\nObjective function (test set)\n \n \nOur method\nBatch n=104\nBatch n=105\nBatch n=106\n10\n−1\n10\n0\n10\n1\n10\n2\n10\n3\n10\n4\n0.1455\n0.146\n0.1465\n0.147\n0.1475\n0.148\n0.1485\n0.149\n0.1495\nEvaluation set A\ntime (in seconds)\nObjective function (test set)\n \n \nOur method\nSG ρ=5.103\nSG ρ=104\nSG ρ=2.104\n10\n−1\n10\n0\n10\n1\n10\n2\n10\n3\n10\n4\n0.107\n0.108\n0.109\n0.11\n0.111\n0.112\nEvaluation set B\ntime (in seconds)\nObjective function (test set)\n \n \nOur method\nSG ρ=5.103\nSG ρ=104\nSG ρ=2.104\n10\n−1\n10\n0\n10\n1\n10\n2\n10\n3\n10\n4\n0.083\n0.084\n0.085\n0.086\n0.087\nEvaluation set C\ntime (in seconds)\nObjective function (test set)\n \n \nOur method\nSG ρ=5.103\nSG ρ=104\nSG ρ=2.104\nFigure 1. Top: Comparison between online and batch learning for various training set sizes. Bottom: Comparison between our method\nand stochastic gradient (SG) descent with different learning rates ρ. In both cases, the value of the objective function evaluated on the\ntest set is reported as a function of computation time on a logarithmic scale. V alues of the objective function greater than its initial value\nare truncated.\nduring the ﬁrst iterations, using a learning rate of the form\nρ/( t + t0) for the stochastic gradient descent, and initializ-\ning A0 = t0I and B0 = t0D0 for the matrices At and Bt,\nwhere t0 is a new parameter.\n5.2. Application to Inpainting\nOur last experiment demonstrates that our algorithm can\nbe used for a difﬁcult large-scale image processing task,\nnamely, removing the text (inpainting) from the damaged\n12-Megapixel image of Figure 2. Using a multi-threaded\nversion of our implementation, we have learned a dictio-\nnary with 256 elements from the roughly 7×106 undam-\naged 12×12 color patches in the image with two epochs in\nabout 500 seconds on a 2.4GHz machine with eight cores.\nOnce the dictionary has been learned, the text is removed\nusing the sparse coding technique for inpainting of Mairal\net al. (2008). Our intent here is of course not to evaluate\nour learning procedure in inpainting tasks, which would re-\nquire a thorough comparison with state-the-art techniques\non standard datasets. Instead, we just wish to demonstrate\nthat the proposed method can indeed be applied to a re-\nalistic, non-trivial image processing task on a large im-\nage. Indeed, to the best of our knowledge, this is the ﬁrst\ntime that dictionary learning is used for image restoration\non such large-scale data. For comparison, the dictionaries\nused for inpainting in the state-of-the-art method of Mairal\net al. (2008) are learned (in batch mode) on only 200,000\npatches.\n6. Discussion\nWe have introduced in this paper a new stochastic online al-\ngorithm for learning dictionaries adapted to sparse coding\ntasks, and proven its convergence. Preliminary experiments\ndemonstrate that it is signiﬁcantly faster than batch alterna-\ntives on large datasets that may contain millions of training\nexamples, yet it does not require learning rate tuning like\nregular stochastic gradient descent methods. More exper-\niments are of course needed to better assess the promise\nof this approach in image restoration tasks such as denois-\ning, deblurring, and inpainting. Beyond this, we plan to\nuse the proposed learning framework for sparse coding in\ncomputationally demanding video restoration tasks (Prot-\nter & Elad, 2009), with dynamic datasets whose size is not\nﬁxed, and also plan to extend this framework to different\nloss functions to address discriminative tasks such as image\nclassiﬁcation (Mairal et al., 2009), which are more sensitive\nto overﬁtting than reconstructive ones, and various matrix\nfactorization tasks, such as non-negative matrix factoriza-\ntion with sparseness constraints and sparse principal com-\nponent analysis.\nAcknowledgments\nThis paper was supported in part by ANR under grant\nMGA. The work of Guillermo Sapiro is partially supported\nby ONR, NGA, NSF, ARO, and DARPA.\n695\nOnline Dictionary Learning for Sparse Coding\nFigure 2. Inpainting example on a 12-Megapixel image. Top:\nDamaged and restored images. Bottom: Zooming on the dam-\naged and restored images. (Best seen in color)\nReferences\nAharon, M., & Elad, M. (2008). Sparse and redundant\nmodeling of image content using an image-signature-\ndictionary. SIAM Imaging Sciences, 1, 228–247.\nAharon, M., Elad, M., & Bruckstein, A. M. (2006). The K-\nSVD: An algorithm for designing of overcomplete dic-\ntionaries for sparse representations. IEEE Transactions\nSignal Processing, 54, 4311-4322\nBertsekas, D. (1999). Nonlinear programming. Athena\nScientiﬁc Belmont, Mass.\nBickel, P ., Ritov, Y ., & Tsybakov, A. (2007). Simultaneous\nanalysis of Lasso and Dantzig selector. preprint.\nBonnans, J., & Shapiro, A. (1998). Optimization prob-\nlems with perturbation: A guided tour. SIAM Review,\n40, 202–227.\nBorwein, J., & Lewis, A. (2006). Convex analysis and non-\nlinear optimization: theory and examples. Springer.\nBottou, L. (1998). Online algorithms and stochastic ap-\nproximations. In D. Saad (Ed.), Online learning and\nneural networks.\nBottou, L., & Bousquet, O. (2008). The tradeoffs of large\nscale learning. Advances in Neural Information Process-\ning Systems, 20, 161–168.\nChen, S., Donoho, D., & Saunders, M. (1999). Atomic de-\ncomposition by basis pursuit. SIAM Journal on Scientiﬁc\nComputing, 20, 33–61.\nEfron, B., Hastie, T., Johnstone, I., & Tibshirani, R. (2004).\nLeast angle regression. Annals of Statistics, 32, 407–\n499.\nElad, M., & Aharon, M. (2006). Image denoising via sparse\nand redundant representations over learned dictionaries.\nIEEE Transactions Image Processing, 54, 3736–3745.\nFisk, D. (1965). Quasi-martingale. Transactions of the\nAmerican Mathematical Society , 359–388.\nFriedman, J., Hastie, T., H ¨olﬂing, H., & Tibshirani, R.\n(2007). Pathwise coordinate optimization. Annals of\nStatistics, 1, 302–332.\nFu, W. (1998). Penalized Regressions: The Bridge V er-\nsus the Lasso. Journal of computational and graphical\nstatistics, 7, 397–416.\nFuchs, J. (2005). Recovery of exact sparse representations\nin the presence of bounded noise. IEEE Transactions\nInformation Theory, 51, 3601–3608.\nLee, H., Battle, A., Raina, R., & Ng, A. Y . (2007). Efﬁcient\nsparse coding algorithms. Advances in Neural Informa-\ntion Processing Systems, 19, 801–808.\nMairal, J., Elad, M., & Sapiro, G. (2008). Sparse represen-\ntation for color image restoration. IEEE Transactions\nImage Processing, 17, 53–69.\nMairal, J., Bach, F., Ponce, J., Sapiro, G., & Zisserman,\nA. (2009). Supervised dictionary learning. Advances in\nNeural Information Processing Systems, 21, 1033–1040.\nMallat, S. (1999). A wavelet tour of signal processing, sec-\nond edition. Academic Press, New Y ork.\nOlshausen, B. A., & Field, D. J. (1997). Sparse coding with\nan overcomplete basis set: A strategy employed by V1?\nVision Research, 37, 3311–3325.\nOsborne, M., Presnell, B., & Turlach, B. (2000). A new\napproach to variable selection in least squares problems.\nIMA Journal of Numerical Analysis, 20, 389–403.\nProtter, M., & Elad, M. (2009). Image sequence denoising\nvia sparse and redundant representations. IEEE Trans-\nactions Image Processing, 18, 27–36.\nRaina, R., Battle, A., Lee, H., Packer, B., & Ng, A. Y .\n(2007). Self-taught learning: transfer learning from un-\nlabeled data. Proceedings of the 26th International Con-\nference on Machine Learning, 759–766.\nTibshirani, R. (1996). Regression shrinkage and selection\nvia the Lasso. Journal of the Royal Statistical Society\nSeries B, 67, 267–288.\nV an der V aart, A. (1998).Asymptotic Statistics. Cambridge\nUniversity Press.\nZou, H., & Hastie, T. (2005). Regularization and variable\nselection via the elastic net. Journal of the Royal Statis-\ntical Society Series B, 67, 301–320.\n696",
  "values": {
    "Respect for Law and public interest": "Yes",
    "Respect for Persons": "Yes",
    "Interpretable (to users)": "Yes",
    "Privacy": "Yes",
    "User influence": "Yes",
    "Transparent (to users)": "Yes",
    "Justice": "Yes",
    "Fairness": "Yes",
    "Not socially biased": "Yes",
    "Critiqability": "Yes",
    "Autonomy (power to decide)": "Yes",
    "Collective influence": "Yes",
    "Non-maleficence": "Yes",
    "Beneficence": "Yes",
    "Explicability": "Yes",
    "Deferral to humans": "Yes"
  }
}