{
  "pdf": "1553374.1553486",
  "title": "Large-scale deep unsupervised learning using graphics processors",
  "author": "Rajat Raina, Anand Madhavan, Andrew Y. Ng",
  "paper_id": "1553374.1553486",
  "text": "Large-scale Deep Unsupervised Learning using Graphics Processors\nRajat Raina rajatr@cs.stanford.edu\nAnand Madhavan manand@stanford.edu\nAndrew Y. Ng ang@cs.stanford.edu\nComputer Science Department, Stanford University, Stanford CA 94305 USA\nAbstract\nThe promise of unsupervised learning meth-\nods lies in their potential to use vast amounts\nof unlabeled data to learn complex, highly\nnonlinear models with millions of free param-\neters. We consider two well-known unsuper-\nvised learning models, deep belief networks\n(DBNs) and sparse coding, that have recently\nbeen applied to a ﬂurry of machine learning\napplications (Hinton & Salakhutdinov, 2006;\nRaina et al., 2007). Unfortunately, current\nlearning algorithms for both models are too\nslow for large-scale applications, forcing re-\nsearchers to focus on smaller-scale models, or\nto use fewer training examples.\nIn this paper, we suggest massively paral-\nlel methods to help resolve these problems.\nWe argue that modern graphics processors\nfar surpass the computational capabilities of\nmulticore CPUs, and have the potential to\nrevolutionize the applicability of deep unsu-\npervised learning methods. We develop gen-\neral principles for massively parallelizing un-\nsupervised learning tasks using graphics pro-\ncessors. We show that these principles can\nbe applied to successfully scaling up learning\nalgorithms for both DBNs and sparse coding.\nOur implementation of DBN learning is up to\n70 times faster than a dual-core CPU imple-\nmentation for large models. For example, we\nare able to reduce the time required to learn a\nfour-layer DBN with 100 million free param-\neters from several weeks to around a single\nday. For sparse coding, we develop a simple,\ninherently parallel algorithm, that leads to a\n5 to 15-fold speedup over previous methods.\nAppearing in Proceedings of the 26 th International Confer-\nence on Machine Learning, Montreal, Canada, 2009. Copy-\nright 2009 by the author(s)/owner(s).\n1. Introduction\nWe consider two well-known unsupervised learning\nmodels, deep belief networks (DBNs) and sparse cod-\ning, that can learn hierarchical representations of their\ninput (Olshausen & Field, 1996; Hinton & Salakhutdi-\nnov, 2006). With the invention of increasingly eﬃcient\nlearning algorithms over the past decade, these mod-\nels have been applied to a number of machine learning\napplications, including computer vision, text modeling\nand collaborative ﬁltering, among others. These mod-\nels are especially well-suited to problems with high-\ndimensional inputs, over which they can learn rich\nmodels with many latent variables or layers. When\napplied to images, these models can easily have tens\nof millions of free parameters, and ideally, we would\nwant to use millions of unlabeled training examples\nto richly cover the input space. Unfortunately, with\ncurrent algorithms, parameter learning can take weeks\nusing a conventional implementation on a single CPU.\nPartly due to such daunting computational require-\nments, typical applications of DBNs and sparse cod-\ning considered in the literature generally contain many\nfewer free parameters (e.g., see Table 1), or are trained\non a fraction of the available input examples.\nIn our view, if the goal is to deploy better machine\nlearning applications, the diﬃculty of learning large\nmodels is a severe limitation. To take a speciﬁc case\nstudy, for two widely-studied statistical learning tasks\nin natural language processing—language modeling\nand spelling correction—it has been shown that sim-\nple, classical models can outperform newer, more com-\nplex models, just because the simple models can be\ntractably learnt using orders of magnitude more input\ndata (Banko & Brill, 2001; Brants et al., 2007).\nAnalogously, in our view, scaling up existing DBN and\nsparse coding models to use more parameters, or more\ntraining data, might produce very signiﬁcant perfor-\nmance beneﬁts. For example, it has been shown that\nsparse coding exhibits a qualitatively diﬀerent and\nhighly selective behavior called “end-stopping” when\n873\n\nLarge-scale Deep Unsupervised Learning using Graphics Processors\nTable 1. A rough estimate of the number of free parame-\nters (in millions) in some recent deep belief network appli-\ncations reported in the literature, compared to our desired\nmodel. To pick the applications, we looked through several\nresearch papers and picked the ones for which we could re-\nliably tell the number of parameters in the model. All the\nmodels do not implement exactly the same algorithm, and\nthe applications cited may not have used the largest-scale\nmodels possible, so this is not an exact comparison; but the\norder of magnitude diﬀerence between our desired model\nand recent work is striking.\nPublished source Application Params\nHinton et al., 2006 Digit images 1.6mn\nHinton & Salakhutdinov Face images 3.8mn\nSalakhutdinov & Hinton Sem. hashing 2.6mn\nRanzato & Szummer Text 3mn\nOur model 100mn\nthe model is large, but not otherwise (Lee et al., 2006).\nThere has been a lot of recent work on scaling up DBN\nand sparse coding algorithms, sometimes with entire\nresearch papers devoted to ingenious methods devised\nspeciﬁcally for each of these models (Hinton et al.,\n2006; Bengio et al., 2006; Murray & Kreutz-Delgado,\n2006; Lee et al., 2006; Kavukcuoglu et al., 2008).\nMeanwhile, the raw clock speed of single CPUs has\nbegun to hit a hardware power limit, and most of\nthe growth in processing power is increasingly ob-\ntained by throwing together multiple CPU cores, in-\nstead of speeding up a single core (Gelsinger, 2001;\nFrank, 2002). Recent work has shown that several\npopular learning algorithms such as logistic regres-\nsion, linear SVMs and others can be easily imple-\nmented in parallel on multicore architectures, by hav-\ning each core perform the required computations for a\nsubset of input examples, and then combining the re-\nsults centrally (Dean & Ghemawat, 2004; Chu et al.,\n2006). However, standard algorithms for DBNs and\nsparse coding are diﬃcult to parallelize with such\n“data-parallel” schemes, because they involve itera-\ntive, stochastic parameter updates, where any update\ndepends on the previous updates. This makes the up-\ndates hard to massively parallelize at a coarse, data-\nparallel level (e.g., by computing the updates in par-\nallel and summing them together centrally) without\nlosing the critical stochastic nature of the updates. It\nappears that ﬁne-grained parallelism might be needed\nto successfully parallelize these tasks.\nIn this paper, we exploit the power of modern graph-\nics processors (GPUs) to tractably learn large DBN\nand sparse coding models. The typical graphics card\nshipped with current desktops contains over a hun-\ndred processing cores, and has a peak memory band-\nwidth several times higher than modern CPUs. The\nGlobal Memory (1GB)\n…\n30 MPs\nMP1\nShared \nMemory\n(16K)\nSP SP SP SP\nSP SP SP SP\nMP2\nShared \nMemory\n(16K)\nMP30\nShared \nMemory\n(16K)\n100 GB/s\n(coalesced)\nSP SP SP SP\nSP SP SP SP\nSP SP SP SP\nSP SP SP SP\n1000 \nGB/s\nFigure 1. Simpliﬁed schematic for the Nvidia GeForce\nGTX 280 graphics card, with 240 total cores (30 multi-\nprocessors with 8 stream processors each).\nhardware can work concurrently with thousands of\nthreads, and is able to schedule these threads on the\navailable cores with very little overhead. Such ﬁne-\ngrained parallelism makes GPUs increasingly attrac-\ntive for general-purpose computation that is hard to\nparallelize on other distributed architectures.\nThere is of course a tradeoﬀ—this parallelism is ob-\ntained by devoting many more transistors to data pro-\ncessing, rather than to caching and control ﬂow, as\nin a regular CPU core. This puts constraints on the\ntypes of instructions and memory accesses that can be\neﬃciently implemented. Thus, the main challenge in\nsuccessfully applying GPUs to a machine learning task\nis to redesign the learning algorithms to meet these\nconstraints as far as possible. While a thorough intro-\nduction to graphics processor architecture is beyond\nthe scope of this paper, we now review the basic ideas\nbehind successful computation with GPUs.\n2. Computing with graphics processors\nWe illustrate the principles of GPU computing using\nNvidia’s CUDA programming model (Harris, 2008).\nFigure 1 shows a simpliﬁed schematic of a typical\nNvidia GPU. The GPU hardware provides two levels of\nparallelism: there are several multiprocessors (MPs),\nand each multiprocessor contains several stream pro-\ncessors (SPs) that run the actual computation. The\ncomputation is organized into groups of threads, called\n“blocks”, such that each block is scheduled to run on\na multiprocessor, and within a multiprocessor, each\nthread is scheduled to run on a stream processor.\nAll threads within a block (and thus executing on the\nsame multiprocessor) have shared access to a small\namount (16 KB) of very fast “shared memory,” and\nthey can synchronize with each other at diﬀerent\npoints in their execution. All threads also have access\nto a much larger GPU-wide “global memory” (cur-\nrently up to 4 GB) which is slower than the shared\nmemory, but is optimized for certain types of simul-\n874\nLarge-scale Deep Unsupervised Learning using Graphics Processors\ntaneous access patterns called “coalesced” accesses.\nBrieﬂy, memory access requests from threads in a\nblock are said to be coalesced if the threads access\nmemory in sequence (i.e., the k-th thread accesses the\nk-th consecutive location in memory). 1 When memory\naccesses are coalesced, the hardware can perform them\nin parallel for all stream processors, and the eﬀective\naccess speed (between the stream processors and the\nglobal memory) is several times faster than the access\nspeed between a CPU and RAM.\nSince GPU computation and within-GPU memory ac-\ncesses themselves are highly parallel, in many algo-\nrithms, the main bottleneck arises in transferring data\nbetween RAM and the GPU’s global memory. For ex-\nample, the total time taken to multiply two 1000x1000\nmatrices using our GPU conﬁguration (and a vendor-\nsupplied linear algebra package) is roughly 20 millisec-\nonds, but the actual computation takes only 0.5% of\nthat time, with the remaining time being used for\ntransfer in and out of global memory. A partial so-\nlution is to perform memory transfers only in large\nbatches, grouped over several computations. In our\nexample, if we were doing 25 diﬀerent matrix multi-\nplications and were able to perform memory transfers\nin large chunks (by transferring all inputs together,\nand transferring all outputs together), then as much\nas 25% of the total time is spent in computation. Thus,\neﬃcient use of the GPU’s parallelism requires careful\nconsideration of the data ﬂow in the application.\n3. Preliminaries\nWe now introduce the unsupervised learning problems\nwe consider in this paper, and analyze the speciﬁc is-\nsues faced in applying GPUs to those problems. We\nconsider an unsupervised learning task where we are\ngiven a large unlabeled dataset {x(1), x (2), . . . , x (m)},\nwith each input x(i) ∈ Rk. The goal is to learn a\nmodel for the inputs x, and to then apply the model\nto speciﬁc machine learning tasks. For example, each\nunlabeled input x(i) ∈ R900 might represent a 30x30\npixel image of a handwritten character (represented as\na vector of pixel intensities). We might want to learn\na model for the complex 900-dimensional space of in-\nputs, and then use this model to classify new hand-\nwritten characters using only very little labeled data.\n3.1. Deep Belief Networks\nDBNs are multilayer neural network models that learn\nhierarchical representations for their input data. Hin-\n1For simplicity, we ignore certain other technical condi-\ntions that are easy to obey in practice. We also omit dis-\ncussion of two other types of memory—constant and tex-\nture memory—that are optimized for other speciﬁc types\nof access patterns that we do not use in our applications.\nton et al. (2006) proposed an unsupervised algorithm\nfor learning DBNs, in which the DBN is greedily built\nup layer-by-layer, starting from the input data. Each\nlayer is learnt using a probabilistic model called a re-\nstricted Boltzmann machine (RBM). Brieﬂy, an RBM\ncontains a set of stochastic hidden units h that are fully\nconnected in an undirected model to a set of stochas-\ntic visible units x. Assuming binary-valued units, the\nRBM deﬁnes the following joint distribution:\nP (x, h) ∝ exp\n( ∑\ni,jxiwijhj + ∑\ni cixi + ∑\nj bjhj\n)\nwhere the weights w and biases b and c are parame-\nters to be tuned. The conditional distributions can be\nanalytically computed:\nP (hj|x) = sigmoid( bj + ∑\ni wijxi) (1)\nP (xi|h) = sigmoid( ci + ∑\nj wijhj) (2)\nMaximum likelihood parameter learning for an RBM\ncan be eﬃciently approximated by contrastive diver-\ngence updates (Hinton, 2002), where we start with the\nunlabeled examples as the visible units, alternately\nsample the hidden units h and visible units x using\nGibbs sampling (Equations 1-2), and update the pa-\nrameters as:\nwij := wij + η (⟨xihj⟩data − ⟨xihj⟩sample) (3)\nci := ci + η (⟨xi⟩data − ⟨xi⟩sample) (4)\nbj := bj + η (⟨hj⟩data − ⟨hj⟩sample) (5)\nwhere η is the learning rate, ⟨·⟩data represents expecta-\ntions with the visible units tied to the input examples,\nand ⟨·⟩sample represents expectations after T ≥ 1 itera-\ntions of Gibbs sampling. Since each update requires a\nGibbs sampling operation, and the updates have to be\napplied over many unlabeled examples to reach con-\nvergence, unsupervised learning of the parameters can\ntake several days to complete on a modern CPU.\n3.2. Sparse Coding\nSparse coding is an algorithm for constructing suc-\ncinct representations of input data (Olshausen & Field,\n1996). Using our earlier example, if each input x(i) ∈\nR900 represents a handwritten character image, sparse\ncoding attempts to learn that each handwritten char-\nacter is composed of only a few building blocks, such\nas pen strokes (instead of 900 arbitrary intensity val-\nues). Such a higher-level representation can then be\napplied to classiﬁcation tasks, where it leads to good\nresults even with limited labeled data (Raina et al.,\n2007; Bradley & Bagnell, 2008).\nSpeciﬁcally, given inputs x ∈ Rk, sparse coding at-\ntempts to ﬁnd basis vectors b = {b1, b 2, . . . , b n}, bj ∈\nRk such that each input x can be represented as a lin-\near combination of a few basis vectors: x ≈ ∑\nj ajbj,\nwhere aj ∈ R represents the activation of basis bj,\n875\nLarge-scale Deep Unsupervised Learning using Graphics Processors\nand most of the aj values are zero (or, the vector a\nis sparse). The basis vectors are found by solving the\nfollowing optimization problem (Lee et al., 2006):\nminimizeb,a 1\n2\n∑\ni ∥x(i) − ∑\nj a(i)\nj bj∥2 + β ∑\ni,j|a(i)\nj |\ns.t. ∥bj∥ ≤ 1, ∀j ∈ {1, ..., n}\nwhere the ﬁrst term in the objective function encour-\nages good reconstruction ( x(i) ≈ ∑\nj bja(i)\nj ), and the\nsecond term encourages sparsity by penalizing non-\nzero activations (Tibshirani, 1996). The optimization\nproblem is not jointly convex in both b and a vari-\nables, but it is convex in either one of those variables,\nif the other is kept ﬁxed. This suggests an alternating\nminimization algorithm with two steps: ﬁrst, keep-\ning b ﬁxed, we optimize over a, which leads to an L1-\nregularized least squares problem, that can be solved\nusing custom-designed solvers (Efron et al., 2004; Lee\net al., 2006; Andrew & Gao, 2007). Then, we keep a\nﬁxed, and optimize over b using convex optimization\ntechniques (Lee et al., 2006). For problems with high-\ndimensional inputs and large numbers of basis vectors,\nthe ﬁrst step is particularly time consuming as it in-\nvolves a non-diﬀerentiable objective function, and the\noverall learning algorithm can take several days.\n4. GPUs for unsupervised learning\nBoth the above algorithms repeatedly execute the fol-\nlowing computations: pick a small number of unla-\nbeled examples, compute an update (by contrastive di-\nvergence or by solving a convex optimization problem),\nand apply it to the parameters. To successfully apply\nGPUs to such unsupervised learning algorithms, we\nneed to satisfy two major requirements. First, memory\ntransfers between RAM and the GPU’s global memory\nneed to be minimized, or grouped into large chunks.\nFor machine learning applications, we can achieve this\nby storing all parameters permanently in GPU global\nmemory during learning. Unlabeled examples usually\ncannot all be stored in global memory, but they should\nbe transferred only occasionally into global memory in\nas large chunks as possible. With both parameters and\nunlabeled examples in GPU global memory, the up-\ndates can be computed without any memory transfer\noperations, with any intermediate computations also\nstored in global memory.\nA second requirement is that the learning updates\nshould be implemented to ﬁt the two level hierar-\nchy of blocks and threads, in such a way that shared\nmemory can be used where possible, and global mem-\nory accesses can be coalesced. Often, blocks can ex-\nploit data parallelism (e.g., each block can work on\na separate input example), while threads can exploit\nmore ﬁne-grained parallelism because they have access\nto very fast shared memory and can be synchronized\n(e.g., each thread can work on a single coordinate of\nthe input example assigned to the block). Further,\nthe graphics hardware can hide memory latencies for\nblocks waiting on global memory accesses by schedul-\ning a ready-to-run block in that time. To fully use such\nlatency hiding, it is beneﬁcial to use a large number\nof independently executing blocks. In some cases, as\ndiscussed for sparse coding in Section 6, we can com-\npletely redesign the updates to be inherently parallel\nand require less synchronization between threads.\nWe thus arrive at the following template algorithm for\napplying GPUs to unsupervised learning tasks:\nAlgorithm 1 Parallel unsupervised learning on GPUs\nInitialize parameters in global memory.\nwhile convergence criterion is not satisﬁed do\nPeriodically transfer a large number of unlabeled\nexamples into global memory.\nPick a few of the unlabeled examples at a time,\nand compute the updates in parallel using the\nGPU’s two-level parallelism (blocks and threads).\nend while\nTransfer learnt parameters from global memory.\n5. Learning large deep belief networks\nWe apply Algorithm 1 to learning large DBNs using\nthe contrastive divergence updates in Equations (3-\n5). The parameters w, c and b for all the DBN layers\nare maintained permanently in global memory during\ntraining. The updates require repeated Gibbs sam-\npling using the distributions in Equations (1-2). These\ndistributions can be rewritten using matrix notation:\nP (h|x) = vectorSigmoid(b + wT x)\nP (x|h) = vectorSigmoid(c + wh)\nwhere vectorSigmoid(·) represents the elementwise sig-\nmoid function, and x, h are vectors containing an el-\nement corresponding to each visible and hidden unit\nrespectively. The above computations can be batched\ntogether for several examples for further eﬃciency.\nThe matrix operations can be performed in parallel us-\ning optimized linear algebra packages for the GPU, and\nthe sigmoid computation and sampling can be done by\na simple parallelization scheme where each block works\non a single example, and each thread in the block\nworks on a single element of the example. Finally,\nonce the samples have been generated, the updates\ncan again be applied in parallel using linear algebra\npackages: e.g., w := w + η\n(\n⟨xT h⟩data − ⟨xT h⟩sample\n)\nWe extend our method to learning deep belief networks\nwith “overlapping patches” (Figure 2). This model is\nmost easily understood with hidden and visible units\narranged in a 2-D array (e.g., when the input is an\n876\nLarge-scale Deep Unsupervised Learning using Graphics Processors\nHidden UnitsBHidden UnitsA\nInput image\nPatch A\nPatch B\nwA, bA, cA wB, bB, cB\n. . . . . .\nFigure 2. A schematic of the overlapping patches model.\nTwo patches A and B in the input image are shown, with\neach patch connected to a diﬀerent set of hidden units.\nThe connections are parameterized by their own sets of\nparameters wA, b A, c A and wB, b B, c B.\nimage and each visible unit is a pixel). The input im-\nage is tiled by equally-spaced, equal-sized patches (or\nreceptive ﬁelds), and each patch is fully connected to\na unique group of hidden units. There is no weight\nsharing in this model, and each connection is param-\neterized by a free parameter. Because of the overlap-\nping patches, all the parameters in the model depend\non each other, making learning hard. However, Gibbs\nsampling can still be performed in parallel for this\nmodel: each visible unit depends on hidden units at\nmany diﬀerent locations, but the sampling operation\nx|h can be implemented using only coalesced global\nmemory accesses (implementation details omitted).\nThese overlapping patch RBMs can be stacked on top\nof each other, such that the second-layer RBM con-\ntains hidden units connected locally to ﬁrst-layer hid-\nden units, and so on. The resulting deep networks\nhave a very large number of units, but only sparse,\nlocal connections, which make learning tractable even\nfor models with more than 100 million parameters.\nExperimental Results: We compare our GPU-\nbased algorithm against CPU-based methods using the\nfollowing multicore hardware:\n• GPU: Nvidia GeForce GTX 280 graphics card\nwith 1GB memory. Dual-core CPU @ 3.16GHz.\nReported results show the total running time (in-\ncluding all computation, memory transfer, etc.).\n• Single CPU : Single core @ 3.16GHz.\n• Dual-core CPU : Two cores, each @ 3.16GHz.\n(Identical machine as for the GPU result.)\nThe CPU-based method was implemented using two\nhighly optimized multithreaded linear algebra pack-\nages: ATLAS BLAS (Whaley et al., 2001) and Goto\nBLAS (Goto & Van De Geijn, 2008). Consistent\nwith previous results, we found that Goto BLAS was\nfaster (Bengio, 2007), so we report CPU results us-\ning it. As input, we used a large dataset of natural\nimages (van Hateren & van der Schaaﬀ, 1997) and ob-\ntained input examples by randomly extracting square\nimage patches of the required size. Following previ-\nous work, we used Gaussian visible units and binary\nhidden units, and trained a sparse RBM by adding an\nadditional penalty term to the objective (Lee et al.,\n2007)—however, these modiﬁcations do not aﬀect the\nrunning time results signiﬁcantly. For learning, we\nperformed one-step contrastive divergence updates us-\ning a mini-batch of 192 examples.\nTable 2 shows the running time for processing 1 million\nexamples for RBMs of varying size (denoted by num-\nber of visible units × number of hidden units). The\nGPU method is between 12 to 72 times faster. The\nspeedup obtained is highest for large RBMs, where\nthe computations involve large matrices and can be\nmore eﬃciently parallelized by using a large number\nof concurrent blocks (which allows the graphics hard-\nware to better hide memory latencies). The largest\nmodel in Table 2 has 45 million parameters, and our\nGPU method can update these parameters using a mil-\nlion examples in about 29 minutes. In comparison, our\nmulticore CPU takes more than a day per million ex-\namples. Since we would ideally want to use tens of\nmillions of training examples for learning such a large\nmodel, the CPU method is impractical for such tasks.\nTable 3 shows a similar running time comparison for\ntwo “overlapping patch” models (see table caption for\ndetails). The GPU method is about 10 times faster\nthan the dual-core CPU. This speedup is somewhat\nlower than the speedup observed for a fully connected\nRBM (Table 2), because Gibbs sampling in the over-\nlapping patch model requires many operations involv-\ning small matrices (one weight matrix per patch), in-\nstead of only a few operations involving large matri-\nces. Using the overlapping patch model, we can learn a\nfour-layer DBN with 96 million parameters, and 25600,\n82944, 8192, 4608 and 1024 units respectively in the\ninput layer and the four successive hidden layers. Such\nmodels are at least an order of magnitude larger than\npreviously published work on DBNs.\nFinally, we note that the overlapping patches model\ncan be modiﬁed to share parameters in all patches,\nsuch that, for example, wA = wB in Figure 2. If over-\nlapping patches are tiled one pixel apart, this model is\nidentical to the convolutional RBM model (Desjardins\n& Bengio, 2008; Lee et al., 2009). Contrastive diver-\ngence learning in this model can be implemented by\nusing convolutions to perform the Gibbs sampling op-\neration h|x. For small to medium ﬁlter (patch) sizes,\nspatial convolution can be implemented very eﬃciently\nusing GPUs, by having each block read a ﬁlter into\nshared memory, then reading the input image column-\n877\nLarge-scale Deep Unsupervised Learning using Graphics Processors\nTable 2. Average running time in seconds for processing 1 million input examples for learning an RBM, with contrastive\ndivergence updates applied in batches of 192 examples each. The size of the RBM in each column is denoted by the\nnumber of visible units × number of hidden units. The GPU speedup is computed w.r.t. the fastest CPU-based result.\nPackage Architecture 576x1024 1024x4096 2304x16000 4096x11008\nGoto BLAS Single CPU 563s 3638s 172803s 223741s\nGoto BLAS Dual-core CPU 497s 2987s 93586s 125381s\nGPU 38.6s 184s 1376s 1726s\nGPU Speedup 12.9x 16.2x 68.0x 72.6x\nTable 3. Average time in seconds for processing 1 million\nexamples for the overlapping patch model, with contrastive\ndivergence updates applied in batches of 192 examples\neach. The model size in each column is denoted by the\nnumber of visible units × number of hidden units (but note\nthat the units are not fully connected). The two models\nwere created by taking 144x144 pixel and 192x192 pixel\ninputs respectively; the size of each patch is 24x24 pixels,\nthere are 192 hidden units connected to each patch, and\nneighboring patches are 8 pixels apart. Overall, the models\nhave 28 million and 54 million free parameters respectively.\nPackage Arch. 20736x49152 36864x92928\nGoto Single CPU 38455s 77246s\nGoto Dual-core 32236s 65235s\nGPU 3415s 6435s\nGPU Speedup 9.4x 10.1x\nby-column into shared memory, and ﬁnally aggregat-\ning the output elements aﬀected by that ﬁlter and that\ninput image column. It can be shown that by ordering\noperations in this way, we use only fast shared memory\naccesses and coalesced global memory accesses. 2 For\nexample, on computing the convolution of 32 128x128\nimages with 32 16x16 ﬁlters, our GPU implementation\nof spatial convolution (including the time to transfer\nimages/ﬁlters into GPU memory) is over 100 times\nfaster than either spatial convolution implemented in\nC or FFT-based convolution in Matlab.\n6. Parallel sparse coding\nWe now consider the sparse coding optimization prob-\nlem discussed in Section 3.2. Following the tem-\nplate in Algorithm 1, we maintain the basis param-\neters b permanently in global memory, and transfer\ninput examples to GPU global memory periodically in\nlarge batches. Following the alternating minimization\nmethod, each update itself consists of two steps: the\nﬁrst, simpler part of the update involves optimizing\nover b, given ﬁxed a:\nminimize b\n∑\ni ∥x(i) − ∑\nj a(i)\nj bj∥2 s.t. ∥bj∥ ≤ 1, ∀j\nWe solve this problem using projected gradient de-\nscent, where we follow the gradient of the quadratic\nobjective function, and project at each step to the\n2For larger ﬁlter sizes FFT-based convolution is gener-\nally better, and a GPU FFT package can be used.\nfeasible set. 3 This method is guaranteed to converge\nto the optimal b and can be straightforwardly imple-\nmented using a GPU linear algebra package.\nThe other part of the update involves optimizing over\na, given ﬁxed b. Since the activation a(i) for each ex-\nample x(i) is now independent of the activations for\nother examples, it suﬃces to consider the following\ncanonical L1-regularized least squares problem for a\nsingle input example x:\nminimize a 1\n2 ∥x − ∑\nj ajbj∥2 + β ∑\nj |aj| (6)\nThe objective function is not diﬀerentiable because of\nthe second term. This problem has recently received\nwide attention because of its robust feature selection\nproperties (Tibshirani, 1996; Ng, 2004), and custom\nalgorithms have been designed to solve it (Efron et al.,\n2004; Lee et al., 2006; Andrew & Gao, 2007). Some of\nthese algorithms use sparse linear algebra operations\nto achieve eﬃciency. We instead present a very diﬀer-\nent algorithm that is inherently parallel and thus uses\nthe GPU hardware more eﬃciently.\n6.1. Parallel L1-regularized least squares\nOur algorithm is based on the observation that in the\noptimization problem in Equation (6), if we vary only\none of the activations aj, while keeping the other acti-\nvations ﬁxed, the optimal value a∗\nj can be easily com-\nputed (Friedman et al., 2007). Letting B be a matrix\nwith bj as its j-th column, and rj = bT\nj bj:\na∗\nj =\n\n\n\n0 if |gj − rjaj| ≤ β\n(−g j + rjaj + β)/r j if gj − rjaj > β\n(−g j + rjaj − β)/r j if gj − rjaj < −β\nwhere g = ∇ a\n1\n2 ∥x − ∑\nj ajbj∥2 = BT Ba − BT x.\nThe updates can be eﬃciently performed in parallel by\nhaving thread j compute just one coordinate a∗\nj . Fur-\nther, since we usually batch several examples together,\nwe can precompute the matrix BT B, the vector BT x\nand the vector r once in parallel, store the result in\nglobal memory, and perform only eﬃcient accesses to\ncompute all a∗\nj values.4\n3The projection operation is particularly simple: for\neach basis vector bj, if ∥bj∥ > 1 then rescale bj to have\nnorm 1, otherwise keep bj unchanged.\n4To see why, note that to compute a∗\nj , thread j needs to\n878\nLarge-scale Deep Unsupervised Learning using Graphics Processors\nThus, we propose the following iterative algorithm: at\neach iteration, starting at the current activation values\na = ˆa, we compute all the optimal coordinate values\na∗\nj in parallel as outlined above. Then, we perform\na line search in the direction of vector d = a∗ − ˆa.\nThe line search consists of ﬁnding a step size t > 0\nsuch that the value of the objective function at the\npoint a = ˆa + td is lower than the value at a = ˆa.\nThis line search, including the function evaluations,\ncan be run in parallel. 5 We then move to the new\npoint a = ˆa + td, and iterate. We declare convergence\nwhen the objective value decreases by less than a 10 −6\nfraction of the previous objective value.\nSince the direction d is a nonnegative linear combina-\ntion of descent directions along the coordinate axes:\ndj = a∗\nj − ˆaj, d must itself be a descent direction for\nthe objective function. Thus, at each iteration, a step\nsize t > 0 can always be found that reduces the value\nof the objective function, and the overall algorithm is\nguaranteed to converge to the optimal solution.\nThis algorithm uses ﬁne-grained parallelism by having\neach thread compute just one coordinate of the solu-\ntion. Such highly multithreaded execution is especially\nwell-suited for graphics processors, as the hardware is\nable to hide memory latency (for threads blocked on\nmemory accesses) by scheduling other threads that are\nnot blocked on memory accesses, and leads to high uti-\nlization of the available cores.\nExperimental Results: We again compare our\nmethod against a multicore CPU baseline (Lee et al.,\n2006). We used optimized Matlab code provided by\nLee et al. For the CPU multicore results, we executed\nthe same Matlab code with multithreading enabled.\nTable 4 shows the running time for applying sparse\ncoding basis updates (including both basis and activa-\ntion optimization) for m = 5000 examples, with mini-\nbatches of 1000 examples. Each example x ∈ R1024\ncompute gj − rjaj = P\nt(BT B)tjat − (BT x)j − rjaj. Con-\nsider the elements thread j accesses: (i) ( BT B)tj: Accesses\ncan be coalesced if BT B is stored in row-major order. (ii)\nBy maintaining a in shared memory, all threads can access\nthe same element at simultaneously, as well as access the\nelements aj that are diﬀerent for each thread. (For the in-\nterested reader, we add that this avoids “bank conﬂicts” in\nshared memory. See CUDA reference manual for details.)\n(iii) (B T x)j and rj: Can be coalesced as thread j accesses\nthe j-th location.\n5Details: By substituting a = ˆa + td in the original\nobjective function, the line search reduces to minimizing a\n1-D function of the form f (t) = α 2t2 +α 1t+α 0 +β∥ˆa+td∥1,\nwhere the values α 2, α 1, α 0 can be computed in parallel.\nFor the 1-D line search over f (t), we simply try a ﬁxed set\nof positive step sizes, and pick the largest step size that\nreduces the value of the objective function.\nTable 4. Average running time for updating sparse coding\nparameters on 5000 input examples. The GPU speedup is\ncomputed w.r.t. the fastest CPU-based result. The spar-\nsity value refers to the average percentage of the 1024 acti-\nvations that were nonzero at the optimal solution; diﬀerent\nsparsity was obtained by using diﬀerent β values. Note that\n3-10% is a reasonable range as it corresponds to around 30\nto 100 nonzero activations per input example.\nMethod Sparsity≈3% 6% 10%\nSingle CPU 215s 403s 908s\nDual-core 191s 375s 854s\nGPU 37.0s 41.5s 55.8s\nSpeedup 5.2x 9.0x 15.3x\nwas obtained via a randomly sampled 32x32 pixel nat-\nural image patch. We used n = 1024 basis vectors,\ninitialized randomly. The majority of sparse coding\ntime in Lee et al.’s method is taken by the activa-\ntion learning step, especially when many activations\nare nonzero at the optimum. By eﬀectively paralleliz-\ning this step, our GPU method is up to 15 times faster\nthan a dual-core implementation.\n7. Discussion\nGraphics processors are able to exploit ﬁner-grained\nparallelism than current multicore architectures or dis-\ntributed clusters. They are designed to maintain thou-\nsands of active threads at any time, and to schedule the\nthreads on hundreds of cores with very low scheduling\noverhead. The map-reduce framework (Dean & Ghe-\nmawat, 2004) has been successfully applied to par-\nallelize a class of machine learning algorithms (Chu\net al., 2006). However, that method relies exclusively\non data parallelism—each core might work indepen-\ndently on a diﬀerent set of input examples—with no\nfurther subdivision of work. In contrast, the two-level\nparallelism oﬀered by GPUs is much more powerful:\nthe top-level GPU blocks can already exploit data par-\nallelism, and GPU threads can further subdivide the\nwork in each block, often working with just a single\nelement of an input example.\nGPUs have been applied to certain problems in ma-\nchine learning, including SVMs (Catanzaro et al.,\n2008), and supervised learning in convolutional net-\nworks (Chellapilla et al., 2006). To continue this line\nof work, and to encourage further applications of deep\nbelief networks and sparse coding, we will make our\nsource code publicly available.\nAcknowledgments: We give warm thanks to Roger\nGrosse, Honglak Lee and the anonymous reviewers for\nhelpful comments, and to Ethan Dreyfuss, Ian Good-\nfellow and Quoc Le for help with assembling the hard-\nware. This work was supported by the DARPA trans-\nfer learning program under contract number F A8750-\n879\nLarge-scale Deep Unsupervised Learning using Graphics Processors\n05-2-0249, and by the Oﬃce of Naval Research under\nMURI N000140710747.\nReferences\nAndrew, G., & Gao, J. (2007). Scalable training of L1-\nregularized log-linear models. International Confer-\nence on Machine Learning (pp. 33–40).\nBanko, M., & Brill, E. (2001). Scaling to very very\nlarge corpora for natural language disambiguation.\nAnnual Meeting of the Association for Computa-\ntional Linguistics (pp. 26–33).\nBengio, Y. (2007). Speeding up stochastic gradient\ndescent. Neural Information Processing Systems\nWorkshop on Eﬃcient Machine Learning.\nBengio, Y., Lamblin, P., Popovici, D., & Larochelle,\nH. (2006). Greedy layer-wise training of deep net-\nworks. Neural Information Processing Systems (pp.\n153–160).\nBradley, D., & Bagnell, J. A. (2008). Diﬀerentiable\nsparse coding. Neural Information Processing Sys-\ntems (pp. 113–120).\nBrants, T., Popat, A. C., Xu, P., Och, F. J., & Dean,\nJ. (2007). Large language models in machine trans-\nlation. Conference on Empirical Methods in Natural\nLanguage Processing (EMNLP-CoNLL).\nCatanzaro, B. C., Sundaram, N., & Keutzer, K.\n(2008). Fast support vector machine training and\nclassiﬁcation on graphics processors. International\nConference on Machine Learning (pp. 104–111).\nChellapilla, K., Puri, S., & Simard, P. (2006). High\nperformance convolutional neural networks for doc-\nument processing. International Workshop on Fron-\ntiers in Handwriting Recognition.\nChu, C. T., Kim, S. K., Lin, Y. A., Yu, Y., Bradski,\nG. R., Ng, A. Y., & Olukotun, K. (2006). Map-\nreduce for machine learning on multicore. Neural\nInformation Processing Systems (pp. 281–288).\nDean, J., & Ghemawat, S. (2004). Mapreduce: Sim-\npliﬁed data processing on large clusters. Operating\nSystem Design and Implementation (pp. 137–150).\nDesjardins, G., & Bengio, Y. (2008). Empirical evalua-\ntion of convolutional RBMs for vision. Tech Report.\nEfron, B., Hastie, T., Johnstone, I., & Tibshirani, R.\n(2004). Least angle regression. Ann. Stat., 32, 407.\nFrank, D. (2002). Power-constrained CMOS scaling\nlimits. IBM Jour. of Res. and Devel. , 46, 235–244.\nFriedman, J., Hastie, T., Hﬂing, H., & Tibshirani, R.\n(2007). Pathwise coordinate optimization. Ann.\nApp. Stat. , 2, 302–332.\nGelsinger, P. (2001). Microprocessors for the new mil-\nlennium: Challenges, opportunities and new fron-\ntiers. ISSCC Tech. Digest, 22–25.\nGoto, K., & Van De Geijn, R. (2008). High-\nperformance implementation of the level-3 BLAS.\nACM Trans. Math. Softw., 35, 1–14.\nHarris, M. (2008). Many-core GPU computing with\nNVIDIA CUDA. Int. Conf. Supercomputing (p. 1).\nHinton, G. E. (2002). Training products of experts by\nminimizing contrastive divergence. Neural Compu-\ntation, 14, 1771–1800.\nHinton, G. E., Osindero, S., & Teh, Y.-W. (2006). A\nfast learning algorithm for deep belief nets. Neural\nComputation, 18, 1527–1554.\nHinton, G. E., & Salakhutdinov, R. R. (2006). Reduc-\ning the dimensionality of data with neural networks.\nScience, 313, 504–507.\nKavukcuoglu, K., Ranzato, M., & LeCun, Y. (2008).\nFast inference in sparse coding algorithms with ap-\nplications to object recognition. NYU Tech Report.\nLee, H., Battle, A., Raina, R., & Ng, A. Y. (2006). Ef-\nﬁcient sparse coding algorithms. Neural Information\nProcessing Systems (pp. 801–808).\nLee, H., Chaitanya, E., & Ng, A. Y. (2007). Sparse\ndeep belief net model for visual area V2. Neural\nInformation Processing Systems (pp. 873–880).\nLee, H., Grosse, R., Ranganath, R., & Ng, A. Y.\n(2009). Convolutional deep belief networks for scal-\nable unsupervised learning of hierarchical repre-\nsentations. International Conference on Machine\nLearning (to appear) .\nMurray, J. F., & Kreutz-Delgado, K. (2006). Learn-\ning sparse overcomplete codes for images. J. VLSI\nSignal Processing Systems, 45, 97–110.\nNg, A. Y. (2004). Feature selection, L1 vs. L2 regu-\nlarization, and rotational invariance. International\nConference on Machine Learning (pp. 78–85).\nOlshausen, B. A., & Field, D. J. (1996). Emergence\nof simple-cell receptive ﬁeld properties by learning a\nsparse code for natural images. Nature, 381, 607–609.\nRaina, R., Battle, A., Lee, H., Packer, B., & Ng, A. Y.\n(2007). Self-taught learning: Transfer learning from\nunlabeled data. International Conference on Ma-\nchine Learning (pp. 759–766).\nRanzato, M. A., & Szummer, M. (2008). Semi-\nsupervised learning of compact document represen-\ntations with deep networks. International Confer-\nence on Machine Learning (pp. 792–799).\nSalakhutdinov, R., & Hinton, G. (2007). Semantic\nHashing. SIGIR Workshop on Information Retrieval\nand Applications of Graphical Models.\nTibshirani, R. (1996). Regression shrinkage and selec-\ntion via the lasso. J. R. Stat. Soc. B., 58, 267–288.\nvan Hateren, J. H., & van der Schaaﬀ, A. (1997). In-\ndependent component ﬁlters of natural images com-\npared with simple cells in primary visual cortex.\nRoyal Soc. Lond. B, 265, 359–366.\nWhaley, R. C., Petitet, A., & Dongarra, J. J. (2001).\nAutomated empirical optimization of software and\nthe ATLAS project. Parallel Computing, 27, 3–35.\n880",
  "values": {
    "Respect for Law and public interest": "Yes",
    "Autonomy (power to decide)": "Yes",
    "Critiqability": "Yes",
    "Explicability": "Yes",
    "Collective influence": "Yes",
    "User influence": "Yes",
    "Respect for Persons": "Yes",
    "Privacy": "Yes",
    "Non-maleficence": "Yes",
    "Deferral to humans": "Yes",
    "Interpretable (to users)": "Yes",
    "Not socially biased": "Yes",
    "Transparent (to users)": "Yes",
    "Beneficence": "No",
    "Fairness": "No",
    "Justice": "No"
  }
}