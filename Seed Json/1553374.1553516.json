{
  "pdf": "1553374.1553516",
  "title": "Feature hashing for large scale multitask learning",
  "author": "Kilian Weinberger, Anirban Dasgupta, John Langford, Alex Smola, Josh Attenberg",
  "paper_id": "1553374.1553516",
  "text": "Feature Hashing for Large Scale Multitask Learning\nKilian Weinberger KILIAN @Y AHOO-INC .COM\nAnirban Dasgupta ANIRBAN @Y AHOO-INC .COM\nJohn Langford JL @HUNCH .NET\nAlex Smola ALEX @SMOLA .ORG\nJosh Attenberg JOSH @CIS .POL Y.EDU\nY ahoo! Research, 2821 Mission College Blvd., Santa Clara, CA 95051 USA\nAbstract\nEmpirical evidence suggests that hashing is an\neffective strategy for dimensionality reduction\nand practical nonparametric estimation. In this\npaper we provide exponential tail bounds for fea-\nture hashing and show that the interaction be-\ntween random subspaces is negligible with high\nprobability. We demonstrate the feasibility of\nthis approach with experimental results for a new\nuse case — multitask learning with hundreds of\nthousands of tasks.\n1. Introduction\nKernel methods use inner products as the basic tool for\ncomparisons between objects. That is, given objects\nx\n1,...,x n ∈ X for some domain X, they rely on\nk(xi,xj): = ⟨φ(xi),φ(xj)⟩ (1)\nto compare the features φ(xi) of xi and φ(xj) of xj respec-\ntively.\nEq. (1) is often famously referred to as the kernel-trick.I t\nallows the use of inner products between very high dimen-\nsional feature vectors φ(xi) and φ(xj) implicitly through\nthe deﬁnition of a positive semi-deﬁnite kernel matrix k\nwithout ever having to compute a vector φ(xi) directly.\nThis can be particularly powerful in classiﬁcation settings\nwhere the original input representation has a non-linear de-\ncision boundary. Often, linear separability can be achieved\nin a high dimensional feature space φ(x\ni).\nIn practice, for example in text classiﬁcation, researchers\nfrequently encounter the opposite problem: the original in-\nput space is almost linearly separable (often because of the\nAppearing in Proceedings of the 26 th International Conference\non Machine Learning, Montreal, Canada, 2009. Copyright 2009\nby the author(s)/owner(s).\nexistence of handcrafted non-linear features), yet, the train-\ning set may be prohibitively large in size and very high di-\nmensional. In such a case, there is no need to map the input\nvectors into a higher dimensional feature space. Instead,\nlimited memory makes storing a kernel matrix infeasible.\nFor this common scenario several authors have recently\nproposed an alternative, but highly complimentary vari-\nation of the kernel-trick, which we refer to as the\nhashing-trick: one hashes the high dimensional input vec-\ntors x into a lower dimensional feature space R\nm with\nφ : X → Rm (Langford et al., 2007; Shi et al., 2009). The\nparameter vector of a classiﬁer can therefore live in Rm in-\nstead of in the original input spaceR d (or in Rn in the case\nof kernel matrices), where m ≪ n and m ≪ d. Different\nfrom random projections, the hashing-trick preserves spar-\nsity and introduces no additional overhead to store projec-\ntion matrices.\nTo our knowledge, we are the ﬁrst to provide exponential\ntail bounds on the canonical distortion of these hashed inner\nproducts. We also show that the hashing-trick can be partic-\nularly powerful in multi-task learning scenarios where the\noriginal feature spaces are the cross-product of the data, X,\nand the set of tasks, U . We show that one can use differ-\nent hash functions for each task φ\n1,...,φ |U |, to map the\ndata into one joint space with little interference. Sharing\namongst the different tasks is achieved with an additional\nhash function φ\n0 that also maps into the same joint space.\nThe hash function φ0 is shared amongst all |U | tasks and\nallows to learn their common components.\nWhile many potential applications exist for the hashing-\ntrick, as a particular case study we focus on collaborative\nemail spam ﬁltering. In this scenario, hundreds of thou-\nsands of users collectively label emails as spam or not-\nspam, and each user expects a personalized classiﬁer that\nreﬂects their particular preferences. Here, the set of tasks,\nU , is the number of email users (this can be very large for\nopen systems such as Yahoo Mail\nTMor GmailTM), and the\nfeature space spans the union of vocabularies in multitudes\n1113\n\nFeature Hashing for Large Scale Multitask Learning\nof languages.\nThis paper makes four main contributions: 1. In sec-\ntion 2 we introduce specialized hash functions with unbi-\nased inner-products that are directly applicable to a large\nvariety of kernel-methods. 2. In section 3 we provide ex-\nponential tail bounds that help explain why hashed feature\nvectors have repeatedly lead to, at times surprisingly, strong\nempirical results. 3. In the same chapter we show that\nthe interference between independently hashed subspaces\nis negligible with high probability, which allows large-scale\nmulti-task learning in a very compressed space. 4. In sec-\ntion 5 we introduce collaborative email-spam ﬁltering as a\nnovel application for hash representations and provide ex-\nperimental results on large-scale real-world spam data sets.\n2. Hash Functions\nWe introduce a variant on the hash kernel proposed by (Shi\net al., 2009). This scheme is modiﬁed through the introduc-\ntion of a signed sum of hashed features whereas the original\nhash kernels use an unsigned sum. This modiﬁcation leads\nto an unbiased estimate, which we demonstrate and further\nutilize in the following section.\nDeﬁnition 1 Denote by h a hash function h : N →\n{1,...,m }. Moreover , denote by ξ a hash function ξ :\nN →{ ± 1}. Then for vectors x, x\n′ ∈ ℓ2 we deﬁne the\nhashed feature map φ and the corresponding inner product\nas\nφ(h,ξ)\ni (x)=\n∑\nj:h(j)=i\nξ(j)xj (2)\nand ⟨x, x′⟩φ :=\n⣨\nφ(h,ξ)(x),φ(h,ξ)(x′)\n⟩\n. (3)\nAlthough the hash functions in deﬁnition 1 are deﬁned over\nthe natural numbers N, in practice we often consider hash\nfunctions over arbitrary strings. These are equivalent, since\neach ﬁnite-length string can be represented by a unique nat-\nural number. Usually, we abbreviate the notation φ\n(h,ξ)(·)\nby just φ(·). Two hash functions φ and φ′ are different\nwhen φ = φ(h,ξ) and φ′ = φ(h′,ξ′) such that either h′ ̸= h\nor ξ ̸= ξ′. The purpose of the binary hash ξ is to remove\nthe bias inherent in the hash kernel of (Shi et al., 2009).\nIn a multi-task setting, we obtain instances in combina-\ntion with tasks, (x, u) ∈ X × U . We can naturally ex-\ntend our deﬁnition 1 to hash pairs, and will write φu(x)=\nφ((x, u)).\n3. Analysis\nThe following section is dedicated to theoretical analysis\nof hash kernels and their applications. In this sense, the\npresent paper continues where (Shi et al., 2009) falls short:\nwe prove exponential tail bounds. These bounds hold for\ngeneral hash kernels, which we later apply to show how\nhashing enables us to do large-scale multitask learning ef-\nﬁciently. We start with a simple lemma about the bias and\nvariance of the hash kernel. The proof of this lemma ap-\npears in appendix A.\nLemma 2 The hash kernel is unbiased, that is\nE\nφ[⟨x, x′⟩φ]= ⟨x, x′⟩. Moreover , the variance is\nσ2\nx,x′ = 1\nm\n(∑\ni̸=j x2\ni x′\nj\n2\n+ xix′\ni\nxjx′\nj\n)\n, and thus, for\n∥x∥2 = ∥x′∥2 =1 , σ2\nx,x′ = O\n(1\nm\n)\n.\nThis suggests that typical values of the hash kernel should\nbe concentrated within O( 1√ m ) of the target value. We use\nChebyshev’s inequality to show that half of all observations\nare within a range of\n√\n2σ. This, together with Talagrand’s\nconvex distance inequality, enables us to construct expo-\nnential tail bounds.\n3.1. Concentration of Measure Bounds\nIn this subsection we show that under a hashed feature-map\nthe length of each vector is preserved with high probability.\nTalagrand’s inequality (Ledoux, 2001) is a key tool for the\nproof of the following theorem (detailed in the appendix B).\nTheorem 3 Let ϵ< 1 be a ﬁxed constant and x be a given\ninstance. Let η=\n∥x∥∞\n∥x∥2\n. Under the assumptions above, the\nhash kernel satisﬁes the following inequality\nPr\n{\n|∥x∥2\nφ −∥ x∥2\n2\n|\n∥x∥2\n2\n≥\n√\n2σx,x + ϵ\n}\n≤ exp\n(\n−\n√ ϵ\n4η\n)\n.\nNote that an analogous result would also hold for the orig-\ninal hash kernel of (Shi et al., 2009), the only modiﬁca-\ntion being the associated bias terms. The above result can\nalso be utilized to show a concentration bound on the inner\nproduct between two general vectors x and x\n′.\nCorollary 4 F or two vectorsx and x′, let us deﬁne\nσ := max(σx,x,σx′,x′ ,σx−x ′,x−x ′ )\nη:= min\n(∥x∥∞\n∥x∥2\n, ∥x′∥∞\n∥x′∥2\n, ∥x − x′∥∞\n∥x − x′∥2\n)\n.\nAlso let Δ=∥x ∥2 + ∥x′∥2 + ∥x − x′∥2. Under the as-\nsumptions above, we have that\nPr\n[\n|⟨x, x′⟩φ −⟨ x, x′⟩| >(\n√\n2σ+ϵ)Δ/2\n]\n<3e−\n√ϵ\n4η .\nThe proof for this corollary can be found in appendix C. We\ncan also extend the bound in Theorem 3 for the maximal\n1114\nFeature Hashing for Large Scale Multitask Learning\ncanonical distortion over large sets of distances between\nvectors as follows:\nCorollary 5 Denote by X = {x1,...,x n} a set of vectors\nwhich satisfy ∥xi − xj ∥∞ ≤ η∥xi − xj ∥2 for all pairs i, j.\nIn this case with probability 1 − δwe have for all i, j\n|∥xi − xj ∥2\nφ −∥ xi − xj ∥2\n2\n|\n∥xi − xj ∥2\n2\n≤\n√\n2\nm +6 4η2 log2 n\n2δ.\nThis means that the number of observations n (or corre-\nspondingly the size of the un-hashed kernel matrix) only\nenters logarithmically in the analysis.\nProof We apply the bound of Theorem 3 to each dis-\ntance individually. Note the bound σ2 ≤ 1\nm for all nor-\nmalized vectors. Also, since we have n(n−1)\n2 pairs of dis-\ntances the union bound yields a corresponding factor. Solv-\ning δ≥ n(n−1)\n2 e−\n√ϵ\n4η for ϵ and easy inequalities proves the\nclaim.\n3.2. Multiple Hashing\nNote that the tightness of the union bound in Corollary 5\ndepends crucially on the magnitude of η. In other words,\nfor large values of η, that is, whenever some terms in x\nare very large, even a single collision can already lead to\nsigniﬁcant distortions of the embedding. This issue can\nbe amended by trading off sparsity with variance. A vec-\ntor of unit length may be written as (1,0,0,0,... ),o r\nas\n(\n1√\n2 , 1√\n2 , 0,...\n)\n, or more generally as a vector with c\nnonzero terms of magnitude c− 1\n2 . This is relevant, for in-\nstance whenever the magnitudes of x follow a known pat-\ntern, e.g. when representing documents as bags of words\nsince we may simply hash frequent words several times.\nThe following corollary gives an intuition as to how the\nconﬁdence bounds scale in terms of the replications:\nLemma 6 If we let x\n′ = 1√ c (x ,...,x ) then:\n1. It is norm preserving: ∥x∥2 = ∥x′∥2 .\n2. It reduces component magnitude by 1√ c = ∥x′ ∥∞\n∥x∥∞\n.\n3. V ariance increases to σ2\nx′,x′ = 1\nc σ2\nx,x+ c−1\nc 2 ∥x∥4\n2 .\nApplying Lemma 6 to Theorem 3, a large magnitude can\nbe decreased at the cost of an increased variance.\n3.3. Approximate Orthogonality\nFor multitask learning, we must learn a different parameter\nvector for each related task. When mapped into the same\nhash-feature space we want to ensure that there is little in-\nteraction between the different parameter vectors. Let U be\na set of different tasks,u ∈ U being a speciﬁc one. Let w be\na combination of the parameter vectors of tasks in U \\{u}.\nWe show that for any observation x for task u, the inter-\naction of w with x in the hashed feature space is minimal.\nFor each x, let the image of x under the hash feature-map\nfor task u be denoted as φu(x)= φ(ξ,h)((x, u)).\nTheorem 7 Let w ∈ Rm be a parameter vector for tasks\nin U \\{ u}. In this case the value of the inner product\n⟨w,φu(x)⟩ is bounded by\nPr {|⟨w,φu(x)⟩| >ϵ }≤ 2e\n− ϵ2/2\nm−1 ∥w∥2\n2∥x∥2\n2 +ϵ∥w∥∞ ∥x∥∞ /3\nProof We use Bernstein’s inequality (Bernstein, 1946),\nwhich states that for independent random variables Xj ,\nwith E[Xj]=0 ,i fC> 0 is such that |Xj |≤ C, then\nPr\n⎡\n⎣\nn∑\nj=1\nXj >t\n⎤\n⎦≤ exp\n(\n− t\n2/2∑n\nj=1 E\n[\nX2\nj\n]\n+ Ct/3\n)\n. (4)\nWe have to compute the concentration property of\n⟨w,φu(x)⟩ = ∑\nj xjξ(j)wh(j). Let Xj = xjξ(j)wh(j).\nBy the deﬁnition of h and ξ, Xj are independent. Also,\nfor each j, since w depends only on the hash-functions for\nU \\{ u}, wh(j) is independent of ξ(j). Thus, E[Xj]=\nE(ξ,h)\n[\nxjξ(j)wh(j)\n]\n=0 . For each j, we also have |Xj | <\n∥x∥∞ ∥w∥∞ =: C. Finally, ∑\nj E[X2\nj ] is given by\nE\n⎡\n⎣∑\nj\n(xjξ(j)wh(j))2\n⎤\n⎦ =\n1\nm\n∑\nj,ℓ\nx2\nj w2\nℓ = 1\nm ∥x∥2\n2 ∥w∥2\n2\nThe claim follows by plugging both terms and C into the\nBernstein inequality (4).\nTheorem 7 bounds the inﬂuence of unrelated tasks with any\nparticular instance. In section 5 we demonstrate the real-\nworld applicability with empirical results on a large-scale\nmulti-task learning problem.\n4. Applications\nThe advantage of feature hashing is that it allows for sig-\nniﬁcant storage compression for parameter vectors: storing\nw in the raw feature space na ¨ıvely requires O(d) numbers,\nwhen w ∈ R\nd. By hashing, we are able to reduce this to\nO(m) numbers while avoiding costly matrix-vector multi-\nplications common in Locality Sensitive Hashing. In addi-\ntion, the sparsity of the resulting vector is preserved.\n1115\nFeature Hashing for Large Scale Multitask Learning\nThe beneﬁts of the hashing-trick leads to applications in\nalmost all areas of machine learning and beyond. In par-\nticular, feature hashing is extremely useful whenever large\nnumbers of parameters with redundancies need to be stored\nwithin bounded memory capacity.\nPersonalization (Daume, 2007) introduced a very sim-\nple but strikingly effective method for multitask learning.\nEach task updates its very speciﬁc own (local) weights and\na set of common (global) weights that are shared amongst\nall tasks. Theorem 7 allows us to hash all these multiple\nclassiﬁers into one feature space with little interaction. To\nillustrate, we explore this setting in the context of spam-\nclassiﬁer personalization.\nSuppose we have thousands of users U and want to per-\nform related but not identical classiﬁcation tasks for each\nof the them. Users provide labeled data by marking emails\nas spam or not-spam. Ideally, for each user u ∈ U ,w e\nwant to learn a predictor w\nu based on the data of that user\nsolely. However, webmail users are notoriously lazy in la-\nbeling emails and even those that do not contribute to the\ntraining data expect a working spam ﬁlter. Therefore, we\nalso need to learn an additional global predictor w\n0 to allow\ndata sharing amongst all users.\nStoring all predictors wi requires O(d × (|U | + 1)) mem-\nory. In a task like collaborative spam-ﬁltering, |U |, the\nnumber of users can be in the hundreds of thousands and\nthe size of the vocabulary is usually in the order of mil-\nlions. The na ¨ıve way of dealing with this is to elimi-\nnate all infrequent tokens. However, spammers target this\nmemory-vulnerability by maliciously misspelling words\nand thereby creating highly infrequent but spam-typical\ntokens that “fall under the radar” of conventional classi-\nﬁers. Instead, if all words are hashed into a ﬁnite-sized\nfeature vector, infrequent but class-indicative tokens get a\nchance to contribute to the classiﬁcation outcome. Further,\nlarge scale spam-ﬁlters (e.g. Yahoo Mail\nTMor GMailTM)\ntypically have severe memory and time constraints, since\nthey have to handle billions of emails per day. To guaran-\ntee a ﬁnite-size memory footprint we hash all weight vec-\ntors w\n0,...,w |U | into a joint, signiﬁcantly smaller, feature\nspace Rm with different hash functions φ0,...,φ |U |. The\nresulting hashed-weight vector wh ∈ Rm can then be writ-\nten as:\nwh = φ0(w0)+\n∑\nu∈U\nφu(wu). (5)\nNote that in practice the weight vector wh can be learned\ndirectly in the hashed space. All un-hashed weight vectors\nnever need to be computed. Given a new document/email\nx of user u ∈ U , the prediction task now consists of calcu-\nlating ⟨φ\n0(x)+φ u(x),w h⟩. Due to hashing we have two\nsources of error – distortion ϵd of the hashed inner prod-\nucts and the interference with other hashed weight vectors\nϵi. More precisely:\n⟨φ0(x)+φ u(x),w h⟩ = ⟨x, w0 + wu⟩ + ϵd + ϵi. (6)\nThe interference error consists of all collisions between\nφ\n0(x) or φu(x) with hash functions of other users,\nϵi =\n∑\nv∈U,v̸=0\n⟨φ0(x),φv(wv)⟩ +\n∑\nv∈U,v̸=u\n⟨φu(x),φv(wv)⟩. (7)\nTo show that ϵi is small with high probability we can\napply Theorem 7 twice, once for each term of (7).\nWe consider each user’s classiﬁcation to be a separate\ntask, and since ∑\nv∈U,v̸=0 wv is independent of the hash-\nfunction φ0, the conditions of Theorem 7 apply with w =∑\nv̸=0 wv and we can employ it to bound the second term,∑\nv∈U,v̸=0 ⟨φu(x),φu(wv)⟩. The second application is\nidentical except that all subscripts “0” are substituted with\n“u”. For lack of space we do not derive the exact bounds.\nThe distortion error occurs because each hash function that\nis utilized by user u can self-collide:\nϵ\nd =\n∑\nv∈{u,0}\n|⟨φv(x),φv(wv)⟩−⟨ x, wv ⟩|. (8)\nTo show that ϵd is small with high probability, we apply\nCorollary 4 once for each possible values of v.\nIn section 5 we show experimental results for this set-\nting. The empirical results are stronger than the theoretical\nbounds derived in this subsection—our technique outper-\nforms a single global classiﬁer on hundreds thousands of\nusers. In the same section we provide an intuitive explana-\ntion for these strong results.\nMassively Multiclass Estimation We can also regard\nmassively multi-class classiﬁcation as a multitask problem,\nand apply feature hashing in a way similar to the person-\nalization setting. Instead of using a different hash func-\ntion for each user, we use a different hash function for each\nclass.\n(Shi et al., 2009) apply feature hashing to problems with\na high number of categories. They show empirically that\njoint hashing of the feature vector φ(x, y) can be efﬁciently\nachieved for problems with millions of features and thou-\nsands of classes.\nCollaborative Filtering Assume that we are given a very\nlarge sparse matrix M where the entry M\nij indicates what\naction user i took on instance j. A common example for\nactions and instances is user-ratings of movies (Bennett &\nLanning, 2007). A successful method for ﬁnding common\nfactors amongst users and instances for predicting unob-\nserved actions is to factorize M into M = U\n⊤W .I f w e\n1116\nFeature Hashing for Large Scale Multitask Learning\nNEU\nVotre\nApotheke\n...\n1\n0\n-1\n0\n0\n-1\n0\n1\n0\n...\ntext document (email) bag of words hashed, \nsparse vector\nx NEU\nUSER123_NEU\nVotre\nUSER123_Votre\nApotheke\nUSER123_Apotheke\n...\nφ\nφ0(x)+φu(x)\nbag of words\n(personalized)\nFigure 1. The hashed personalization summarized in a schematic\nlayout. Each token is duplicated and one copy is individualized\n(e.g. by concatenating each word with a unique user identiﬁer).\nThen, the global hash function maps all tokens into a low dimen-\nsional feature space where the document is classiﬁed.\nhave millions of users performing millions of actions, stor-\ning U and W in memory quickly becomes infeasible. In-\nstead, we may choose to compress the matrices U and W\nusing hashing. For U, W ∈ Rn×d denote by u, w ∈ Rm\nvectors with\nui =\n∑\nj,k:h(j,k)=i\nξ(j, k)Ujk and wi =\n∑\nj,k:h′ (j,k)=i\nξ′(j, k)Wjk.\nwhere (h, ξ) and (h′,ξ′) are independently chosen hash\nfunctions. This allows us to approximate matrix elements\nM\nij =[ U ⊤W]ij via\nMφ\nij :=\n∑\nk\nξ(k,i )ξ′(k,j )uh(k,i)wh′ (k,j).\nThis gives a compressed vector representation of M that\ncan be efﬁciently stored.\n5. Results\nWe evaluated our algorithm in the setting of personaliza-\ntion. As data set, we used a proprietary email spam-\nclassiﬁcation task of n =3 .2 million emails, properly\nanonymized, collected from |U | = 433167 users. Each\nemail is labeled as spam or not-spam by one user in U . Af-\nter tokenization, the data set consists of 40 million unique\nwords.\nFor all experiments in this paper, we used the V owpal Wab-\nbit implementation\n1 of stochastic gradient descent on a\nsquare-loss. In the mail-spam literature the misclassiﬁca-\ntion of not-spam is considered to be much more harmful\nthan misclassiﬁcation of spam. We therefore follow the\nconvention to set the classiﬁcation threshold during test\ntime such that exactly 1% of the not − spam test data is\nclassiﬁed as spam Our implementation of the personalized\nhash functions is illustrated in Figure 1. To obtain a person-\nalized hash function φ\nu for user u, we concatenate a unique\n1http://hunch.net/∼vw/\n\u0015\u0010\u0014\u001b\u0001\u0015\u0010\u0014\u0017\u0001\u0015\u0010\u0014\u0014\u0001\u0015\u0010\u0014\u0014\u0001 \u0015\u0001\n\u0015\u0010\u0015\u0016\u0001\n\u0014\u0010\u001b\u0016\u0001\n\u0014\u0010\u001a\u0014\u0001\n\u0014\u0010\u0019\u001b\u0001\u0014\u0010\u0019\u001a\u0001\n\u0014\u0010\u0014\u0014\u0001\n\u0014\u0010\u0016\u0014\u0001\n\u0014\u0010\u0018\u0014\u0001\n\u0014\u0010\u0019\u0014\u0001\n\u0014\u0010\u001b\u0014\u0001\n\u0015\u0010\u0014\u0014\u0001\n\u0015\u0010\u0016\u0014\u0001\n\u0015\u001b\u0001 \u0016\u0014\u0001 \u0016\u0016\u0001 \u0016\u0018\u0001 \u0016\u0019\u0001\n\r\u000b\u0002\b\u0001\b\u0006\r\r\u0013\f\u0002\u000e\u0004\u0001\u0014\f\u0004\u0007\u0002\u000f\u0011\u0004\u0001\u000e\n\u0001\u0003\u0002\r\u0004\u0007\u0006\t\u0004\u0015\u0001\n\u0003\u0001\u0003\u0006\u000e\r\u0001\u0006\t\u0001\u0005\u0002\r\u0005\u0013\u000e\u0002\u0003\u0007\u0004\u0001\n\u0006\t\u000b\u0003\u0002\t\u0011\u0007\u0002\u000e\u0007\u0005\u0004\u0001\n\f\u0005\r\u000e\u000b\n\u0002\t\b\u000f\u0005\u0004\u0001\n\u0003\u0002\u000e\u0005\t\b\n\u0005\u0001\nFigure 2. The decrease of uncaught spam over the baseline clas-\nsiﬁer averaged over all users. The classiﬁcation threshold was\nchosen to keep the not-spam misclassiﬁcation ﬁxed at 1%.\nThe hashed global classiﬁer (global-hashed ) converges relatively\nsoon, showing that the distortion error ϵd vanishes. The personal-\nized classiﬁer results in an average improvement of up to 30%.\nuser-id to each word in the email and then hash the newly\ngenerated tokens with the same global hash function.\nThe data set was collected over a span of 14 days. We\nused the ﬁrst 10 days for training and the remaining 4 days\nfor testing. As baseline, we chose the purely global classi-\nﬁer trained over all users and hashed into 2\n26 dimensional\nspace. As 226 far exceeds the total number of unique words\nwe can regard the baseline to be representative for the clas-\nsiﬁcation without hashing. All results are reported as the\namount of spam that passed the ﬁlter undetected, relative\nto this baseline (eg. a value of 0.80 indicates a 20% reduc-\ntion in spam for the user)\n2.\nFigure 2 displays the average amount of spam in users’ in-\nboxes as a function of the number of hash keys m, relative\nto the baseline above. In addition to the baseline, we eval-\nuate two different settings.\nThe global-hashed curve represents the relative\nspam catch-rate of the global classiﬁer after hashing\n⟨φ\n0(w0),φ0(x)⟩.A t m =2 26 this is identical to the\nbaseline. Early convergence at m =2 22 suggests that at\nthis point hash collisions have no impact on the classiﬁ-\ncation error and the baseline is indeed equivalent to that\nobtainable without hashing.\nIn the personalized setting each user u ∈ U gets her own\nclassiﬁer φu(wu) as well as the global classiﬁer φ0(w0).\nWithout hashing the feature space explodes, as the cross\nproduct of u = 400K users and n =4 0M tokens results\nin 16 trillion possible unique personalized features. Fig-\nure 2 shows that despite aggressive hashing, personaliza-\ntion results in a 30% spam reduction once the hash table is\nindexed by 22 bits.\n2As part of our data sharing agreement, we agreed not to in-\nclude absolute classiﬁcation error-rates.\n1117\nFeature Hashing for Large Scale Multitask Learning\n\u0010\u0001\n\u0010\n\u0012\u0001\n\u0010\n\u0014\u0001\n\u0010\n\u0016\u0001\n\u0010\n\u0018\u0001\n\u0011\u0001\n\u0011\n\u0012\u0001\n\u0011\n\u0014\u0001\n\u0011\u0018\u0001 \u0012\u0010\u0001 \u0012\u0012\u0001 \u0012\u0014\u0001 \u0012\u0016\u0001\n\r\u000b\u0002\b\u0001\b\u0006\r\r\u0013\f\u0002\u000e\u0004\u0001\u0014\f\u0004\u0007\u0002\u000f\u0011\u0004\u0001\u000e\n\u0001\u0003\u0002\r\u0004\u0007\u0006\t\u0004\u0015\u0001\n\u0003\u0001\u0003\u0006\u000e\r\u0001\u0006\t\u0001\u0005\u0002\r\u0005\u0013\u000e\u0002\u0003\u0007\u0004\u0001\n\f\u0010\r\u0001\n\f\u0011\r\u0001\n\f\u0012\t\u0013\r\u0001\n\f\u0014\t\u0017\r\u0001\n\f\u0018\t\u0011\u0015\r\u0001\n\f\u0011\u0016\t\u0013\u0011\r\u0001\n\f\u0013\u0012\t\u0016\u0014\r\u0001\n\f\u0016\u0014\t\u0019\u000b\u0001\n\u0003\u0002\b\u0004\u0006\u0005\u0007\u0004\u0001\nFigure 3. Results for users clustered by training emails. For ex-\nample, the bucket [8, 15] consists of all users with eight to ﬁfteen\ntraining emails. Although users in buckets with large amounts of\ntraining data do beneﬁt more from the personalized classiﬁer (up-\nto 65% reduction in spam), even users that did not contribute to\nthe training corpus at all obtain almost 20% spam-reduction.\nUser clustering One hypothesis for the strong results in\nFigure 2 might originate from the non-uniform distribution\nof user votes — it is possible that by using personalization\nand feature hashing we beneﬁt a small number of users who\nhave labeled many emails, degrading the performance of\nmost users (who have labeled few or no emails) in the pro-\ncess. In fact, in real life, a large fraction of email users do\nnot contribute at all to the training corpus and only interact\nwith the classiﬁer during test time. The personalized ver-\nsion of the test email φ\nu(xu) is then hashed into buckets\nof other tokens and only adds interference noise ϵi to the\nclassiﬁcation.\nTo show that we improve the performance of most users,\nit is therefore important that we not only report averaged\nresults over all emails, but explicitly examine the effects\nof the personalized classiﬁer for users depending on their\ncontribution to the training set. To this end, we place users\ninto exponentially growing buckets based on their num-\nber of training emails and compute the relative reduction\nof uncaught spam for each bucket individually. Figure 3\nshows the results on a per-bucket basis. We do not compare\nagainst a purely local approach, with no global component,\nsince for a large fraction of users—those without training\ndata—this approach cannot outperform random guessing.\nIt might appear surprising that users in the bucket with none\nor very little training emails (the line of bucket [0] is iden-\ntical to bucket [1]) also beneﬁt from personalization. After\nall, their personalized classiﬁer was never trained and can\nonly add noise at test-time. The classiﬁer improvement of\nthis bucket can be explained by the subjective deﬁnition of\nspam and not-spam. In the personalized setting the indi-\nvidual component of user labeling is absorbed by the local\nclassiﬁers and the global classiﬁer represents the common\ndeﬁnition of spam and not-spam. In other words, the global\npart of the personalized classiﬁer obtains better generaliza-\ntion properties, beneﬁting all users.\n6. Related Work\nA number of researchers have tackled related, albeit differ-\nent problems.\n(Rahimi & Recht, 2008) use Bochner’s theorem and sam-\npling to obtain approximate inner products for Radial Ba-\nsis Function kernels. (Rahimi & Recht, 2009) extend this\nto sparse approximation of weighted combinations of ba-\nsis functions. This is computationally efﬁcient for many\nfunction spaces. Note that the representation is dense.\n(Li et al., 2007) take a complementary approach: for sparse\nfeature vectors, φ(x), they devise a scheme of reducing the\nnumber of nonzero terms even further. While this is in prin-\nciple desirable, it does not resolve the problem of φ(x) be-\ning high dimensional. More succinctly, it is necessary to\nexpress the function in the dual representation rather than\nexpressing f as a linear function, where w is unlikely to be\ncompactly represented: f(x)= ⟨φ(x),w ⟩.\n(Achlioptas, 2003) provides computationally efﬁcient ran-\ndomization schemes for dimensionality reduction. Instead\nof performing a dense d·m dimensional matrix vector mul-\ntiplication to reduce the dimensionality for a vector of di-\nmensionality d to one of dimensionality m, as is required\nby the algorithm of (Gionis et al., 1999), he only requires\n1\n3\nof that computation by designing a matrix consisting only\nof entries {−1, 0,1}.\n(Shi et al., 2009) propose a hash kernel to deal with the is-\nsue of computational efﬁciency by a very simple algorithm:\nhigh-dimensional vectors are compressed by adding up all\ncoordinates which have the same hash value — one only\nneeds to perform as many calculations as there are nonzero\nterms in the vector. This is a signiﬁcant computational sav-\ning over locality sensitive hashing (Achlioptas, 2003; Gio-\nnis et al., 1999).\nSeveral additional works provide motivation for the investi-\ngation of hashing representations. For example, (Ganchev\n& Dredze, 2008) provide empirical evidence that the\nhashing-trick can be used to effectively reduce the memory\nfootprint on many sparse learning problems by an order of\nmagnitude via removal of the dictionary. Our experimen-\ntal results validate this, and show that much more radical\ncompression levels are achievable. In addition, (Langford\net al., 2007) released the V owpal Wabbit fast online learn-\ning software which uses a hash representation similar to\nthat discussed here.\n7. Conclusion\nIn this paper we analyze the hashing-trick for dimensional-\nity reduction theoretically and empirically. As part of our\ntheoretical analysis we introduce unbiased hash functions\nand provide exponential tail bounds for hash kernels. These\n1118\nFeature Hashing for Large Scale Multitask Learning\ngive further insight into hash-spaces and explain previously\nmade empirical observations. We also derive that random\nsubspaces of the hashed space are likely to not interact,\nwhich makes multitask learning with many tasks possible.\nOur empirical results validate this on a real-world applica-\ntion within the context of spam ﬁltering. Here we demon-\nstrate that even with a very large number of tasks and\nfeatures, all mapped into a joint lower dimensional hash-\nspace, one can obtain impressive classiﬁcation results with\nﬁnite memory guarantee.\nReferences\nAchlioptas, D. (2003). Database-friendly random projec-\ntions: Johnson-lindenstrauss with binary coins. Journal\nof Computer and System Sciences, 66, 671–687.\nBennett, J., & Lanning, S. (2007). The Netﬂix Prize. Pro-\nceedings of Conference on Knowledge Discovery and\nData Mining Cup and Workshop 2007.\nBernstein, S. (1946). The theory of probabilities. Moscow:\nGastehizdat Publishing House.\nDaume, H. (2007). Frustratingly easy domain adaptation.\nAnnual Meeting of the Association for Computational\nLinguistics (p. 256).\nGanchev, K., & Dredze, M. (2008). Small statistical mod-\nels by random feature mixing. Workshop on Mobile Lan-\nguage Processing, Annual Meeting of the Association for\nComputational Linguistics.\nGionis, A., Indyk, P ., & Motwani, R. (1999). Similarity\nsearch in high dimensions via hashing. Proceedings of\nthe 25th VLDB Conference (pp. 518–529). Edinburgh,\nScotland: Morgan Kaufmann.\nLangford, J., Li, L., & Strehl, A. (2007). V ow-\npal wabbit online learning project (Technical Report).\nhttp://hunch.net/?p=309.\nLedoux, M. (2001). The concentration of measure phe-\nnomenon. Providence, RI: AMS.\nLi, P ., Church, K., & Hastie, T. (2007). Conditional random\nsampling: A sketch-based sampling technique for sparse\ndata. In B. Sch ¨olkopf, J. Platt and T. Hoffman (Eds.),\nAdvances in neural information processing systems 19,\n873–880. Cambridge, MA: MIT Press.\nRahimi, A., & Recht, B. (2008). Random features for large-\nscale kernel machines. In J. Platt, D. Koller, Y . Singer\nand S. Roweis (Eds.), Advances in neural information\nprocessing systems 20. Cambridge, MA: MIT Press.\nRahimi, A., & Recht, B. (2009). Randomized kitchen\nsinks. In L. Bottou, Y . Bengio, D. Schuurmans and\nD. Koller (Eds.), Advances in neural information pro-\ncessing systems 21. Cambridge, MA: MIT Press.\nShi, Q., Petterson, J., Dror, G., Langford, J., Smola, A.,\nStrehl, A., & Vishwanathan, V . (2009). Hash kernels.\nProc. Intl. Workshop on Artiﬁcial Intelligence and Statis-\ntics 12.\nA. Mean and Variance\nProof [Lemma 2] To compute the expectation we expand\n⟨x, x′⟩φ =\n∑\ni,j\nξ(i)ξ(j)xix′\njδh(i),h(j). (9)\nSince Eφ[⟨x, x′⟩φ]= Eh[Eξ[⟨x, x′⟩φ]], taking expecta-\ntions over ξwe see that only the terms i = j have nonzero\nvalue, which shows the ﬁrst claim. For the variance we\ncompute Eφ[⟨x, x′⟩2\nφ]. Expanding this, we get:\n⟨x, x′⟩2\nφ=\n∑\ni,j,k,l\nξ(i)ξ(j)ξ(k)ξ(l)xix′\njxkx′\nl\nδh(i),h(j)δh(k),h(l).\nThis expression can be simpliﬁed by noting that:\nEξ [ξ(i)ξ(j)ξ(k)ξ(l)]= δijδkl +[1 − δijkl](δikδjl + δilδjk).\nPassing the expectation over ξthrough the sum, this allows\nus to break down the expansion of the variance into two\nterms.\nEφ[⟨x, x′⟩2\nφ]=\n∑\ni,k\nxix′\nixkx′\nk\n+\n∑\ni̸=j\nx2\ni\nx′\nj\n2\nEh\n[\nδh(i),h(j)\n]\n+\n∑\ni̸=j\nxix′\ni\nxjx′\nj\nEh\n[\nδh(i),h(j)\n]\n= ⟨x, x′⟩2 + 1\nm\n⎛\n⎝ ∑\ni̸=j\nx2\ni x′\nj\n2\n+\n∑\ni̸=j\nxix′\ni\nxjx′\nj\n⎞\n⎠\nby noting that Eh\n[\nδh(i),h(j)\n]\n= 1\nm for i ̸= j. Using the fact\nthat σ2 = Eφ[⟨x, x′⟩2\nφ]− Eφ[⟨x, x′⟩φ]2 proves the claim.\nB. Concentration of Measure\nOur proof uses Talagrand’s convex distance inequality. We\nﬁrst deﬁne a weighted Hamming distance function between\ntwo hash-function φ and φ\n′ as follows.\nd(φ, φ′) = sup\n∥α∥2≤1\n∑\ni\nαiI(h(i) ̸= h′(i) or ξ(i) ̸= ξ′(i))\n=\n√\n|{i : h(i) ̸= h′(i) or ξ(i) ̸= ξ′(i)}|\n1119\nFeature Hashing for Large Scale Multitask Learning\nNext denote by d(φ, A) the distance between a hash func-\ntion and a set A of hash functions, that is d(φ, A)=\ninfφ′ ∈A d(φ, φ′). In this case Talagrand’s convex distance\ninequality (Ledoux, 2001) holds. If Pr(A) denotes the total\nprobability mass of the set A, then\nPr {d(φ, A) ≥ s}≤ [Pr(A)]−1 e−s 2/4. (10)\nProof [Theorem 3] Without loss of generality assume that\n∥x∥2 =1 . We can then easily generalize to the general x\ncase. From Lemma 2 it follows that the variance of ∥x∥2\nφ is\ngiven by σ2\nx,x = 2\nN [1 −∥ x∥4\n4\n] and E(∥x∥2\nφ\n)=1 .\nChebyshev’s inequality states that P(|X − E(X)|≤√\n2σ) ≥ 1\n2 . We can therefore denote\nA :=\n{\nφ where\n⏐⏐\n⏐∥x∥\n2\nφ − 1\n⏐⏐\n⏐ ≤\n√\n2σx,x\n}\n.\nand obtain Pr(A) ≥ 1\n2 . From Talagrand’s inequality (10)\nwe know that Pr({φ : d(φ, A) ≥ s}) ≤ 2e−s 2/4. Now as-\nsume that we have a pair of hash functions φ and φ′, with\nφ′ ∈ A. Let us deﬁne the difference of their hashed inner-\nproducts as δ:= ∥x∥2\nφ −⟨ x, x⟩φ′ . By the triangle inequal-\nity and because φ′ ∈ A, we can state that\n⏐⏐\n⏐∥x∥\n2\nφ − 1\n⏐⏐\n⏐ ≤\n⏐\n⏐\n⏐∥x∥\n2\nφ −⟨ x, x⟩φ′\n⏐⏐\n⏐ +\n⏐\n⏐\n⏐⟨x, x⟩\nφ′ − 1\n⏐\n⏐\n⏐\n≤| δ| +\n√\n2σ. (11)\nLet us now denote the coordinate-wise difference between\nthe hashed features as vi := φ′\ni(x) − φi(x). With this def-\ninition, we can express δ in terms of v: δ= ∑\ni φi(x)2 −\nφ′\ni\n(x)2 = −2 ⟨φ′(x),v ⟩ + ∥v∥2\n2. By applying the Cauchy-\nSchwartz inequality to the inner product ⟨φ′(x),v ⟩, we ob-\ntain |δ|≤2∥φ ′(x)∥2∥v∥2 + ∥v∥2\n2\n. Plugging this into (11)\nleads us to⏐⏐\n⏐∥x∥\n2\nφ − 1\n⏐⏐\n⏐ ≤ 2∥φ\n′(x)∥2∥v∥2 + ∥v∥2\n2 +\n√\n2σx,x. (12)\nNext, we bound ∥v∥2 in terms of d(φ, φ′). To do this, ex-\npand vi = ∑\nj xj(ξ′\njδh′ (j)i − ξjδh(j)i). As ξj ∈{ +1, −1},\nwe know that |ξj − ξ′\nj |≤2. Further, xj ≤∥ x∥∞ and we\ncan write\n|vi|≤ 2 ∥x∥∞\n∑\nj\nδh(j)i + δh′ (j)i. (13)\nWe can now make two observations: First note that∑\ni\n∑\nj δh(j)i +δh′ (j)i is at most 2t where t = |{j : h(j) ̸=\nh′(j)}|. Second, from the deﬁnition of the distance func-\ntion, we get that d(φ, φ′) ≥\n√\nt. Putting these together,\n∑\ni\n|vi|≤ 4 ∥x∥∞ t ≤ 4 ∥x∥∞ d2(φ, φ′)\n∥vi∥2\n2 =\n∑\ni\n|vi|2 ≤ 16 ∥x∥2\n∞ d4(φ, φ′).\n(The last inequality holds because in the worst case all mass\nis concentrated in a single entry of vi.) As a next step we\nwill express ∥φ′(x)∥2 in terms of σx,x. Because φ′ ∈ A,\nwe obtain that\n∥φ′(x)∥2 =\n√\n⟨x, x⟩φ′ ≤ (1+\n√\n2σx,x)1/2 ≤ 1+σx,x/\n√\n2.\nTo simplify our notation, let us deﬁne β =1+ σx,x/\n√\n2.\nPlugging our upper bounds for ∥v∥2 and ∥φ′(x)∥2 into (12)\nleads to⏐⏐\n⏐∥x∥\n2\nφ − 1\n⏐⏐\n⏐ ≤ 8 ∥x∥\n∞ βd2(φ, φ′)(β+2 ∥x∥∞ d2(φ, φ′))+\n√\n2σ.\nAs we have not speciﬁed our particular choice of φ′,w e\ncan now choose it to be the closest vector to φ within A,\nie such that d(φ, φ′)= d(φ, A). By Talagrand’s inequality,\nwe know that with probability at least1−2e−s 2/4 we obtain\nd(φ, A) ≤ s and therefore with high probability:\n⏐⏐⏐∥x∥2\nφ − 1\n⏐⏐⏐ ≤ 8 ∥x∥∞ βs2 +1 6∥x∥2\n∞ s4 +\n√\n2σ.\nA change of variables s2 =\n√\nβ2+ϵ−β\n4∥x∥∞\ngives us that⏐⏐\n⏐∥x∥\n2\nφ − 1\n⏐⏐\n⏐ ≤\n√\n2σ + ϵ w.p. 1 − 2e−s 2/4. Noting that\ns2 =(\n√\nβ2 + ϵ − β)/4 ∥x∥∞ ≥ √ ϵ/4 ∥x∥∞ , lets us ob-\ntain our ﬁnal result⏐\n⏐\n⏐∥x∥\n2\nφ − 1\n⏐⏐\n⏐ ≤\n√\n2σ+ ϵ w.p. 1 − 2e− √ ϵ/4∥x∥∞ .\nFinally, for a general x, we can derive the above result for\ny = x\n∥x∥2\n. Replacing ∥y∥∞ = ∥x∥∞\n∥x∥2\nwe get the following\nversion for a general x,\nPr\n{\n|∥x∥2\nφ −∥ x∥2\n2\n|\n∥x∥2\n2\n≥\n√\n2σx,x + ϵ\n}\n≤ exp\n(\n−\n√ ϵ∥x∥2\n4∥x∥∞\n)\nC. Inner Product\nProof [Corollary 4] We have that 2 ⟨x, x′⟩φ = ∥x∥2\nφ\n+\n∥x′∥2\nφ −∥ x − x′∥2\nφ\n. Taking expectations, we have the stan-\ndard inner product inequality. Thus,\n|2 ⟨φu(x),φu(x)⟩−2 ⟨x, x⟩| ≤ |∥ φu(x)∥2 −∥ x∥2 |\n+ |∥φu(x′)∥2 −∥ x′∥2 | + |∥φ(x − x′)∥2 −∥ x − x′∥2 |\nUsing union bound, with probability 1 − 3 exp\n(\n−\n√ ϵ\n4η\n)\n,\neach of the terms above is bounded using Theorem 3. Thus,\nputting the bounds together, we have that, with probability\n1 − 3 exp\n(\n−\n√ ϵ\n4η\n)\n,\n|2 ⟨φu(x),φu(x)⟩−2 ⟨x, x⟩| ≤\n≤ (\n√\n2σ+ ϵ)(∥x∥2 + ∥x′∥2 + ∥x − x′∥2)\n1120",
  "values": {
    "Respect for Persons": "Yes",
    "Not socially biased": "Yes",
    "Transparent (to users)": "Yes",
    "Beneficence": "Yes",
    "Interpretable (to users)": "Yes",
    "Autonomy (power to decide)": "Yes",
    "Justice": "Yes",
    "Respect for Law and public interest": "Yes",
    "Privacy": "Yes",
    "Fairness": "Yes",
    "User influence": "Yes",
    "Deferral to humans": "Yes",
    "Collective influence": "Yes",
    "Explicability": "Yes",
    "Non-maleficence": "Yes",
    "Critiqability": "Yes"
  }
}