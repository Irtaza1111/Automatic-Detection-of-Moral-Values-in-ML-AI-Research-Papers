{
  "pdf": "1553374.1553523",
  "title": "Learning structural SVMs with latent variables",
  "author": "Chun-Nam John Yu, Thorsten Joachims",
  "paper_id": "1553374.1553523",
  "text": "Learning Structural SVMs with Latent Variables\nChun-Nam John Y u cnyu@cs.cornell.edu\nThorsten\nJoachims tj@cs.cornell.edu\nDepartment of Computer Science, Cornell University, Ithaca, NY 14850 USA\nAbstract\nWe present a large-margin formulation and\nalgorithm for structured output prediction\nthat allows the use of latent variables. Our\nproposal covers a large range of applica-\ntion problems, with an optimization problem\nthat can be solved eﬃciently using Concave-\nConvex Programming. The generality and\nperformance of the approach is demonstrated\nthrough three applications including motif-\nﬁnding, noun-phrase coreference resolution,\nand optimizing precision at k in information\nretrieval.\n1. Introduction\nIn many structured prediction tasks, there is useful\nmodeling information that is not available as part of\nthe training data ( x1, y1), ..., (xn, yn). In noun phrase\ncoreference resolution, for example, one is typically\ngiven the clustering y of noun-phrases for a training\ndocument x, but not the set of informative links that\nconnects the noun phrases together into clusters. Sim-\nilarly, in machine translation, one may be given the\ntranslation y of sentence x, but not the linguistic struc-\nture h (e.g. parse trees, word alignments) that con-\nnects them. This missing information h, even if not\nobservable, is crucial for expressing high-ﬁdelity mod-\nels for these tasks. It is important to include these\ninformation in the model as latent variables.\nLatent variables have long been used to model observa-\ntions in generative probabilistic models such as Hidden\nMarkov Models. In discriminative models, however,\nthe use of latent variables is much less explored. Re-\ncently, there has been some work on Conditional Ran-\ndom Fields (Wang et al., 2006) with latent variables.\nEven less explored is the use of latent variables in\nAppearing in Pr oceedings of the 26 th International Confer-\nence on Machine Learning, Montreal, Canada, 2009. Copy-\nright 2009 by the author(s)/owner(s).\nlarge-margin structured output learning such as Max-\nMargin Markov Networks or Structural SVMs (Taskar\net al., 2003; Tsochantaridis et al., 2004). While these\nnon-probabilistic models oﬀer excellent performance\non many structured prediction tasks in the fully ob-\nserved case, they currently do not support the use of\nlatent variables, which excludes many interesting ap-\nplications.\nIn this paper, we propose an extension of the Struc-\ntural SVM framework to include latent variables. We\nidentify a particular, yet rather general, formulation\nfor which there exists an eﬃcient algorithm to ﬁnd a\nlocal optimum using the Concave-Convex Procedure.\nThe resulting algorithm is similarly modular as the\nStructural SVM algorithms for the fully observed case.\nTo illustrate the generality of our Latent Structural\nSVM algorithm, we provide experimental results on\nthree diﬀerent applications in computational biology,\nnatural language processing, and information retrieval.\n1.1. Related W orks\nMany of the early works in introducing latent vari-\nables into discriminative models were motivated by\ncomputer vision applications, where it is natural to use\nlatent variables to model human body parts or parts of\nobjects in detection tasks. The work in (Wang et al.,\n2006) introduces Hidden Conditional Random Fields,\na discriminative probabilistic latent variable model for\nstructured prediction, with applications to two com-\nputer vision tasks. In natural language processing\nthere is also work in applying discriminative proba-\nbilistic latent variable models, for example the training\nof PCFG with latent annotations in a discriminative\nmanner (Petrov & Klein, 2007). The non-convex likeli-\nhood functions of these problems are usually optimized\nusing gradient-based methods.\nThe Concave-Convex Procedure (Yuille & Rangara-\njan, 2003) employed in our work is a general frame-\nwork for minimizing non-convex functions which falls\ninto the class of DC (Diﬀerence of Convex) program-\nming. In recent years there have been numerous appli-\n1169\n\nLearning Structural SVMs with Latent V ariables\ncations of the algorithm in machine learning, includ-\ning\ntraining non-convex SVMs and transductive SVMs\n(Collobert et al., 2006). The approach in (Smola et al.,\n2005) employs CCCP to handle missing data in SVMs\nand Gaussian Processes and is closely related to our\nwork. However our approach is non-probabilistic and\navoids the computation of partition functions, which is\nparticularly attractive for structured prediction. Very\nrecently the CCCP algorithm has also been applied to\nobtain tighter non-convex loss bounds on structured\nlearning (Chapelle et al., 2008).\nIn the computer vision community there are recent\nworks on training Hidden CRF using the max-margin\ncriterion (Felzenszwalb et al., 2008; Wang & Mori,\n2008). In these works they focus on classiﬁcation prob-\nlems only and their training problem formulations are\na special case of our proposal below. Interestingly, the\nalgorithm in (Felzenszwalb et al., 2008) coincides with\nour approach for binary classiﬁcation but was derived\nin a diﬀerent way.\n2. Structural SVMs\nSuppose we are given a training set of input-output\nstructure pairs S = {(x1, y1), . . . ,(xn, yn)} ∈(X × Y )n.\nWe want to learn a linear prediction rule of the form\nfw(x) = argmax y∈Y [w · Φ(x, y)] , (1)\nwhere Φ is a joint feature vector that describes the\nrelationship between input x and structured output y,\nwith w being the parameter vector. The optimization\nproblem of computing this argmax is typically referred\nto as the “inference” or “prediction” problem.\nWhen training Structural SVMs, the parameter vec-\ntor w is determined by minimizing the (regularized)\nrisk on the training set ( x1, y1), ..., (xn, yn). Risk is\nmeasured through a user-supplied loss function ∆( y, ˆy)\nthat quantiﬁes how much the prediction ˆy diﬀers from\nthe correct output y. Note that ∆ is typically non-\nconvex and discontinuous and there are usually ex-\nponentially many possible structures ˆ y in the output\nspace Y. The Structural SVM formulation (Tsochan-\ntaridis et al., 2004) overcomes these diﬃculties by re-\nplacing the loss function ∆ with a piecewise linear con-\nvex upper bound (margin rescaling)\n∆(yi, ˆyi(w)) ≤ max\nˆy∈Y\n[∆(yi, ˆy)+w·Φ(xi, ˆy)]−w·Φ(xi, yi)\nwhere ˆyi(w) = argmax y∈Y w · Φ(xi, y).\nTo train Structural SVMs we then solve the following\nconvex optimization problem:\nmin\nw\n1\n2 ∥w∥2+C\nn∑\ni=1\n[\nmax\nˆy∈ Y\n[∆(yi, ˆy)+ w·Φ(xi, ˆy)]−w·Φ(xi, yi)\n]\n.\nDespite the typically exponential size of Y, this op-\ntimization problem can be solved eﬃciently using\ncutting-plane or stochastic gradient methods. Struc-\ntural SVMs give excellent performance on many struc-\ntured prediction tasks, especially when the model Φ\nis high-dimensional and it is necessary to optimize to\nnon-standard loss functions ∆.\n3. Structural SVM with Latent\nVariables\nAs argued in the introduction, however, in many appli-\ncations the input-output relationship is not completely\ncharacterized by ( x, y) ∈ X × Y pairs in the training\nset alone, but also depends on a set of unobserved la-\ntent variables h ∈ H . To generalize the Structural\nSVM formulation, we extend our joint feature vector\nΦ(x, y) with an extra argument h to Φ(x, y, h) to de-\nscribe the relation among input x, output y, and latent\nvariable h. We want to learn a prediction rule of the\nform\nfw(x) = argmax (y,h)∈Y×H [w · Φ(x, y, h)] . (2)\nAt ﬁrst glance, a natural way to extend the loss func-\ntion ∆ is to again include the latent variables h ∈ H\nsimilar to above, to give\n∆((yi, h∗\ni (w)), (ˆyi(w), ˆhi(w))),\nwhere h∗\ni (w) = argmax h∈H w · Φ(xi, yi, h) and\n(ˆyi(w), ˆhi(w)) = argmax (y,h)∈Y×H w · Φ(xi, y, h).\nEssentially, this extended loss measures the diﬀerence\nbetween the pair (ˆ yi(w), ˆhi(w)) given by the predic-\ntion rule and the best latent variable h∗\ni (w) that ex-\nplains the input-output pair ( xi, yi) in the training set.\nLike in the fully observed case, we can derive a hinge-\nloss style upper bound\n∆((yi, h∗\ni (w)), (ˆyi(w), ˆhi(w)))\n≤ ∆((yi, h∗\ni (w)), (ˆyi(w), ˆhi(w)))\n− [w · Φ(xi, yi, h∗\ni (w)) − w · Φ(xi, ˆyi(w), ˆhi(w))]\n=\n(\nmax\n(ˆy,ˆh)∈Y × H\nw·Φ(xi, ˆy, ˆh)\n)\n+∆((yi, h∗\ni (w)), (ˆyi(w), ˆhi(w)))\n−\n(\nmax\nh∈H\nw · Φ(xi, yi, h)\n)\n. (3)\nIn the case of Structural SVMs without latent vari-\nables, the complex dependence on w within the loss ∆\ncan be removed using the following inequality, com-\nmonly referred to as “loss-augmented inference” in\n1170\nLearning Structural SVMs with Latent V ariables\nStructural SVM training:\n(\nmax\nˆy∈\nY\nw · Φ(x, ˆy)\n)\n+ ∆(yi, ˆyi(w))\n≤ max\nˆy∈Y\n[w · Φ(xi, ˆyi) + ∆(yi, ˆy)].\n(4)\nWhen latent variables are included, however, the de-\npendence of ∆ on the latent variables h∗\ni (w) of the\ncorrect label yi prevents us from applying this trick.\nTo circumvent this diﬃculty, let us rethink the deﬁni-\ntion of loss function from Equation (3). As we will see\nbelow, many real world applications do not require the\nloss functions to depend on the oﬀending h∗\ni (w). In ap-\nplications such as parsing and object recognition, the\nlatent variables serve as indicator for mixture compo-\nnents or intermediate representations and are not part\nof the output. As a result, the natural loss functions\nthat we are interested in for these tasks usually do not\ndepend on the latent variables.\nWe therefore focus on the case where the loss function\n∆ does not depend on the latent variable h∗\ni (w):\n∆((yi, h∗\ni (w)), (ˆyi(w), ˆhi(w))) = ∆( yi, ˆyi(w), ˆhi(w)).\nNote that however the loss function may still depend\non ˆhi(w). In the case where the latent variable ˆh is a\npart of the prediction which we care about we can still\ndeﬁne useful asymmetric loss functions ∆( y, ˆy, ˆh) for\nlearning. The applications of noun phrase coreference\nresolution and optimizing for precision@ k in document\nretrieval in the experiments section below are good\nexamples of this.\nWith the redeﬁned loss ∆( yi, ˆyi(w), ˆhi(w)), the bound\nin Equation (3) becomes\n∆((yi, h∗\ni (w)), (ˆyi(w), ˆhi(w)))\n≤\n(\nmax\n(ˆy,ˆh)∈Y × H\n[w·Φ(xi, ˆy, ˆh) + ∆(yi, ˆy, ˆh)]\n)\n−\n(\nmax\nh∈H\nw·Φ(xi, yi, h)\n)\n.\nUsing the same reasoning as for fully observed Struc-\ntural SVMs, this gives rise to the following optimiza-\ntion problem for Structural SVMs with latent vari-\nables:\nmin\nw\n1\n2 ∥w∥2 +C\nn∑\ni=1\n(\nmax\n( ˆy,ˆh)∈Y×H\n[w·Φ(xi, ˆy, ˆh) + ∆(yi, ˆy, ˆh)]\n)\n− C\nn∑\ni=1\n(\nmax\nh∈H\nw · Φ(xi, yi, h)\n)\n. (5)\nIt is easy to observe that the above formulation reduces\nto the usual Structural SVM formulation in the ab-\nsence of latent variables. The formulation can also be\neasily extended to include kernels, although the usual\nextra cost of computing inner products in nonlinear\nkernel feature space applies.\nFinally, note that the redeﬁned loss distinguishes our\napproach from transductive structured output learn-\ning (Zien et al., 2007). When the loss ∆ depends only\non the fully observed label yi, it rules out the possi-\nbility of transductive learning, but the restriction also\nresults in simpler optimization problems compared to\nthe transductive cases (for example, the approach in\n(Zien et al., 2007) involves constraint removals to deal\nwith dependence on h∗\ni (w) within the loss ∆).\n4. Solving the Optimization Problem\nA key property of Equation (5) that follows from the\nredeﬁned loss ∆(yi, ˆyi(w), ˆhi(w)) is that it can be writ-\nten as the diﬀerence of two convex functions:\nmin\nw\n[\n1\n2 ∥w∥2 +C\nn∑\ni=1\nmax\n( ˆy,ˆh)∈Y × H\n[w·Φ(xi, ˆy, ˆh)+∆( yi, ˆy, ˆh)]\n]\n−\n[\nC\nn∑\ni=1\nmax\nh∈H\nw · Φ(xi, yi, h)\n]\n.\nThis allows us to solve the optimization problem us-\ning the Concave-Convex Procedure (CCCP) (Yuille &\nRangarajan, 2003). The general template for a CCCP\nalgorithm for minimizing a function f (w) − g(w),\nwhere f and g are convex, works as follows:\nAlgorithm 1 Conca ve-Convex Procedure (CCCP)\n1: Set t = 0 and initialize w0\n2: rep\neat\n3: Find hyperplane vt such that −g(w) ≤\n−g(wt) + (w − wt) · vt for all w\n4: Solve wt+1 = argmin w f (w) + w · vt\n5: Set t = t + 1\n6: until [f (wt) − g(wt)] − [f (wt−1) − g(wt−1)] < ϵ\nThe CCCP algorithm is guaranteed to decrease the\nob\njective function at every iteration and to converge\nto a local minimum or saddle point (Yuille & Rangara-\njan, 2003). Line 3 constructs a hyperplane that upper\nbounds the concave part of the objective −g, so that\nthe optimization problem solved at line 4 is convex.\nIn terms of the optimization problem for Latent Struc-\ntural SVM, the step of computing the upper bound for\nthe concave part in line 3 involves computing\nh∗\ni = argmax h∈H wt · Φ(xi, yi, h) (6)\nfor each i. We call this the “latent variable comple-\ntion” problem. The hyperplane constructed is vt =∑n\ni=1 Φ(xi, yi, h∗\ni ).\n1171\nLearning Structural SVMs with Latent V ariables\nComputing the new iterate wt+1 in line 1 involves solv-\ning the standard Structural SVM optimization prob-\nlem by completing yi with the latent variables h∗\ni as if\nthey were completely observed:\nmin\nw\n1\n2 ∥w∥2 +C\nn∑\ni=1\nmax\n( ˆy,ˆh)∈Y×H\n[w · Φ(xi, ˆy, ˆh)+∆( yi, ˆy, ˆh)]\n− C\n∑n\ni=1\nw · Φ(xi, yi, h∗\ni ). (7)\nThus the CCCP algorithm applied to Structural SVM\nwith latent variables gives rise to a very intuitive\nalgorithm that alternates between imputing the la-\ntent variables h∗\ni that best explain the training pair\n(xi, yi) and solving the Structural SVM optimization\nproblem while treating the latent variables as com-\npletely observed. This is similar to the iterative pro-\ncess of Expectation Maximization (EM). But unlike\nEM which maximizes the expected log likelihood un-\nder the marginal distribution of the latent variables,\nwe are minimizing the regularized loss against a single\nlatent variable h∗\ni that best explains ( xi, yi).\nIn our implementation, we used an improved version of\nthe cutting plane algorithm called the proximal bundle\nmethod (Kiwiel, 1990) to solve the standard Structural\nSVM problem in Equation (7). In our experience the\nproximal bundle method usually converges using fewer\niterations than the cutting plane algorithm (Joachims\net al., To appear) in the experiments below. The al-\ngorithm also ﬁts very nicely into the CCCP algorith-\nmic framework when it is employed to solve the stan-\ndard Structural SVM optimization problem inside the\nCCCP loop. The solution wt−1 from the last iteration\ncan be used as a starting point in a new CCCP iter-\nation, without having to reconstruct all the cuts from\nscratch. We will provide some computational experi-\nence at the end of the experiments section.\n5. Experiments\nBelow we demonstrate three applications of our La-\ntent Structural SVM algorithm. Some of them have\nbeen discussed in the machine learning literature be-\nfore, but we will show that our Latent Structural\nSVM framework provides new and straightforward so-\nlution approaches with good predictive performance.\nA software package implementing the Latent Struc-\ntural SVM algorithm is available for download at\nhttp://www.cs.cornell.edu/∼cnyu/latentssvm/.\n5.1. Discriminative Motif Finding\nOur development of the Latent Structural SVM was\nmotivated by a motif ﬁnding problem in yeast DNA\nthrough collaboration with computational biologists.\nMotifs are repeated patterns in DNA sequences that\nare believed to have biological signiﬁcance. Our\ndataset consists of ARSs (autonomously replicating se-\nquences) screened in two yeast species S. kluyveri and\nS. cerevisiae. Our task is to predict whether a par-\nticular sequence is functional (i.e., whether they start\nthe replication process) in S. cerevisiae and to ﬁnd\nout the motif responsible. All the native ARSs in S.\ncerevisiae are labeled as positive, since by deﬁnition\nthey are functional. The ones that showed ARS ac-\ntivity in S. kluyveri were then further tested to see\nwhether they contain functional ARS in S. cerevisiae,\nsince they might have lost their function due to se-\nquence divergence of the two species during evolution.\nThey are labeled as positive if functional and negative\notherwise. In this problem the latent variable h is the\nposition of the motif in the positive sequences, since\ncurrent experimental procedures do not have enough\nresolution to pinpoint their locations. Altogether we\nhave 124 positive examples and 75 negative examples.\nIn addition we have 6460 sequences from the yeast in-\ntergenic regions for background model estimation.\nPopular methods for motif ﬁnding includes methods\nbased on EM (Bailey & Elkan, 1995) and Gibbs-\nsampling. For this particular yeast dataset we believe\na discriminative approach, especially one incorporat-\ning large-margin separation, is beneﬁcial because of\nthe close relationship and DNA sequence similarity\namong the diﬀerent yeast species in the dataset.\nLet xi denote the ith base (A, C, G, or T) in our\ninput sequence x of length n. We use the common\nposition-speciﬁc weight matrix plus background model\napproach in our deﬁnition of feature vector:\nΨ(x, y, h)=\n\n\n\n\n\n\n\n\nl∑\nj=1\nφ(j)\nP SM(xh+j) +\nh∑\ni=1\nφBG(xi) +\nn∑\ni=h+l+1\nφBG(xi)\n[if y = +1]\nn∑\ni=1\nφBG(xi) [if y = −1],\nwhere φ(j)\nP SM is the feature count for the jth position\nof the motif in the position-speciﬁc weight matrix, and\nφBG is the feature count for the background model (we\nuse a Markov background model of order 3).\nFor the positive sequences, we randomly initialized the\nmotif position h uniformly over the whole length of\nthe sequence. We optimized over the zero-one loss ∆\nfor classiﬁcation and performed a 10-fold cross vali-\ndation. We make use of the set of 6460 intergenic\nsequences in training by treating them as negative ex-\namples (but they are excluded in the test sets). Instead\nof penalizing their slack variables by C in the objec-\ntive we only penalize these examples by C/50 to avoid\n1172\nLearning Structural SVMs with Latent V ariables\nTable 1. Classiﬁcation Error on Yeast DNA (10-fold CV)\nError rate\nGibbs sampler ( l = 11) 32.49%\nGibbs\nsampler ( l = 17) 31.47%\nLatent Structural SVM ( l = 11) 11.09%\nLatent Structural SVM ( l = 17) 12.00%\noverwhelming the training set with negative examples\n(with\nthe factor 1 /50 picked by cross-validation). We\ntrained models using regularization constant C from\n{0.1, 1, 10, 100, 1000} times the size of the training set\n(5992 for each fold), and each model is re-trained 10\ntimes using 10 diﬀerent random seeds.\nAs control we ran a Gibbs sampler (Ng & Keich, 2008)\non the same dataset, with the same set of intergenic\nsequences for background model estimation. It reports\ngood signals on motif lengths l = 11 and l = 17,\nwhich we compare our algorithm against. To provide a\nstronger baseline we optimize the classiﬁcation thresh-\nold of the Gibbs sampler on the test set and report\nthe best accuracy over all possible thresholds. Table\n1 compares the accuracies of the Gibbs sampler and\nour method averaged across 10 folds. Our algorithm\nshows a signiﬁcant improvement over the Gibbs sam-\npler (with p-value < 10−4 in a paired t-test). As for\nthe issue of local minima, the standard deviations on\nthe classiﬁcation error over the 10 random seeds, av-\neraged over 10 folds, are 0.0648 for l = 11 and 0.0546\nfor l = 17. There are variations in solution quality\ndue to local minima in the objective, but they are rel-\natively mild in this task and can be overcome with a\nfew random restarts.\nIn this application the Latent Structural SVM allows\nus to exploit discriminative information to better de-\ntect motif signals compared to traditional unsuper-\nvised probabilistic model for motif ﬁnding. Currently\nwe are working with our collaborators on ways to inter-\npret the position-speciﬁc weight matrix encoded in the\nweight vector trained by the Latent Structural SVM.\n5.2. Noun Phrase Coreference via Clustering\nIn noun phrase coreference resolution we would like\nto determine which noun phrases in a text refer to\nthe same real-world entity. In (Finley & Joachims,\n2005) the task is formulated as a correlation clustering\nproblem trained with Structural SVMs. In correlation\nclustering the objective function maximizes the sum\nof pairwise similarities. However this might not be\nthe most appropriate objective, because in a cluster of\ncoreferent noun phrases of size k, many of the O(k2)\nlinks contain only very weak signals. For example, it is\nFigure 1. The circles are the clusters deﬁned by the label\ny. The set of solid edges is one spanning forest h that\nis consistent with y. The dotted edges are examples of\nincorrect links that will be penalized by the loss function.\ndiﬃcult to determine whether a mention of the name\n‘Tom’ at the beginning of a text and a pronoun ‘he’\nat the end of the text are coreferent directly without\nscanning through the whole text.\nFollowing the intuition that humans might determine\nif two noun phrases are coreferent by reasoning tran-\nsitively over strong coreference links (Ng & Cardie,\n2002), we model the problem of noun phrase corefer-\nence as a single-link agglomerative clustering problem.\nEach input x contains all n noun phrases in a docu-\nment, and all the pairwise features xij between the\nith and jth noun phrases. The label y is a partition\nof the n noun phrases into coreferent clusters. The\nlatent variable h is a spanning forest of ‘strong’ coref-\nerence links that is consistent with the clustering y. A\nspanning forest h is consistent with a clustering y if\nevery cluster in y is a connected component in h (i.e.,\na tree), and there are no edges in h that connects two\ndistinct clusters in y (Figure 1).\nTo score a clustering y with a latent spanning forest\nh, we use a linear scoring model that adds up all the\nedge scores for edges in h, parameterized by w:\nw · Φ(x, y, h) =\n∑\n(i,j)∈h\nw · xij.\nTo predict a clustering y from an input x (argmax\nin Equation (2)), we can run any Maximum Span-\nning Tree algorithm such as Kruskal’s algorithm on\nthe complete graph of n noun phrases in x, with edge\nweights deﬁned by w · xij. The output h is a spanning\nforest instead of a spanning tree because two trees will\nremain disconnected if all edges connecting the two\ntrees have negative weights. We then output the clus-\ntering deﬁned by the forest h as our prediction y.\nFor the loss function ∆, we would like to pick one that\nsupports eﬃcient computation in the loss-augmented\ninference, while at the same time penalizing incorrect\nspanning trees appropriately for our application. We\n1173\nLearning Structural SVMs with Latent V ariables\nTable 2. Clustering Accuracy on MUC6 Data\nMITRE Loss Pair Loss\nSVM-cluster 41.3 2.89\nLaten\nt Structural SVM 44.1 2.66\nLatent Structural SVM\n(modiﬁed loss, r = 0.01) 35.6 4.11\npropose the loss function\n∆(\ny, ˆy, ˆh) = n(y) − k(y) −\n∑\n(i,j)∈ˆh\nl(y, (i, j)), (8)\nwhere n(y) and k(y) are the number of vertices and\nthe number of clusters in the correct clustering y. The\nfunction l(y, (i, j)) returns 1 if i and j are within the\nsame cluster in y, and -1 otherwise. It is easy to see\nthat this loss function is non-negative and zero if and\nonly if the spanning forest ˆh deﬁnes the same clustering\nas y. Since this loss function is linearly decomposable\ninto the edges in ˆh, the loss-augmented inference can\nalso be computed eﬃciently using Kruskal’s algorithm.\nSimilarly the step of completing the latent variable\nh given a clustering y, which involves computing a\nhighest scoring spanning forest that is consistent with\ny, can also be done with the same algorithm.\nTo evaluate our algorithm, we performed experiments\non the MUC6 noun phrase coreference dataset. There\nare 60 documents in the dataset and we use the ﬁrst\n30 for training and the remaining 30 for testing. The\npairwise features xij are the same as those in (Ng\n& Cardie, 2002). The regularization parameter C is\npicked from 10 −2 to 10 6 using a 10-fold cross valida-\ntion procedure. The spanning forest h for each correct\nclustering y is initialized by connecting all coreferent\nnoun phrases in chronological order (the order in which\nthey appear in the document), so that initially each\ntree in the spanning forest is a linear chain.\nTable 2 shows the result of our algorithm compared\nto the SVM correlation clustering approach in (Finley\n& Joachims, 2005). We present the results using the\nsame loss functions as in (Finley & Joachims, 2005).\nPair loss is the proportion of all O(n2) edges incor-\nrectly classiﬁed. MITRE loss is a loss proposed for\nevaluating noun phrase coreference that is related to\nthe F1-score (Vilain et al., 1995).\nWe can see from the ﬁrst two lines in the table that our\nmethod performs well on the Pair loss but worse on the\nMITRE loss when compared with the SVM correlation\nclustering approach. Error analysis reveals that our\nmethod trained with the loss deﬁned by Equation (8) is\nvery conservative when predicting links between noun\nphrases, having high precision but rather low recall.\nTherefore we adapt our loss function to make it more\nsuitable for minimizing the MITRE loss. We modiﬁed\nthe loss function in Equation (8) to penalize less for\nadding edges that incorrectly link two distinct clusters,\nusing a penalty r < 1 instead of 1 for each incorrect\nedge added. With the modiﬁed loss (with r = 0 .01\npicked via cross-validation) our method performs much\nbetter than the SVM correlation clustering approach\non the MITRE loss (p-value < 0.03 in a Z-test).\nUnlike the SVM correlation clustering approach, where\napproximate inference is required, our inference proce-\ndure involves only simple and eﬃcient maximum span-\nning tree calculations. For this noun phrase corefer-\nence task, the new formulation with Latent Structural\nSVM improves both the prediction performance and\ntraining eﬃciency over conventional Structural SVMs.\n5.3. Optimizing for Precision@k in Ranking\nOur last example application is related to optimizing\nfor precision@ k in document retrieval. Precision@ k\nis deﬁned to be the number of relevant documents in\nthe top k positions given by a ranking, divided by k.\nFor each example in the training set, the pattern x\nis a collection of n documents {x1, . . . ,xn} associated\nwith a query q, and the label y ∈ {− 1, 1}n classiﬁes\nwhether each document in the collection is relevant to\nthe query or not. However for the purpose of evalu-\nating and optimizing for information retrieval perfor-\nmance measures such as precision@ k and NDCG@ k,\nthe partial order of the documents given by the label\ny is insuﬃcient. The label y does not tell us which the\ntop k documents are. To deal with this problem, we\ncan postulate the existence of a latent total order h on\nall documents related to the query, with h consistent\nwith the partial order given by label y. To be precise,\nlet hj be the index of the jth most relevant document,\nsuch that xhj ≥tot xhj+1 for j from 1 to n − 1, where\n≥tot is a total order of relevance on the documents\nxi, and let >tot be its strict version. The label y is\nconsistent with the latent variable h if yi > y j implies\nxi >tot xj, so that all relevant documents in y comes\nbefore the non-relevant documents in the total order\nh. For optimizing for precision@ k in this section, we\ncan restrict h to be ﬁrst k documents h1, . . . , hk.\nWe use the following construction for the feature vec-\ntor (in a linear feature space):\nΦ(x, y, h) = 1\nk\n∑k\nj=1\nxhj .\nThe\nfeature vector only consists of contributions from\nthe top k documents selected by h, when all other doc-\numents in the label y are ignored (with the restriction\nthat h has to be consistent with y).\n1174\nLearning Structural SVMs with Latent V ariables\nw\nFigure 2. Laten t Structural SVM tries to optimize for ac-\ncuracy near the region for the top k documents (circled),\nwhen a good general ranking direction w is given\nFor the loss we use the following precision@ k loss:\n∆(y, ˆy, ˆh) = min{1, n(y)\nk } − 1\nk\nk∑\nj=1\n[yhj == 1].\nThis loss function is essentially one minus precision@ k,\nwith slight modiﬁcations when there are less than k\nrelevant documents in a collection. We replace 1 by\nn(y)/k so that the loss can be minimized to zero, where\nn(y) is the total number of relevant documents in y.\nIntuitively, with this particular design of the feature\nvector and the loss function, the algorithm is trying to\noptimize for the classiﬁcation accuracy in the region\nnear the top k documents, while ignoring most of the\ndocuments in the rest of the feature space (Figure 2).\nAll the inference problems required for this application\nare eﬃcient to solve. Prediction requires sorting based\non the score w · xj in decreasing order and picking the\ntop k. The loss-augmented inference requires sorting\nbased on the score w · xj − [yj == 1] and picking the\ntop k for ˆh. Latent variable completion for y requires\na similar sorting procedure on w · xj and picking the\ntop k, but during sorting the partial order given by the\nlabel y has to be respected (so that xi comes before xj\nwhen either yi > y j, or yi == yj and w · xi > w · xj).\nTo evaluate our algorithm, we ran experiments on\nthe OHSUMED tasks of the LETOR 3.0 dataset (Liu\net al., 2007). We use the per-query-normalized version\nof the features in all our training and testing below,\nand employ exactly the same training, test, and vali-\ndation sets split as given.\nFor this application it is vital to have a good initial-\nization of the latent varibles h. Simple initialization\nstrategies such as randomly picking k relevant docu-\nments indicated by the label y does not work for these\ndatasets with noisy relevance judgements, which usu-\nally give the trivial zero vector as solution. Instead\nwe adopt the following initialization strategy. Using\nthe same training and validation sets in each fold, we\ntrained a model optimizing for weighted average clas-\nTable 3. Precision@k on OHSUMED dataset (5-fold CV)\nOHSUMED P@1 P@3 P@5 P@10\nRanking SVM 0.597 0.543 0.532 0.486\nListNet 0.652\n0.602 0.550 0.498\nLatent Structural SVM 0.680 0.573 0.567 0.494\nInitial Weight Vector 0.626 0.557 0.524 0.464\n 0\n 10\n 20\n 30\n 40\n 50\n 60\n 70\n 0.01  1  100  10000  1e+06  1e+08\nNumber of CCCP iterations\nC\nMotif\nNP Coref\nOHSUMED prec@5\nFigure 3. Num ber of CCCP iterations against C\nsiﬁcation accuracy (weighted by the reciprocal of the\nnumber of documents associated by each query). Then\nfor each fold the trained model is used as the initial\nweight vector to optimize for precision@ k.\nWe can see from Table 3 that our Latent Struc-\ntural SVM approach performs better than the Ranking\nSVM (Herbrich et al., 2000; Joachims, 2002) on pre-\ncision@1,3,5,10, one of the stronger baselines in the\nLETOR 3.0 benchmark. We also essentially tie with\nListNet (Cao et al., 2007), one of the best overall rank-\ning method in the LETOR 3.0 benchmark. As a san-\nity check, we also report the performance of the initial\nweight vectors used for initializing the CCCP. The La-\ntent Structural SVM consistently improves upon these,\nshowing that the good performance is not simply a re-\nsult of good initialization.\n5.4. Eﬃciency of the Optimization Algorithm\nFigure 3 shows the number of iterations required for\nconvergence for the three tasks for diﬀerent values of\nthe parameter C, averaged across all folds in their re-\nspective cross validation procedures. We ﬁx the pre-\ncision ϵ at 0 .001 for the motif ﬁnding and optimizing\nfor precision@ k tasks, and use ϵ = 0 .05 for the noun\nphrase coreference task due to a diﬀerent scaling of the\nloss function. We can see that in general the number of\nCCCP iterations required only grows very mildly with\nC, and most runs ﬁnish within 50 iterations. As the\ncost of each CCCP iteration is no more than solving a\nstandard Structural SVM optimization problem (with\n1175\nLearning Structural SVMs with Latent V ariables\nthe completion of latent variables), the total number\nof\nCCCP iterations gives us a rough estimate of the\ncost of training Latent Structural SVMs, which is not\nparticularly expensive. In practice the cost is even\nlower because we do not need to solve the optimiza-\ntion problem to high precision in the early iterations,\nand we can also reuse solution from the previous iter-\nation for warm start in a new CCCP iteration.\n6. Conclusions\nWe have presented a framework and formulation for\nlearning Structural SVMs with latent variables. We\nidentify a particular case that covers a wide range of\napplication problems, yet aﬀords an eﬃcient training\nalgorithms using Convex-Concave Programming. The\nalgorithm is modular and easily adapted to new appli-\ncations. We demonstrated the generality of the Latent\nStructural SVM with three applications, and a future\nresearch direction will be to explore further applica-\ntions of this algorithm in diﬀerent domains.\nAcknowledgments\nThis work is supported by NSF Award IIS-0713483.\nWe would like to thank Tom Finley and Professor Uri\nKeich for the datasets, and the anonymous reviewers\nfor their helpful suggestions to improve this paper.\nReferences\nBailey, T., & Elkan, C. (1995). Unsupervised Learning\nof Multiple Motifs in Biopolymers Using Expecta-\ntion Maximization. Machine Learning, 21, 51–80.\nCao, Z., Qin, T., Liu, T., Tsai, M., & Li, H. (2007).\nLearning to rank: from pairwise approach to listwise\napproach. Proc. of the Int. Conf. on Mach. Learn.\n(pp. 129–136).\nChapelle, O., Do, C., Le, Q., Smola, A., & Teo, C.\n(2008). Tighter bounds for structured estimation.\nAdv. in Neural Inf. Process. Syst. (pp. 281–288).\nCollobert, R., Sinz, F., Weston, J., & Bottou, L.\n(2006). Trading convexity for scalability. Proc. of\nthe Int. Conf. on Mach. Learn. (pp. 201–208).\nFelzenszwalb, P., McAllester, D., & Ramanan, D.\n(2008). A Discriminatively Trained, Multiscale, De-\nformable Part Model. Proc. Computer Vision and\nPattern Recognition Conf. (pp. 1–8).\nFinley, T., & Joachims, T. (2005). Supervised cluster-\ning with support vector machines. Proc. of the Int.\nConf. on Mach. Learn. (p. 217).\nHerbrich, R., Graepel, T., & Obermayer, K. (2000).\nLarge margin rank boundaries for ordinal regression.\nIn Advances in large margin classiﬁers , chapter 7,\n115–132. MIT Press.\nJoachims, T. (2002). Optimizing search engines using\nclickthrough data. ACM SIGKDD Conf. on Knowl-\nedge Discovery and Data Mining (pp. 133–142).\nJoachims, T., Finley, T., & Yu, C. (To appear).\nCutting-plane training of structural SVMs. Machine\nLearning.\nKiwiel, K. (1990). Proximity control in bundle\nmethods for convex nondiﬀerentiable minimization.\nMathematical Programming, 46, 105–122.\nLiu, T., Xu, J., Qin, T., Xiong, W., & Li, H. (2007).\nLETOR: Benchmark dataset for research on learn-\ning to rank for information retrieval. SIGIR Work-\nshop on Learning to Rank for Information Retrieval.\nNg, P., & Keich, U. (2008). GIMSAN: a Gibbs motif\nﬁnder with signiﬁcance analysis. Bioinformatics, 24,\n2256.\nNg, V., & Cardie, C. (2002). Improving machine learn-\ning approaches to coreference resolution. Proc. of\nAssoc. for Computational Linguistics (p. 104).\nPetrov, S., & Klein, D. (2007). Discriminative Log-\nLinear Grammars with Latent Variables. Adv. in\nNeural Inf. Process. Syst. (p. 1153).\nSmola, A., Vishwanathan, S., & Hofmann, T. (2005).\nKernel methods for missing variables. Proc. of the\nInt. Conf. on Artif. Intell. and Stat. (p. 325).\nTaskar, B., Guestrin, C., & Koller, D. (2003). Max-\nmargin Markov networks. Adv. in Neural Inf. Pro-\ncess. Syst. (p. 51).\nTsochantaridis, I., Hofmann, T., Joachims, T., & Al-\ntun, Y. (2004). Support vector machine learning for\ninterdependent and structured output spaces. Proc.\nof the Int. Conf. on Mach. Learn. (p. 104).\nVilain, M., Burger, J., Aberdeen, J., Connolly, D., &\nHirschman, L. (1995). A model-theoretic coreference\nscoring scheme. Proceedings of the 6th conference on\nMessage understanding (pp. 45–52).\nWang, S., Quattoni, A., Morency, L., Demirdjian, D.,\n& Darrell, T. (2006). Hidden Conditional Random\nFields for Gesture Recognition. Proc. Computer Vi-\nsion and Pattern Recognition Conf. (p. 1521).\nWang, Y., & Mori, G. (2008). Max-margin hidden con-\nditional random ﬁelds for human action recognition\n(Technical Report TR 2008-21). School of Comput-\ning Science, Simon Fraser University.\nYuille, A., & Rangarajan, A. (2003). The Concave-\nConvex Procedure. Neural Computation , 15, 915.\nZien, A., Brefeld, U., & Scheﬀer, T. (2007). Trans-\nductive support vector machines for structured vari-\nables. Proc. of the Int. Conf. on Mach. Learn. (pp.\n1183–1190).\n1176",
  "values": {
    "Fairness": "Yes",
    "Justice": "Yes",
    "User influence": "Yes",
    "Respect for Persons": "Yes",
    "Privacy": "Yes",
    "Collective influence": "Yes",
    "Not socially biased": "Yes",
    "Beneficence": "Yes",
    "Transparent (to users)": "Yes",
    "Critiqability": "Yes",
    "Interpretable (to users)": "Yes",
    "Respect for Law and public interest": "Yes",
    "Deferral to humans": "Yes",
    "Explicability": "Yes",
    "Non-maleficence": "Yes",
    "Autonomy (power to decide)": "Yes"
  }
}