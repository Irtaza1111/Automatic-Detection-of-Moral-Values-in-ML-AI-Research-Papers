{
  "pdf": "1803.06373v1",
  "title": "Adversarial Logit Pairing",
  "author": "Harini Kannan*, Alexey Kurakin, Ian Goodfellow",
  "paper_id": "1803.06373v1",
  "text": "arXiv:1803.06373v1  [cs.LG]  16 Mar 2018\nAdversarial Logit Pairing\nHarini Kannan* 1 Alexey Kurakin 1 Ian Goodfellow 1\nAbstract\nIn this paper, we develop improved techniques\nfor defending against adversarial examples at\nscale. First, we implement the state of the art\nversion of adversarial training at unprecedented\nscale on ImageNet and investigate whether it\nremains effective in this setting—an important\nopen scientiﬁc question (\nAthalye et al. , 2018).\nNext, we introduce enhanced defenses using a\ntechnique we call logit pairing, a method that en-\ncourages logits for pairs of examples to be sim-\nilar. When applied to clean examples and their\nadversarial counterparts, logit pairing improves\naccuracy on adversarial examples over vanilla ad-\nversarial training; we also ﬁnd that logit pairing\non clean examples only is competitive with ad-\nversarial training in terms of accuracy on two\ndatasets. Finally, we show that adversarial logit\npairing achieves the state of the art defense on\nImagenet against PGD white box attacks, with\nan accuracy improvement from 1.5% to 27.9% .\nAdversarial logit pairing also successfully dam-\nages the current state of the art defense against\nblack box attacks on Imagenet (\nTram` er et al.,\n2018), dropping its accuracy from 66.6% to\n47.1%. With this new accuracy drop, adversar-\nial logit pairing ties with Tram` er et al. (2018) for\nthe state of the art on black box attacks on Ima-\ngeNet.\n1. Introduction\nMany deep learning models today are vulnerable to adver-\nsarial examples, or inputs that have been intentionally op-\ntimized to cause misclassiﬁcation. In the context of com-\nputer vision, object recognition classiﬁers incorrectly r ec-\nognize images that have been modiﬁed with small, often\n1Google Brain, Mountain View, CA, USA. Correspon-\ndence to: Harini Kannan <hkannan@google.com>, Alexey\nKurakin <kurakin@google.com>, Ian Goodfellow <goodfel-\nlow@google.com>.\n*Work done as a member of the Google Brain Residency pro-\ngram (g.co/brainresidency)\nimperceptible perturbations. It is important to develop mod-\nels that are robust to adversarial perturbations for a varie ty\nof reasons:\n• so that machine learning can be used in situations\nwhere an attacker may attempt to interfere with the\noperation of the deployed system,\n• so that machine learning is more useful for model-\nbased optimization,\n• to gain a better understanding of how to provide per-\nformance guarantees for models under distribution\nshift,\n• to gain a better understanding of how to enforce\nsmoothness assumptions, etc.\nIn this paper, we investigate defenses against such adversar-\nial attacks. The contributions of this paper are the follow-\ning:\n• We implement the state of the art version of adversar-\nial training at unprecedented scale and investigate its\neffectiveness on the ImageNet dataset.\n• We propose logit pairing , a method that encourages\nthe logits for two pairs of examples to be similar. We\npropose two ﬂavors of logit pairing: clean and adver-\nsarial.\n• We show that clean logit pairing is a method with\nminimal computational cost that defends against PGD\nblack box attacks almost as well as adversarial train-\ning for two datasets.\n• We show that adversarial logit pairing is a method\nthat leads to higher accuracy when subjected to white\nbox and black box attacks. We achieve the current\nstate of the art on black-box and white-box accuracies\nwith our model trained with adversarial logit pairing.\n• We show that attacks constructed with our adversari-\nally trained models substantially damage the current\nstate of the art for black box defenses on ImageNet\n(\nTram` er et al., 2018). We then show that our models\nare resistant to these attacks.\nAdversarial Logit Pairing\n2. Deﬁnitions and threat models\nDefense mechanisms are intended to provide security under\nparticular threat models. The threat model speciﬁes the ca-\npabilities of the adversary. In this paper, we always assume\nthe adversary is capable of forming attacks that consist of\nperturbations of limited L∞ norm. This is a simpliﬁed task\nchosen because it is more amenable to benchmark evalua-\ntions. Realistic attackers against computer vision system s\nwould likely use different attacks that are difﬁcult to charac-\nterize with norm balls, such as\nBrown et al. (2017). We con-\nsider two different threat models characterizing amounts o f\ninformation the adversary can have:\n1. White box: the attacker has full information about the\nmodel (i.e. knows the architecture, parameters, etc.).\n2. Black box: the attacker has no information about the\nmodel’s architecture or parameters, and no ability to\nsend queries to the model to gather more information.\n3. The challenges of defending ImageNet\nclassiﬁers\nMultiple methods to defend against adversarial ex-\namples have been proposed (\nBuckman et al., 2018;\nGoodfellow et al., 2014; Kolter & Wong, 2017;\nMadry et al., 2017; Papernot et al., 2016; Szegedy et al.,\n2013; Tram` er et al., 2018; Xu et al. , 2017). Recently,\nAthalye et al. (2018) broke several defenses proposed for\nthe white box setting that relied on empirical testing to\nestablish their level of robustness. In our work we choose\nto focus on\nMadry et al. (2017) because it is a method\nthat has withstood intense scrutiny even in the white box\nsetting.\nAthalye et al. (2018) endorsed Madry et al. (2017)\nas the only such method that they were not able to break.\nHowever, they observe that the defense from\nMadry et al.\n(2017) has not been shown to scale to ImageNet. There\nare also certiﬁed defenses ( Aditi Raghunathan, 2018;\nAman Sinha, 2018; Kolter & Wong, 2017) that provide\nguaranteed robustness, but the total amount of robustness\nthey guarantee is small compared to the amount empirically\nclaimed by\nMadry et al. (2017). This leaves Madry et al.\n(2017) as a compelling defense to study because it provides\na large beneﬁt that has withstood intensive scrutiny.\nIn this paper, we implement the\nMadry et al. (2017) defense\nat ImageNet scale for the ﬁrst time and evaluate it using\nthe same attack methodology as has been used at smaller\nscale. Our results provide an important conclusive answer\nto an open question (\nAthalye et al. , 2018) about whether\nthis defense strategy scales.\nThe defense used by Madry et al. (2017) consists\nof using adversarial training ( Goodfellow et al., 2014;\nSzegedy et al., 2013) with an attack called “projected gradi-\nent descent” (PGD). Their PGD attack consists of initializ-\ning the search for an adversarial example at a random point\nwithin the allowed norm ball, then running several itera-\ntions of the basic iterative method (\nKurakin et al., 2017b) to\nﬁnd an adversarial example. The noisy initial point creates\na stronger attack than other previous iterative methods such\nas BIM (\nKurakin et al., 2017a), and performing adversarial\ntraining with this stronger attack makes their defense more\nsuccessful (\nMadry et al., 2017). Kurakin et al. (2017a) ear-\nlier reported that adversarial training with (non-noisy) BIM\nadversarial examples did not result in general robustness t o\na wide variety of attacks.\nAll previous attempted defenses on ImageNet\n(\nKurakin et al., 2017a; Tram` er et al., 2018) report er-\nror rates of 99 percent on strong, multi-step white box\nattacks. We, for the ﬁrst time, scale the Madry et al.\n(2017) defense to this setting and successfully apply it.\nFurthermore, we also introduce an enhanced defense that\ngreatly improves over this baseline and improves the\namount of robustness achieved.\n4. Methods\n4.1. Adversarial training\nMadry et al. (2017) suggests that PGD is a universal ﬁrst\norder adversary – in other words, developing robustness\nagainst PGD attacks also implies resistance against many\nother ﬁrst order attacks. We use adversarial training with\nPGD as the underlying basis for our methods:\narg min\nθ\nE(x,y)∈ ˆpdata\n(\nmax\nδ∈ S\nL(θ, x + δ, y)\n)\n(1)\nwhere ˆpdata is the underlying training data distribution,\nL(θ, x, y) is a loss function at data point x which has true\nclass y for a model with parameters θ, and the maximiza-\ntion with respect to δ is approximated using noisy BIM.\nWe ﬁnd that we achieve better performance not by liter-\nally solving the min-max problem described by\nMadry et al.\n(2017). Instead, we train on a mixture of clean and ad-\nversarial examples, as recommended by Goodfellow et al.\n(2014); Kurakin et al. (2017a):\narg min\nθ\n[\nE(x,y)∈ ˆpdata\n(\nmax\nδ∈ S\nL(θ, x + δ, y)\n)\n+\nE(x,y)∈ ˆpdata\n(\nL(θ, x, y)\n) ]\n(2)\nThis formulation helps to maintain good accuracy on\nclean examples. We call this defense formulation mixed-\nminibatch PGD (M-PGD). We note that though we have\nAdversarial Logit Pairing\nvaried the defense slightly from the one used in Madry et al.\n(2017), we still use the attack from Madry et al. (2017),\nwhich is also endorsed by Athalye et al. (2018).\n4.2. Logit pairing\nWe propose logit pairing, a method to encourage the logits\nfrom two images to be similar to each other. For a model\nthat takes inputs x and computes a vector of logits z =\nf (x), logit pairing adds a loss\nλL (f (x), f(x′))\nfor pairs of training examples x and x′, where λ is a coef-\nﬁcient determining the strength of the logit pairing penalt y\nand L is a loss function encouraging the logits to be similar.\nIn this paper we use L2 loss for L, but other losses such as\nL1 or Huber could also be suitable choices.\nWe explored two logit pairing techniques which are de-\nscribed below. We found each of them to be useful: ad-\nversarial logit pairing obtains the best-yet defense again st\nthe Madry attack, while clean logit pairing, and a related\nidea we call logit squeezing, provide competitive defenses\nat signiﬁcantly reduced cost.\n4.2.1. A DVERSARIAL LOGIT PAIRING\nAdversarial logit pairing (ALP) matches the logits from a\nclean image x and its corresponding adversarial image x′.\nIn traditional adversarial training, the model is trained t o\nassign both x and x′ to the same output class label, but the\nmodel does not receive any information indicating that x′ is\nmore similar to x than to another example of the same class.\nALP provides an extra regularization term encouraging sim-\nilar embeddings of the clean and adversarial versions of the\nsame example, helping guide the model towards better in-\nternal representations of the data.\nConsider a model with parameters θ trained on a minibatch\nM of clean examples {x(1), . . . ,x(m)} and corresponding\nadversarial examples { ˜x(1), . . . , ˜x(m)}. Let f (x; θ) be the\nfunction mapping from inputs to logits of the model. Let\nJ(M, θ) be the cost function used for adversarial training\n(the cross-entropy loss applied to train the classiﬁer on each\nexample in the minibatch, plus any weight decay, etc.). Ad-\nversarial logit pairing consists of minimizing the loss\nJ(M, θ) + λ 1\nm\nm∑\ni=1\nL\n(\nf (x(i); θ), f( ˜x(i); θ)\n)\n.\n4.2.2. C LEAN LOGIT PAIRING\nIn clean logit pairing (CLP), x and x′ are two randomly\nselected clean training examples, and thus are typically not\neven from the same class. Let J (clean)(M, θ) be the loss\nfunction used to train a classiﬁer on a minibatch M, such\nas a cross-entropy loss and any other loss terms such as\nweight decay. Clean logit pairing consists of minimizing\nthe loss\nJ (clean)(M, θ) + λ 2\nm\nm\n2∑\ni=1\nL\n(\nf (x(i); θ), f(x(i+ m\n2 ); θ)\n)\n.\nWe included experiments with clean logit pairing in order\nto perform an ablation study, understanding the contribu-\ntion of the pairing loss itself relative to the formation of\nclean and adversarial pairs. To our surprise, inducing sim-\nilarity between random pairs of logits led to high levels of\nrobustness on MNIST and SVHN. This leads us to suggest\nclean logit pairing as a method worthy of study in its own\nright rather than just as a baseline. CLP is surprisingly ef-\nfective and has signiﬁcantly lower computation cost than\nadversarial training or ALP .\nWe note that our best results with CLP relied on adding\nGaussian noise to the input during training, a standard\nneural network regularization technique (\nSietsma & Dow,\n1991).\n4.2.3. C LEAN LOGIT SQUEEZING\nSince clean logit pairing led to high accuracies, we hy-\npothesized that the model was learning to predict logits\nof smaller magnitude and therefore being penalized for be-\ncoming overconﬁdent. To this end, we tested penalizing the\nnorm of the logits, which we refer to as “logit squeezing”\nfor the rest of the paper. For MNIST, it turned out that logit\nsqueezing gave us better results than logit pairing.\n5. Adversarial logit pairing results and\ndiscussion\n5.1. Results on MNIST\nHere, we ﬁrst present results with adversarial logit pairin g\non MNIST. We found that the exact value of the logit pair-\ning weight did not matter too much on MNIST as long as it\nwas roughly between 0.2 and 1. As long as some logit pair-\ning was added, the accuracy on adversarial examples im-\nproved compared to vanilla adversarial training. We used a\nﬁnal logit pairing weight of 1 in the values reported in Table\n1. A weight of 1 corresponds to weighting both the adver-\nsarial logit pairing loss and the cross-entropy loss equall y.\nWe used the LeNet model as in Madry et al. (2017). We\nalso used the same attack parameters they used: total adver-\nsarial perturbation of 76.5/255 (0.3), perturbation per st ep\nof 2.55/255 (0.01), and 40 total attack steps with 1 random\nrestart. Similar to\nMadry et al. (2017), we generated black\nbox examples for MNIST by independently initializing and\nadversarially training a copy of the LeNet model. We then\nused the PGD attack on this model to generate the black\nAdversarial Logit Pairing\nbox examples.\nMethod White Box Black Box Clean\nM-PGD 93.2% 96.0% 98.5%\nALP 96.4% 97.5% 98.8%\nTable 1. Comparison of adversarial logit pairing and vanilla adver-\nsarial training on MNIST. All accuracies reported are for the PGD\nattack.\nAs shown in Table 1, adversarial logit pairing achieves state\nof the art on MNIST for the PGD attack. It improves white\nbox accuracy from 93.2% to 96.4%, and it improves black\nbox accuracy from 96.0% to 97.5%.\n5.2. Results on SVHN\nMethod White Box Black Box Clean\nM-PGD 44.4% 55.4% 96.9%\nALP 46.9% 56.2% 96.2%\nTable 2. Comparison of adversarial logit pairing and vanilla adver-\nsarial training on SVHN. All accuracies reported are for the PGD\nattack.\nOur PGD attack parameters for SVHN were as follows: a\ntotal epsilon perturbation of 12/255, a per-step epsilon of\n3/255, and 10 attack iterations.\nFor SVHN, we used the RevNet-9 model (\nGomez et al. ,\n2017). RevNets are similar to ResNets in that they both use\nresidual connections, have similar architectures, and get\nsimilar accuracies on multiple datasets. However, RevNets\nhave large memory savings compared to ResNets, as their\nmemory usage is constant and does not scale with the num-\nber of layers. Because of this, we used RevNets in order\nto take advantage of larger batch sizes and quicker conver-\ngence times.\nSimilar to MNIST, most logit pairing values from 0.5 to\n1 worked, and as long as some logit pairing was added, it\ngreatly improved accuracies. However, making the logit\npairing values too large (e.g. anything larger than 2) did no t\nlead to any beneﬁt and was roughly the same as vanilla ad-\nversarial training. The ﬁnal adversarial logit pairing wei ght\nused in Table\n2 was 0.5.\n5.3. Results on ImageNet\n5.3.1. M OTIVATION\nPrior to this work, the standard baseline of PGD adversarial\ntraining had not yet been scaled to ImageNet. Kurakin et\nal. (\n2017a) showed that adversarial training with one-step\nattacks confers robustness to other one-step attacks, but i s\nunable to make a difference with multi-step attacks. Train-\ning on multi-step attacks did not help either.\nMadry et al.\n(2017) demonstrated successful defenses based on multi-\nstep noisy PGD adversarial training on MNIST and CIFAR-\n10, but did not scale the process to ImageNet.\nHere, we implement and scale the state of the art adver-\nsarial training method from CIFAR-10 and MNIST to Ima-\ngeNet for the ﬁrst time. We then implement our adversarial\nlogit pairing method for comparison.\n5.3.2. I MPLEMENTATION DETAILS\nThe cost of adversarial training scales with the number of\nattack steps because a full round of backpropagation is per-\nformed with each step. This means that a rough estimate of\nthe total adversarial training time of a model can be found\nby multiplying the total clean training time by the num-\nber of attack steps. With ImageNet, this can be especially\ncostly without any optimizations.\nTo effectively scale up adversarial training with PGD to Im-\nageNet, we implemented synchronous distributed training\nin Tensorﬂow with 53 workers: 50 were used for gradi-\nent aggregation, and 3 were left as backup replicas. Each\nworker had one p100 card. We experimented with asyn-\nchronous gradient updates, but we found that it led to stale\ngradients and poor convergence. Additionally, we used 17\nparameter servers that ran on CPUs. Large batch training\nhelped to scale up adversarial training as well: each replic a\nhad a batch size of 32, for an effective batch size of 1600\nimages. We found that the total time to convergence was\napproximately 6 days.\nSimilar to\nKurakin et al. (2017a), we use the InceptionV3\nmodel to implement adversarial training on ImageNet in\norder to better compare results.\nLike\nSzegedy et al. (2016), we used RMSProp for our op-\ntimizer, a starting learning rate of 0.045, a learning rate\ndecay every two epochs at an exponential rate of 0.94, and\nmomentum of 0.9.\nFinally, we used the Cleverhans library (\nNicolas Papernot,\n2017) to implement our adversarial attacks.\n5.3.3. T ARGETED VS . UNTARGETED ATTACKS\nAthalye et al. (2018) state that on ImageNet, accuracy on\ntargeted attacks is a much more meaningful metric to use\nthan accuracy on untargeted attacks. They state that this\nis because untargeted attacks can cause misclassiﬁcation o f\nvery similar classes (e.g. images of two very similar dog\nbreeds), which is not meaningful. This is consistent with\nobservations by\nKurakin et al. (2017a).\nTo that end, as Athalye et al. (2018) recommends, all accu-\nracies we report on ImageNet are for targeted attacks, and\nAdversarial Logit Pairing\nall adversarial training was done with targeted attacks.\n5.3.4. R ESULTS AND DISCUSSION\nResults with adversarial logit pairing. We present our\nmain ImageNet results in Tables 3 and 4. All accuracies\nreported refer to the worst case accuracies among all at-\ntacks we tried in each of the two threat models we consider\n(white box and black box). We used the following attacks\nin our attack suite, which are all from\nMadry et al. (2017),\nKurakin et al. (2017a), and Tram` er et al. (2018): Step-LL,\nStep-Rand, R+Step-LL, R+Step-Rand, Iter-Rand, Iter-LL,\nPGD-Rand, and PGD-LL. The sufﬁxes ”Rand” and ”LL”\ndenote targeting a random class and targeting the least\nlikely class, respectively. For the multi-step attacks in o ur\nsuite, we varied the size of the total adversarial perturba-\ntion, the size of the perturbation per step, and the number\nof attack steps. Below are the maximum values of each of\nthese sizes that we tried:\n• Size of total adversarial perturbation: 16/255 on a\nscale of 0 to 1\n• Size of total adversarial perturbation per step: 2/255\non a scale of 0 to 1\n• Number of attack steps: 10\nAll accuracies reported are on the ImageNet validation set.\nWhite Box White Box\nMethod T op 1 T op 5\nRegular training 0.7% 4.4 %\nTram` er et al. (2018) 1.3% 6.5 %\nKurakin et al. (2017a) 1.5% 5.5 %\nM-PGD 3.9% 10.3%\nALP 27.9% 55.4%\nTable 3. Comparison of adversarial logit pairing and vanilla adver-\nsarial training on ImageNet. All accuracies reported are fo r white\nbox accuracy on the ImageNet validation set.\nBlack Box Black Box\nMethod T op 1 T op 5\nM-PGD 36.5% 62.3%\nALP 46.7% 74.0%\nTram` er et al. (2018) 47.1% 74.3%\nTable 4. Comparison of adversarial logit pairing and vanilla adver-\nsarial training on ImageNet. All accuracies reported are fo r black\nbox accuracy on the ImageNet validation set.\nDamaging Ensemble Adversarial Training. Ensemble\nadversarial training ( Tram` er et al., 2018) reported the state\nof the art for ImageNet black box attacks at 66.6% Top-\n1 black box accuracy for InceptionV3. Here, we present\na black box attack that signiﬁcantly damages the defense\nproposed in Ensemble Adversarial Training.\nWe construct a black box attack by taking an ALP-trained\nImageNet model and constructing a transfer attack with\nthat model. Out of all of the attacks we tried, we found\nthat the Iter-Rand attack (\nKurakin et al., 2017a) was the\nstrongest against Ensemble Adversarial Training. This at-\ntack reduces the accuracy of Ensemble Adversarial Train-\ning from 66.6% Top-1 black box accuracy to 47.1%.\nWe hypothesize that the reason this attack was so strong is\nbecause it came from a model that had used multi-step ad-\nversarial training. The attacks used in\nTram` er et al. (2018)\nall came from models that had been trained with one or\ntwo steps of adversarial training. Black box results from\nMadry et al. (2017) generally show that examples from ad-\nversarially trained models are more likely to transfer to\nother models.\nThus, we recommend adversarial training with full iterative\nattacks to provide a minimal level of white box and black\nbox robustness on ImageNet. When testing black box ac-\ncuracy on ImageNet, we recommend using attacks from\nmodels that have been adversarially trained with multiple\nsteps to get a sense of the strongest possible black box at-\ntack. Adversarial training with one step attacks (even with\nensemble training) on ImageNet can be broken in both the\nwhite box and black box case.\nDiscussion. Firstly, our results show that PGD adversarial\ntraining can lead to convergence on ImageNet when com-\nbined with synchronous gradient updates and large batch\nsizes. Scaling adversarial training to ImageNet had not\nbeen previously shown before and had been an open ques-\ntion (\nAthalye et al. , 2018). Multi-step adversarial training\ndoes show an improvement on white box accuracies from\nthe previous state-of-the-art, from 1.5% to 3.9%.\nSecondly, we see that ALP further improves white box ac-\ncuracy from the adversarial training baseline – showing an\nimprovement from 3.9% to 27.9%. Adversarial logit pair-\ning also improves black box accuracy from the M-PGD\nbaseline, going from 36.5% to 47.1%.\nFinally, these results show that adversarial logit pairing\nachieves state of the art on ImageNet on white box attacks\n– with a drastic 20x improvement over the previous state of\nthe art (\nKurakin et al., 2017a; Tram` er et al., 2018). We do\nthis while still matching the black box results of Ensemble\nAdversarial Training, the current state-of-the-art black box\ndefense (Tram` er et al., 2018).\nWe hypothesize that adversarial logit pairing works well\nbecause it provides an additional prior that regularizes th e\nAdversarial Logit Pairing\nmodel toward a more accurate understanding of the classes.\nIf we train the model with only the cross-entropy loss, it\nis prone to learning spurious functions that ﬁt the training\ndistribution but have undeﬁned behavior off the training\nmanifold. Adversarial training adds additional informati on\nabout the structure of the space. By adding an assump-\ntion that small perturbations should not change the class,\nregardless of direction, adversarial training introduces an-\nother prior that forces the model to select functions that\nhave sensible behavior over a much larger region. How-\never, adversarial training does not include any informatio n\nabout the relationship between a clean adversarial example\nand the adversarial version of the same example. In adver-\nsarial training, we might take an image of a cat, perturb\nit so the model thinks it is a dog, and then ask the model\nto still recognize the image as a cat. There is no signal to\ntell the model that the adversarial example is similar speci f-\nically to the individual cat image that started the process.\nAdversarial logit pairing forces the explanations of a clea n\nexample and the corresponding adversarial example to be\nsimilar. This is essentially a prior encouraging the model\nto learn logits that are a function of the truly meaningful\nfeatures in the image (position of cat ears, etc.) and ignore\nthe features that are spurious (off-manifold directions in tro-\nduced by adversarial perturbations). We can also think of\nthe process as distilling (\nHinton et al., 2015) the knowledge\nfrom the clean domain into the adversarial domain and vice\nversa.\nSimilar to the dip in clean accuracy on CIFAR-10 reported\nby\nMadry et al. (2017), we found that our models have a\nslight dip in clean accuracy to 72%. However, we believe\nthis is outweighed by the large gains in adversarial accura-\ncies.\n5.3.5. C OMPARISON OF DIFFERENT ARCHITECTURES\nModel architecture plays a role in adversarial robustness\n(\nCubuk et al., 2017), and models with higher capacities\ntend to be more robust ( Kurakin et al., 2017a; Madry et al. ,\n2017). Since ImageNet is a particularly challenging dataset,\nwe think that studying different model architectures in con -\njunction with adversarial training would be valuable. In\nthis work, we primarily studied InceptionV3 to offer bet-\nter comparisons to previous literature. With the rest of our\navailable computational resources, we were able to study\nan additional model (ResNet-101) to see if residual con-\nnections impacted adversarial robustness. We used ALP to\ntrain the models, and results are reported in Tables\n5 and 6.\n5.4. Clean logit pairing results\nWe experimented with clean logit pairing on MNIST, and\nwe found that it gave surprisingly high results on white\nbox and black box accuracies. As mentioned in our meth-\nMethod White Box T op 1 White Box T op 5\nInceptionV3 27.9% 55.4%\nResNet-101 30.2% 55.8%\nTable 5. Comparison of InceptionV3 and ResNet101 on Ima-\ngeNet. All accuracies reported are for white box accuracy on\nthe ImageNet validation set.\nMethod Black Box T op 1 Black Box T op 5\nInceptionV3 46.7% 74.0%\nResNet-101 36.0% 62.2%\nTable 6. Comparison of InceptionV3 and ResNet101 on Ima-\ngeNet. All accuracies reported are for black box accuracy on\nthe ImageNet validation set.\nods section, we augmented images with Gaussian noise\nﬁrst and then applied clean logit pairing or logit squeezing .\nLogit squeezing resulted in slightly higher PGD accuracies\nthan CLP (detailed in Figure\n1). Table 7 contains our ﬁ-\nnal MNIST results on clean logit squeezing. For evaluation\nwith PGD, we used the same attack parameters as our eval-\nuation for adversarial logit pairing.\nMethod White box Black box Clean\nM-PGD 93.2% 96.0% 98.8%\nLogit squeezing 86.4% 96.8% 99.0%\nTable 7. Comparison of clean logit squeezing and vanilla adver-\nsarial training on MNIST. All accuracies reported are for the PGD\nattack.\nAs Table 7 shows, clean logit squeezing is competitive with\nadversarial training, despite the large reduction in compu ta-\ntional cost.\nWe also experimented with changing the weight of logit\npairing and logit squeezing to see if it acts as a controllabl e\nparameter, and results are in Figure 1.\nOne thing to note about Figure 1 is that simply augmenting\nimages with Gaussian noise is enough to bring up PGD ac-\ncuracy to around 25 % – about 2.5 times better than guess-\ning at random. We would like to emphasize that the noise\nwas added during training time, not test time. Noise and\nother randomized test time defenses have been shown to be\nbroken by\nAthalye et al. (2018). Going from nearly 0 per-\ncent PGD accuracy to 25 percent with just Gaussian noise\nsuggests that there could be other simple changes to train-\ning procedures that result in better robustness against at-\ntacks.\nThe below table reports results on SVHN with the PGD\nattack. Below are the attack parameters used:\nAdversarial Logit Pairing\n0 0 .2 0 .4 0 .6 0 .8 10\n10\n20\n30\n40\n50\n60\n70\n80\n90\n100\nLogit pairing or logit squeezing weight\nPGD accuracy\nWeight vs. MNIST PGD accuracies\nLogit squeezing\nLogit pairing\nFigure 1. V arying the logit pairing weight for MNIST\n• Size of total adversarial perturbation: 12/255 on a\nscale of 0 to 1\n• Size of total adversarial perturbation per step: 3/255\non a scale of 0 to 1\n• Number of attack steps: 10\nMethod White Box Black Box Clean\nM-PGD 44.4% 55.4% 96.9%\nCLP 39.1% 55.8% 95.5%\nTable 8. Clean logit pairing results on SVHN.\nAs the above tables show, clean logit pairing is competitive\nwith adversarial training for black box results, despite the\nlarge reduction in computational cost. Adversarial training\nwith multi-step attacks scales with the number of steps per\nattack because full backpropagation is completed with each\nattack step. In other words, if the normal training time of a\nmodel is N, adversarial training with k steps per attack will\nroughly cause the full training time of the model to be kN.\nIn contrast, the cost of CLP in terms of ﬂoating point oper-\nations and memory consumption is O(1) in the sense that\nit does not scale with the number or size of hidden layers\nin the model, input image size, or number of attack steps.\nIt does scale with the number of logits, but this is negligi-\nble compared to the other factors. Typically the number\nof logits is determined by the task (10 for CIFAR-10, 100\nfor ImageNet) and remains ﬁxed, while other factors like\nmodel size are desirable to increase. For example, binary\nclassiﬁcation is a common task in many real world applica-\ntions like spam and fraud detection.\nWe hope that CLP points the way to further effective de-\nfenses that are essentially free. Defenses with low computa -\ntional cost are more likely to be adopted since they require\nfewer resources. The future of machine learning security\nis much brighter if security can be accomplished without a\nmajor tradeoff against training efﬁciency.\n6. Comparison to other possible approaches\nLogit pairing is similar to two other approaches that have\nbeen previously shown to improve adversarial robustness:\nlabel smoothing and mixup.\nLabel smoothing (\nSzegedy et al., 2016) consists of train-\ning a classiﬁer using soft targets for the cross-entropy los s\nrather than hard targets. The correct class is given a tar-\nget probability of 1 − δ and the remaining δ probability\nmass is divided uniformly between the incorrect classes.\nThis technique is somewhat related to our work because\nsmaller logits will generally cause smoother output distri -\nbutions, but note that label smoothing would be satisﬁed\nto have very large logits so long as the probabilities af-\nter normalization are smooth.\nWarde-Farley & Goodfellow\n(2016) showed that label smoothing offers a small amount\nof robustness to adversarial examples, and it is included by\ndefault in the CleverHans tutorial on adversarial examples\n(\nNicolas Papernot, 2017).\nMixup (Zhang et al., 2017) trains the model on input points\nthat are interpolated between training examples. At these\ninterpolated input points, the output target is formed by sim-\nilarly interpolating between the target distributions for each\nof the training examples.\nZhang et al. (2017) reports that\nmixup increases robustness to adversarial examples.\nWe present our results comparing adversarial logit pair-\ning to label smoothing and mixup in Table 9. Here, we\nuse ResNet-101 on ImageNet, and all evaluations are with\nPGD. We ﬁnd that adversarial logit pairing provides a much\nstronger defense than either of these two approaches.\nMethod T op 1 T op 5\nMixup 0.1% 1.5%\nLabel smoothing 1.6% 10.0%\nALP 30.2% 55.8%\nTable 9. White box accuracies under Madry et al. (2017) attack on\nImageNet for label smoothing, mixup, and adversarial logit pair-\ning.\nBesides these related methods of defense against ad-\nversarial examples, ALP is also similar to a method\nof semi-supervised learning : virtual adversarial training\nAdversarial Logit Pairing\n(Miyato et al. , 2017). Virtual adversarial training (V A T) is\na method designed to learn from unlabeled data by training\nthe model to resist adversarial perturbations of unlabeled\ndata. The goal of V A T is to reduce test error when training\nwith a small set of labeled examples, not to cause robust-\nness to adversarial examples. V A T consists of:\n1. Construct adversarial examples by perturbing unla-\nbeled examples\n2. Speciﬁcally, make the adversarial examples by maxi-\nmizing the KL divergence between the predictions on\nthe clean examples and the predictions on the adver-\nsarial examples.\n3. During model training, add a loss term that minimizes\nKL divergence between predictions on clean and ad-\nversarial examples.\nALP does not include (1) or (2) but does resemble (3). Both\nALP and V A T encourage the full distribution of predictions\non clean and adversarial examples to be similar. V A T does\nso using a non-symmetric loss applied to the output prob-\nabilities; ALP does so using a symmetric loss applied to\nthe logits. During the design of our defense, we found\nthat V A T offered an improvement over the baseline Madry\nmodel on MNIST, but ALP consistently performed better\nthan V A T on MNIST across several hyperparameter values.\nALP also performed better than V A T with the direction of\nthe KL ﬂipped. We therefore focused on further developing\nALP . The better performance of ALP than V A T may be due\nto the fact that the KL divergence can suffer from saturat-\ning gradients or it may be due to the fact that the KL diver-\ngence is invariant to a shift of all the logits for an individu al\nexample while the logit pairing loss is not. Logit pairing en -\ncourages the logits for the clean and adversarial example to\nbe centered on the same mean logit value, which doesn’t\nchange the information in the output probabilities but may\naffect the learning dynamics.\n7. Conclusion and Future Work\nIn conclusion, we implement adversarial training at un-\nprecendented scale and present logit pairing as a defense.\nThe experiments in this paper were run on NVIDIA p100s,\nbut with the recent availability of much more powerful\nhardware (NVIDIA v100s, Cloud TPUs, etc.), we believe\nthat defenses for adversarial examples on ImageNet will\nbecome even more scalable. Speciﬁcally our contributions\nare:\n• We answer the open question as to whether adversarial\ntraining scales to ImageNet.\n• We introduce adversarial logit pairing (ALP), an ex-\ntension to adversarial training that greatly increases\nits effectiveness.\n• We introduce clean logit pairing and logit squeezing,\nlow-cost alternatives to adversarial training that can\nincrease the adoption of robust machine learning due\nto their requirement of very few resources.\n• We demonstrate that ALP-trained models can gener-\nate attacks strong enough to signiﬁcantly damage the\npreviously state of the art Ensemble Adversarial Train-\ning defense, which was used by all 10 of the top de-\nfense teams in the NIPS 2017 competition on adver-\nsarial examples.\n• We show that adversarial logit pairing achieves the\nstate of the art defense for white box and black box\nattacks on ImageNet.\nOur results suggest that feature pairing (matching adversa r-\nial and clean intermediate features instead of logits) may\nalso prove useful in the future.\nOne limitation to our defenses is that they are not currently\ncertiﬁed or veriﬁed (there is no proof that the true robust-\nness of the system is similar to the robustness that we mea-\nsured empirically). Research into certiﬁcation and veri-\nﬁcation methods (\nAditi Raghunathan, 2018; Aman Sinha,\n2018; Katz et al., 2017; Kolter & Wong, 2017) could make\nit possible to certify or verify these same networks in futur e\nwork. Current certiﬁcation methods do not scale to the size\nof models we trained here or are only able to provide tight\ncertiﬁcation bounds for models that were trained to be easy\nto certify using a speciﬁc certiﬁcation method.\nWe would like to note that these defense mechanisms are\nnot yet sufﬁcient to secure machine learning in a real sys-\ntem (see many of the concerns raised by (\nBrown et al.,\n2017) and Gilmer et al. (2018)), and that attacks could be\ndeveloped against our work in the future. Here, we use\nALP in conjunction with the PGD attack since it is the\nstrongest attack presented so far, but since ALP is indepen-\ndent of the actual attack it is used with, it is conceivable\nthat ALP could be used in conjunction with future attacks\nto develop stronger defenses. In conclusion, we present\nour defense as the current state of the art of research into\ndefenses, and we believe it will serve as one step along the\npath to a complete defense in the future.\nAcknowledgements\nWe thank Tom Brown for helpful feedback on drafts of this\narticle.\nAdversarial Logit Pairing\nReferences\nAditi Raghunathan, Jacob Steinhardt, Percy Liang. Certi-\nﬁed defenses against adversarial examples. International\nConference on Learning Representations , 2018. URL\nhttps://openreview.net/forum?id=Bys4ob-Rb.\nAman Sinha, Hongseok Namkoong, John Duchi.\nCertiﬁable distributional robustness with princi-\npled adversarial training. International Confer-\nence on Learning Representations , 2018. URL\nhttps://openreview.net/forum?id=Hk6kPgZA-.\nAthalye, Anish, Carlini, Nicholas, and Wagner, David.\nObfuscated gradients give a false sense of secu-\nrity: Circumventing defenses to adversarial exam-\nples. Technical report, arXiv, 2018. URL\nhttps://arxiv.org/abs/1802.00420.\nBrown, Tom B, Man´ e, Dandelion, Roy, Aurko, Abadi,\nMart´ ın, and Gilmer, Justin. Adversarial patch. arXiv\npreprint arXiv:1712.09665, 2017.\nBuckman, Jacob, Roy, Aurko, Raffel, Colin, and\nGoodfellow, Ian. Thermometer encoding: One hot\nway to resist adversarial examples. ICLR, 2018. URL\nhttps://openreview.net/forum?id=S18Su--CW .\nCubuk, E. D., Zoph, B., Schoenholz, S., and Le,\nQ. Intriguing properties of adversarial exam-\nples. Technical report, arXiv, 2017. URL\nhttps://arxiv.org/pdf/1711.02846.pdf.\nGilmer, Justin, Metz, Luke, Faghri, Fartash, Schoen-\nholz, Samuel S., Raghu, Maithra, Wattenberg,\nMartin, and Goodfellow, Ian. Adversarial\nspheres. In ICLR 2018 workshop , 2018. URL\nhttps://arxiv.org/abs/1801.02774.\nGomez, Aidan, Ren, Mengye, Urtasun, Raquel, and Grosse,\nRoger. The reversible residual network: Backpropaga-\ntion without storing activations. In NIPS 2017 , 2017.\nURL\nhttps://arxiv.org/abs/1707.04585.\nGoodfellow, Ian J, Shlens, Jonathon, and Szegedy, Chris-\ntian. Explaining and harnessing adversarial examples.\narXiv preprint arXiv:1412.6572 , 2014.\nHinton, Geoffrey, Vinyals, Oriol, and Dean, Jeff. Distill-\ning the knowledge in a neural network. arXiv preprint\narXiv:1503.02531, 2015.\nKatz, Guy, Barrett, Clark, Dill, David L, Julian, Kyle, and\nKochenderfer, Mykel J. Reluplex: An efﬁcient smt\nsolver for verifying deep neural networks. In Interna-\ntional Conference on Computer Aided V eriﬁcation , pp.\n97–117. Springer, 2017.\nKolter, J Zico and Wong, Eric. Provable defenses against\nadversarial examples via the convex outer adversarial\npolytope. arXiv preprint arXiv:1711.00851 , 2017.\nKurakin, Alexey, Goodfellow, Ian, and Bengio, Samy. Ad-\nversarial machine learning at scale. In ICLR 2017, 2017a.\nURL\nhttps://arxiv.org/abs/1611.01236.\nKurakin, Alexey, Goodfellow, Ian, and Bengio,\nSamy. Adversarial examples in the physical\nworld. In ICLR’2017 W orkshop , 2017b. URL\nhttps://arxiv.org/abs/1607.02533.\nMadry, A., Makelov, A., Schmidt, L., Tsipras, D., and\nVladu, A. Towards deep learning models resistant to ad-\nversarial attacks. Technical report, arXiv, 2017. URL\nhttps://arxiv.org/pdf/1706.06083.pdf.\nMiyato, Takeru, Maeda, Shin-ichi, Koyama, Masanori, and\nIshii, Shin. Virtual adversarial training: a regulariza-\ntion method for supervised and semi-supervised learning.\narXiv preprint arXiv:1704.03976 , 2017.\nNicolas Papernot, Nicholas Carlini, Ian Goodfellow\nReuben Feinman Fartash Faghri Alexander Matyasko\nKaren Hambardzumyan Yi-Lin Juang Alexey Kurakin\nRyan Sheatsley Abhibhav Garg Y en-Chen Lin. clev-\nerhans v2.0.0: an adversarial machine learning library.\narXiv preprint arXiv:1610.00768 , 2017.\nPapernot, Nicolas, McDaniel, Patrick, Wu, Xi, Jha,\nSomesh, and Swami, Ananthram. Distillation as a de-\nfense to adversarial perturbations against deep neural net-\nworks. In Security and Privacy (SP), 2016 IEEE Sympo-\nsium on, pp. 582–597. IEEE, 2016.\nSietsma, J. and Dow, R. Creating artiﬁcial neural networks\nthat generalize. Neural Networks, 4(1):67–79, 1991.\nSzegedy, Christian, Zaremba, Wojciech, Sutskever, Ilya,\nBruna, Joan, Erhan, Dumitru, Goodfellow, Ian, and Fer-\ngus, Rob. Intriguing properties of neural networks. arXiv\npreprint arXiv:1312.6199, 2013.\nSzegedy, Christian, V anhoucke, Vincent, Ioffe, Sergey,\nShlens, Jon, and Wojna, Zbigniew. Rethinking the in-\nception architecture for computer vision. In Proceedings\nof the IEEE Conference on Computer Vision and Pattern\nRecognition, pp. 2818–2826, 2016.\nTram` er, F., Kurakin, A., Papernot, N., Boneh, D., and\nMcDaniel, P . Ensemble adversarial training: At-\ntacks and defenses. In ICLR 2018 , 2018. URL\nhttps://arxiv.org/abs/1705.07204.\nWarde-Farley, David and Goodfellow, Ian. Adversarial per-\nturbation of deep neural networks. In Hazan, Tamir, Pa-\npandreou, George, and Tarlow, Daniel (eds.), Perturba-\ntion, Optimization, and Statistics . MIT Press, 2016.\nAdversarial Logit Pairing\nXu, Weilin, Evans, David, and Qi, Y anjun. Feature squeez-\ning: Detecting adversarial examples in deep neural net-\nworks. arXiv preprint arXiv:1704.01155 , 2017.\nZhang, H., Cisse, M., Dauphin, Y ., and Lopez-\nPaz, D. Mixup: Beyond empirical risk mini-\nmization. Technical report, arXiv, 2017. URL\nhttps://arxiv.org/pdf/1710.09412.pdf.",
  "values": {
    "Privacy": "No",
    "Collective influence": "No",
    "User influence": "No",
    "Fairness": "No",
    "Justice": "No",
    "Non-maleficence": "No",
    "Beneficence": "No",
    "Explicability": "No",
    "Critiqability": "No",
    "Deferral to humans": "No",
    "Autonomy (power to decide)": "No",
    "Respect for Persons": "No",
    "Not socially biased": "No",
    "Respect for Law and public interest": "No",
    "Interpretable (to users)": "No",
    "Transparent (to users)": "No"
  }
}