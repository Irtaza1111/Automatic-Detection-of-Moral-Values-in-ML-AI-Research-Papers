{
  "pdf": "1808.06601v2",
  "title": "1808.06601v2",
  "author": "Unknown",
  "paper_id": "1808.06601v2",
  "text": "Video-to-Video Synthesis\nTing-Chun Wang1, Ming-Yu Liu1, Jun-Yan Zhu2, Guilin Liu 1,\nAndrew Tao1, Jan Kautz 1, Bryan Catanzaro 1\n1NVIDIA, 2MIT CSAIL\n{tingchunw,mingyul,guilinl,atao,jkautz,bcatanzaro}@nvidia.com,\njunyanz@mit.edu\nAbstract\nWe study the problem of video-to-video synthesis, whose goal is to learn a mapping\nfunction from an input source video (e.g., a sequence of semantic segmentation\nmasks) to an output photorealistic video that precisely depicts the content of the\nsource video. While its image counterpart, the image-to-image translation problem,\nis a popular topic, the video-to-video synthesis problem is less explored in the\nliterature. Without modeling temporal dynamics, directly applying existing image\nsynthesis approaches to an input video often results in temporally incoherent videos\nof low visual quality. In this paper, we propose a video-to-video synthesis approach\nunder the generative adversarial learning framework. Through carefully-designed\ngenerators and discriminators, coupled with a spatio-temporal adversarial objective,\nwe achieve high-resolution, photorealistic, temporally coherent video results on\na diverse set of input formats including segmentation masks, sketches, and poses.\nExperiments on multiple benchmarks show the advantage of our method compared\nto strong baselines. In particular, our model is capable of synthesizing 2K resolution\nvideos of street scenes up to 30 seconds long, which signiﬁcantly advances the\nstate-of-the-art of video synthesis. Finally, we apply our method to future video\nprediction, outperforming several competing systems. Code, models, and more\nresults are available at our website.\n1 Introduction\nThe capability to model and recreate the dynamics of our visual world is essential to building\nintelligent agents. Apart from purely scientiﬁc interests, learning to synthesize continuous visual\nexperiences has a wide range of applications in computer vision, robotics, and computer graphics.\nFor example, in model-based reinforcement learning [2, 24], a video synthesis model ﬁnds use in\napproximating visual dynamics of the world for training the agent with less amount of real experience\ndata. Using a learned video synthesis model, one can generate realistic videos without explicitly\nspecifying scene geometry, materials, lighting, and dynamics, which would be cumbersome but\nnecessary when using a standard graphics rendering engine [35].\nThe video synthesis problem exists in various forms, including future video prediction [15, 18, 42, 45,\n50, 65, 68, 71, 77] and unconditional video synthesis [59, 67, 69]. In this paper, we study a new form:\nvideo-to-video synthesis. At the core, we aim to learn a mapping function that can convert an input\nvideo to an output video. To the best of our knowledge, a general-purpose solution to video-to-video\nsynthesis has not yet been explored by prior work, although its image counterpart, the image-to-image\ntranslation problem, is a popular research topic [6, 31, 33, 43, 44, 63, 66, 73, 82, 83]. Our method is\ninspired by previous application-speciﬁc video synthesis methods [58, 60, 61, 75].\nWe cast the video-to-video synthesis problem as a distribution matching problem, where the goal is\nto train a model such that the conditional distribution of the synthesized videos given input videos\nresembles that of real videos. To this end, we learn a conditional generative adversarial model [20]\nPreprint. Work in progress.\narXiv:1808.06601v2  [cs.CV]  3 Dec 2018\nFigure 1: Generating a photorealistic video from an input segmentation map video on Cityscapes.\nTop left: input. Top right: pix2pixHD. Bottom left: COVST. Bottom right: vid2vid (ours). Click the\nimage to play the video clip in a browser.\ngiven paired input and output videos, With carefully-designed generators and discriminators, and a\nnew spatio-temporal learning objective, our method can learn to synthesize high-resolution, photore-\nalistic, temporally coherent videos. Moreover, we extend our method to multimodal video synthesis.\nConditioning on the same input, our model can produce videos with diverse appearances.\nWe conduct extensive experiments on several datasets on the task of converting a sequence of\nsegmentation masks to photorealistic videos. Both quantitative and qualitative results indicate that\nour synthesized footage looks more photorealistic than those from strong baselines. See Figure 1\nfor example. We further demonstrate that the proposed approach can generate photorealistic 2K\nresolution videos, up to 30 seconds long. Our method also grants users ﬂexible high-level control\nover the video generation results. For example, a user can easily replace all the buildings with trees in\na street view video. In addition, our method works for other input video formats such as face sketches\nand body poses, enabling many applications from face swapping to human motion transfer. Finally,\nwe extend our approach to future prediction and show that our method can outperform existing\nsystems. Please visit our website for code, models, and more results.\n2 Related Work\nGenerative Adversarial Networks (GANs). We build our model on GANs [20]. During GAN\ntraining, a generator and a discriminator play a zero-sum game. The generator aims to produce\nrealistic synthetic data so that the discriminator cannot differentiate between real and the synthesized\ndata. In addition to noise distributions [14, 20, 55], various forms of data can be used as input to the\ngenerator, including images [33, 43, 82], categorical labels [52, 53], and textual descriptions [56, 80].\nSuch conditional models are called conditional GANs, and allow ﬂexible control over the output of\nthe model. Our method belongs to the category of conditional video generation with GANs. However,\ninstead of predicting future videos conditioning on the current observed frames [41, 50, 69], our\nmethod synthesizes photorealistic videos conditioning on manipulable semantic representations, such\nas segmentation masks, sketches, and poses.\nImage-to-image translation algorithms transfer an input image from one domain to a corresponding\nimage in another domain. There exists a large body of work for this problem [6, 31, 33, 43, 44, 63, 66,\n73, 82, 83]. Our approach is their video counterpart. In addition to ensuring that each video frame\nlooks photorealistic, a video synthesis model also has to produce temporally coherent frames, which\nis a challenging task, especially for a long duration video.\nUnconditional video synthesis. Recent work [59, 67, 69] extends the GAN framework for un-\nconditional video synthesis, which learns a generator for converting a random vector to a video.\n2\nVGAN [69] uses a spatio-temporal convolutional network. TGAN [59] projects a latent code to a\nset of latent image codes and uses an image generator to convert those latent image codes to frames.\nMoCoGAN [67] disentangles the latent space to motion and content subspaces and uses a recurrent\nneural network to generate a sequence of motion codes. Due to the unconditional setting, these\nmethods often produce low-resolution and short-length videos.\nFuture video prediction. Conditioning on the observed frames, video prediction models are trained\nto predict future frames [15, 18, 36, 41, 42, 45, 50, 65, 68, 71, 72, 77]. Many of these models are trained\nwith image reconstruction losses, often producing blurry videos due to the classic regress-to-the-mean\nproblem. Also, they fail to generate long duration videos even with adversarial training [42, 50]. The\nvideo-to-video synthesis problem is substantially different because it does not attempt to predict\nobject motions or camera motions. Instead, our approach is conditional on an existing video and can\nproduce high-resolution and long-length videos in a different domain.\nVideo-to-video synthesis. While video super-resolution [61, 62], video matting and blending [3, 12],\nand video inpainting [74] can be considered as special cases of the video-to-video synthesis problem,\nexisting approaches rely on problem-speciﬁc constraints and designs. Hence, these methods cannot\nbe easily applied to other applications. Video style transfer [10, 22, 28, 58], transferring the style of a\nreference painting to a natural scene video, is also related. In Section 4 , we show that our method\noutperforms a strong baseline that combines a recent video style transfer with a state-of-the-art\nimage-to-image translation approach.\n3 Video-to-Video Synthesis\nLet sT\n1 ≡{ s1, s2, ..., sT} be a sequence of source video frames. For example, it can be a sequence\nof semantic segmentation masks or edge maps. Let xT\n1 ≡{ x1, x2, ..., xT} be the sequence of\ncorresponding real video frames. The goal of video-to-video synthesis is to learn a mapping function\nthat can convert sT\n1 to a sequence of output video frames, ˜xT\n1 ≡ {˜x1, ˜x2, ..., ˜xT}, so that the\nconditional distribution of ˜xT\n1 given sT\n1 is identical to the conditional distribution of xT\n1 given sT\n1 .\np(˜xT\n1|sT\n1 ) = p(xT\n1|sT\n1 ). (1)\nThrough matching the conditional video distributions, the model learns to generate photorealistic,\ntemporally coherent output sequences as if they were captured by a video camera.\nWe propose a conditional GAN framework for this conditional video distribution matching task. Let\nG be a generator that maps an input source sequence to a corresponding output frame sequence:\nxT\n1 = G(sT\n1 ). We train the generator by solving the minimax optimization problem given by\nmax\nD\nmin\nG\nE(xT\n1,sT\n1 )[log D(xT\n1 , sT\n1 )] + EsT\n1\n[log(1− D(G(sT\n1 ), sT\n1 ))], (2)\nwhere D is the discriminator. We note that as solving (2), we minimize the Jensen-Shannon divergence\nbetween p(˜xT\n1|sT\n1 ) and p(xT\n1|sT\n1 ) as shown by Goodfellow et al. [20].\nSolving the minimax optimization problem in (2) is a well-known, challenging task. Careful designs\nof network architectures and objective functions are essential to achieve good performance as shown\nin the literature [14, 21, 30, 37, 49, 51, 55, 73, 80]. We follow the same spirit and propose new network\ndesigns and a spatio-temporal objective for video-to-video synthesis as detailed below.\nSequential generator . To simplify the video-to-video synthesis problem, we make a Markov\nassumption where we factorize the conditional distribution p(˜xT\n1|sT\n1 ) to a product form given by\np(˜xT\n1|sT\n1 ) =\nT∏\nt=1\np(˜xt|˜xt−1\nt−L, st\nt−L). (3)\nIn other words, we assume the video frames can be generated sequentially, and the generation of the\nt-th frame ˜xt only depends on three factors: 1) current source frame st, 2) past L source frames st−1\nt−L,\nand 3) past L generated frames ˜xt−1\nt−L. We train a feed-forward network F to model the conditional\ndistribution p(˜xt|˜xt−1\nt−L, st\nt−L) using ˜xt = F (˜xt−1\nt−L, st\nt−L). We obtain the ﬁnal output˜xT\n1 by applying\nthe function F in a recursive manner. We found that a smallL (e.g., L = 1) causes training instability,\nwhile a large L increases training time and GPU memory but with minimal quality improvement. In\nour experiments, we set L = 2.\n3\nVideo signals contain a large amount of redundant information in consecutive frames. If the optical\nﬂow [46] between consecutive frames is known, we can estimate the next frame by warping the\ncurrent frame [54, 70]. This estimation would be largely correct except for the occluded areas. Based\non this observation, we model F as\nF (˜xt−1\nt−L, st\nt−L) = (1− ˜mt)⊙ ˜wt−1(˜xt−1) + ˜mt⊙ ˜ht, (4)\nwhere⊙ is the element-wise product operator and 1 is an image of all ones. The ﬁrst part corresponds\nto pixels warped from the previous frame, while the second part hallucinates new pixels. The\ndeﬁnitions of the other terms in Equation 4 are given below.\n• ˜wt−1 = W (˜xt−1\nt−L, st\nt−L) is the estimated optical ﬂow from ˜xt−1 to ˜xt, and W is the optical\nﬂow prediction network. We estimate the optical ﬂow using both input source images st\nt−L and\npreviously synthesized images ˜xt−1\nt−L. By ˜wt−1(˜xt−1), we warp ˜xt−1 based on ˜wt−1.\n• ˜ht = H(˜xt−1\nt−L, st\nt−L) is the hallucinated image, synthesized directly by the generator H.\n• ˜mt = M(˜xt−1\nt−L, st\nt−L) is the occlusion mask with continuous values between 0 and 1. M denotes\nthe mask prediction network. Our occlusion mask is soft instead of binary to better handle the\n“zoom in” scenario. For example, when an object is moving closer to our camera, the object will\nbecome blurrier over time if we only warp previous frames. To increase the resolution of the\nobject, we need to synthesize new texture details. By using a soft mask, we can add details by\ngradually blending the warped pixels and the newly synthesized pixels.\nWe use residual networks [26] for M, W , and H. To generate high-resolution videos, we adopt a\ncoarse-to-ﬁne generator design similar to the method of Wang et. al [73].\nAs using multiple discriminators can mitigate the mode collapse problem during GANs training [19,\n67, 73], we also design two types of discriminators as detailed below.\nConditional image discriminator DI. The purpose of DI is to ensure that each output frame\nresembles a real image given the same source image. This conditional discriminator should output 1\nfor a true pair (xt, st) and 0 for a fake one (˜xt, st).\nConditional video discriminator DV . The purpose of DV is to ensure that consecutive output\nframes resemble the temporal dynamics of a real video given the same optical ﬂow. While DI\nconditions on the source image, DV conditions on the ﬂow. Let wt−2\nt−K be K− 1 optical ﬂow for the\nK consecutive real images xt−1\nt−K. This conditional discriminator DV should output 1 for a true pair\n(xt−1\nt−K, wt−2\nt−K) and 0 for a fake one (˜xt−1\nt−K, wt−2\nt−K).\nWe introduce two sampling operators to facilitate the discussion. First, let φI be a random image\nsampling operator such that φI(xT\n1 , sT\n1 ) = (xi, si) where i is an integer uniformly sampled from 1 to\nT . In other words, φI randomly samples a pair of images from (xT\n1 , sT\n1 ). Second, we deﬁne φV as a\nsampling operator that randomly retrieve K consecutive frames. Speciﬁcally,φV (wT −1\n1 , xT\n1 , sT\n1 ) =\n(wi−2\ni−K, xi−1\ni−K, si−1\ni−K) where i is an integer uniformly sampled from K + 1 to T + 1. This operator\nretrieves K consecutive frames and the corresponding K− 1 optical ﬂow images. With φI and φV ,\nwe are ready to present our learning objective function.\nLearning objective function. We train the sequential video synthesis function F by solving\nmin\nF\n(\nmax\nDI\nLI(F, DI) + max\nDV\nLV (F, DV )\n)\n+ λWLW (F ), (5)\nwhereLI is the GAN loss on images deﬁned by the conditional image discriminatorDI,LV is the\nGAN loss on K consecutive frames deﬁned byDV , andLW is the ﬂow estimation loss. The weight\nλW is set to 10 throughout the experiments based on a grid search. In addition to the loss terms\nin Equation 5, we use the discriminator feature matching loss [40, 73] and VGG feature matching\nloss [16, 34, 73] as they improve the convergence speed and training stability [73]. Please see the\nappendix for more details.\nWe further deﬁne the image-conditional GAN lossLI [33] using the operator φI\nEφI (xT\n1,sT\n1 )[log DI(xi, si)] + EφI (˜xT\n1,sT\n1 )[log(1− DI(˜xi, si))]. (6)\nSimilarly, the video GAN lossLV is given by\nEφV (wT −1\n1 ,xT\n1,sT\n1 )[log DV (xi−1\ni−K, wi−2\ni−K)] + EφV (wT −1\n1 ,˜xT\n1,sT\n1 )[log(1− DV (˜xi−1\ni−K, wi−2\ni−K))]. (7)\n4\nRecall that we synthesize a video ˜xT\n1 by recursively applying F .\nThe ﬂow lossLW includes two terms. The ﬁrst is the endpoint error between the ground truth and\nthe estimated ﬂow, and the second is the warping loss when the ﬂow warps the previous frame to the\nnext frame. Let wt be the ground truth ﬂow from xt to xt+1. The ﬂow loss LW is given by\nLW = 1\nT− 1\nT −1∑\nt=1\n(\n|| ˜wt− wt∥1 +∥ ˜wt(xt)− xt+1∥1\n)\n. (8)\nForeground-background prior .When using semantic segmentation masks as the source video, we\ncan divide an image into foreground and background areas based on the semantics. For example,\nbuildings and roads belong to the background, while cars and pedestrians are considered as the\nforeground. We leverage this strong foreground-background prior in the generator design to further\nimprove the synthesis performance of the proposed model.\nIn particular, we decompose the image hallucination network H into a foreground model ˜hF,t =\nHF (st\nt−L) and a background model ˜hB,t = HB(˜xt−1\nt−L, st\nt−L). We note that background motion\ncan be modeled as a global transformation in general, where optical ﬂow can be estimated quite\naccurately. As a result, the background region can be generated accurately via warping, and the\nbackground hallucination network HB only needs to synthesize the occluded areas. On the other hand,\na foreground object often has a large motion and only occupies a small portion of the image, which\nmakes optical ﬂow estimation difﬁcult. The network HF has to synthesize most of the foreground\ncontent from scratch. With this foreground–background prior, F is then given by\nF (˜xt−1\nt−L, st\nt−L) = (1− ˜mt)⊙ ˜wt−1(˜xt−1) + ˜mt⊙\n(\n(1− mB,t)⊙ ˜hF,t + mB,t⊙ ˜hB,t\n)\n, (9)\nwhere mB,t is the background mask derived from the ground truth segmentation mask st. This prior\nimproves the visual quality by a large margin with the cost of minor ﬂickering artifacts. In Table 2,\nour user study shows that most people prefer the results with foreground–background modeling.\nMultimodal synthesis. The synthesis network F is a unimodal mapping function. Given an input\nsource video, it can only generate one output video. To achieve multimodal synthesis [19, 73, 83], we\nadopt a feature embedding scheme [73] for the source video that consists of instance-level semantic\nsegmentation masks. Speciﬁcally, at training time, we train an image encoderE to encode the ground\ntruth real image xt into a d-dimensional feature map ( d = 3 in our experiments). We then apply\nan instance-wise average pooling to the map so that all the pixels within the same object share the\nsame feature vectors. We then feed both the instance-wise averaged feature map zt and the input\nsemantic segmentation mask st to the generator F . Once training is done, we ﬁt a mixture of Gaussian\ndistribution to the feature vectors that belong to the same object class. At test time, we sample a\nfeature vector for each object instance using the estimated distribution of that object class. Given\ndifferent feature vectors, the generator F can synthesize videos with different visual appearances.\n4 Experiments\nImplementation details. We train our network in a spatio-temporally progressive manner. In\nparticular, we start with generating low-resolution videos with few frames, and all the way up to\ngenerating full resolution videos with 30 (or more) frames. Our coarse-to-ﬁne generator consists of\nthree scales: 512× 256, 1024× 512, and 2048× 1024 resolutions, respectively. The mask prediction\nnetwork M and ﬂow prediction network W share all the weights except for the output layer. We\nuse the multi-scale PatchGAN discriminator architecture [33, 73] for the image discriminator DI. In\naddition to multi-scale in the spatial resolution, our multi-scale video discriminator DV also looks\nat different frame rates of the video to ensure both short-term and long-term consistency. See the\nappendix for more details.\nWe train our model for 40 epochs using the ADAM optimizer [39] with lr = 0.0002 and (β1, β2) =\n(0.5, 0.999) on an NVIDIA DGX1 machine. We use the LSGAN loss [49]. Due to the high image\nresolution, even with one short video per batch, we have to use all the GPUs in DGX1 (8 V100 GPUs,\neach with 16GB memory) for training. We distribute the generator computation task to 4 GPUs and\nthe discriminator task to the other 4 GPUs. Training takes∼ 10 days for 2K resolution.\nDatasets. We evaluate the proposed approach on several datasets.\n5\nTable 1: Comparison between competing video-to-video synthesis approaches on Cityscapes.\nFréchet Inception Dist. I3D ResNeXt\npix2pixHD 5.57 0.18\nCOVST 5.55 0.18\nvid2vid (ours) 4.66 0.15\nHuman Preference Score short seq. long seq.\nvid2vid (ours) / pix2pixHD 0.87 / 0.13 0.83 / 0.17\nvid2vid (ours) / COVST 0.84 / 0.16 0.80 / 0.20\nTable 2: Ablation study. We compare the proposed approach to its three variants.\nHuman Preference Score\nvid2vid (ours) / no background–foreground prior 0.80 / 0.20\nvid2vid (ours) / no conditional video discriminator 0.84 / 0.16\nvid2vid (ours) / no flow warping 0.67 / 0.33\nTable 3: Comparison between future video prediction methods on Cityscapes.\nFréchet Inception Dist. I3D ResNeXt\nPredNet 11.18 0.59\nMCNet 10.00 0.43\nvid2vid (ours) 3.44 0.18\nHuman Preference Score\nvid2vid (ours) / PredNet 0.92 / 0.08\nvid2vid (ours) / MCNet 0.98 / 0.02\n• Cityscapes [13]. The dataset consists of 2048× 1024 street scene videos captured in several\nGerman cities. Only a subset of images in the videos contains ground truth semantic segmentation\nmasks. To obtain the input source videos, we use those images to train a DeepLabV3 semantic\nsegmentation network [11] and apply the trained network to segment all the videos. We use\nthe optical ﬂow extracted by FlowNet2 [32] as the ground truth ﬂow w. We treat the instance\nsegmentation masks computed by the Mask R-CNN [25] as our instance-level ground truth. In\nsummary, the training set contains 2975 videos, each with 30 frames. The validation set consists\nof 500 videos, each with 30 frames. Finally, we test our method on three long sequences from\nthe Cityscapes demo videos, with 600, 1100, and 1200 frames, respectively. We will show that\nalthough trained on short videos, our model can synthesize long videos.\n• Apolloscape [29] consists of 73 street scene videos captured in Beijing, whose video lengths vary\nfrom 100 to 1000 frames. Similar to Cityscapes, Apolloscape is constructed for the image/video\nsemantic segmentation task. But we use it for synthesizing videos using the semantic segmentation\nmask. We split the dataset into half for training and validation.\n• Face video dataset [57]. We use the real videos in the FaceForensics dataset, which contains\n854 videos of news brieﬁng from different reporters. We use this dataset for the sketch video to\nface video synthesis task. To extract a sequence of sketches from a video, we ﬁrst apply a face\nalignment algorithm [38] to localize facial landmarks in each frame. The facial landmarks are\nthen connected to create the face sketch. For background, we extract Canny edges outside the\nface regions. We split the dataset into 704 videos for training and 150 videos for validation.\n• Dance video dataset. We download YouTube dance videos for the pose to human motion\nsynthesis task. Each video is about 3∼ 4 minutes long at 1280× 720 resolution, and we crop the\ncentral 512× 720 regions. We extract human poses with DensePose [23] and OpenPose [7], and\ndirectly concatenate the results together. Each training set includes a dance video from a single\ndancer, while the test set contains videos of other dance motions or from other dancers.\nBaselines. We compare our approach to two baselines trained on the same data.\n• pix2pixHD [73] is the state-of-the-art image-to-image translation approach. When applying the\napproach to the video-to-video synthesis task, we process input videos frame-by-frame.\n• COVST is built on the coherent video style transfer [10] by replacing the stylization network with\npix2pixHD. The key idea in COVST is to warp high-level deep features using optical ﬂow for\nachieving temporally coherent outputs. No additional adversarial training is applied. We feed in\nground truth optical ﬂow to COVST, which is impractical for real applications. In contrast, our\nmodel estimates optical ﬂow from source videos.\nEvaluation metrics. We use both subjective and objective metrics for evaluation.\n• Human preference score. We perform a human subjective test for evaluating the visual quality\nof synthesized videos. We use the Amazon Mechanical Turk (AMT) platform. During each\n6\nFigure 2: Apolloscape results. Left: pix2pixHD. Center: COVST. Right: proposed. The input\nsemantic segmentation mask video is shown in the left video. Click the image to play the video clip\nin a browser.\nFigure 3: Example multi-modal video synthesis results. These synthesized videos contain different\nroad surfaces. Click the image to play the video clip in a browser.\nFigure 4: Example results of changing input semantic segmentation masks to generate diverse videos.\nLeft: tree→building. Right: building→tree. The original video is shown in Figure 3. Click the image\nto play the video clip in a browser.\ntest, an AMT participant is ﬁrst shown two videos at a time (results synthesized by two different\nalgorithms) and then asked which one looks more like a video captured by a real camera. We\nspeciﬁcally ask the worker to check for both temporal coherence and image quality. A worker\nmust have a life-time task approval rate greater than 98% to participate in the evaluation. For each\nquestion, we gather answers from 10 different workers. We evaluate the algorithm by the ratio\nthat the algorithm outputs are preferred.\n• Fréchet Inception Distance (FID) [27] is a widely used metric for implicit generative models, as\nit correlates well with the visual quality of generated samples. The FID was originally developed\nfor evaluating image generation. We propose a variant for video evaluation, which measures both\nvisual quality and temporal consistency. Speciﬁcally, we use a pre-trained video recognition CNN\nas a feature extractor after removing the last few layers from the network. This feature extractor\nwill be our “inception” network. For each video, we extract a spatio-temporal feature map with\nthis CNN. We then compute the mean ˜µ and covariance matrix ˜Σ for the feature vectors from\nall the synthesized videos. We also calculate the same quantities µ and Σ for the ground truth\nvideos. The FID is then calculated as∥µ− ˜µ∥2 + Tr\n(\nΣ + ˜Σ− 2\n√\nΣ˜Σ\n)\n. We use two different\npre-trained video recognition CNNs in our evaluation: I3D [8] and ResNeXt [76].\nMain results. We compare the proposed approach to the baselines on the Cityscapes benchmark,\nwhere we apply the learned models to synthesize 500 short video clips in the validation set. As shown\nin Table 1, our results have a smaller FID and are often favored by the human subjects. We also\nreport the human preference scores on the three long test videos. Again, the videos rendered by our\napproach are considered more realistic by the human subjects. The human preference scores for the\nApolloscape dataset are given in the appendix.\n7\nFigure 5: Example face→sketch→face results. Each set shows the original video, the extracted edges,\nand our synthesized video. Click the image to play the video clip in a browser.\nFigure 6: Example dance→pose→dance results. Each set shows the original dancer, the extracted\nposes, and the synthesized video. Click the image to play the video clip in a browser.\nFigures 1 and 2 show the video synthesis results. Although each frame rendered bypix2pixHD is\nphotorealistic, the resulting video lacks temporal coherence. The road lane markings and building\nappearances are inconsistent across frames. While improving upon pix2pixHD,COVST still suffers\nfrom temporal inconsistency. On the contrary, our approach produces a high-resolution, photorealistic,\ntemporally consistent video output. We can generate 30-second long videos, showing that our\napproach synthesizes convincing videos with longer lengths.\nWe conduct an ablation study to analyze several design choices of our method. Speciﬁcally, we\ncreate three variants. In one variant, we do not use the foreground-background prior, which is termed\nno background–foreground prior. That is, instead of using Equation 9, we use Equation 4.\nThe second variant is no conditional video discriminator where we do not use the video\ndiscriminator DV for training. In the last variant, we remove the optical ﬂow prediction network\nW and the mask prediction network M from the generator F in Equation 4 and only use H for\nsynthesis. This variant is referred to as no flow warping. We use the human preference score on\nCityscapes for this ablation study. Table 2 shows that the visual quality of output videos degrades\nsigniﬁcantly without the ablated components. To evaluate the effectiveness of different components\nin our network, we also experimented with directly using ground truth ﬂows instead of estimated\nﬂows by our network. We found the results visually similar, which suggests that our network is robust\nto the errors in the estimated ﬂows.\nMultimodal results. Figure 3 shows example multimodal synthesis results. In this example, we\nkeep the sampled feature vectors of all the object instances in the video the same except for the road\ninstance. The ﬁgure shows temporally smooth videos with different road appearances.\nSemantic manipulation. Our approach also allows the user to manipulate the semantics of source\nvideos. In Figure 4, we show an example of changing the semantic labels. In the left video, we\nreplace all trees with buildings in the original segmentation masks and synthesize a new video. On\nthe right, we show the result of replacing buildings with trees.\nSketch-to-video synthesis for face swapping. We train a sketch-to-face synthesis video model\nusing the real face videos in the FaceForensics dataset [57]. As shown in Figure 5, our model can\nconvert sequences of sketches to photorealistic output videos. This model can be used to change the\nfacial appearance of the original face videos [5].\nPose-to-video synthesis for human motion transfer . We also apply our method to the task of\nconverting sequences of human poses to photorealistic output videos. We note that the image\ncounterpart was studied in recent works [4, 17, 47, 48]. As shown in Figure 6, our model learns to\nsynthesize high-resolution photorealistic output dance videos that contain unseen body shapes and\nmotions. Our method can change the clothing [79, 81] for the same dancer (Figure 6 left) as well as\ntransfer the visual appearance to new dancers (Figure 6 right) as explored in concurrent work [1,9,78].\n8\nFigure 7: Future video prediction results. Top left: ground truth. Top right: PredNet [45]. Bottom\nleft: MCNet [68]. Bottom right: ours. Click the image to play the video clip in a browser.\nFuture video prediction. We show an extension of our approach to the future video prediction\ntask: learning to predict the future video given a few observed frames. We decompose the task\ninto two sub-tasks: 1) synthesizing future semantic segmentation masks using the observed frames,\nand 2) converting the synthesized segmentation masks into videos. In practice, after extracting the\nsegmentation masks from the observed frames, we train a generator to predict future semantic masks.\nWe then use the proposed video-to-video synthesis approach to convert the predicted segmentation\nmasks to a future video.\nWe conduct both quantitative and qualitative evaluations with comparisons to two start-of-the-art\napproaches: PredNet [45] and MCNet [68]. We follow the prior work [41, 70] and report the human\npreference score. We also include the FID scores. As shown in Table 3, our model produces smaller\nFIDs, and the human subjects favor our resulting videos. In Figure 7, we visualize the future video\nsynthesis results. While the image quality of the results from the competing algorithms degrades\nsigniﬁcantly over time, ours remains consistent.\n5 Discussion\nWe present a general video-to-video synthesis framework based on conditional GANs. Through\ncarefully-designed generators and discriminators as well as a spatio-temporal adversarial objective,\nwe can synthesize high-resolution, photorealistic, and temporally consistent videos. Extensive\nexperiments demonstrate that our results are signiﬁcantly better than the results by state-of-the-art\nmethods. Our method also compares favorably against the competing video prediction methods.\nAlthough our approach outperforms previous methods, our model still fails in a couple of situations.\nFor example, our model struggles in synthesizing turning cars due to insufﬁcient information in\nlabel maps. This could be potentially addressed by adding additional 3D cues, such as depth maps.\nFurthermore, our model still can not guarantee that an object has a consistent appearance across\nthe whole video. Occasionally, a car may change its color gradually. This issue might be alleviated\nif object tracking information is used to enforce that the same object shares the same appearance\nthroughout the entire video. Finally, when we perform semantic manipulations such as turning trees\ninto buildings, visible artifacts occasionally appear as building and trees have different label shapes.\nThis might be resolved if we train our model with coarser semantic labels, as the trained model would\nbe less sensitive to label shapes.\nAcknowledgements We thank Karan Sapra, Fitsum Reda, and Matthieu Le for generating the\nsegmentation maps for us. We also thank Lisa Rhee and Miss Ketsuki for allowing us to use their\ndance videos for training. We thank William S. Peebles for proofreading the paper.\n9\nReferences\n[1] K. Aberman, M. Shi, J. Liao, D. Lischinski, B. Chen, and D. Cohen-Or. Deep video-based performance\ncloning. arXiv preprint arXiv:1808.06847, 2018.\n[2] K. Arulkumaran, M. P. Deisenroth, M. Brundage, and A. A. Bharath. Deep reinforcement learning: A brief\nsurvey. IEEE Signal Processing Magazine, 34(6):26–38, 2017.\n[3] X. Bai, J. Wang, D. Simons, and G. Sapiro. Video snapcut: robust video object cutout using localized\nclassiﬁers. ACM Transactions on Graphics (TOG), 28(3):70, 2009.\n[4] G. Balakrishnan, A. Zhao, A. V . Dalca, F. Durand, and J. Guttag. Synthesizing images of humans in unseen\nposes. In IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2018.\n[5] D. Bitouk, N. Kumar, S. Dhillon, P. Belhumeur, and S. K. Nayar. Face swapping: automatically replacing\nfaces in photographs. In ACM SIGGRAPH, 2008.\n[6] K. Bousmalis, N. Silberman, D. Dohan, D. Erhan, and D. Krishnan. Unsupervised pixel-level domain\nadaptation with generative adversarial networks. In IEEE Conference on Computer Vision and Pattern\nRecognition (CVPR), 2017.\n[7] Z. Cao, T. Simon, S.-E. Wei, and Y . Sheikh. Realtime multi-person 2D pose estimation using part afﬁnity\nﬁelds. In IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2017.\n[8] J. Carreira and A. Zisserman. Quo vadis, action recognition? a new model and the kinetics dataset. In\nIEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2017.\n[9] C. Chan, S. Ginosar, T. Zhou, and A. A. Efros. Everybody dance now. In European Conference on\nComputer Vision (ECCV) Workshop, 2018.\n[10] D. Chen, J. Liao, L. Yuan, N. Yu, and G. Hua. Coherent online video style transfer. In IEEE International\nConference on Computer Vision (ICCV), 2017.\n[11] L.-C. Chen, G. Papandreou, F. Schroff, and H. Adam. Rethinking atrous convolution for semantic image\nsegmentation. arXiv preprint arXiv:1706.05587, 2017.\n[12] T. Chen, J.-Y . Zhu, A. Shamir, and S.-M. Hu. Motion-aware gradient domain video composition.IEEE\nTrans. Image Processing, 22(7):2532–2544, 2013.\n[13] M. Cordts, M. Omran, S. Ramos, T. Rehfeld, M. Enzweiler, R. Benenson, U. Franke, S. Roth, and\nB. Schiele. The Cityscapes dataset for semantic urban scene understanding. In IEEE Conference on\nComputer Vision and Pattern Recognition (CVPR), 2016.\n[14] E. Denton, S. Chintala, A. Szlam, and R. Fergus. Deep generative image models using a Laplacian pyramid\nof adversarial networks. In Advances in Neural Information Processing Systems (NIPS), 2015.\n[15] E. L. Denton and V . Birodkar. Unsupervised learning of disentangled representations from video. In\nAdvances in Neural Information Processing Systems (NIPS), 2017.\n[16] A. Dosovitskiy and T. Brox. Generating images with perceptual similarity metrics based on deep networks.\nIn Advances in Neural Information Processing Systems (NIPS), 2016.\n[17] P. Esser, E. Sutter, and B. Ommer. A variational u-net for conditional appearance and shape generation. In\nIEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2018.\n[18] C. Finn, I. Goodfellow, and S. Levine. Unsupervised learning for physical interaction through video\nprediction. In Advances in Neural Information Processing Systems (NIPS), 2016.\n[19] A. Ghosh, V . Kulharia, V . Namboodiri, P. H. Torr, and P. K. Dokania. Multi-agent diverse generative\nadversarial networks. In IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2018.\n[20] I. Goodfellow, J. Pouget-Abadie, M. Mirza, B. Xu, D. Warde-Farley, S. Ozair, A. Courville, and Y . Bengio.\nGenerative adversarial networks. In Advances in Neural Information Processing Systems (NIPS), 2014.\n[21] I. Gulrajani, F. Ahmed, M. Arjovsky, V . Dumoulin, and A. C. Courville. Improved training of wasserstein\nGANs. In Advances in Neural Information Processing Systems (NIPS), 2017.\n[22] A. Gupta, J. Johnson, A. Alahi, and L. Fei-Fei. Characterizing and improving stability in neural style\ntransfer. In IEEE International Conference on Computer Vision (ICCV), 2017.\n[23] R. A. Güler, N. Neverova, and I. Kokkinos. Densepose: Dense human pose estimation in the wild. In IEEE\nConference on Computer Vision and Pattern Recognition (CVPR), 2018.\n[24] D. Ha and J. Schmidhuber. World models. In Advances in Neural Information Processing Systems (NIPS),\n2018.\n[25] K. He, G. Gkioxari, P. Dollár, and R. Girshick. Mask R-CNN. In IEEE International Conference on\nComputer Vision (ICCV), 2017.\n[26] K. He, X. Zhang, S. Ren, and J. Sun. Deep residual learning for image recognition. In IEEE Conference\non Computer Vision and Pattern Recognition (CVPR), 2016.\n[27] M. Heusel, H. Ramsauer, T. Unterthiner, B. Nessler, and S. Hochreiter. GANs trained by a two time-scale\nupdate rule converge to a local Nash equilibrium. In Advances in Neural Information Processing Systems\n(NIPS), 2017.\n[28] H. Huang, H. Wang, W. Luo, L. Ma, W. Jiang, X. Zhu, Z. Li, and W. Liu. Real-time neural style transfer\nfor videos. In IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2017.\n[29] X. Huang, X. Cheng, Q. Geng, B. Cao, D. Zhou, P. Wang, Y . Lin, and R. Yang. The ApolloScape dataset\nfor autonomous driving. In IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2018.\n10\n[30] X. Huang, Y . Li, O. Poursaeed, J. E. Hopcroft, and S. J. Belongie. Stacked generative adversarial networks.\nIn IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2017.\n[31] X. Huang, M.-Y . Liu, S. Belongie, and J. Kautz. Multimodal unsupervised image-to-image translation. In\nECCV, 2018.\n[32] E. Ilg, N. Mayer, T. Saikia, M. Keuper, A. Dosovitskiy, and T. Brox. Flownet 2.0: Evolution of optical\nﬂow estimation with deep networks. In IEEE Conference on Computer Vision and Pattern Recognition\n(CVPR), 2017.\n[33] P. Isola, J.-Y . Zhu, T. Zhou, and A. A. Efros. Image-to-image translation with conditional adversarial\nnetworks. In IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2017.\n[34] J. Johnson, A. Alahi, and L. Fei-Fei. Perceptual losses for real-time style transfer and super-resolution. In\nEuropean Conference on Computer Vision (ECCV), 2016.\n[35] J. T. Kajiya. The rendering equation. In ACM SIGGRAPH, 1986.\n[36] N. Kalchbrenner, A. v. d. Oord, K. Simonyan, I. Danihelka, O. Vinyals, A. Graves, and K. Kavukcuoglu.\nVideo pixel networks. arXiv preprint arXiv:1610.00527, 2016.\n[37] T. Karras, T. Aila, S. Laine, and J. Lehtinen. Progressive growing of GANs for improved quality, stability,\nand variation. In International Conference on Learning Representations (ICLR), 2018.\n[38] D. E. King. Dlib-ml: A machine learning toolkit. Journal of Machine Learning Research, 2009.\n[39] D. Kingma and J. Ba. Adam: A method for stochastic optimization. In International Conference on\nLearning Representations (ICLR), 2015.\n[40] A. B. L. Larsen, S. K. Sønderby, H. Larochelle, and O. Winther. Autoencoding beyond pixels using a\nlearned similarity metric. In International Conference on Machine Learning (ICML), 2016.\n[41] A. X. Lee, R. Zhang, F. Ebert, P. Abbeel, C. Finn, and S. Levine. Stochastic adversarial video prediction.\narXiv preprint arXiv:1804.01523, 2018.\n[42] X. Liang, L. Lee, W. Dai, and E. P. Xing. Dual motion GAN for future-ﬂow embedded video prediction.\nIn Advances in Neural Information Processing Systems (NIPS), 2017.\n[43] M.-Y . Liu, T. Breuel, and J. Kautz. Unsupervised image-to-image translation networks. InAdvances in\nNeural Information Processing Systems (NIPS), 2017.\n[44] M.-Y . Liu and O. Tuzel. Coupled generative adversarial networks. In Advances in Neural Information\nProcessing Systems (NIPS), 2016.\n[45] W. Lotter, G. Kreiman, and D. Cox. Deep predictive coding networks for video prediction and unsupervised\nlearning. In International Conference on Learning Representations (ICLR), 2017.\n[46] B. D. Lucas, T. Kanade, et al. An iterative image registration technique with an application to stereo vision.\nInternational Joint Conference on Artiﬁcial Intelligence (IJCAI), 1981.\n[47] L. Ma, X. Jia, Q. Sun, B. Schiele, T. Tuytelaars, and L. Van Gool. Pose guided person image generation.\nIn Advances in Neural Information Processing Systems (NIPS), 2017.\n[48] L. Ma, Q. Sun, S. Georgoulis, L. Van Gool, B. Schiele, and M. Fritz. Disentangled person image generation.\nIn IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2018.\n[49] X. Mao, Q. Li, H. Xie, R. Y . Lau, Z. Wang, and S. P. Smolley. Least squares generative adversarial\nnetworks. In IEEE International Conference on Computer Vision (ICCV), 2017.\n[50] M. Mathieu, C. Couprie, and Y . LeCun. Deep multi-scale video prediction beyond mean square error. In\nInternational Conference on Learning Representations (ICLR), 2016.\n[51] T. Miyato, T. Kataoka, M. Koyama, and Y . Yoshida. Spectral normalization for generative adversarial\nnetworks. In International Conference on Learning Representations (ICLR), 2018.\n[52] T. Miyato and M. Koyama. cGANs with projection discriminator. InInternational Conference on Learning\nRepresentations (ICLR), 2018.\n[53] A. Odena, C. Olah, and J. Shlens. Conditional image synthesis with auxiliary classiﬁer GANs. In\nInternational Conference on Machine Learning (ICML), 2017.\n[54] K. Ohnishi, S. Yamamoto, Y . Ushiku, and T. Harada. Hierarchical video generation from orthogonal\ninformation: Optical ﬂow and texture. In AAAI, 2018.\n[55] A. Radford, L. Metz, and S. Chintala. Unsupervised representation learning with deep convolutional\ngenerative adversarial networks. In International Conference on Learning Representations (ICLR), 2015.\n[56] S. Reed, Z. Akata, X. Yan, L. Logeswaran, B. Schiele, and H. Lee. Generative adversarial text to image\nsynthesis. In International Conference on Machine Learning (ICML), 2016.\n[57] A. Rössler, D. Cozzolino, L. Verdoliva, C. Riess, J. Thies, and M. Nießner. Faceforensics: A large-scale\nvideo dataset for forgery detection in human faces. arXiv preprint arXiv:1803.09179, 2018.\n[58] M. Ruder, A. Dosovitskiy, and T. Brox. Artistic style transfer for videos. InGerman Conference on Pattern\nRecognition, 2016.\n[59] M. Saito, E. Matsumoto, and S. Saito. Temporal generative adversarial nets with singular value clipping.\nIn IEEE International Conference on Computer Vision (ICCV), 2017.\n[60] A. Schödl, R. Szeliski, D. H. Salesin, and I. Essa. Video textures. ACM Transactions on Graphics (TOG),\n2000.\n11\n[61] E. Shechtman, Y . Caspi, and M. Irani. Space-time super-resolution.IEEE Transactions on Pattern Analysis\nand Machine Intelligence (TPAMI), 27(4):531–545, 2005.\n[62] W. Shi, J. Caballero, F. Huszár, J. Totz, A. P. Aitken, R. Bishop, D. Rueckert, and Z. Wang. Real-time\nsingle image and video super-resolution using an efﬁcient sub-pixel convolutional neural network. InIEEE\nConference on Computer Vision and Pattern Recognition (CVPR), 2016.\n[63] A. Shrivastava, T. Pﬁster, O. Tuzel, J. Susskind, W. Wang, and R. Webb. Learning from simulated and\nunsupervised images through adversarial training. In IEEE Conference on Computer Vision and Pattern\nRecognition (CVPR), 2017.\n[64] K. Simonyan and A. Zisserman. Very deep convolutional networks for large-scale image recognition.\narXiv preprint arXiv:1409.1556, 2014.\n[65] N. Srivastava, E. Mansimov, and R. Salakhudinov. Unsupervised learning of video representations using\nlstms. In International Conference on Machine Learning (ICML), 2015.\n[66] Y . Taigman, A. Polyak, and L. Wolf. Unsupervised cross-domain image generation. In International\nConference on Learning Representations (ICLR), 2017.\n[67] S. Tulyakov, M.-Y . Liu, X. Yang, and J. Kautz. MoCoGAN: Decomposing motion and content for video\ngeneration. In IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2018.\n[68] R. Villegas, J. Yang, S. Hong, X. Lin, and H. Lee. Decomposing motion and content for natural video\nsequence prediction. In International Conference on Learning Representations (ICLR), 2017.\n[69] C. V ondrick, H. Pirsiavash, and A. Torralba. Generating videos with scene dynamics. In Advances in\nNeural Information Processing Systems (NIPS), 2016.\n[70] C. V ondrick and A. Torralba. Generating the future with adversarial transformers. InIEEE Conference on\nComputer Vision and Pattern Recognition (CVPR), 2017.\n[71] J. Walker, C. Doersch, A. Gupta, and M. Hebert. An uncertain future: Forecasting from static images using\nvariational autoencoders. In European Conference on Computer Vision (ECCV), 2016.\n[72] J. Walker, K. Marino, A. Gupta, and M. Hebert. The pose knows: Video forecasting by generating pose\nfutures. In IEEE International Conference on Computer Vision (ICCV), 2017.\n[73] T.-C. Wang, M.-Y . Liu, J.-Y . Zhu, A. Tao, J. Kautz, and B. Catanzaro. High-resolution image synthesis\nand semantic manipulation with conditional GANs. In IEEE Conference on Computer Vision and Pattern\nRecognition (CVPR), 2018.\n[74] Y . Wexler, E. Shechtman, and M. Irani. Space-time video completion. In IEEE Conference on Computer\nVision and Pattern Recognition (CVPR), 2004.\n[75] Y . Wexler, E. Shechtman, and M. Irani. Space-time completion of video.IEEE Transactions on Pattern\nAnalysis and Machine Intelligence (TPAMI), 29(3), 2007.\n[76] S. Xie, R. Girshick, P. Dollár, Z. Tu, and K. He. Aggregated residual transformations for deep neural\nnetworks. In IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2017.\n[77] T. Xue, J. Wu, K. Bouman, and B. Freeman. Visual dynamics: Probabilistic future frame synthesis via\ncross convolutional networks. In Advances in Neural Information Processing Systems (NIPS), 2016.\n[78] C. Yang, Z. Wang, X. Zhu, C. Huang, J. Shi, and D. Lin. Pose guided human video generation. InEuropean\nConference on Computer Vision (ECCV), 2018.\n[79] S. Yang, T. Ambert, Z. Pan, K. Wang, L. Yu, T. Berg, and M. C. Lin. Detailed garment recovery from a\nsingle-view image. arXiv preprint arXiv:1608.01250, 2016.\n[80] H. Zhang, T. Xu, H. Li, S. Zhang, X. Huang, X. Wang, and D. Metaxas. StackGAN: Text to photo-realistic\nimage synthesis with stacked generative adversarial networks. In IEEE International Conference on\nComputer Vision (ICCV), 2017.\n[81] Z.-H. Zheng, H.-T. Zhang, F.-L. Zhang, and T.-J. Mu. Image-based clothes changing system.Computational\nVisual Media, 3(4):337–347, 2017.\n[82] J.-Y . Zhu, T. Park, P. Isola, and A. A. Efros. Unpaired image-to-image translation using cycle-consistent\nadversarial networks. In IEEE International Conference on Computer Vision (ICCV), 2017.\n[83] J.-Y . Zhu, R. Zhang, D. Pathak, T. Darrell, A. A. Efros, O. Wang, and E. Shechtman. Toward multimodal\nimage-to-image translation. In Advances in Neural Information Processing Systems (NIPS), 2017.\n12\nResidual blocks\n...\nResidual blocks\n...\nResidual blocks\n...\nResidual blocks\n...\nSemantic maps\nPrevious images\nIntermediate image\nFlow mapMask\nFigure 8: The network architecture ( G1) for low-res videos. Our network takes in a number of\nsemantic label maps and previously generated images, and outputs the intermediate frame as well as\nthe ﬂow map and the mask.\n...\nResidual blocksG2\n...\n...\n...\n...\nG2\nG1\nFigure 9: The network architecture (G2) for higher resolution videos. The label maps and previous\nframes are downsampled and fed into the low-res network G1. Then, the features from the high-res\nnetwork and the last layer of the low-res network are summed and fed into another series of residual\nblocks to output the ﬁnal images.\nA Network Architecture\nA.1 Generators\nOur network adopts a coarse-to-ﬁne architecture. For the lowest resolution, the network takes in a\nnumber of semantic label maps st\nt−L and previously generated frames ˜xt−1\nt−L as input. The label maps\nare concatenated together and undergo several residual blocks to form intermediate high-level features.\nWe apply the same processing for the previously generated images. Then, these two intermediate\nlayers are added and fed into two separate residual networks to output the hallucinated image ˜ht as\nwell as the ﬂow map ˜wt and the mask ˜mt (Figure 8).\nNext, to build from low-res results to higher-res results, we use another network G2 on top of the\nlow-res network G1 (Figure 9). In particular, we ﬁrst downsample the inputs and fed them intoG1.\nThen, we extract features from the last feature layer of G1 and add them to the intermediate feature\nlayer of G2. These summed features are then fed into another series of residual blocks to output the\nhigher resolution images.\nA.2 Discriminators\nFor our image discriminator DI, we adopt the multi-scale PatchGAN architecture [33, 73]. We also\ndesign a temporally multi-scale video discriminator DV by downsampling the frame rates of the\nreal/generated videos. In the ﬁnest scale, the discriminator takesK consecutive frames in the original\nsequence as input. In the next scale, we subsample the video by a factor of K (i.e., skipping every\n13\nK− 1 intermediate frames), and the discriminator takes consecutive K frames in this new sequence\nas input. We do this for up to three scales in our implementation and ﬁnd that this helps us ensure\nboth short-term and long-term consistency. Note that DV is also multi-scale in the spatial domain as\nDI.\nA.3 Feature matching loss\nIn our learning objective function, we also add VGG feature matching loss and discriminator feature\nmatching loss to improve the training stability. For VGG feature matching loss, we use the VGG\nnetwork [64] as a feature extractor and minimize L1 losses between the extracted features from\nthe real and the generated images. In particular, we add ∑\ni\n1\nPi\n[||ψ(i)(x)− ψ(i)(G(s))||1] to our\nobjective, where ψ(i) denotes the i-th layer with Pi elements of the VGG network. Similarly, we\nadopt the discriminator feature matching loss, to match the statistics of features extracted by the GAN\ndiscriminators. We use both the image discriminator DI and the video discriminator DV .\nB Evaluation for the Apolloscape Dataset\nWe provide both the FID and the human preference score on the Apolloscape dataset. For both\nmetrics, our method outperforms the other baselines.\nTable 4: Comparison between competing video-to-video synthesis approaches on Apolloscape.\nInception Net. of FID I3D ResNeXt\npix2pixHD 2.33 0.128\nCOVST 2.36 0.128\nvid2vid (ours) 2.24 0.125\nHuman Preference Score\nvid2vid (ours) / pix2pixHD 0.61 / 0.39\nvid2vid (ours) / COVST 0.59 / 0.41\n14",
  "values": {
    "Justice": "Yes",
    "Respect for Persons": "Yes",
    "Fairness": "Yes",
    "Privacy": "Yes",
    "Interpretable (to users)": "Yes",
    "Transparent (to users)": "Yes",
    "Autonomy (power to decide)": "Yes",
    "Collective influence": "Yes",
    "Not socially biased": "Yes",
    "User influence": "Yes",
    "Explicability": "Yes",
    "Beneficence": "Yes",
    "Respect for Law and public interest": "Yes",
    "Non-maleficence": "Yes",
    "Critiqability": "Yes",
    "Deferral to humans": "Yes"
  }
}