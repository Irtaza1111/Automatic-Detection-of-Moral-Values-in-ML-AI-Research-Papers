{
  "pdf": "1905.02450v5",
  "title": "MASS: Masked Sequence to Sequence Pre-training for Language Generation",
  "author": "Kaitao Song, Xu Tan, Tao Qin, Jianfeng Lu, Tie-Yan Liu",
  "paper_id": "1905.02450v5",
  "text": "MASS: Masked Sequence to Sequence Pre-training for Language Generation\nKaitao Song * 1 Xu Tan* 2 Tao Qin2 Jianfeng Lu 1 Tie-Yan Liu2\nAbstract\nPre-training and ﬁne-tuning, e.g., BERT (De-\nvlin et al., 2018), have achieved great success\nin language understanding by transferring knowl-\nedge from rich-resource pre-training task to the\nlow/zero-resource downstream tasks. Inspired\nby the success of BERT, we propose MAsked\nSequence to Sequence pre-training (MASS)\nfor encoder-decoder based language generation.\nMASS adopts the encoder-decoder framework\nto reconstruct a sentence fragment given the re-\nmaining part of the sentence: its encoder takes\na sentence with randomly masked fragment (sev-\neral consecutive tokens) as input, and its decoder\ntries to predict this masked fragment. In this way,\nMASS can jointly train the encoder and decoder to\ndevelop the capability of representation extraction\nand language modeling. By further ﬁne-tuning\non a variety of zero/low-resource language gen-\neration tasks, including neural machine transla-\ntion, text summarization and conversational re-\nsponse generation (3 tasks and totally 8 datasets),\nMASS achieves signiﬁcant improvements over\nbaselines without pre-training or with other pre-\ntraining methods. Specially, we achieve state-of-\nthe-art accuracy (37.5 in terms of BLEU score)\non the unsupervised English-French translation,\neven beating the early attention-based supervised\nmodel (Bahdanau et al., 2015b)1.\n1. Introduction\nPre-training and ﬁne-tuning are widely used when target\ntasks are of low or zero resource in terms of training data,\n*Equal contribution 1Key Laboratory of Intelligent Percep-\ntion and Systems for High-Dimensional Information of Min-\nistry of Education, Nanjing University of Science and Technol-\nogy 2Microsoft Research. Correspondence to: Tao Qin <tao-\nqin@microsoft.com>.\nProceedings of the 36 th International Conference on Machine\nLearning, Long Beach, California, PMLR 97, 2019. Copyright\n2019 by the author(s).\n1We release the codes in https://github.com/\nmicrosoft/MASS.\nwhile pre-training has plenty of data (Girshick et al., 2014;\nSzegedy et al., 2015; Ouyang et al., 2015; Dai & Le, 2015;\nHoward & Ruder, 2018; Radford et al., 2018; Devlin et al.,\n2018). For example, in computer vision, models are usually\npre-trained on the large scale ImageNet dataset and then ﬁne-\ntuned on downstream tasks like object detection (Szegedy\net al., 2015; Ouyang et al., 2015) or image segmenta-\ntion (Girshick et al., 2014). Recently, pre-training methods\nsuch as ELMo (Peters et al., 2018), OpenAI GPT (Radford\net al., 2018) and BERT (Devlin et al., 2018) have attracted a\nlot of attention in natural language processing, and achieved\nstate-of-the-art accuracy in multiple language understanding\ntasks such as sentiment classiﬁcation (Socher et al., 2013),\nnatural language inference (Bowman et al., 2015), named\nentity recognition (Tjong Kim Sang & De Meulder, 2003)\nand SQuAD question answering (Rajpurkar et al., 2016),\nwhich usually have limited supervised data. Among the\npre-training methods mentioned above, BERT is the most\nprominent one by pre-training the bidirectional encoder rep-\nresentations on a large monolingual corpus through masked\nlanguage modeling and next sentence prediction.\nDifferent from language understanding, language generation\naims to generate natural language sentences conditioned on\nsome inputs, including tasks like neural machine translation\n(NMT) (Cho et al., 2014; Bahdanau et al., 2015a; Vaswani\net al., 2017), text summarization (Ayana et al., 2016; Suzuki\n& Nagata, 2017; Gehring et al., 2017) and conversational re-\nsponse generation (Shang et al., 2015; Vinyals & Le, 2015).\nLanguage generation tasks are usually data-hungry, and\nmany of them are low-resource or even zero-source in terms\nof training data. Directly applying a BERT like pre-training\nmethod on these natural language generation tasks is not fea-\nsible, since BERT is designed for language understanding,\nwhich are usually handled by just one encoder or decoder.\nTherefore, how to design pre-training methods for the lan-\nguage generation tasks (which usually adopt the encoder-\ndecoder based sequence to sequence learning framework) is\nof great potential and importance.\nIn this paper, inspired by BERT, we propose a novel ob-\njective for pre-training: MAsked Sequence to Sequence\nlearning (MASS) for language generation. MASS is based\non the sequence to sequence learning framework: its en-\ncoder takes a sentence with a masked fragment (several\nconsecutive tokens) as input, and its decoder predicts this\narXiv:1905.02450v5  [cs.CL]  21 Jun 2019\nMASS: Masked Sequence to Sequence Pre-training for Language Generation\nmasked fragment conditioned on the encoder representa-\ntions. Unlike BERT or a language model that pre-trains\nonly the encoder or decoder, MASS is carefully designed to\npre-train the encoder and decoder jointly in two steps: 1) By\npredicting the fragment of the sentence that is masked on the\nencoder side, MASS can force the encoder to understand\nthe meaning of the unmasked tokens, in order to predict\nthe masked tokens in the decoder side; 2) By masking the\ninput tokens of the decoder that are unmasked in the source\nside, MASS can force the decoder rely more on the source\nrepresentation other than the previous tokens in the target\nside for next token prediction, better facilitating the joint\ntraining between encoder and decoder.\nMASS just needs to pre-train one model and then ﬁne-tune\non a variety of downstream tasks. We use transformer as the\nbasic sequence to sequence model and pre-train on the WMT\nmonolingual corpus2, and then ﬁne-tune on three different\nlanguage generation tasks including NMT, text summariza-\ntion and conversational response generation. Considering\nthe downstream tasks cover cross-lingual task like NMT, we\npre-train one model on multiple languages. We explore the\nlow-resource setting for all the three tasks, and also consider\nunsupervised NMT which is a purely zero-resource set-\nting. For NMT, the experiments are conducted on WMT14\nEnglish-French, WMT16 English-German and WMT16\nEnglish-Romanian datasets. For unsupervised NMT, we\ndirectly ﬁne-tune the pre-trained model on monolingual\ndata with back-translation loss (Lample et al., 2018), in-\nstead of using additional denoising auto-encoder loss as\nin Lample et al. (2018). For low-resource NMT, we ﬁne-\ntune our model on limited bilingual data. For the other two\ntasks, we conduct experiments on: 1) the Gigaword corpus\nfor abstractive text summarization; 2) the Cornell Movie\nDialog corpus for conversational response generation. Our\nmethod achieves improvements on all these tasks as well\nas both the zero- and low-resource settings, demonstrating\nour method is effective and applicable to a wide range of\nsequence generation tasks.\nThe contributions of this work are listed as follows: 1) We\npropose MASS, a masked sequence to sequence pre-training\nmethod for language generation; 2) We apply MASS on a\nvariety of language generation tasks including NMT, text\nsummarization and conversational response generation, and\nachieve signiﬁcant improvements, demonstrating the effec-\ntiveness of our proposed method. Specially, we achieve a\nstate-of-the art BLEU score for unsupervised NMT on two\nlanguage pairs: English-French and English-German, and\noutperform the previous unsupervised NMT method (Lam-\nple & Conneau, 2019) by more than 4 points on English-\nFrench and 1 point on French-English in terms of BLEU\n2The monolingual data for each language is downloaded from\nhttp://www.statmt.org/wmt16/translation-task.html.\nscore, and even beating the early attention-based supervised\nmodel (Bahdanau et al., 2015b).\n2. Related Work\nThere are a lot of works on sequence to sequence learning\nand the pre-training for natural language processing. We\nbrieﬂy review several popular approaches in this section.\n2.1. Sequence to Sequence Learning\nSequence to sequence learning (Cho et al., 2014; Bahdanau\net al., 2015a; Wu et al., 2016; Gehring et al., 2017; Vaswani\net al., 2017) is a challenging task in artiﬁcial intelligence,\nand covers a variety of language generation applications\nsuch as NMT (Cho et al., 2014; Bahdanau et al., 2015a;\nWu et al., 2016; Gehring et al., 2017; Vaswani et al., 2017;\nTan et al., 2019; Artetxe et al., 2017; Lample et al., 2017;\n2018; He et al., 2018; Hassan et al., 2018; Song et al., 2018;\nShen et al., 2018), text summarization (Ayana et al., 2016;\nSuzuki & Nagata, 2017; Gehring et al., 2017), question\nanswering (Yuan et al., 2017; Fedus et al., 2018) and con-\nversational response generation (Shang et al., 2015; Vinyals\n& Le, 2015).\nSequence to sequence learning has attracted much attention\nin recent years due to the advance of deep learning. How-\never, many language generations tasks such as NMT lack\npaired data but have plenty of unpaired data. Therefore, the\npre-training on unpaired data and ﬁne-tuning with small-\nscale paired data will be helpful for these tasks, which is\nexactly the focus of this work.\n2.2. Pre-training for NLP tasks\nPre-training has been widely used in NLP tasks to learn\nbetter language representation. Previous works mostly fo-\ncus on natural language understanding tasks, and can be\nclassiﬁed into feature-based approaches and ﬁne-tuning ap-\nproaches. Feature-based approaches mainly leverage pre-\ntraining to provide language representations and features\nto the downstream tasks, which includes word-level rep-\nresentations (Brown et al., 1992; Ando & Zhang, 2005;\nBlitzer et al., 2006; Collobert & Weston, 2008; Mikolov\net al., 2013; Pennington et al., 2014) and sentence-level rep-\nresentations (Kiros et al., 2015; Logeswaran & Lee, 2018;\nLe & Mikolov, 2014), as well as context sensitive features\nfrom the NMT model (McCann et al., 2017) and ELMo (Pe-\nters et al., 2018). Fine-tuning approaches mainly pre-train\na model on language modeling objective and then ﬁne-\ntune the model on the downstream tasks with supervised\ndata (Dai & Le, 2015; Howard & Ruder, 2018; Radford\net al., 2018; Devlin et al., 2018). Speciﬁcally, Devlin et al.\n(2018) proposed BERT based on masked language modeling\nand next sentence prediction and achieved a state-of-the-art\nMASS: Masked Sequence to Sequence Pre-training for Language Generation\nX6\nX8_ X7X1 X2 _ _ _ _ _ _ X4 X5X3\nEncoder Decoder\n__\nX3 X5X4\nAttention\nFigure 1. The encoder-decoder framework for our proposed MASS. The token “ ” represents the mask symbol [M].\naccuracy on multiple language understanding tasks in the\nGLUE benchmark (Wang et al., 2018) and SQuAD (Ra-\njpurkar et al., 2016).\nThere are also some works pre-training the encoder-decoder\nmodel for language generation. Dai & Le (2015); Ra-\nmachandran et al. (2016) leverage a language model or\nauto-encoder to pre-train the encoder and decoder. Their\nimprovements, although observed, are limited and not as\ngeneral and signiﬁcant as the pre-training methods (e.g.,\nBERT) for language understanding. Zhang & Zong (2016)\ndesigned a sentence reordering task for pre-training, but\nonly for the encoder part of the encoder-decoder model.\nZoph et al. (2016); Firat et al. (2016) pre-train the model\non similar rich-resource language pairs and ﬁne-tuned on\nthe target language pair, which relies on supervised data on\nother language pairs. Recently, XLM (Lample & Conneau,\n2019) pre-trained BERT-like models both for the encoder\nand decoder, and achieved the previous state of the art re-\nsults on unsupervised machine translation. However, the\nencoder and decoder in XLM are pre-trained separately and\nthe encoder-decoder attention mechanism cannot be pre-\ntrained, which are sub-optimal for sequence to sequence\nbased language generation tasks.\nDifferent from previous works, our proposed MASS is care-\nfully designed to pre-train both the encoder and decoder\njointly using only unlabeled data, and can be applied to\nmost language generations tasks.\n3. MASS\nIn this section, we ﬁrst introduce the basic framework of\nsequence to sequence learning, and then propose MASS\n(MAsked Sequence to Sequence pre-training). We then\ndiscuss the differences between MASS and previous pre-\ntraining methods including the masked language modeling\nin BERT and standard language modeling.\n3.1. Sequence to Sequence Learning\nWe denote (x,y ) ∈ (X,Y) as a sentence pair, where\nx = (x1,x 2,...,x m) is the source sentence with m to-\nkens, and y = (y1,y 2,...,y n) is the target sentence with\nn tokens, and X andY are the source and target do-\nmains. A sequence to sequence model learns the param-\neter θ to estimate the conditional probability P (y|x;θ),\nand usually uses log likelihood as the objective function:\nL(θ; (X,Y)) = Σ (x,y)∈(X ,Y) logP (y|x;θ). The condi-\ntional probabilityP (y|x;θ) can be further factorized accord-\ning to the chain rule: P (y|x;θ) = ∏n\nt=1P (yt|y<t,x ;θ),\nwherey<t is the proceeding tokens before positiont.\nA major approach to sequence to sequence learning is the\nencoder-decoder framework: The encoder reads the source\nsequence and generates a set of representations; the decoder\nestimates the conditional probability of each target token\ngiven the source representations and its preceding tokens.\nAttention mechanism (Bahdanau et al., 2015a) is further\nintroduced between the encoder and decoder to ﬁnd which\nsource representation to focus on when predicting the cur-\nrent token.\n3.2. Masked Sequence to Sequence Pre-training\nWe introduce a novel unsupervised prediction task in this\nsection. Given an unpaired source sentence x∈X , we\ndenotex\\u:v as a modiﬁed version ofx where its fragment\nfrom positionu tov are masked, 0<u<v <m andm is\nthe number of tokens of sentencex. We denotek =v−u+1\nas the number of tokens being masked from position u to\nv. We replace each masked token by a special symbol [M],\nand the length of the masked sentence is not changed.xu:v\ndenotes the sentence fragment ofx fromu tov.\nMASS pre-trains a sequence to sequence model by predict-\ning the sentence fragmentxu:v taking the masked sequence\nx\\u:v as input. We also use the log likelihood as the objec-\ntive function:\nL(θ;X ) = 1\n|X| Σx∈X logP (xu:v|x\\u:v;θ)\n= 1\n|X| Σx∈X log\nv∏\nt=u\nP (xu:v\nt |xu:v\n<t,x\\u:v;θ).\n(1)\nWe show an example in Figure 1, where the input sequence\nhas 8 tokens with the fragment x3x4x5x6 being masked.\nNote that the model only predicts the masked fragment\nx3x4x5x6, givenx3x4x5 as the decoder input for position\n4− 6, and the decoder takes the special mask symbol [M]\nas inputs for the other positions (e.g., position 1− 3 and\nMASS: Masked Sequence to Sequence Pre-training for Language Generation\nX8X4 X7X1 X2 X3 _ X6 _ _ _ _ __\nEncoder Decoder\n__\nX5\nAttention\n(a) Masked language modeling in BERT (k = 1)\nX1 X2 X3 X5 X6X4 X8X7\nX7X6X3__ __ _ _ _ _ _ X1 X2 X4 X5\nEncoder DecoderAttention (b) Standard language modeling (k =m)\nFigure 2. The model structure of MASS whenk = 1 andk =m. Masked language modeling in BERT can be viewed as the casek = 1\nand standard language modeling can be viewed as the casek =m.\n7− 8). While our method works for any neural network\nbased encoder-decoder frameworks, we choose Transformer\nin our experiments, considering that it achieves state-of-the-\nart performances in multiple sequence to sequence learning\ntasks.\nActually, the masked language modeling in BERT (Devlin\net al., 2018) and the standard language modeling (Bengio\net al., 2003; Mikolov et al., 2010) in GPT (Radford et al.,\n2018) can be viewed as special cases of MASS. We have\nan important hyperparameterk, which denotes the length\nof the masked fragment of the sentence. Our method with\ndifferentk values can cover the special cases that are related\nto previous pre-training methods, as shown in Table 1.\nWhenk = 1, the masked fragment in the source sentence\ncontains only one token, and the decoder predicts this token\nwithout any tokens as input but conditioned on the unmasked\nsource tokens, as shown in Figure 2a. It becomes the masked\nlanguage modeling as used in BERT. One may argue that\nthe model structure is a little bit different from the masked\nlanguage model. However, since all the input tokens of the\ndecoder are masked, the decoder is itself like a non-linear\nclassiﬁer, analogous to the softmax matrix used in BERT.\nIn this case, the conditional probability isP (xu|x\\u;θ) and\nu is the position of the masked token, which is exactly the\nformulation of masked language modeling used in BERT3.\nWhen k = m where m is the number of tokens in sen-\ntencex, all the tokens on the encoder side are masked and\nthe decoder needs to predict all tokens given previous to-\nkens, as shown in Figure 2b. The conditional probability\nisP (x1:m|x\\1:m;θ), and it becomes the standard language\nmodeling in GPT, conditioned on null information from the\nencoder as all the tokens in the encoder side are masked.\n3.3. Discussions\nMASS is a pre-training method for language generation.\nWhile its special cases are related to the previous methods\nincluding the standard language modeling in GPT and the\nmasked language modeling in BERT, it is different from\n3One may argue that the masked language modeling in BERT\nrandomly masks multiple tokens rather than just one token at a\ntime. However, the key idea behind masking language modeling\nin BERT is to leverage bidirectional context information. Masking\nmultiple tokens at a time is mainly for training speedup.\nLength Probability Model\nk = 1 P (xu|x\\u;θ) masked LM in BERT\nk =m P (x1:m|x\\1:m;θ) standard LM in GPT\nk ∈ (1,m ) P (xu:v|x\\u:v;θ) methods in between\nTable 1. Masked language modeling in BERT and standard lan-\nguage modeling, as special cases covered in MASS.\nthese methods in general.\n• Standard language modeling has long been used for\npre-training, and the most prominent ones are the re-\ncently proposed ELMo (Peters et al., 2018) and Ope-\nnAI GPT (Radford et al., 2018). BERT introduces\ntwo pre-training tasks (masked language modeling and\nnext sentence prediction) for natural language under-\nstanding, and uses one encoder to extract the repre-\nsentation for a single sentence or a pair of sentences.\nBoth standard language modeling and BERT can just\npre-train the encoder or decoder separately. While\nachieving promising results on language understand-\ning tasks, they are not suitable for language genera-\ntion tasks which typically leverage an encoder-decoder\nframework for conditional sequence generation.\n• MASS is designed to jointly pre-train the encoder and\ndecoder for language generation tasks. First, by only\npredicting the masked tokens through a sequence to\nsequence framework, MASS forces the encoder to un-\nderstand the meaning of the unmasked tokens, and\nalso encourages the decoder to extract useful infor-\nmation from the encoder side. Second, by predicting\nconsecutive tokens in the decoder side, the decoder\ncan build better language modeling capability than just\npredicting discrete tokens. Third, by further masking\nthe input tokens of the decoder which are not masked\nin the encoder side (e.g., when predicting fragment\nx3x4x5x6, only the tokensx3x4x5 are taken as the in-\nput and other tokens are masked with[M]), the decoder\nis encouraged to extract more useful information from\nthe encoder side, rather than leveraging the abundant\ninformation from the previous tokens.\nMASS: Masked Sequence to Sequence Pre-training for Language Generation\n4. Experiments and Results\nIn this section, we describe the experimental details about\nMASS pre-training and ﬁne-tuning on a variety of language\ngeneration tasks, including NMT, text summarization, con-\nversational response generation.\n4.1. MASS Pre-training\nModel Conﬁguration We choose Transformer (Vaswani\net al., 2017) as the basic model structure, which consists\nof 6-layer encoder and 6-layer decoder with 1024 embed-\nding/hidden size and 4096 feed-forward ﬁlter size. For\nneural machine translation task, we pre-train our model on\nthe monolingual data of the source and target languages. We\nrespectively conduct experiments on three language pairs:\nEnglish-French, English-German, and English-Romanian.\nFor other language generation tasks, including text summa-\nrization and conversational response generation, we pre-\ntrain the model with only English monolingual data re-\nspectively. To distinguish between the source and target\nlanguages in neural machine translation task, we add a lan-\nguage embedding to each token of the input sentence for the\nencoder and decoder, which is also learnt end-to-end. We\nimplement our method based on codebase of XLM 4.\nDatasets We use all of the monolingual data from WMT\nNews Crawl datasets5, which covers 190M, 62M and 270M\nsentences from year 2007 to 2017 for English, French, Ger-\nman respectively. We also include a low-resource language,\nRomanian, in the pre-training stage, to verify the effective-\nness of MASS pre-trained with low-resource monolingual\ndata. We use all of the available Romanian sentences from\nNews Crawl dataset and augment it with WMT16 data,\nwhich results in 2.9M sentences. We remove the sentences\nwith length over 175. For each task, we jointly learn a\n60,000 sub-word units with Byte-Pair Encoding (Sennrich\net al., 2016) between source and target languages.\nPre-Training Details We mask the fragment by replac-\ning the consecutive tokens with special symbols [M], with\nrandom start positionu. Following Devlin et al. (2018), the\nmasked tokens in the encoder will be a [M] token 80% of\nthe time, a random token 10% of the time and a unchanged\ntoken 10% of the time. We set the fragment length k as\nroughly 50% of the total number of tokens in the sentence\nand also study differentk to compare their accuracy changes.\nTo reduce the memory and computation cost, we removed\nthe padding in the decoder (the masked tokens) but keep the\npositional embedding of the unmasked tokens unchanged\n(e.g., if the ﬁrst two tokens are masked and removed, the\n4https://github.com/facebookresearch/XLM\n5While we choose the WMT monolingual data in the current\nsetting, pre-training on Wikipedia data is also feasible.\nposition for the third token is still 2 but not 0). In this way,\nwe can get similar accuracy and reduce 50% computation\nin the decoder. We use Adam optimizer (Kingma & Ba,\n2015) with a learning rate of 10−4 for the pre-training. The\nmodel are trained on 8 NVIDIA V100 GPU cards and each\nmini-batch contains 3000 tokens for pre-training.\nTo verify the effectiveness of MASS, we ﬁne-tune the pre-\ntrained model on three language generation tasks: NMT,\ntext summarization and conversational response generation.\nWe explore the low-resource setting on these tasks where\nwe just leverage few training data for ﬁne-tuning to simulate\nthe low-resource scenario. For NMT, we mainly investigate\nthe zero-resource (unsupervised) setting, as unsupervised\nNMT has become a challenging task in recent years (Artetxe\net al., 2017; Lample et al., 2017; 2018).\n4.2. Fine-Tuning on NMT\nIn this section, we ﬁrst describe the experiments on the\nunsupervised NMT, and then introduce the experiments on\nlow-resource NMT.\nExperimental Setting For unsupervised NMT, there is no\nbilingual data to ﬁne-tune the pre-trained model. Therefore,\nwe leverage the monolingual data that is also used in the\npre-training stage. Different from Artetxe et al. (2017);\nLample et al. (2017; 2018); Leng et al. (2019), we just\nuse back-translation to generate pseudo bilingual data for\ntraining, without using denoising auto-encoder 6. During\nﬁne-tuning, we use Adam optimizer (Kingma & Ba, 2015)\nwith initial learning rate 10−4, and the batch size is set as\n2000 tokens for each GPU. During evaluation, we calculate\nthe BLEU score with multi-bleu.pl 7 on newstest2014 for\nEnglish-French, and newstest2016 for English-German and\nEnglish-Romanian.\nResults on Unsupervised NMT Our results are shown\nin Table 2. On all the 6 translation directions, our method\noutperforms all of the previous results, including the meth-\nods without pre-training (Lample et al., 2018) and with\npre-training (Lample & Conneau, 2019). XLM (Lample\n& Conneau, 2019) is the previous state-of-the-art method\nwhich leverage BERT like pre-training in encoder and de-\ncoder, which covers several pre-training methods: masked\nlanguage model (MLM) and causal language model (CLM).\nOur method still outperforms XLM by 4.1 BLEU points on\nen-fr.\nCompared with Other Pre-training Methods We also\ncompare MASS with the previous pre-training methods for\n6MASS is better than denoising auto-encoder as we will show\nin Table 3.\n7https://github.com/moses-smt/mosesdecoder/blob/master/\nscripts/generic/multi-bleu.perl\nMASS: Masked Sequence to Sequence Pre-training for Language Generation\nMethod Setting en - fr fr - en en - de de - en en - ro ro - en\nArtetxe et al. (2017) 2-layer RNN 15.13 15.56 6.89 10.16 - -\nLample et al. (2017) 3-layer RNN 15.05 14.31 9.75 13.33 - -\nYang et al. (2018) 4-layer Transformer 16.97 15.58 10.86 14.62 - -\nLample et al. (2018) 4-layer Transformer 25.14 24.18 17.16 21.00 21.18 19.44\nXLM (Lample & Conneau, 2019) 6-layer Transformer 33.40 33.30 27.00 34.30 33.30 31.80\nMASS 6-layer Transformer 37.50 34.90 28.30 35.20 35.20 33.10\nTable 2. The BLEU score comparisons between MASS and the previous works on unsupervised NMT. Results on en-fr and fr-en pairs are\nreported on newstest2014 and the others are on newstest2016. Since XLM uses different combinations of MLM and CLM in the encoder\nand decoder, we report the highest BLEU score for XLM on each language pair.\nlanguage generation tasks. The ﬁrst baseline is BERT+LM,\nwhich use masked language modeling in BERT to pre-train\nthe encoder and the standard language modeling to pre-train\nthe decoder. The second baseline is DAE, which simply\nuses denoising auto-encoder (Vincent et al., 2008) to pre-\ntrain the encoder and decoder. We pre-train the model with\nBERT+LM and DAE, and ﬁne-tune on the unsupervised\ntranslation pairs with same ﬁne-tuning strategy of XLM\n(i.e., DAE loss + back-translation). These methods are also\nconﬁgured with the 6-layer Transformer setting.\nAs shown in Table 3, BERT+LM achieves higher BLEU\nscore than DAE, and MASS outperforms both BERT+LM\nand DAE on all the unsupervised translation pairs. While\nDAE usually leverages some denoising methods like ran-\ndomly masking tokens or swapping adjacent tokens, the\ndecoder can still easily learn to copy the unmasked tokens\nthrough encoder-decoder attention8. On the other hand, the\ndecoder in DAE takes the full sentence as the input, which\nis enough to predict the next token like the language model,\nand is not forced to extract additional useful representation\nfrom the encoder.\nExperiments on Low-Resource NMT In the low-\nresource NMT setting, we respectively sample 10K, 100K,\n1M paired sentence from the bilingual training data of\nWMT14 English-French, WMT16 English-German and\nWMT16 English-Romanian, to explore the performance of\nour method in different low-resource scenarios. We use the\nsame BPE codes learned in the pre-trained stage to tokenize\nthe training sentence pairs. We ﬁne-tune the pre-trained\nmodel on the paired data for 20,000 steps with Adam op-\ntimizer and the learning rate is set as 10−4. We choose\nthe best model according to the accuracy on development\nset. We report the BLEU scores on the same testsets used\nin the unsupervised setting. As shown in Figure 3, MASS\noutperforms the baseline models that are trained only on\n8The popular encoder-decoder based model structures (Wu\net al., 2016; Gehring et al., 2017; Vaswani et al., 2017) all adopt\nresidual connection (He et al., 2016). Therefore, the token genera-\ntion in the top layer of the decoder side can directly depend on the\ntoken embedding in the encoder side through residual connection\nand attention.\nMethod en-fr fr-en en-de de-en en-ro ro-en\nBERT+LM 33.4 32.3 24.9 32.9 31.7 30.4\nDAE 30.1 28.3 20.9 27.5 28.8 27.6\nMASS 37.5 34.9 28.3 35.2 35.2 33.1\nTable 3. The BLEU score comparisons between MASS and other\npre-training methods. The results for BERT+LM are directly taken\nfrom the MLM+CLM setting in XLM (Lample & Conneau, 2019)\nas they use the same pre-training methods.\nthe bilingual data without any pre-training on all the six\ntranslation directions, demonstrating the effectiveness of\nour method in the low-resource scenarios.\n4.3. Fine-Tuning on Text Summarization\nExperiment Setting Text summarization is the task of\ncreating a short and ﬂuent summary of a long text document,\nwhich is a typical sequence generation task. We ﬁne-tune the\npre-trained model on text summarization task with different\nscales (10K, 100K, 1M and 3.8M) of training data from the\nGigaword corpus (Graff et al., 2003) 9, which consists of\na total of 3.8M article-title pairs in English. We take the\narticle as the encoder input and title as the decoder input for\nﬁne-tuning. We report the F1 score of ROUGE-1, ROUGE-\n2 and ROUGE-L on the Gigaword testset during evaluation.\nWe use beam search with a beam size of 5 for inference.\nResults Our results are illustrated in Figure 4. We com-\npare MASS with the model that is trained only on the paired\ndata without any pre-training. MASS consistently outper-\nforms the baseline on different scales of ﬁne-tuning data\n(more than 10 ROUGE points gain on 10K data and 5\nROUGE points gain on 100K data), which demonstrates\nthat MASS is effective in low-resource scenarios with dif-\nferent scale of training data on this task.\nCompared with Other Pre-Training Methods We fur-\nther compare MASS with the pre-training methods of\nBERT+LM and DAE described in Section 4.2, with 3.8M\n9https://github.com/harvardnlp/sent-summary\nMASS: Masked Sequence to Sequence Pre-training for Language Generation\n(a) en-fr\n (b) fr-en\n (c) en-de\n (d) de-en\n (e) en-ro\n (f) ro-en\nFigure 3. The BLEU score comparisons between MASS and the baseline on low-resource NMT with different scales of paired data.\n(a) RG-1 (F)\n (b) RG-2 (F)\n (c) RG-L (F)\nFigure 4. The comparisons between MASS and the baseline on\ntext summarization task with different scales of paired data. The\nresults are reported in ROUGE-1 (RG-1), ROUGE-2 (RG-2) and\nROUGE-L (RG-L) respectively. F stands for F1-score.\nMethod RG-1 (F) RG-2 (F) RG-L (F)\nBERT+LM 37.75 18.45 34.85\nDAE 35.97 17.17 33.14\nMASS 38.73 19.71 35.96\nTable 4. The comparisons between MASS and two other pre-\ntraining methods in terms of ROUGE score on the text summariza-\ntion task with 3.8M training data.\ndata on the text summarization task. As shown in Table 4,\nMASS consistently outperforms the two pre-training meth-\nods on the three ROUGE scores.\n4.4. Fine-Tuning on Conversational Response\nGeneration\nExperimental Setting Conversational response gener-\nation generates a ﬂexible response for the conversa-\ntion (Shang et al., 2015; Vinyals & Le, 2015). We conduct\nexperiments on the Cornell movie dialog corpus (Danescu-\nNiculescu-Mizil & Lee, 2011)10 that contains 140K conver-\nsation pairs. We randomly sample 10K/20K pairs as the\nvalidation/test set and the remaining data is used for training.\nWe adopt the same optimization hyperparameters from the\npre-training stage for ﬁne-tuning. We report the results with\nperplexity (PPL) following Vinyals & Le (2015).\nResults We compare MASS with the baseline that is\ntrained on the available data pairs. We conduct experiments\n10https://github.com/suriyadeepan/datasets/tree/master/seq2seq/\ncornell movie corpus\non the 10K pairs (randomly chosen) and the whole 110K\npairs, and show the results in Table 5. MASS achieves lower\nPPL than the baseline on both the 10K and 110K data.\nMethod Data = 10K Data = 110K\nBaseline 82.39 26.38\nBERT+LM 80.11 24.84\nMASS 74.32 23.52\nTable 5. The comparisons between MASS and other baseline meth-\nods in terms of PPL on Cornell Movie Dialog corpus.\nCompared with Other Pre-Training Methods We also\ncompare MASS with the pre-training methods ofBERT+LM\nand DAE on conversational response generation. As shown\nin Table 5, MASS consistently outperforms the two pre-\ntraining methods with lower PPL on 10K and 110K training\ndata respectively.\n4.5. Analysis of MASS\nStudy of Different k The length of the masked fragment\nk is an important hyperparameter of MASS and we have\nvariedk in Section 3.2 to cover the special cases of masked\nlanguage modeling in BERT and standard language mod-\neling. In this section, we study the performance of MASS\nwith differentk, where we choosek from 10% to 90% per-\ncentage of the sentence length m with a step size of 10%,\nplus withk = 1 andk =m.\nWe observe both the performance of MASS after pre-\ntraining, as well as the performance after ﬁne-tuning on\nseveral language generation tasks, including unsupervised\nEnglish-French translation, text summarization and conver-\nsational response generation. We ﬁrst show the perplexity\n(PPL) of the pre-training model on the English and French\nlanguages with different k. We choose the English and\nFrench sentences from newstest2013 of WMT En-Fr as\nthe validation set, and plot the PPL in Figure 5a (English)\nand 5b (French). It can be seen that the pre-trained model\nachieves the best validation PPL when k is between 50%\nand 70% of the sentence length m. We then observe the\nperformance on ﬁne-tuning tasks. We show the curve of\nthe validation BLEU scores on unsupervised En-Fr trans-\nMASS: Masked Sequence to Sequence Pre-training for Language Generation\n(a)\n (b)\n (c)\n (d)\n (e)\nFigure 5. The performances of MASS with different masked lengthsk, in both pre-training and ﬁne-tuning stages, which include: the PPL\nof the pre-trained model on English (Figure a) and French (Figure b) sentences from WMT newstest2013 on English-French translation;\nthe BLEU score of unsupervised English-French translation on WMT newstest2013 (Figure c); the ROUGE score (F1 score in RG-2) on\nthe validation set of text summarization (Figure d); the PPL on the validation set of conversational response generation (Figure e).\nMethod BLEU Method BLEU Method BLEU\nDiscrete 36.9 Feed 35.3 MASS 37.5\nTable 6. The comparison between MASS and the ablation methods\nin terms of BLEU score on the unsupervised en-fr translation.\nlation in Figure 5c, the validation ROUGE scores on text\nsummarization in Figure 5d, and the validation PPL on con-\nversational response generation in Figure 5e. It can be seen\nthat MASS achieves best performance on these downstream\ntasks whenk is nearly 50% of the sentence lengthm. There-\nfore, we setk = 50% ofm for MASS in our experiments.\nActually,k = 50% of m is a good balance between the\nencoder and decoder. Too few valid tokens in the encoder\nside or in the decoder side will bias the model to concentrate\nmore on the other side, which is not suitable for language\ngeneration task that typically leverages the encoder-decoder\nframework to extract the sentence representation in the en-\ncoder, as well as to model and generate the sentence in the\ndecoder. The extreme cases are k = 1 (masked language\nmodeling in BERT) andk =m (standard language model-\ning), as illustrated in Figure 2. Neither k = 1 nork = m\ncan achieve good performance on the downstream language\ngeneration tasks, as shown in Figure 5.\nAblation Study of MASS In our masked sequence to se-\nquence pre-training, we have two careful designs: (1) We\nmask consecutive tokens in the encoder side, and thus pre-\ndict consecutive tokens in the decoder side, which can build\nbetter language modeling capability than just predicting\ndiscrete tokens. (2) We mask the input tokens of the de-\ncoder which are not masked in the encoder side (e.g., when\npredicting fragmentx3x4x5x6 in Figure 1, only the tokens\nx3x4x5 are taken as the input and other tokens are masked\nwith [M]), to encourage the decoder to extract more useful\ninformation from the encoder side, rather than leveraging\nthe abundant information from the previous tokens. In this\nsection, we conduct two ablation studies to verify the ef-\nfectiveness of the two designs in MASS. The ﬁrst study is\nto randomly mask discrete tokens instead of consecutive\ntokens in MASS, denoted as Discrete. The second study\nis to feed all the tokens to the decoder instead of masking\nthe input tokens of the decoder that are not masked in the\nencoder side, denoted as Feed. We compare MASS with the\ntwo ablation methods on the unsupervised English-French\ntranslation, as shown in Table 6. It can be seen that bothDis-\ncrete and Feed perform worse than MASS, demonstrating\nthe effectiveness of the two designs in MASS.\n5. Conclusion\nIn this work, we have proposed MASS: masked sequence to\nsequence pre-training for language generation tasks, which\nreconstructs a sentence fragment given the remaining part\nof the sentence in the encoder-decoder framework. MASS\njust needs to pre-train one model and then ﬁne-tune on\nmultiple language generation tasks such as neural machine\ntranslation, text summarization and conversational response\ngeneration. Through experiments on the three above tasks\nand total eight datasets, MASS achieved signiﬁcant improve-\nments over the baseline without pre-training or with other\npre-training methods. More speciﬁcally, MASS achieved\nthe state-of-the-art BLEU scores for unsupervised NMT on\nthree language pairs, outperforming the previous state-of-\nthe-art by more than 4 BLEU points on English-French.\nFor future work, we will apply MASS to more language\ngeneration tasks such as sentence paraphrasing, text style\ntransfer and post editing, as well as other sequence genera-\ntion tasks (Ren et al., 2019). We will also investigate more\nof the theoretical and empirical analysis on our masked\nsequence to sequence pre-training method.\nAcknowledgements\nThis work was partially supported by the National Key Re-\nsearch and Development Program of China under Grant\n2018YFB1004904. We thank Yichong Leng, Weicong Chen,\nYi Zhuang, Hao Sun and Yi Ren for the further develop-\nMASS: Masked Sequence to Sequence Pre-training for Language Generation\nment on the work of MASS. We also thank the anonymous\nreviewers for their valuable comments on our paper.\nReferences\nAndo, R. K. and Zhang, T. A framework for learning pre-\ndictive structures from multiple tasks and unlabeled data.\nJournal of Machine Learning Research , 6(Nov):1817–\n1853, 2005.\nArtetxe, M., Labaka, G., Agirre, E., and Cho, K. Unsuper-\nvised neural machine translation. CoRR, 2017.\nAyana, Shen, S., Liu, Z., and Sun, M. Neural headline\ngeneration with minimum risk training. ArXiv, 2016.\nBahdanau, D., Cho, K., and Bengio, Y . Neural machine\ntranslation by jointly learning to align and translate.ICLR\n2015, 2015a.\nBahdanau, D., Cho, K., and Bengio, Y . Neural machine\ntranslation by jointly learning to align and translate.\n2015b.\nBengio, Y ., Ducharme, R., Vincent, P., and Jauvin, C. A\nneural probabilistic language model. Journal of machine\nlearning research, 3(Feb):1137–1155, 2003.\nBlitzer, J., McDonald, R., and Pereira, F. Domain adapta-\ntion with structural correspondence learning. In EMNLP,\npp. 120–128. Association for Computational Linguistics,\n2006.\nBowman, S. R., Angeli, G., Potts, C., and Manning, C. D.\nA large annotated corpus for learning natural language\ninference. In EMNLP, 2015.\nBrown, P. F., Desouza, P. V ., Mercer, R. L., Pietra, V . J. D.,\nand Lai, J. C. Class-based n-gram models of natural lan-\nguage. Computational linguistics, 18(4):467–479, 1992.\nCho, K., van Merrienboer, B., G¨ulc ¸ehre, C ¸., Bahdanau, D.,\nBougares, F., Schwenk, H., and Bengio, Y . Learning\nphrase representations using RNN encoder-decoder for\nstatistical machine translation. In EMNLP, 2014.\nCollobert, R. and Weston, J. A uniﬁed architecture for\nnatural language processing: Deep neural networks with\nmultitask learning. In ICML, pp. 160–167. ACM, 2008.\nDai, A. M. and Le, Q. V . Semi-supervised sequence learning.\nIn NIPS, pp. 3079–3087, 2015.\nDanescu-Niculescu-Mizil, C. and Lee, L. Chameleons in\nimagined conversations: A new approach to understand-\ning coordination of linguistic style in dialogs. In ACL\nWorkshop, 2011.\nDevlin, J., Chang, M.-W., Lee, K., and Toutanova, K. Bert:\nPre-training of deep bidirectional transformers for lan-\nguage understanding. CoRR, 2018.\nFedus, W., Goodfellow, I., and Dai, A. Maskgan: Better\ntext generation via ﬁlling in the . In ICLR, 2018.\nFirat, O., Sankaran, B., Al-Onaizan, Y ., Vural, F. T. Y .,\nand Cho, K. Zero-resource translation with multi-lingual\nneural machine translation. In EMNLP, pp. 268–277,\n2016.\nGehring, J., Auli, M., Grangier, D., Yarats, D., and Dauphin,\nY . N. Convolutional sequence to sequence learning. In\nICML, volume 70, pp. 1243–1252, 2017.\nGirshick, R., Donahue, J., Darrell, T., and Malik, J. Rich\nfeature hierarchies for accurate object detection and se-\nmantic segmentation. In CVPR, pp. 580–587, 2014.\nGraff, David, Kong, Junbo, Chen, Ke, Maeda, and Kazuaki.\nEnglish gigaword. In Linguistic Data Consortium, 2003.\nHassan, H., Aue, A., Chen, C., Chowdhary, V ., Clark,\nJ., Federmann, C., Huang, X., Junczys-Dowmunt, M.,\nLewis, W., Li, M., et al. Achieving human parity on auto-\nmatic chinese to english news translation. arXiv preprint\narXiv:1803.05567, 2018.\nHe, K., Zhang, X., Ren, S., and Sun, J. Deep residual\nlearning for image recognition. In CVPR, pp. 770–778,\n2016.\nHe, T., Tan, X., Xia, Y ., He, D., Qin, T., Chen, Z., and\nLiu, T.-Y . Layer-wise coordination between encoder and\ndecoder for neural machine translation. In Advances in\nNeural Information Processing Systems, pp. 7944–7954,\n2018.\nHoward, J. and Ruder, S. Universal language model ﬁne-\ntuning for text classiﬁcation. In ACL, volume 1, pp. 328–\n339, 2018.\nKingma, D. P. and Ba, J. Adam: A method for stochastic\noptimization. In ICLR, 2015.\nKiros, R., Zhu, Y ., Salakhutdinov, R. R., Zemel, R., Urtasun,\nR., Torralba, A., and Fidler, S. Skip-thought vectors. In\nNIPS, pp. 3294–3302, 2015.\nLample, G. and Conneau, A. Cross-lingual language model\npretraining. CoRR, abs/1901.07291, 2019.\nLample, G., Conneau, A., Denoyer, L., and Ranzato, M.\nUnsupervised machine translation using monolingual cor-\npora only. CoRR, 2017.\nLample, G., Ott, M., Conneau, A., Denoyer, L., and Ran-\nzato, M. Phrase-based & neural unsupervised machine\ntranslation. In EMNLP, pp. 5039–5049, 2018.\nMASS: Masked Sequence to Sequence Pre-training for Language Generation\nLe, Q. and Mikolov, T. Distributed representations of sen-\ntences and documents. In ICML, pp. 1188–1196, 2014.\nLeng, Y ., Tan, X., Qin, T., Li, X.-Y ., and Liu, T.-Y . Unsu-\npervised pivot translation for distant languages. In ACL,\n2019.\nLogeswaran, L. and Lee, H. An efﬁcient framework for\nlearning sentence representations. CORR, 2018.\nMcCann, B., Bradbury, J., Xiong, C., and Socher, R.\nLearned in translation: Contextualized word vectors. In\nNIPS, pp. 6294–6305, 2017.\nMikolov, T., Karaﬁ´at, M., Burget, L., ˇCernock`y, J., and\nKhudanpur, S. Recurrent neural network based language\nmodel. In Eleventh Annual Conference of the Interna-\ntional Speech Communication Association , 2010.\nMikolov, T., Sutskever, I., Chen, K., Corrado, G. S., and\nDean, J. Distributed representations of words and phrases\nand their compositionality. In NIPS, pp. 3111–3119,\n2013.\nOuyang, W., Li, H., Zeng, X., and Wang, X. Learning deep\nrepresentation with large-scale attributes. In CVPR, pp.\n1895–1903, 2015.\nPennington, J., Socher, R., and Manning, C. Glove: Global\nvectors for word representation. In EMNLP, pp. 1532–\n1543, 2014.\nPeters, M., Neumann, M., Iyyer, M., Gardner, M., Clark, C.,\nLee, K., and Zettlemoyer, L. Deep contextualized word\nrepresentations. In NAACL, volume 1, pp. 2227–2237,\n2018.\nRadford, A., Narasimhan, K., Salimans, T., and Sutskever,\nI. Improving language understanding by generative pre-\ntraining. 2018.\nRajpurkar, P., Zhang, J., Lopyrev, K., and Liang, P. Squad:\n100,000+ questions for machine comprehension of text.\nCoRR, 2016.\nRamachandran, P., Liu, P. J., and Le, Q. V . Unsupervised\npretraining for sequence to sequence learning. CoRR,\nabs/1611.02683, 2016.\nRen, Y ., Tan, X., Qin, T., Zhao, S., Zhao, Z., and Liu, T.-Y .\nAlmost unsupervised text to speech and automatic speech\nrecognition. In ICML, 2019.\nSennrich, R., Haddow, B., and Birch, A. Neural machine\ntranslation of rare words with subword units. In ACL,\nvolume 1, pp. 1715–1725, 2016.\nShang, L., Lu, Z., and Li, H. Neural responding machine\nfor short-text conversation. In ACL, volume 1, pp. 1577–\n1586, 2015.\nShen, Y ., Tan, X., He, D., Qin, T., and Liu, T.-Y . Dense\ninformation ﬂow for neural machine translation. In Pro-\nceedings of the 2018 Conference of the North American\nChapter of the Association for Computational Linguistics:\nHuman Language Technologies, V olume 1 (Long Papers),\npp. 1294–1303, June 2018.\nSocher, R., Perelygin, A., Wu, J., Chuang, J., Manning,\nC. D., Ng, A., and Potts, C. Recursive deep models for\nsemantic compositionality over a sentiment treebank. In\nEMNLP, pp. 1631–1642, 2013.\nSong, K., Tan, X., He, D., Lu, J., Qin, T., and Liu, T.-Y .\nDouble path networks for sequence to sequence learning.\nIn Proceedings of the 27th International Conference on\nComputational Linguistics, pp. 3064–3074, 2018.\nSuzuki, J. and Nagata, M. Cutting-off redundant repeating\ngenerations for neural abstractive summarization. In ACL,\npp. 291–297, 2017.\nSzegedy, C., Liu, W., Jia, Y ., Sermanet, P., Reed, S.,\nAnguelov, D., Erhan, D., Vanhoucke, V ., and Rabinovich,\nA. Going deeper with convolutions. In CVPR, pp. 1–9,\n2015.\nTan, X., Ren, Y ., He, D., Qin, T., and Liu, T.-Y . Multilingual\nneural machine translation with knowledge distillation.\nIn ICLR, 2019.\nTjong Kim Sang, E. F. and De Meulder, F. Introduction to\nthe conll-2003 shared task: Language-independent named\nentity recognition. In NAACL, pp. 142–147. Association\nfor Computational Linguistics, 2003.\nVaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones,\nL., Gomez, A. N., Kaiser, L., and Polosukhin, I. Attention\nis all you need. In NIPS, pp. 6000–6010, 2017.\nVincent, P., Larochelle, H., Bengio, Y ., and Manzagol, P.-A.\nExtracting and composing robust features with denoising\nautoencoders. In ICML, pp. 1096–1103. ACM, 2008.\nVinyals, O. and Le, Q. V . A neural conversational model.\nCoRR, abs/1506.05869, 2015.\nWang, A., Singh, A., Michael, J., Hill, F., Levy, O., and\nBowman, S. R. Glue: A multi-task benchmark and analy-\nsis platform for natural language understanding. CoRR,\nabs/1804.07461, 2018.\nWu, Y ., Schuster, M., Chen, Z., Le, Q. V ., Norouzi, M.,\nMacherey, W., Krikun, M., Cao, Y ., Gao, Q., Macherey,\nK., Klingner, J., Shah, A., Johnson, M., Liu, X., Kaiser,\nL., Gouws, S., Kato, Y ., Kudo, T., Kazawa, H., Stevens,\nK., Kurian, G., Patil, N., Wang, W., Young, C., Smith, J.,\nRiesa, J., Rudnick, A., Vinyals, O., Corrado, G., Hughes,\nM., and Dean, J. Google’s neural machine translation\nMASS: Masked Sequence to Sequence Pre-training for Language Generation\nsystem: Bridging the gap between human and machine\ntranslation. CoRR, abs/1609.08144, 2016.\nYang, Z., Chen, W., Wang, F., and Xu, B. Unsupervised\nneural machine translation with weight sharing. In ACL,\npp. 46–55, 2018.\nYuan, X., Wang, T., Gulcehre, C., Sordoni, A., Bachman, P.,\nZhang, S., Subramanian, S., and Trischler, A. Machine\ncomprehension by text-to-text neural question generation.\nIn Proceedings of the 2nd Workshop on Representation\nLearning for NLP, pp. 15–25, 2017.\nZhang, J. and Zong, C. Exploiting source-side monolingual\ndata in neural machine translation. In EMNLP, pp. 1535–\n1545, 2016.\nZoph, B., Yuret, D., May, J., and Knight, K. Transfer\nlearning for low-resource neural machine translation. In\nEMNLP, pp. 1568–1575, 2016.",
  "values": {
    "Non-maleficence": "Yes",
    "Beneficence": "Yes",
    "Justice": "Yes",
    "Fairness": "Yes",
    "Respect for Law and public interest": "Yes",
    "Explicability": "Yes",
    "Respect for Persons": "Yes",
    "Not socially biased": "Yes",
    "Collective influence": "Yes",
    "Critiqability": "Yes",
    "User influence": "Yes",
    "Privacy": "Yes",
    "Deferral to humans": "Yes",
    "Transparent (to users)": "Yes",
    "Autonomy (power to decide)": "Yes",
    "Interpretable (to users)": "Yes"
  }
}