{
  "pdf": "NIPS-2008-deflation-methods-for-sparse-pca-Paper",
  "title": "Deflation Methods for Sparse PCA",
  "author": "Lester W. Mackey",
  "paper_id": "NIPS-2008-deflation-methods-for-sparse-pca-Paper",
  "text": "Deﬂation Methods for Sparse PCA\nLester Mackey\nC\nomputer Science Division\nUniversity of California, Berkeley\nBerkeley, CA 94703\nAbstract\nIn analogy to the PCA setting, the sparse PCA problem is often solved by iter-\natively alternating between two subtasks: cardinality-constrained rank-one vari-\nance maximization and matrix deﬂation. While the former has received a great\ndeal of attention in the literature, the latter is seldom analyzed and is typically\nborrowed without justiﬁcation from the PCA context. In this work, we demon-\nstrate that the standard PCA deﬂation procedure is seldom appropriate for the\nsparse PCA setting. To rectify the situation, we ﬁrst develop several deﬂation al-\nternatives better suited to the cardinality-constrained context. We then reformulate\nthe sparse PCA optimization problem to explicitly reﬂect the maximumadditional\nvariance objective on each round. The result is a generalized deﬂation procedure\nthat typically outperforms more standard techniques on real-world datasets.\n1 Introduction\nPrincipal component analysis (PCA) is a popular change of variables technique used in data com-\npression, predictive modeling, and visualization. The goal of PCA is to extract several principal\ncomponents, linear combinations of input variables that together best account for the variance in a\ndata set. Often, PCA is formulated as an eigenvalue decomposition problem: each eigenvector of\nthe sample covariance matrix of a data set corresponds to the loadings or coefﬁcients of a principal\ncomponent. A common approach to solving this partial eigenvalue decomposition is to iteratively\nalternate between two subproblems: rank-one variance maximization and matrix deﬂation. The ﬁrst\nsubproblem involves ﬁnding the maximum-variance loadings vector for a given sample covariance\nmatrix or, equivalently, ﬁnding the leading eigenvector of the matrix. The second involves modifying\nthe covariance matrix to eliminate the inﬂuence of that eigenvector.\nA primary drawback of PCA is its lack of sparsity. Each principal component is a linear combination\nof all variables, and the loadings are typically non-zero. Sparsity is desirable as it often leads to\nmore interpretable results, reduced computation time, and improved generalization. Sparse PCA\n[8, 3, 16, 17, 6, 18, 1, 2, 9, 10, 12] injects sparsity into the PCA process by searching for “pseudo-\neigenvectors”, sparse loadings that explain a maximal amount variance in the data.\nIn analogy to the PCA setting, many authors attempt to solve the sparse PCA problem by itera-\ntively alternating between two subtasks: cardinality-constrained rank-one variance maximization\nand matrix deﬂation. The former is an NP-hard problem, and a variety of relaxations and approx-\nimate solutions have been developed in the literature [1, 2, 9, 10, 12, 16, 17]. The latter subtask\nhas received relatively little attention and is typically borrowed without justiﬁcation from the PCA\ncontext. In this work, we demonstrate that the standard PCA deﬂation procedure is seldom appro-\npriate for the sparse PCA setting. To rectify the situation, we ﬁrst develop several heuristic deﬂation\nalternatives with more desirable properties. We then reformulate the sparse PCA optimization prob-\nlem to explicitly reﬂect the maximum additional variance objective on each round. The result is a\ngeneralized deﬂation procedure that typically outperforms more standard techniques on real-world\ndatasets.\n1\nThe remainder of the paper is organized as follows. In Section 2 we discuss matrix deﬂation as it re-\nlates to PCA and sparse PCA. We examine the failings of typical PCA deﬂation in the sparse setting\nand develop several alternative deﬂation procedures. In Section 3, we present a reformulation of the\nstandard iterative sparse PCA optimization problem and derive a generalized deﬂation procedure\nto solve the reformulation. Finally, in Section 4, we demonstrate the utility of our newly derived\ndeﬂation techniques on real-world datasets.\nNotation\nI is the identity matrix. Sp\n+ is the set of all symmetric, positive semideﬁnite matrices in Rp×p.\nCard(x) represents the cardinality of or number of non-zero entries in the vector x.\n2 Deﬂation methods\nA matrix deﬂation modiﬁes a matrix to eliminate the inﬂuence of a given eigenvector, typically by\nsetting the associated eigenvalue to zero (see [14] for a more detailed discussion). We will ﬁrst\ndiscuss deﬂation in the context of PCA and then consider its extension to sparse PCA.\n2.1 Hotelling’s deﬂation and PCA\nIn the PCA setting, the goal is to extract the r leading eigenvectors of the sample covariance matrix,\nA0 ∈Sp\n+, as its eigenvectors are equivalent to the loadings of the ﬁrst r principal components.\nHotelling’s deﬂation method [11] is a simple and popular technique for sequentially extracting these\neigenvectors. On the t-th iteration of the deﬂation method, we ﬁrst extract the leading eigenvector\nof At−1,\nxt = argmax\nx:xT x=1\nxT At−1x (1)\nand we then use Hotelling’s deﬂation to annihilate xt:\nAt = At−1−xtxT\nt At−1xtxT\nt . (2)\nThe deﬂation step ensures that the t + 1-st leading eigenvector of A0 is the leading eigenvector of\nAt. The following proposition explains why.\nProposition 2.1. If λ1≥. . .≥λp are the eigenvalues ofA∈Sp\n+, x1, . . . , xp are the corresponding\neigenvectors, and ˆA = A−xjxT\nj AxjxT\nj for some j∈1, . . . , p, then ˆA has eigenvectors x1, . . . , xp\nwith corresponding eigenvalues λ1, . . . , λj−1, 0, λj+1, . . . , λp.\nPROOF.\nˆAxj = Axj−xjxT\nj AxjxT\nj xj = Axj−xjxT\nj Axj = λjxj−λjxj = 0xj.\nˆAxi = Axi−xjxT\nj AxjxT\nj xi = Axi−0 = λixi,∀i̸= j.\nThus, Hotelling’s deﬂation preserves all eigenvectors of a m atrix and annihilates a selected eigen-\nvalue while maintaining all others. Notably, this implies that Hotelling’s deﬂation preserves positive-\nsemideﬁniteness. In the case of our iterative deﬂation method, annihilating the t-th leading eigen-\nvector of A0 renders the t + 1-st leading eigenvector dominant in the next round.\n2.2 Hotelling’s deﬂation and sparse PCA\nIn the sparse PCA setting, we seek r sparse loadings which together capture the maximum amount\nof variance in the data. Most authors [1, 9, 16, 12] adopt the additional constraint that the loadings\nbe produced in a sequential fashion. To ﬁnd the ﬁrst such ”pseudo-eigenvector”, we can consider a\ncardinality-constrained version of Eq. (1):\nx1 = argmax\nx:xT x=1,Card(x)≤k1\nxT A0x. (3)\n2\nThat leaves us with the question of how to best extract subsequ ent pseudo-eigenvectors. A common\napproach in the literature [1, 9, 16, 12] is to borrow the iterative deﬂation method of the PCA setting.\nTypically, Hotelling’s deﬂation is utilized by substituting an extracted pseudo-eigenvector for a true\neigenvector in the deﬂation step of Eq. (2). This substitution, however, is seldom justiﬁed, for the\nproperties of Hotelling’s deﬂation, discussed in Section 2.1, depend crucially on the use of a true\neigenvector.\nTo see what can go wrong when Hotelling’s deﬂation is applied to a non-eigenvector, consider the\nfollowing example.\nExample. Let C =\n(\n2 1\n1 1\n)\n, a 2×2 matrix. The eigenvalues of C are λ1 = 2 .6180 and λ2 =\n.3820. Let x = (1 , 0)T , a sparse pseudo-eigenvector, and ˆC = C−xxT Cxx T , the corresponding\ndeﬂated matrix. Then ˆC =\n(\n0 1\n1 1\n)\nwith eigenvalues ˆλ1 = 1 .6180 and ˆλ2 =−.6180. Thus,\nHotelling’s deﬂation does not in general preserve positive-semideﬁniteness when applied to a non-\neigenvector.\nThat Sp\n+ is not closed under pseudo-eigenvector Hotelling’s deﬂation is a serious failing, for most\niterative sparse PCA methods assume a positive-semideﬁnite matrix on each iteration. A second,\nrelated shortcoming of pseudo-eigenvector Hotelling’s deﬂation is its failure to render a pseudo-\neigenvector orthogonal to a deﬂated matrix. IfA is our matrix of interest,x is our pseudo-eigenvector\nwith variance λ = xT Ax, and ˆA = A−xxT AxxT is our deﬂated matrix, then ˆAx = Ax−\nxxT AxxT x = Ax−λx is zero iff x is a true eigenvector. Thus, even though the “variance” of\nx w.r.t. ˆA is zero (x T ˆAx = xT Ax−xT xxT AxxT x = λ−λ = 0 ), “covariances” of the form\nyT ˆAx for y̸= x may still be non-zero. This violation of the Cauchy-Schwarz inequality betrays a\nlack of positive-semideﬁniteness and may encourage the reappearance ofx as a component of future\npseudo-eigenvectors.\n2.3 Alternative deﬂation techniques\nIn this section, we will attempt to rectify the failings of pseudo-eigenvector Hotelling’s deﬂation by\nconsidering several alternative deﬂation techniques better suited to the sparse PCA setting. Note\nthat any deﬂation-based sparse PCA method (e.g. [1, 9, 16, 12]) can utilize any of the deﬂation\ntechniques discussed below.\n2.3.1 Projection deﬂation\nGiven a data matrix Y ∈Rn×p and an arbitrary unit vector in x∈Rp, an intuitive way to remove\nthe contribution of x from Y is to project Y onto the orthocomplement of the space spanned by x:\nˆY = Y (I−xxT ). If A is the sample covariance matrix of Y , then the sample covariance of ˆY is\ngiven by ˆA = (I−xxT )A(I−xxT ), which leads to our formulation for projection deﬂation:\nProjection deﬂation\nAt = At−1−xtxT\nt At−1−At−1xtxT\nt + xtxT\nt At−1xtxT\nt = (I−xtxT\nt )At−1(I−xtxT\nt ) (4)\nNote that when xt is a true eigenvector of At−1 with eigenvalue λt, projection deﬂation reduces to\nHotelling’s deﬂation:\nAt = At−1−xtxT\nt At−1−At−1xtxT\nt + xtxT\nt At−1xtxT\nt\n= At−1−λtxtxT\nt −λtxtxT\nt + λtxtxT\nt\n= At−1−xtxT\nt At−1xtxT\nt .\nHowever, in the general case, when xt is not a true eigenvector, projection deﬂation maintains the\ndesirable properties that were lost to Hotelling’s deﬂation. For example, positive-semideﬁniteness\nis preserved:\n∀y, yT Aty = yT (I−xtxT\nt )At−1(I−xtxT\nt )y = zT At−1z\nwhere z = ( I−xtxT\nt )y. Thus, if At−1∈Sp\n+, so is At. Moreover, At is rendered left and right\northogonal to xt, as (I−xtxT\nt )xt = xt−xt = 0 and At is symmetric. Projection deﬂation therefore\nannihilates all covariances with xt:∀v, vT Atxt = xT\nt Atv = 0.\n3\n2.3.2 Schur complement deﬂation\nS\nince our goal in matrix deﬂation is to eliminate the inﬂuence, as measured through variance and\ncovariances, of a newly discovered pseudo-eigenvector, it is reasonable to consider the conditional\nvariance of our data variables given a pseudo-principal component. While this conditional variance\nis non-trivial to compute in general, it takes on a simple closed form when the variables are normally\ndistributed. Let x∈Rp be a unit vector and W∈Rp be a Gaussian random vector, representing the\njoint distribution of the data variables. If W has covariance matrix Σ, then (W, W x)has covariance\nmatrix V =\n( Σ Σx\nxT Σ xT Σx\n)\n, and V ar(W|W x) = Σ −ΣxxT Σ\nxT Σx w henever xT Σx̸= 0 [15].\nThat is, the conditional variance is the Schur complement of the vector variance xT Σx in the full\ncovariance matrix V . By substituting sample covariance matrices for their population counterparts,\nwe arrive at a new deﬂation technique:\nSchur complement deﬂation\nAt = At−1−At−1xtxT\nt At−1\nxT\nt At−1xt\n( 5)\nSchur complement deﬂation, like projection deﬂation, preserves positive-semideﬁniteness. To\nsee this, suppose At−1 ∈Sp\n+. Then, ∀v, vT Atv = vT At−1v−vT At−1xtxT\nt At−1v\nxT\nt At−1xt\n≥0 a s\nvT At−1vxT\nt At−1xt−(vT At−1xt)2≥0 by the Cauchy-Schwarz inequality and xT\nt At−1xt≥0\nas At−1∈Sp\n+.\nFurthermore, Schur complement deﬂation renders xt left and right orthogonal to At, since At is\nsymmetric and Atxt = At−1xt−At−1xtxT\nt At−1xt\nxT\nt At−1xt\n= At−1xt−At−1xt = 0.\nAdditionally, Schur complement deﬂation reduces to Hotelling’s deﬂation whenxt is an eigenvector\nof At−1 with eigenvalue λt̸= 0:\nAt = At−1−At−1xtxT\nt At−1\nxT\nt At−1xt\n= At−1−λtxtxT\nt λt\nλt\n= At−1−xtxT\nt At−1xtxT\nt .\nWh\nile we motivated Schur complement deﬂation with a Gaussianity assumption, the technique ad-\nmits a more general interpretation as a column projection of a data matrix. Suppose Y ∈Rn×p\nis a mean-centered data matrix, x∈Rp has unit norm, and ˆY = ( I−Y xxT Y T\n||Y x ||2 )Y , the projection\nof the columns of Y onto the orthocomplement of the space spanned by the pseudo-principal com-\nponent, Y x. If Y has sample covariance matrix A, then the sample covariance of ˆY is given by\nˆA = 1\nn Y T (I−Y\nxxT Y T\n||Y x ||2 )T (I−Y xxT Y T\n||Y x ||2 )Y = 1\nn Y T (I−Y\nxxT Y T\n||Y x ||2 )Y = A−AxxT A\nxT A x .\n2.3.3 Orthogonalized deﬂation\nWhile projection deﬂation and Schur complement deﬂation address the concerns raised by per-\nforming a single deﬂation in the non-eigenvector setting, new difﬁculties arise when we attempt to\nsequentially deﬂate a matrix with respect to a series of non-orthogonal pseudo-eigenvectors.\nWhenever we deal with a sequence of non-orthogonal vectors, we must take care to distinguish\nbetween the variance explained by a vector and the additional variance explained, given all pre-\nvious vectors. These concepts are equivalent in the PCA setting, as true eigenvectors of a matrix\nare orthogonal, but, in general, the vectors extracted by sparse PCA will not be orthogonal. The\nadditional variance explained by the t-th pseudo-eigenvector, xt, is equivalent to the variance ex-\nplained by the component ofxt orthogonal to the space spanned by all previous pseudo-eigenvectors,\nqt = xt−Pt−1xt, wherePt−1 is the orthogonal projection onto the space spanned byx1, . . . , xt−1.\nOn each deﬂation step, therefore, we only want to eliminate the variance associated with qt. Anni-\nhilating the full vector xt will often lead to “double counting” and could re-introduce components\nparallel to previously annihilated vectors. Consider the following example:\n4\nExample. L et C0 = I. If we apply projection deﬂation w.r.t. x1 = (\n√\n2\n2 ,\n√\n2\n2 )T , the result is\nC1 =\n( 1\n2 −1\n2\n−1\n2\n1\n2\n)\n,\nand x1 is orthogonal to C1. If we next apply projection deﬂation to C1 w.r.t.\nx2 = (1, 0)T , the result, C2 =\n( 0 0\n0 1\n2\n)\n,\nis no longer orthogonal to x1.\nThe authors of [12] consider this issue of non-orthogonality in the context of Hotelling’s deﬂation.\nTheir modiﬁed deﬂation procedure is equivalent to Hotelling’s deﬂation (Eq. (2)) for t = 1 and can\nbe easily expressed in terms of a running Gram-Schmidt decomposition for t > 1:\nOrthogonalized Hotelling’s deﬂation (OHD)\nqt = (I−Qt−1QT\nt−1)xt⏐⏐⏐\n⏐(I−Qt−1QT\nt−1)xt\n⏐⏐⏐\n⏐ (\n 6)\nAt = At−1−qtqT\nt At−1qtqT\nt\nwhere q1 = x1, and q1, . . . , qt−1 form the columns ofQt−1. Since q1, . . . , qt−1 form an orthonormal\nbasis for the space spanned by x1, . . . , xt−1, we have that Qt−1QT\nt−1 =Pt−1, the aforementioned\northogonal projection.\nSince the ﬁrst round of OHD is equivalent to a standard application of Hotelling’s deﬂation, OHD\ninherits all of the weaknesses discussed in Section 2.2. However, the same principles may be applied\nto projection deﬂation to generate an orthogonalized variant that inherits its desirable properties.\nSchur complement deﬂation is unique in that it preserves orthogonality in all subsequent rounds.\nThat is, if a vector v is orthogonal to At−1 for any t, then Atv = At−1v−At−1xtxT\nt At−1v\nxT\nt At−1xt\n= 0 as\nAt−1v = 0. This further implies the following proposition.\nProposition 2.2. Orthogonalized Schur complement deﬂation is equivalent to Schur complement\ndeﬂation.\nProof. Consider the t-th round of Schur complement deﬂation. We may write xt = ot + pt, where\npt is in the subspace spanned by all previously extracted pseudo-eigenvectors and ot is orthogonal\nto this subspace. Then we know that At−1pt = 0 , as pt is a linear combination of x1, . . . , xt−1,\nand At−1xi = 0 ,∀i < t. Thus,xT\nt Atxt = pT\nt Atpt + oT\nt Atpt + pT\nt Atot + oT\nt Atot = oT\nt Atot.\nFurther, At−1xtxT\nt At−1 = At−1ptpT\nt At−1 +At−1ptoT\nt At−1 +At−1otpT\nt At−1 +At−1otoT\nt At−1 =\nAt−1otoT\nt At−1. Hence, At = At−1−At−1otoT\nt At−1\noT\nt At−1ot\n= At−1−At−1qtqT\nt At−1\nqT\nt At−1qt\na s qt = ot\n||ot| | .\nTable 1 compares the properties of the various deﬂation techn iques studied in this section.\nMethod xT\nt Atxt = 0 Atxt = 0 At∈Sp\n+ Asxt = 0 ,∀s > t\nHotelling’s ✓ × × ×\nProjection ✓ ✓ ✓ ×\nSchur complement ✓ ✓ ✓ ✓\nOrth. Hotelling’s ✓ × × ×\nOrth. Projection ✓ ✓ ✓ ✓\nTable 1: Summary of sparse PCA deﬂation method properties\n3\nReformulating sparse PCA\nIn the previous section, we focused on heuristic deﬂation techniques that allowed us to reuse the\ncardinality-constrained optimization problem of Eq. (3). In this section, we explore a more princi-\npled alternative: reformulating the sparse PCA optimization problem to explicitly reﬂect our maxi-\nmization objective on each round.\nRecall that the goal of sparse PCA is to ﬁnd r cardinality-constrained pseudo-eigenvectors which\ntogether explain the most variance in the data. If we additionally constrain the sparse loadings to\n5\nbe generated sequentially, as in the PCA setting and the previ ous section, then a greedy approach of\nmaximizing the additional variance of each new vector naturally suggests itself.\nOn round t, the additional variance of a vector x is given by qT A0q\nqT q w here A0 is the data covari-\nance matrix, q = ( I−Pt−1)x, and Pt−1 is the projection onto the space spanned by previous\npseudo-eigenvectors x1, . . . , xt−1. As qT q = xT (I−Pt−1)(I−Pt−1)x = xT (I−Pt−1)x, max-\nimizing additional variance is equivalent to solving a cardinality-constrained maximum generalized\neigenvalue problem,\nmax\nx\nxT (I−Pt−1)A0(I−Pt−1)x\nsubject to xT (I−Pt−1)x = 1\nCard(x)≤kt.\n(7)\nIf we let qs = (I−Ps−1)xs,∀s≤t−1, then q1, . . . , qt−1 form an orthonormal basis for the space\nspanned by x1, . . . , xt−1. Writing I−Pt−1 = I−∑t−1\ns=1 qsqT\ns = ∏t−1\ns=1 (I−qsqT\ns ) suggests a\ngeneralized deﬂation technique that leads to the solution of Eq. (7) on each round. We imbed the\ntechnique into the following algorithm for sparse PCA:\nAlgorithm 1 G eneralized Deﬂation Method for Sparse PCA\nGiven: A0∈Sp\n+, r∈N,{k1, . . . , kr}⊂N\nExecute:\n1. B0←I\n2. For t := 1, . . . , r\n•xt← argmax\nx:xT Bt−1x=1,Card(x)≤kt\nxT At−1x\n•qt←Bt−1xt\n•At←(I−qtqT\nt )At−1(I−qtqT\nt )\n•Bt←Bt−1(I−qtqT\nt )\n•xt←xt/||xt||\nReturn:{x1, . . . , xr}\nAdding a cardinality constraint to a maximum eigenvalue prob lem renders the optimization problem\nNP-hard [10], but any of several leading sparse eigenvalue methods, including GSLDA of [10],\nDCPCA of [12], and DSPCA of [1] (with a modiﬁed trace constraint), can be adapted to solve this\ncardinality-constrained generalized eigenvalue problem.\n4 Experiments\nIn this section, we present several experiments on real world datasets to demonstrate the value added\nby our newly derived deﬂation techniques. We run our experiments with Matlab implementations\nof DCPCA [12] (with the continuity correction of [9]) and GSLDA [10], ﬁtted with each of the\nfollowing deﬂation techniques: Hotelling’s (HD), projection (PD), Schur complement (SCD), or-\nthogonalized Hotelling’s (OHD), orthogonalized projection (OPD), and generalized (GD).\n4.1 Pit props dataset\nThe pit props dataset [5] with 13 variables and 180 observations has become a de facto standard for\nbenchmarking sparse PCA methods. To demonstrate the disparate behavior of differing deﬂation\nmethods, we utilize each sparse PCA algorithm and deﬂation technique to successively extract six\nsparse loadings, each constrained to have cardinality less than or equal to kt = 4 . We report the\nadditional variances explained by each sparse vector in Table 2 and the cumulative percentage vari-\nance explained on each iteration in Table 3. For reference, the ﬁrst 6 true principal components of\nthe pit props dataset capture 87% of the variance.\n6\nDCPCA GSLDA\nHD PD SCD OHD OPD GD HD PD SCD OHD OPD GD\n2.938 2.938 2.938 2.938 2.938 2.938 2.938 2.938 2.938 2.938 2.938 2.938\n2.209 2.209 2.076 2.209 2.209 2.209 2.107 2.280 2.065 2.107 2.280 2.280\n0.935 1.464 1.926 0.935 1.464 1.477 1.988 2.067 2.243 1.985 2.067 2.072\n1.301 1.464 1.164 0.799 1.464 1.464 1.352 1.304 1.120 1.335 1.305 1.360\n1.206 1.057 1.477 0.901 1.058 1.178 1.067 1.120 1.164 0.497 1.125 1.127\n0.959 0.980 0.725 0.431 0.904 0.988 0.557 0.853 0.841 0.489 0.852 0.908\nTable 2: Additional variance explained by each of the ﬁrst 6 sp arse loadings extracted from the Pit\nProps dataset.\nOn the DCPCA run, Hotelling’s deﬂation explains 73.4% of the variance, while the best performing\nmethods, Schur complement deﬂation and generalized deﬂation, explain approximately 79% of the\nvariance each. Projection deﬂation and its orthogonalized variant also outperform Hotelling’s deﬂa-\ntion, while orthogonalized Hotelling’s shows the worst performance with only 63.2% of the variance\nexplained. Similar results are obtained when the discrete method of GSLDA is used. Generalized\ndeﬂation and the two projection deﬂations dominate, with GD achieving the maximum cumulative\nvariance explained on each round. In contrast, the more standard Hotelling’s and orthogonalized\nHotelling’s underperform the remaining techniques.\nDCPCA GSLDA\nHD PD SCD OHD OPD GD HD PD SCD OHD OPD GD\n22.6% 22.6% 22.6% 22.6% 22.6% 22.6% 22.6% 22.6% 22.6% 22.6% 22.6% 22.6%\n39.6% 39.6% 38.6% 39.6% 39.6% 39.6% 38.8% 40.1% 38.5% 38.8% 40.1% 40.1%\n46.8% 50.9% 53.4% 46.8% 50.9% 51.0% 54.1% 56.0% 55.7% 54.1% 56.0% 56.1%\n56.8% 62.1% 62.3% 52.9% 62.1% 62.2% 64.5% 66.1% 64.4% 64.3% 66.1% 66.5%\n66.1% 70.2% 73.7% 59.9% 70.2% 71.3% 72.7% 74.7% 73.3% 68.2% 74.7% 75.2%\n73.4% 77.8% 79.3% 63.2% 77.2% 78.9% 77.0% 81.2% 79.8% 71.9% 81.3% 82.2%\nTable 3: Cumulative percentage variance explained by the ﬁrs t 6 sparse loadings extracted from the\nPit Props dataset.\n4.2 Gene expression data\nThe Berkeley Drosophila Transcription Network Project (BDTNP) 3D gene expression data\n[4] contains gene expression levels measured in each nucleus of developing Drosophila em-\nbryos and averaged across many embryos and developmental stages. Here, we analyze 0-\n3 1160524183713 s10436-29ap05-02.vpc, an aggregate VirtualEmbryo contain ing 21 genes and\n5759 example nuclei. We run GSLDA for eight iterations with cardinality pattern 9,7,6,5,3,2,2,2\nand report the results in Table 4.\nGSLDA additional variance explained GSLDA cumulative percentage variance\nHD PD SCD OHD OPD GD HD PD SCD OHD OPD GD\nPC 1 1.784 1.784 1.784 1.784 1.784 1.784 21.0% 21.0% 21.0% 21.0% 21.0% 21.0%\nPC 2 1.464 1.453 1.453 1.464 1.453 1.466 38.2% 38.1% 38.1% 38.2% 38.1% 38.2%\nPC 3 1.178 1.178 1.179 1.176 1.178 1.187 52.1% 51.9% 52.0% 52.0% 51.9% 52.2%\nPC 4 0.716 0.736 0.716 0.713 0.721 0.743 60.5% 60.6% 60.4% 60.4% 60.4% 61.0%\nPC 5 0.444 0.574 0.571 0.460 0.571 0.616 65.7% 67.4% 67.1% 65.9% 67.1% 68.2%\nPC 6 0.303 0.306 0.278 0.354 0.244 0.332 69.3% 71.0% 70.4% 70.0% 70.0% 72.1%\nPC 7 0.271 0.256 0.262 0.239 0.313 0.304 72.5% 74.0% 73.4% 72.8% 73.7% 75.7%\nPC 8 0.223 0.239 0.299 0.257 0.245 0.329 75.1% 76.8% 77.0% 75.9% 76.6% 79.6%\nTable 4: Additional variance and cumulative percentage vari ance explained by the ﬁrst 8 sparse\nloadings of GSLDA on the BDTNP VirtualEmbryo.\nThe results of the gene expression experiment show a clear hierarchy among the deﬂation methods.\nThe generalized deﬂation technique performs best, achieving the largest additional variance on every\nround and a ﬁnal cumulative variance of 79.6%. Schur complement deﬂation, projection deﬂation,\nand orthogonalized projection deﬂation all perform comparably, explaining roughly 77% of the total\nvariance after 8 rounds. In last place are the standard Hotelling’s and orthogonalized Hotelling’s\ndeﬂations, both of which explain less than 76% of variance after 8 rounds.\n7\n5 Conclusion\nI\nn this work, we have exposed the theoretical and empirical shortcomings of Hotelling’s deﬂation in\nthe sparse PCA setting and developed several alternative methods more suitable for non-eigenvector\ndeﬂation. Notably, the utility of these procedures is not limited to the sparse PCA setting. Indeed,\nthe methods presented can be applied to any of a number of constrained eigendecomposition-based\nproblems, including sparse canonical correlation analysis [13] and linear discriminant analysis [10].\nAcknowledgments\nThis work was supported by AT&T through the AT&T Labs Fellowship Program.\nReferences\n[1] A. d’Aspremont, L. El Ghaoui, M. I. Jordan, and G. R. G. Lanckriet. A Direct Formulation for\nSparse PCA using Semideﬁnite Programming. In Advances in Neural Information Processing\nSystems (NIPS). Vancouver, BC, December 2004.\n[2] A. d’Aspremont, F. R. Bach, and L. E. Ghaoui. Full regularization path for sparse principal\ncomponent analysis. In Proceedings of the 24th international Conference on Machine Learn-\ning. Z. Ghahramani, Ed. ICML ’07, vol. 227. ACM, New York, NY , 177-184, 2007.\n[3] J. Cadima and I. Jolliffe. Loadings and correlations in the interpretation of principal compo-\nnents. Applied Statistics, 22:203.214, 1995.\n[4] C.C. Fowlkes, C.L. Luengo Hendriks, S.V . Kernen, G.H. Weber, O. Rbel, M.-Y . Huang, S.\nChatoor, A.H. DePace, L. Simirenko and C. Henriquez et al. Cell 133, pp. 364-374, 2008.\n[5] J. Jeffers. Two case studies in the application of principal components. Applied Statistics, 16,\n225-236, 1967.\n[6] I.T. Jolliffe and M. Uddin. A Modiﬁed Principal Component Technique based on the Lasso.\nJournal of Computational and Graphical Statistics, 12:531.547, 2003.\n[7] I.T. Jolliffe, Principal component analysis, Springer Verlag, New York, 1986.\n[8] I.T. Jolliffe. Rotation of principal components: choice of normalization constraints. Journal of\nApplied Statistics, 22:29-35, 1995.\n[9] B. Moghaddam, Y . Weiss, and S. Avidan. Spectral bounds for sparse PCA: Exact and greedy\nalgorithms. Advances in Neural Information Processing Systems, 18, 2006.\n[10] B. Moghaddam, Y . Weiss, and S. Avidan. Generalized spectral bounds for sparse LDA. In Proc.\nICML, 2006.\n[11] Y . Saad, Projection and deﬂation methods for partial pole assignment in linear state feedback,\nIEEE Trans. Automat. Contr., vol. 33, pp. 290-297, Mar. 1998.\n[12] B.K. Sriperumbudur, D.A. Torres, and G.R.G. Lanckriet. Sparse eigen methods by DC pro-\ngramming. Proceedings of the 24th International Conference on Machine learning, pp. 831-\n838, 2007.\n[13] D. Torres, B.K. Sriperumbudur, and G. Lanckriet. Finding Musically Meaningful Words by\nSparse CCA. Neural Information Processing Systems (NIPS) Workshop on Music, the Brain\nand Cognition, 2007.\n[14] P. White. The Computation of Eigenvalues and Eigenvectors of a Matrix. Journal of the Society\nfor Industrial and Applied Mathematics, V ol. 6, No. 4, pp. 393-437, Dec., 1958.\n[15] F. Zhang (Ed.). The Schur Complement and Its Applications. Kluwer, Dordrecht, Springer,\n2005.\n[16] Z. Zhang, H. Zha, and H. Simon, Low-rank approximations with sparse factors I: Basic algo-\nrithms and error analysis. SIAM J. Matrix Anal. Appl., 23 (2002), pp. 706-727.\n[17] Z. Zhang, H. Zha, and H. Simon, Low-rank approximations with sparse factors II: Penalized\nmethods with discrete Newton-like iterations. SIAM J. Matrix Anal. Appl., 25 (2004), pp.\n901-920.\n[18] H. Zou, T. Hastie, and R. Tibshirani. Sparse Principal Component Analysis. Technical Report,\nStatistics Department, Stanford University, 2004.\n8",
  "values": {
    "Not socially biased": "No",
    "Justice": "No",
    "Respect for Persons": "No",
    "Transparent (to users)": "No",
    "Non-maleficence": "No",
    "Fairness": "No",
    "Respect for Law and public interest": "No",
    "Privacy": "No",
    "Deferral to humans": "No",
    "Beneficence": "No",
    "Interpretable (to users)": "No",
    "User influence": "No",
    "Collective influence": "No",
    "Autonomy (power to decide)": "No",
    "Explicability": "No",
    "Critiqability": "No"
  }
}