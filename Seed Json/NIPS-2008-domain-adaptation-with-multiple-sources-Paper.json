{
  "pdf": "NIPS-2008-domain-adaptation-with-multiple-sources-Paper",
  "title": "Domain Adaptation with Multiple Sources",
  "author": "Yishay Mansour, Mehryar Mohri, Afshin Rostamizadeh",
  "paper_id": "NIPS-2008-domain-adaptation-with-multiple-sources-Paper",
  "text": "Domain Adaptation with Multiple Sources\nYishay Mansour\nG\noogle Research and\nTel Aviv Univ.\nmansour@tau.ac.il\nMehryar Mohri\nCourant Institute and\nGoogle Research\nmohri@cims.nyu.edu\nAfshin Rostamizadeh\nCourant Institute\nNew York University\nrostami@cs.nyu.edu\nAbstract\nThis paper presents a theoretical analysis of the problem of domain adaptation\nwith multiple sources. For each source domain, the distribution over the input\npoints as well as a hypothesis with error at most ǫ are given. The problem con-\nsists of combining these hypotheses to derive a hypothesis with small error with\nrespect to the target domain. We present several theoretical results relating to\nthis problem. In particular, we prove that standard convex combinations of the\nsource hypotheses may in fact perform very poorly and that, instead, combinations\nweighted by the source distributions beneﬁt from favorable theoretical guarantees.\nOur main result shows that, remarkably, for any ﬁxed target function, there exists\na distribution weighted combining rule that has a loss of at most ǫ with respect to\nany target mixture of the source distributions. We further generalize the setting\nfrom a single target function to multiple consistent target functions and show the\nexistence of a combining rule with error at most 3ǫ. Finally, we report empirical\nresults for a multiple source adaptation problem with a real-world dataset.\n1 Introduction\nA common assumption in theoretical models of learning such as the standard PAC model [16], as\nwell as in the design of learning algorithms, is that training instances are drawn according to the\nsame distribution as the unseen test examples. In practice, however, there are many cases where this\nassumption does not hold. There can be no hope for generalization, of course, when the training and\ntest distributions vastly differ, but when they are less dissimilar, learning can be more successful.\nA typical situation is that of domain adaptation where little or no labeled data is at one’s disposal\nfor the target domain, but large amounts of labeled data from a source domain somewhat similar to\nthe target, or hypotheses derived from that source, are available instead. This problem arises in a\nvariety of applications in natural language processing [4, 7, 10], speech processing [8, 9, 11, 13–15],\ncomputer vision [12], and many other areas.\nThis paper studies the problem of domain adaptation with multiple sources, which has also received\nconsiderable attention in many areas such as natural language processing and speech processing.\nAn example is the problem of sentiment analysis which consists of classifying a text sample such\nas a movie review, restaurant rating, or discussion boards, or other web pages. Information about a\nrelatively small number of domains such as movies or books may be available, but little or none can\nbe found for more difﬁcult domains such as travel.\nWe will consider the following problem of multiple source adaptation. For each source i ∈ [1, k ],\nthe learner receives the distribution Di of the input points corresponding to that source as well\nas a hypothesis hi with loss at most ǫ on that source. The learner’s task consists of combining\nthe k hypotheses hi, i ∈ [1, k ], to derive a hypothesis h with small loss with respect to the target\ndistribution. The target distribution is assumed to be a mixture of the distributions Di. We will\ndiscuss both the case where the mixture is known to the learner and the case where it is unknown.\n1\nNote that the distribution Di i s deﬁned over the input points and bears no information about the\nlabels. In practice, Di is estimated from large amounts of unlabeled points typically available from\nsource i.\nAn alternative set-up for domain adaptation with multiple sources is one where the learner is not\nsupplied with a good hypothesis hi for each source but where instead he has access to the labeled\ntraining data for each source domain. A natural solution consists then of combining the raw labeled\ndata from each source domain to form a new sample more representative of the target distribution\nand use that to train a learning algorithm. This set-up and the type of solutions just described\nhave been in fact explored extensively in applications [8, 9, 11, 13–15]. However, several empirical\nobservations motivated our study of hypothesis combination, in addition to the theoretical simplicity\nand clarity of this framework.\nFirst, in some applications such as very large-vocabulary speech recognition, often the original raw\ndata used to derive each domain-dependent model is no more available [2, 9]. This is because such\nmodels are typically obtained as a result of training based on many hours of speech with ﬁles oc-\ncupying hundreds of gigabytes of disk space, while the models derived require orders of magnitude\nless space. Thus, combining raw labeled data sets is not possible in such cases. Secondly, a com-\nbined data set can be substantially larger than each domain-speciﬁc data set, which can signiﬁcantly\nincrease the computational cost of training and make it prohibitive for some algorithms. Thirdly,\ncombining labeled data sets requires the mixture parameters of the target distribution to be known,\nbut it is not clear how to produce a hypothesis with a low error rate with respect to any mixture\ndistribution.\nFew theoretical studies have been devoted to the problem of adaptation with multiple sources. Ben-\nDavid et al. [1] gave bounds for single source adaptation, then Blitzer et al. [3] extended the work\nto give a bound on the error rate of a hypothesis derived from a weighted combination of the source\ndata sets for the speciﬁc case of empirical risk minimization. Crammer et al. [5, 6] also addressed\na problem where multiple sources are present but the nature of the problem differs from adaptation\nsince the distribution of the input points is the same for all these sources, only the labels change\ndue to varying amounts of noise. We are not aware of a prior theoretical study of the problem of\nadaptation with multiple sources analyzed here.\nWe present several theoretical results relating to this problem. We examine two types of hypothesis\ncombination. The ﬁrst type is simply based on convex combinations of the k hypotheses hi. We\nshow that this natural and widely used hypothesis combination may in fact perform very poorly in\nour setting. Namely, we give a simple example of two distributions and two matching hypotheses,\neach with zero error for their respective distribution, but such that any convex combination has\nexpected absolute loss of 1/ 2 for the equal mixture of the distributions. This points out a potentially\nsigniﬁcant weakness of a convex combination.\nThe second type of hypothesis combination, which is the main one we will study in this work,\ntakes into account the probabilities derived from the distributions. Namely, the weight of hypothesis\nhi on an input x is proportional to λ iDi(x), were λ is the set of mixture weights. We will refer\nto this method as the distribution weighted hypothesis combination . Our main result shows that,\nremarkably, for any ﬁxed target function, there exists a distribution weighted combining rule that\nhas a loss of at most ǫ with respect to any mixture of the k distributions. We also show that there\nexists a distribution weighted combining rule that has loss at most 3ǫ with respect to any consistent\ntarget function (one for which each hi has loss ǫ on Di) and any mixture of the k distributions. In\nsome sense, our results establish that the distribution weighted hypothesis combination is the “right”\ncombination rule, and that it also beneﬁts from a well-founded theoretical guarantee.\nThe remainder of this paper is organized as follows. Section 2 introduces our theoretical model for\nmultiple source adaptation. In Section 3, we analyze the abstract case where the mixture parameters\nof the target distribution are known and show that the distribution weighted hypothesis combination\nthat uses as weights these mixture coefﬁcients achieves a loss of at most ǫ. In Section 4, we give\na simple method to produce an error of Θ(kǫ ) that does not require the prior knowledge of the\nmixture parameters of the target distribution. Our main results showing the existence of a combined\nhypothesis performing well regardless of the target mixture are given in Section 5 for the case of a\nﬁxed target function, and in Section 6 for the case of multiple target functions. Section 7 reports\nempirical results for a multiple source adaptation problem with a real-world dataset.\n2\n2 Problem Set-Up\nL\net X be the input space, f : X → R the target function to learn, andL : R × R → R a loss function\npenalizing errors with respect to f. The loss of a hypothesis h with respect to a distribution D and\nloss function L is denoted by L(D, h, f ) and deﬁned as L(D, h, f ) = E x∼ D[L(h(x), f (x))] =∑\nx∈X L(h(x), f (x))D(x). We will denote by ∆ the simplex ∆ = {λ : λ i ≥ 0 ∧ ∑k\ni=1 λ i = 1}of\nRk.\nWe consider an adaptation problem with k source domains and a single target domain. The input\nto the problem is the set of k source distributions D1, . . . , D k and k corresponding hypotheses\nh1, . . . , h k such that for all i ∈ [1, k ], L(Di, h i, f ) ≤ ǫ, for a ﬁxed ǫ ≥ 0. The distribution\nof the target domain, DT , is assumed to be a mixture of the k source distributions Dis, that is\nDT (x) = ∑k\ni=1 λ iDi(x), for some unknown mixture weight vectorλ ∈ ∆. The adaptation problem\nconsists of combing the hypotheses his to derive a hypothesis with small loss on the target domain.\nSince the target distribution DT is assumed to be a mixture, we will refer to this problem as the\nmixture adaptation problem.\nA combining rule for the hypotheses takes as an input the his and outputs a single hypothe-\nsis h : X → R. We deﬁne two combining rules of particular interest for our purpose: the lin-\near combining rule which is based on a parameter z ∈ ∆ and which sets the hypothesis to\nh(x) = ∑k\ni=1 zihi(x); and the distribution weighted combining rule also based on a parameter\nz ∈ ∆ which sets the hypothesis to h(x) = ∑k\ni=1\nziDi(x)\nPk\nj=\n1 zj Dj (x) hi(x) when ∑k\nj=1 zjDj(x) > 0.\nThis last condition always holds if Di(x) > 0 for all x ∈ X and some i ∈ [1, k ]. We deﬁne H to\nbe the set of all distribution weighted combining rules. Given the input to the adaptation problem\nwe have implicit information about the target function f. We deﬁne the set of consistent target\nfunctions, F, as follows,\nF = {g : ∀i ∈ [1, k ], L(Di, h i, g ) ≤ ǫ}.\nBy deﬁnition, the target function f is an element of F.\nWe will assume that the following properties hold for the loss function L: (i) L is non-negative:\nL(x, y ) ≥ 0 for all x, y ∈ R; (ii) L is convex with respect to the ﬁrst argument: L(∑k\ni=1 λ ixi, y ) ≤∑k\ni=1 λ iL(xi, y ) for all x1, . . . , x k, y ∈ R and λ ∈ ∆; (iii) L is bounded: there exists M ≥ 0\nsuch that L(x, y ) ≤ M for all x, y ∈ R; (iv) L(x, y ) is continuous in both x and y; and (v) L is\nsymmetric L(x, y ) = L(y, x ). The absolute loss deﬁned by L(x, y ) = |x − y|will serve as our\nprimary motivating example.\n3 Known Target Mixture Distribution\nIn this section we assume that the parameters of the target mixture distribution are known. Thus, the\nlearning algorithm is givenλ ∈∆ such that DT (x) =∑k\ni=1 λ iDi(x). A good starting point would be\nto study the performance of a linear combining rule. Namely the classiﬁer h(x) = ∑k\ni=1 λ ihi(x).\nWhile this seems like a very natural classiﬁer, the following example highlights the problematic\naspects of this approach.\nConsider a discrete domain X = {a, b }and two distributions, Da and Db, such that Da(a) = 1\nand Db(b) = 1 . Namely, each distribution puts all the weight on a single element in X . Consider\nthe target function f, where f (a) = 1 and f (b) = 0 , and let the loss be the absolute loss. Let\nh0 = 0 be the function that outputs 0 for all x ∈ X and similarly h1 = 1 . The hypotheses h1\nand h0 have zero expected absolute loss on the distributions Da and Db, respectively, i.e., ǫ = 0 .\nNow consider the target distribution DT with λ a = λ b = 1 / 2, thus DT (a) = DT (b) = 1 / 2. The\nhypothesis h(x) = (1 / 2)h1(x) + (1/ 2)h0(x) always outputs 1/ 2, and has an absolute loss of 1/ 2.\nFurthermore, for any other parameter z of the linear combining rule, the expected absolute loss of\nh(x) = zh1(x)+ (1− z)h0(x) with respect to DT is exactly 1/ 2. We have established the following\ntheorem.\nTheorem 1. There is a mixture adaptation problem with ǫ = 0 for which any linear combination\nrule has expected absolute loss of 1/ 2.\n3\nNext we show that the distribution weighted combining rule pr oduces a hypothesis with a low ex-\npected loss. Given a mixture DT (x) = ∑k\ni=1 λ iDi(x), we consider the distribution weighted com-\nbining rule with parameter λ , which we denote by hλ. Recall that,\nhλ(x) =\nk∑\ni=1\nλ iDi(x)\n∑k\nj=\n1 λ jDj(x)\nhi(x) =\nk∑\ni=1\nλ iDi(x)\nDT (x) hi(x) .\nU\nsing the convexity of L with respect to the ﬁrst argument, the loss of hλ with respect to DT and a\ntarget f ∈ F can be bounded as follows,\nL(DT , h λ, f ) =\n∑\nx∈X\nL(hλ(x), f (x))DT (x) ≤\n∑\nx∈X\nk∑\ni=1\nλ iDi(x)L(hi(x), f (x)) =\nk∑\ni=1\nλ iǫ i ≤ ǫ,\nwhere ǫ i := L(Di, h i, f ) ≤ ǫ. Thus, we have derived the following theorem.\nTheorem 2. For any mixture adaptation problem with target distributionDλ(x) = ∑k\ni=1 λ iDi(x),\nthe expected loss of the hypothesis hλ is at most ǫ with respect to any target function f ∈ F :\nL(Dλ, h λ, f ) ≤ ǫ.\n4 Simple Adaptation Algorithms\nIn this section we show how to construct a simple distribution weighted hypothesis that has an\nexpected loss guarantee with respect to any mixture. Our hypothesis hu is simply based on equal\nweights, i.e., ui = 1/k , for all i ∈ [1, k ]. Thus,\nhu(x) =\nk∑\ni=1\n(1/k )Di(x)\n∑k\nj=\n1(1/k )Dj(x)\nhi(x) =\nk∑\ni=1\nDi(x)\n∑k\nj=\n1 Dj(x)\nhi(x).\nWe show forhu an expected loss bound ofkǫ, with respect to any mixture distributionDT and target\nfunction f ∈ F . (Proof omitted.)\nTheorem 3. For any mixture adaptation problem the expected loss of hu is at most kǫ, for any\nmixture distribution DT and target function f ∈ F , i.e., L(DT , h u, f ) ≤ kǫ.\nUnfortunately, the hypothesis hu can have an expected absolute loss as large as Ω(kǫ ). (Proof\nomitted.)\nTheorem 4. There is a mixture adaptation problem for which the expected absolute loss of hu is\nΩ(kǫ ). Also, for k = 2 there is an input to the mixture adaptation problem for which the expected\nabsolute loss of hu is 2ǫ − ǫ 2.\n5 Existence of a Good Hypothesis\nIn this section, we will show that for any target function f ∈ F there is a distribution weighted\ncombining rule hz that has a loss of at most ǫ with respect to any mixture DT . We will construct\nthe proof in two parts. In the ﬁrst part, we will show, using a simple reduction to a zero-sum game,\nthat one can obtain a mixture of hzs that guarantees a loss bounded by ǫ. In the second part, which\nis the more interesting scenario, we will show that for any target function f ∈ F there is a single\ndistribution weighted combining rule hz that has loss of at most ǫ with respect to any mixture DT .\nThis later part will require the use of Brouwer ﬁxed point theorem to show the existence of such an\nhz.\n5.1 Zero-sum game\nThe adaptation problem can be viewed as a zero-sum game between two players, NATURE and\nLEARNER. Let the input to the mixture adaptation problem be D1, . . . , D k, h1, . . . , h k and ǫ, and\nﬁx a target function f ∈ F . The playerNATURE picks a distribution Di while the playerLEARNER\nselects a distribution weighted combining rule hz ∈ H . The loss when NATURE plays Di and\nLEARNER plays hz is L(Di, h z, f ). Let us emphasize that the target function f ∈ F is ﬁxed\nbeforehand. The objective of NATURE is to maximize the loss and the objective of LEARNER is to\nminimize the loss. We start with the following lemma,\n4\nLemma 1. G iven any mixed strategy ofNATURE, i.e., a distribution µ over Di’s, then the following\naction ofLEARNER hµ ∈ H has expected loss at most ǫ, i.e., L(Dµ, h µ, f ) ≤ ǫ.\nThe proof is identical to that of Theorem 2. This almost establishes that the value of the game is at\nmost ǫ. The technical part that we need to take care of is the fact that the action space of LEARNER\nis inﬁnite. However, by an appropriate discretization of H we can derive the following theorem.\nTheorem 5. For any target function f ∈ F and any δ > 0, there exists a function h(x) =∑m\nj=1 α jhzj (x), where hzi ∈ H , such that L(DT , h, f ) ≤ ǫ + δ for any mixture distribution\nDT (x) = ∑k\ni=1 λ iDi(x).\nSince we can ﬁx δ > 0 to be arbitrarily small, this implies that a linear mixture of distribution\nweighted combining rules can guarantee a loss of almost ǫ with respect to any product distribution.\n5.2 Single distribution weighted combining rule\nIn the previous subsection, we showed that a mixture of hypotheses in H would guarantee a loss of\nat most ǫ. Here, we will considerably strengthen the result and show that there is asingle hypothesis\nin H for which this guarantee holds. Unfortunately our loss is not convex with respect to h ∈ H, so\nwe need to resort to a more powerful technique, namely the Brouwer ﬁxed point theorem.\nFor the proof we will need that the distribution weighted combining rule hz be continuous in\nthe parameter z. In general, this does hold due to the existence of points x ∈ X for which∑k\nj=1 zjDj(x) = 0 . To avoid this discontinuity, we will modify the deﬁnition of hz to hη\nz, as\nfollows.\nClaim 1. Let U denote the uniform distribution over X , then for any η > 0 and z ∈ ∆, let\nhη\nz : X → R be the function deﬁned by\nhη\nz(x) =\nk∑\ni=1\nziDi(x) + ηU (x)/k∑k\nj=\n1 zjDj(x) + ηU (x)\nhi(x).\nThen, for any distribution D, L(D, h η\nz , f ) is continuous in z.1\nLet us ﬁrst state Brouwer’s ﬁxed point theorem.\nTheorem 6 (Brouwer Fixed Point Theorem). For any compact and convex non-empty set A ⊂ Rn\nand any continuous function f : A → A, there is a point x ∈ A such that f (x) = x.\nWe ﬁrst show that there exists a distribution weighted combining rule hη\nz for which the losses\nL(Di, h η\nz, f ) are all nearly the same.\nLemma 2. For any target function f ∈ F and any η, η ′> 0, there exists z ∈ ∆, with zi ̸= 0 for all\ni ∈ [1, k ], such that the following holds for the distribution weighted combining rule hη\nz ∈ H:\nL(Di, h η\nz, f ) = γ + η ′− η ′\nzik ≤ γ + η ′\nf\nor any 1 ≤ i ≤ k, where γ = ∑k\nj=1 zjL(Dj, h η\nz, f ).\nProof. Fix η ′ > 0 and let Lz\ni = L(Di, h η\nz, f ) for all z ∈ ∆ and i ∈ [1, m ]. Consider the\nmapping φ : ∆ → ∆ deﬁned for all z ∈ ∆ by [φ (z)]i = ( ziLz\ni + η ′/k )/ (∑k\nj=1 zjLz\nj + η ′),\nwhere [φ (z)]i, is the ith coordinate of φ (x), i ∈ [1, m ]. By Claim 1, φ is continuous. Thus,\nby Brouwer’s Fixed Point Theorem, there exists z ∈ ∆ such that φ (z) = z. This implies that\nzi = (ziLz\ni + η ′/k )/ (∑k\nj=1 zjLz\nj + η ′). Since η ′> 0, we must havezi ̸= 0 for any i ∈ [1, m ]. Thus,\nwe can divide byzi and write Lz\ni +η ′/ (zik) = ( ∑k\nj=1 zjLz\nj )+η ′. Therefore, Lz\ni = γ +η ′− η ′/ (zik)\nwith γ = ∑k\nj=1 zjLz\nj .\n1I n addition to continuity, the perturbation to hz, hη\nz, also helps us ensure that none of the mixture weights\nzi is zero in the proof of the Lemma 2 .\n5\nNote that the lemma just presented does not use the structure o f the distribution weighted combining\nrule, but only the fact that the loss is continuous in the parameterz ∈ ∆. The lemma applies as well\nto the linear combination rule and provides the same guarantee. The real crux of the argument is, as\nshown in the next lemma, that γ is small for a distribution weighted combining rule (while it can be\nvery large for a linear combination rule).\nLemma 3. For any target function f ∈ F and any η, η ′ > 0, there exists z ∈ ∆ such that\nL(Dλ, h η\nz, f ) ≤ ǫ + ηM + η ′ for any λ ∈ ∆.\nProof. Let z be the parameter guaranteed in Lemma 2. Then L(Di, h η\nz , f ) = γ + η ′− η ′/ (zik) ≤\nγ + η ′, for 1 ≤ i ≤ k. Consider the mixture Dz, i.e., set the mixture parameter to bez. Consider the\nquantity L(Dz, h η\nz, f ). On the one hand, by deﬁnition, L(Dz, h η\nz, f ) = ∑k\ni=1 ziL(Di, h η\nz, f ) and\nthus L(Dz, h η\nz, f ) = γ . On the other hand,\nL(Dz,h η\nz , f )\n=\nX\nx∈X\nDz(x)L(hη\nz (x), f (x)) ≤\nX\nx∈X\nDz(x)\nDz(x) + ηU (x)\n kX\ni=1\n(ziDi(x) + ηU (x)\nk )L(hi(x), f (x))\n!\n≤\nX\nx∈X\n kX\ni=1\nziDi(x)L(hi(x), f (x))\n!\n+\nX\nx∈X\nηM U (x)\n=\nkX\ni=1\nziL(Di, h i, f ) + ηM =\nkX\ni=1\nziǫ i + ηM ≤ ǫ + ηM .\nTherefore γ ≤ ǫ + ηM . To complete the proof, note that the following inequality holds for any\nmixture Dλ:\nL(Dλ, h η\nz, f ) =\nk∑\ni=1\nλ iL(Di, h η\nz, f ) ≤ γ + η ′,\nwhich is at most ǫ + ηM + η ′.\nBy setting η = δ / (2M ) and η ′= δ/ 2, we can derive the following theorem.\nTheorem 7. For any target functionf ∈ F and any δ > 0, there exists η > 0 and z ∈ ∆, such that\nL(Dλ, h η\nz, f ) ≤ ǫ + δ for any mixture parameter λ .\n6 Arbitrary target function\nThe results of the previous section show that for anyﬁxed target function there is a good distribution\nweighted combining rule. In this section, we wish to extend these results to the case where the target\nfunction is not ﬁxed in advanced. Thus, we seek a single distribution weighted combining rule that\ncan perform well for any f ∈ F and any mixture Dλ. Unfortunately, we are not able to prove a\nbound of ǫ + o(ǫ) but only a bound of 3ǫ. To show this bound we will show that for any f1, f 2 ∈ F\nand any hypothesis h the difference of loss is bounded by at most 2ǫ.\nLemma 4. Assume that the loss function L obeys the triangle inequality, i.e., L(f, h ) ≤ L(f, g ) +\nL(g, h ). Then for any f, f ′∈ F and any mixture DT , the inequality L(DT , h, f ′) ≤ L (DT , h, f ) +\n2ǫ holds for any hypothesis h.\nProof. Since our loss function obeys the triangle inequality, for any functions f, g, h , the following\nholds, L(D, f, h ) ≤ L (D, f, g ) + L(D, g, h ). In our case, we observe that replacing g with any\nf ′ ∈ F gives, L(Dλ, f, h ) ≤ L (Dλ, f ′, h ) + L(Dλ, f, f ′). We can bound the term L(Dλ, f, f ′)\nwith a similar inequality, L(Dλ, f, f ′) ≤ L (Dλ, f, h λ) + L(Dλ, f ′, h λ) ≤ 2ǫ, where hλ is the\ndistribution weighted combining rule produced by choosingz = λ and using Theorem 2. Therefore,\nfor any f, f ′∈ F we have, L(Dλ, f, h ) ≤ L (Dλ, f ′, h ) + 2ǫ, which completes the proof.\nWe derived the following corollary to Theorem 7.\nC\norollary 1. Assume that the loss function L obeys the triangle inequality. Then, for any δ > 0,\nthere exists η > 0 and z ∈ ∆, such that for any mixture parameter λ and any f ∈ F ,\nL(Dλ, h η\nz, f ) ≤ 3ǫ + δ .\n6\n1 2 3 4 5 6\n1.5\n1.6\n1.7\n1.8\n1.9\n2\n2.1MSE\nUniform Mixture Over 4 Domains\n \n \nIn−Domain\nOut−Domain\n0 0.2 0.4 0.6 0.8 11.4\n1.6\n1.8\n2\n2.2\n2.4\nMixture = α book + (1 − α) kitchen\nα\nMSE\n \n \nweighted\nlinear\nbook\nkitchen\n0 0.2 0.4 0.6 0.8 11.4\n1.6\n1.8\n2\n2.2\n2.4\nα\nMSE\nMixture = α dvd + (1 − α) electronics\n \n \nweighted\nlinear\ndvd\nelectronics\n(a) ( b)\nFigure 1: (a) MSE performance for a target mixture of four domains (1: books, 2: dvd, 3: electronics,\n4: kitchen 5: linear, 6: weighted). (b) MSE performance under various mixtures of two source\ndomains, plot left: book andkitchen, plot right: dvd andelectronics.\n7 Empirical results\nThis section reports the results of our experiments with a distribution weighted combining rule using\nreal-world data. In our experiments, we ﬁxed a mixture target distribution Dλ and considered the\ndistribution weighted combining rulehz, with z = λ . Since we used real-world data, we did not have\naccess to the domain distributions. Instead, we modeled each distribution and used large amounts\nof unlabeled data available for each source to estimate the model’s parameters. One could have thus\nexpected potentially signiﬁcantly worse empirical results than the theoretical ones, but this turned\nout not to be an issue in our experiments.\nWe used the sentiment analysis dataset found in [4]. 2 The data consists of review text and rat-\ning labels, taken from amazon.com product reviews within four different categories (domains).\nThese four domains consist of book, dvd, electronics and kitchen reviews, where each do-\nmain contains 2000 data points. 3 In our experiments, we ﬁxed a mixture target distribution Dλ and\nconsidered the distribution weighted combining rule hz, with z = λ .\nIn our ﬁrst experiment, we considered mixtures of all four domains, where the test set was a uniform\nmixture of 600 points, that is the union of 150 points taken uniformly at random from each domain.\nThe remaining 1,850 points from each domain were used to train the base hypotheses. 4 We com-\npared our proposed weighted combining rule to the linear combining rule. The results are shown\nin Figure 1(a). They show that the base hypotheses perform poorly on the mixture test set, which\njustiﬁes the need for adaptation. Furthermore, the distribution weighted combining rule is shown to\nperform at least as well as the worst in-domain performance of a base hypothesis, as expected from\nour bounds. Finally, we observe that this real-world data experiment gives an example in which a\nlinear combining rule performs poorly compared to the distribution weighted combining rule.\nIn other experiments, we considered the mixture of two domains, where the mixture is varied ac-\ncording to the parameter α ∈ {0. 1, 0. 2, . . . , 1. 0}. For each plot in Figure 1 (b), the test set consists\nof 600α points from the ﬁrst domain and 600(1 − α ) points from the second domain, where the\nﬁrst and second domains are made clear in the ﬁgure. The remaining points that were not used for\ntesting were used to train the base hypotheses. The results show the linear shift from one domain to\nthe other, as is evident from the performance of the two base hypotheses. The distribution weighted\ncombining rule outperforms the base hypotheses as well as the linear combining rule.\n2h ttp://www.seas.upenn.edu/˜mdredze/datasets/sentiment/.\n3The rating label, an integer between 1 and 5, was used as a regression label, and the loss measured by the\nmean squared error (MSE). All base hypotheses were generated using Support Vector Regression (SVR) [17]\nwith the trade-off parameters C = 8 , ǫ = 0 . 1, and a Gaussian kernel with parameter g = 0 . 00078. The SVR\nsolutions were obtained using the libSVM software library ( http://www.csie.ntu.edu.tw/˜cjlin/libsvm/).\nOur features were deﬁned as the set of unigrams appearing ﬁve times or more in all domains. This deﬁned\nabout 4000 unigrams. We used a binary feature vector encoding the presence or absence of these frequent\nunigrams to deﬁne our instances. To model the domain distributions, we used a unigram statistical language\nmodel trained on the same corpus as the one used to deﬁne the features. The language model was created using\nthe GRM library (http://www.research.att.com/˜fsmtools/grm/).\n4Each experiment was repeated 20 times with random folds. The standard deviation found was far below\nwhat could be legibly displayed in the ﬁgures.\n7\nThus, our preliminary experiments suggest that the distribu tion weighted combining rule performs\nwell in practice and clearly outperforms a simple linear combining rule. Furthermore, using statis-\ntical language models as approximations to the distribution oracles seem to be sufﬁcient in practice\nand can help produce a good distribution weighted combining rule.\n8 Conclusion\nWe presented a theoretical analysis of the problem of adaptation with multiple sources. Domain\nadaptation is an important problem that arises in a variety of modern applications where limited or\nno labeled data is available for a target application and our analysis can be relevant in a variety of\nsituations. The theoretical guarantees proven for the distribution weight combining rule provide it\nwith a strong foundation. Its empirical performance with a real-world data set further motivates\nits use in applications. Much of the results presented were based on the assumption that the target\ndistribution is some mixture of the source distributions. A further analysis suggests however that\nour main results can be extended to arbitrary target distributions.\nAcknowledgments\nWe thank Jennifer Wortman for helpful comments on an earlier draft of this paper and Ryan McDonald for\ndiscussions and pointers to data sets. The work of M. Mohri and A. Rostamizadeh was partly supported by the\nNew York State Ofﬁce of Science Technology and Academic Research (NYSTAR).\nReferences\n[1] Shai Ben-David, John Blitzer, Koby Crammer, and Fernando Pereira. Analysis of representations for\ndomain adaptation. In Proceedings of NIPS 2006. MIT Press, 2007.\n[2] Jacob Benesty, M. Mohan Sondhi, and Yiteng Huang, editors. Springer Handbook of Speech Processing.\nSpringer, 2008.\n[3] John Blitzer, Koby Crammer, A. Kulesza, Fernando Pereira, and Jennifer Wortman. Learning bounds for\ndomain adaptation. In Proceedings of NIPS 2007. MIT Press, 2008.\n[4] John Blitzer, Mark Dredze, and Fernando Pereira. Biographies, Bollywood, Boom-boxes and Blenders:\nDomain Adaptation for Sentiment Classiﬁcation. In ACL 2007, Prague, Czech Republic, 2007.\n[5] Koby Crammer, Michael Kearns, and Jennifer Wortman. Learning from Data of Variable Quality. In\nProceedings of NIPS 2005, 2006.\n[6] Koby Crammer, Michael Kearns, and Jennifer Wortman. Learning from multiple sources. In Proceedings\nof NIPS 2006, 2007.\n[7] Mark Dredze, John Blitzer, Pratha Pratim Talukdar, Kuzman Ganchev, Joao Graca, and Fernando Pereira.\nFrustratingly Hard Domain Adaptation for Parsing. In CoNLL 2007, Prague, Czech Republic, 2007.\n[8] Jean-Luc Gauvain and Chin-Hui. Maximum a posteriori estimation for multivariate gaussian mixture\nobservations of markov chains.IEEE Transactions on Speech and Audio Processing, 2(2):291–298, 1994.\n[9] Frederick Jelinek. Statistical Methods for Speech Recognition. The MIT Press, 1998.\n[10] Jing Jiang and ChengXiang Zhai. Instance Weighting for Domain Adaptation in NLP. In Proceedings of\nACL 2007, pages 264–271, Prague, Czech Republic, 2007. Association for Computational Linguistics.\n[11] C. J. Legetter and Phil C. Woodland. Maximum likelihood linear regression for speaker adaptation of\ncontinuous density hidden markov models. Computer Speech and Language, pages 171–185, 1995.\n[12] Aleix M. Mart´ ınez. Recognizing imprecisely localized, partially occluded, and expression variant faces\nfrom a single sample per class. IEEE Trans. Pattern Anal. Mach. Intell., 24(6):748–763, 2002.\n[13] S. Della Pietra, V . Della Pietra, R. L. Mercer, and S. Roukos. Adaptive language modeling using minimum\ndiscriminant estimation. In HLT ’91: Proceedings of the workshop on Speech and Natural Language ,\npages 103–106, Morristown, NJ, USA, 1992. Association for Computational Linguistics.\n[14] Brian Roark and Michiel Bacchiani. Supervised and unsupervised PCFG adaptation to novel domains. In\nProceedings of HLT-NAACL, 2003.\n[15] Roni Rosenfeld. A Maximum Entropy Approach to Adaptive Statistical Language Modeling. Computer\nSpeech and Language, 10:187–228, 1996.\n[16] Leslie G. Valiant. A theory of the learnable. ACM Press New York, NY , USA, 1984.\n[17] Vladimir N. Vapnik. Statistical Learning Theory. Wiley-Interscience, New York, 1998.\n8",
  "values": {
    "Critiqability": "No",
    "Explicability": "No",
    "Interpretable (to users)": "No",
    "Collective influence": "No",
    "Non-maleficence": "No",
    "User influence": "No",
    "Not socially biased": "No",
    "Beneficence": "No",
    "Transparent (to users)": "No",
    "Autonomy (power to decide)": "No",
    "Fairness": "No",
    "Respect for Persons": "No",
    "Privacy": "No",
    "Deferral to humans": "No",
    "Respect for Law and public interest": "No",
    "Justice": "No"
  }
}