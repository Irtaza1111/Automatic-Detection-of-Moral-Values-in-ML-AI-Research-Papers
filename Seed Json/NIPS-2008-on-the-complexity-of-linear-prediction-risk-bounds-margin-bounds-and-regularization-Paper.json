{
  "pdf": "NIPS-2008-on-the-complexity-of-linear-prediction-risk-bounds-margin-bounds-and-regularization-Paper",
  "title": "On the Complexity of Linear Prediction: Risk Bounds, Margin Bounds, and Regularization",
  "author": "Sham M. Kakade, Karthik Sridharan, Ambuj Tewari",
  "paper_id": "NIPS-2008-on-the-complexity-of-linear-prediction-risk-bounds-margin-bounds-and-regularization-Paper",
  "text": "On the Complexity of Linear Prediction:\nRisk Bounds, Margin Bounds, and Regularization\nSham M. Kakade\nTTI Chicago\nChicago, IL 60637\nsham@tti-c.org\nKarthik Sridharan\nTTI Chicago\nChicago, IL 60637\nkarthik@tti-c.org\nAmbuj Tewari\nTTI Chicago\nChicago, IL 60637\ntewari@tti-c.org\nAbstract\nThis work characterizes the generalization ability of algorithms whose predic-\ntions are linear in the input vector. To this end, we provide sharp bounds for\nRademacher and Gaussian complexities of (constrained) linear classes, which di-\nrectly lead to a number of generalization bounds. This derivation provides simpli-\nﬁed proofs of a number of corollaries including: risk bounds for linear prediction\n(including settings where the weight vectors are constrained by either L2 or L1\nconstraints), margin bounds (including both L2 and L1 margins, along with more\ngeneral notions based on relative entropy), a proof of the PAC-Bayes theorem,\nand upper bounds on L2 covering numbers (with Lp norm constraints and rela-\ntive entropy constraints). In addition to providing a uniﬁed analysis, the results\nherein provide some of the sharpest risk and margin bounds. Interestingly, our\nresults show that the uniform convergence rates of empirical risk minimization\nalgorithms tightly match the regret bounds of online learning algorithms for linear\nprediction, up to a constant factor of 2.\n1 Introduction\nLinear prediction is the cornerstone of an extensive number of machine learning algorithms, in-\ncluding SVM’s, logistic and linear regression, the lasso, boosting, etc. A paramount question is to\nunderstand the generalization ability of these algorithms in terms of the attendant complexity re-\nstrictions imposed by the algorithm. For example, for the sparse methods (e.g. regularizing based\non L1 norm of the weight vector) we seek generalization bounds in terms of the sparsity level. For\nmargin based methods (e.g. SVMs or boosting), we seek generalization bounds in terms of either\nthe L2 or L1 margins. The focus of this paper is to provide a more uniﬁed analysis for methods\nwhich use linear prediction.\nGiven a training set {(xi, y i)}n\ni=1, the paradigm is to compute a weight vector ˆw which minimizes\nthe F -regularized ℓ-risk. More speciﬁcally,\nˆw = argmin\nw\n1\nn\nn∑\ni=1\nℓ(⟨w, x i⟩ , y i) + λF (w) (1)\nwhere ℓ is the loss function, F is the regularizer, and ⟨w, x⟩ is the inner product between vectors x\nand w. In a formulation closely related to the dual problem, we have:\nˆw = argmin\nw:F (w)≤c\n1\nn\nn∑\ni=1\nℓ(⟨w, x i⟩ , y i) (2)\nwhere, instead of regularizing, a hard restriction over the parameter space is imposed (by the constant\nc). This works provides generalization bounds for an extensive family of regularization functions F .\nRademacher complexities (a measure of the complexity of a function class) provide a direct route\nto obtaining such generalization bounds, and this is the route we take. Such bounds are analogous\nto VC dimensions bounds, but they are typically much sharper and allow for distribution dependent\nbounds. There are a number of methods in the literature to use Rademacher complexities to obtain\neither generalization bounds or margin bounds. Bartlett and Mendelson [2002] provide a general-\nization bound for Lipschitz loss functions. For binary prediction, the results in Koltchinskii and\nPanchenko [2002] provide means to obtain margin bounds through Rademacher complexities.\nIn this work, we provide sharp bounds for Rademacher and Gaussian complexities of linear classes,\nwith respect to a strongly convex complexity function F (as in Equation 1). These bounds provide\nsimpliﬁed proofs of a number of corollaries: generalization bounds for the regularization algorithm\nin Equation 2 (including settings where the weight vectors are constrained by either L2 or L1 con-\nstraints), margin bounds (including L2 and L1 margins, and, more generally, for Lp margins), a\nproof of the PAC-Bayes theorem, and L2 covering numbers (with Lp norm constraints and relative\nentropy constraints). Our bounds are often tighter than previous results and our proofs are all under\nthis more uniﬁed methodology.\nOur proof techniques — reminiscent of those techniques for deriving regret bounds for online learn-\ning algorithms — are rooted in convex duality (following Meir and Zhang [2003]) and use a more\ngeneral notion of strong convexity (as in Shalev-Shwartz and Singer [2006]). Interestingly, the risk\nbounds we provide closely match the regret bounds for online learning algorithms (up to a constant\nfactor of 2), thus showing that the uniform converge rates of empirical risk minimization algorithms\ntightly match the regret bounds of online learning algorithms (for linear prediction). The Discussion\nprovides this more detailed comparison.\n1.1 Related Work\nA staggering number of results have focused on this problem in varied special cases. Perhaps the\nmost extensively studied are margin bounds for the 0-1 loss. For L2-margins (relevant for SVM’s,\nperceptron based algorithms, etc.), the sharpest bounds are those provided by Bartlett and Mendel-\nson [2002] (using Rademacher complexities) and Langford and Shawe-Taylor [2003], McAllester\n[2003] (using the PAC-Bayes theorem). For L1-margins (relevant for Boosting, winnow, etc),\nbounds are provided by Schapire et al. [1998] (using a self-contained analysis) and Langford et al.\n[2001] (using PAC-Bayes, with a different analysis). Another active line of work is on sparse meth-\nods — particularly methods which impose sparsity via L1 regularization (in lieu of the non-convex\nL0 norm). For L1 regularization, Ng [2004] provides generalization bounds for this case, which\nfollow from the covering number bounds of Zhang [2002]. However, these bounds are only stated\nas polynomial in the relevant quantities (dependencies are not provided).\nPrevious to this work, the most uniﬁed framework for providing generalization bounds for linear\nprediction stem from the covering number bounds in Zhang [2002]. Using these covering number\nbounds, Zhang [2002] derives margin bounds in a variety of cases. However, providing sharp gen-\neralization bounds for problems with L1 regularization (or L1 constraints in the dual) requires more\ndelicate arguments. As mentioned, Ng [2004] provides bounds for this case, but the techniques used\nby Ng [2004] would result in rather loose dependencies (the dependence on the sample size n would\nbe n−1/4 rather than n−1/2). We discuss this later in Section 4.\n2 Preliminaries\nOur input space,X , is a subset of a vector space, and our output space is Y. Our samples (X, Y )∈\nX ×Yare distributed according to some unknown distribution P . The inner product between\nvectors x and w is denoted by ⟨w, x⟩, where w ∈S (here, S is a subset of the dual space to\nour input vector space). A norm of a vector x is denoted by ∥x∥, and the dual norm is deﬁned as\n∥w∥⋆ = sup{⟨w, x⟩ :∥x∥≤1}. We further assume that for all x∈X,∥x∥≤X.\nLet ℓ : R×Y→R+ be our loss function of interest. Throughout we shall consider linear predictors\nof form⟨w, x⟩. The expected of loss of w is denoted byL(w) = E[ℓ(⟨w, x⟩ , y )]. As usual, we are\nprovided with a sequence of i.i.d. samples {(xi, y i)}n\ni=1, and our goal is to minimize our expected\nloss. We denote the empirical loss as ˆL(w) = 1\nn\n∑n\ni=1 ℓ(⟨w, xi⟩ , y i).\nThe restriction we make on our complexity function F is that it is a strongly convex function. In\nparticular, we assume it is strongly convex with respect to our dual norm: a function F : S→R is\nsaid to be σ -strongly convex w.r.t. to∥·∥ ∗iff∀u,v∈S,∀α∈[0, 1], we have\nF (α u + (1−α )v)≤αF (u) + (1−α )F (v)−σ\n2 α (1−α )∥u−v∥2\n∗.\nSee Shalev-Shwartz and Singer [2006] for more discussion on this generalized deﬁnition of strong\nconvexity.\nRecall the deﬁnition of the Rademacher and Gaussian complexity of a function class F,\nRn(F) = E\n[\nsup\nf∈F\n1\nn\nn∑\ni=1\nf (xi)ǫ i\n]\nGn(F) = E\n[\nsup\nf∈F\n1\nn\nn∑\ni=1\nf (xi)ǫ i\n]\nwhere, in the former, ǫ i independently takes values in {−1,+1}with equal probability, and, in the\nlatter, ǫ i are independent, standard normal random variables. In both expectations, (x1, . . . , xn) are\ni.i.d.\nAs mentioned in the Introduction, there are number of methods in the literature to use Rademacher\ncomplexities to obtain either generalization bounds or margin bounds. Two results are particularly\nuseful to us. First, Bartlett and Mendelson [2002] provides the following generalization bound for\nLipschitz loss functions. Here, L(f ) = E[ℓ(f (x), y )] is the expected of loss of f : X →R, and\nˆL(f ) = 1\nn\n∑n\ni=1 ℓ(f (xi), y i) is the empirical loss.\nTheorem 1. (Bartlett and Mendelson [2002]) Assume the loss ℓ is Lipschitz (with respect to its\nﬁrst argument) with Lipschitz constant Lℓ and that ℓ is bounded by c. F or any δ > 0 and with\nprobability at least 1−δ simultaneously for all f∈F, we have that\nL(f )≤ˆL(f ) + 2LℓRn(F) + c\n√\nlog(1/δ )\n2n\nwhereRn(F) is the Rademacher complexity of a function class F, and n is the sample size.\nThe second result, for binary prediction, from Koltchinskii and Panchenko [2002] provides a mar-\ngin bound in terms of the Rademacher complexity. The following is a variant of Theorem 2 in\nKoltchinskii and Panchenko [2002]:\nTheorem 2. (Koltchinskii and Panchenko [2002]) The zero-one loss function is given by\nℓ(f (x), y ) = 1[yf (x) ≤0], where y ∈ {+1, −1}. Denote the fraction of the data havingγ -\nmargin mistakes by Kγ (f ) := |{i:yif (xi)<γ}|\nn . Assume that ∀f∈Fwe have supx∈X|f (x)|≤C.\nThen, with probability at least 1−δ over the sample, for all margins γ > 0 and all f∈Fwe have,\nL(f )≤Kγ (f ) + 4Rn(F)\nγ +\n√\nlog(log2 4C\nγ )\nn +\n√\nlog(1/δ )\n2n .\n(We provide a proof in the appendix.) The above results show that if we provide sharp bounds on the\nRademacher complexities then we obtain sharp generalization bounds. Typically, we desire upper\nbounds on the Rademacher complexity that decrease with n.\n3 Complexities of Linear Function Classes\nGiven a subsetW⊆S, deﬁne the associated class of linear functionsFW asFW :={x↦→⟨w, x⟩ :\nw∈W}. Our main theorem bounds the complexity of FW for certain setsW.\nTheorem 3. (Complexity Bounds) Let S be a closed convex set and let F : S →R be σ -strongly\nconvex w.r .t.∥·∥ ∗s.t. inf w∈S F (w) = 0 . Further , letX ={x: ∥x∥≤X}. DeﬁneW ={w∈\nS : F (w)≤W 2\n∗}. Then, we have\nRn(FW )≤XW∗\n√\n2\nσn , Gn(FW )≤XW∗\n√\n2\nσn .\nThe restriction inf w∈SF (w) = 0 is not a signiﬁcant one since adding a constant to F still keeps it\nstrongly convex. Interestingly, the complexity bounds above precisely match the regret bounds for\nonline learning algorithms (for linear prediction), a point which we return to in the Discussion. We\nﬁrst provide a few examples, before proving this result.\n3.1 Examples\n(1) Lp/L q norms. Let S = Rd. Take∥·∥,∥·∥∗to be the Lp, Lq norms for p∈[2, ∞), 1/p +1/q = 1,\nwhere∥x∥p :=\n( ∑d\nj=1|xi|p\n) 1/p\n. Choose F (w) =∥·∥ 2\nq and note that it is 2(q−1)-strongly convex\non Rd w.r.t. itself. SetX ,W as in Theorem 3. Then, we have\nRn(FW )≤XW∗\n√\np−1\nn . (3)\n(2) L∞/L 1 norms. Let S ={w∈Rd : ∥w∥1 = W1 , wj ≥0}be the W1-scaled probability\nsimplex. Take ∥·∥ ,∥·∥ ∗to be the L∞, L1 norms,∥x∥∞= max 1≤j≤d|xj|. Fix a probability\ndistribution µ > 0 and let F (w) = entro µ(w) := ∑\nj(wj/W 1) log(wj/( W1µj)). For any µ,\nentroµ(w) is 1/W 2\n1 -strongly convex on S w.r.t.∥·∥ 1. Set X as in Theorem 3 and let W(E) =\n{w∈S : entro µ(w)≤E}. Then, we have\nRn(FW(E))≤XW 1\n√\n2E\nn . (4)\nNote that if we take µ to be the uniform distribution then for any w∈S we have that trivial upper\nbound of entroµ(w)≤log d. Hence if we let W :=W(log d) with uniform µ and note that it is the\nentire scaled probability simplex. Then\nRn(FW )≤XW 1\n√\n2 log d\nn . (5)\nThe restriction wj ≥0 can be removed in the deﬁnition of S by the standard trick of doubling the\ndimension of x to include negated copies of each coordinate. So, if we have S = {w∈Rd :\n∥w∥1≤W1}and we setX as above andW = S, then we get Rn(FW )≤XW 1\n√\n2 log(2d)/n .\nIn this way, even though the L1 norm is not strongly convex (so our previous Theorem does not\ndirectly apply to it), the class of functions imposed by this L1 norm restriction is equivalent to that\nimposed by the above entropy restriction. Hence, we are able to analyze the generalization properties\nof the optimization problem in Equation 2.\n(3) Smooth norms. A norm is (2, D )-smooth on S if for any x, y∈S,\nd2\ndt2∥x + ty∥2≤2D2∥y∥2 .\nLet∥·∥ be a (2, D )-smooth norm and∥·∥∗be its dual. Lemma 11 in the appendix proves that ∥·∥∗\nis 2/D 2-strongly convex w.r.t. itself. SetX ,W as in Theorem 3. Then, we have\nRn(FW )≤XW∗D√n . (6)\n(4) Bregman divergences. For a strongly convex F , deﬁne the Bregman divergence ∆F (w∥v) :=\nF (w)−F (v)−⟨∇F (v), w−v⟩. It is interesting to note that Theorem 3 is still valid if we choose\nW∗={w∈S : ∆ F (w∥v)≤W 2\n∗}for some ﬁxed v∈S. This is because the Bregman divergence\n∆F (·∥v) inherits the strong convexity of F .\nExcept for (5), none of the above bounds depend explicitly on the dimension of the underlying space\nand hence can be easily extended to inﬁnite dimensional spaces under appropriate assumptions.\n3.2 The Proof\nFirst, some background on convex duality is in order. The Fenchel conjugate of F : S →R is\ndeﬁned as:\nF∗(θ) := sup\nw∈S\n⟨w, θ⟩−F (w) .\nA simple consequence of this deﬁnition is Fenchel-Y oung inequality,\n∀θ, w∈S, ⟨w, θ⟩≤F (w) + F∗(θ) .\nIf F is σ -strongly convex, then F∗is differentiable and\n∀θ, η, F ∗(θ + η)≤F∗(θ) +⟨∇F∗(θ), η⟩ + 1\n2σ ∥η∥2\n∗. (7)\nSee the Appendix in Shalev-Shwartz [2007] for proof. Using this inequality we can control the\nexpectation of F∗applied to a sum of independent random variables.\nLemma 4. Let S be a closed convex set and let F : S→R be σ -strongly convex w.r .t.∥·∥∗. Let Zi\nbe mean zero independent random vectors such that E[∥Zi∥2]≤V 2. Deﬁne Si := ∑\nj≤iZi. Then\nF∗(Si)−iV 2/2σ is a supermartingale. Furthermore, if inf w∈SF (w) = 0 , then E[F∗(Sn)] ≤\nnV 2/2 σ .\nProof. Note that inf w∈SF (w) = 0 implies F∗(0) = 0 . Inequality (7) gives,\nF∗(Si−1+ Zi)≤F∗(Si) +⟨∇F∗(Si−1), Z i⟩ + 1\n2σ ∥Zi∥2\n∗.\nTaking conditional expectation w.r.t.Z1, . . . , Z i−1and noting that Ei−1[Zi] = 0 and Ei−1[∥Zi∥2\n∗]≤\nV 2, we get\nEi−1[F∗(Si)]≤F∗(Si−1) + 0 + V 2\n2σ\nwhere Ei−1[·] abbreviates E[·|Z1, . . . , Z i−1]. To end the proof, note that inf w∈SF (w) = 0 implies\nF∗(0) = 0 .\nLike Meir and Zhang [2003] (see Section 5 therein), we begin by using conjugate duality to bound\nthe Rademacher complexity. To ﬁnish the proof, we exploit the strong convexity of F by applying\nthe above lemma.\nProof. Fix x1, . . . , xn such that∥xi∥≤X. Let θ = 1\nn\n∑\ni ǫ ixi where ǫ i’s are i.i.d. Rademacher or\nGaussian random variables (our proof only requires that E[ǫ i] = 0 and E[ǫ 2\ni ] = 1 ). Choose arbitrary\nλ > 0. By Fenchel’s inequality, we have ⟨w, λθ ⟩≤F (w) + F∗(λ θ) which implies\n⟨w, θ⟩≤F (w)\nλ + F∗(λ θ)\nλ .\nSince, F (w)≤W 2\n∗for all w∈W, we have\nsup\nw∈W\n⟨w, θ⟩≤W 2\n∗\nλ + F∗(λ θ)\nλ .\nTaking expectation (w.r.t. ǫ i’s), we get\nE\n[\nsup\nw∈W\n⟨w, θ⟩\n]\n≤W 2\n∗\nλ + 1\nλ E [F∗(λ θ)] .\nNow set Zi = λǫ ixi\nn (so that Sn = λ θ) and note that the conditions of Lemma 4 are satisﬁed with\nV 2 = λ 2B2/n 2 and hence E[F∗(λ θ)]≤λ 2X 2\n2σn . Plugging this above, we have\nE\n[\nsup\nw∈W\n⟨w, θ⟩\n]\n≤W 2\n∗\nλ + λX 2\n2σn .\nSetting λ =\n√\n2σnW 2\n∗\nX 2 gives\nE\n[\nsup\nw∈W\n⟨w, θ⟩\n]\n≤XW∗\n√\n2\nσn .\nwhich completes the proof.\n4 Corollaries\n4.1 Risk Bounds\nWe now provide generalization error bounds for any Lipschitz loss function ℓ, with Lipschitz con-\nstant Lℓ . Based on the Rademacher generalization bound provided in the Introduction (see Theo-\nrem 1) and the bounds on Rademacher complexity proved in previous section, we obtain the follow-\ning corollaries.\nCorollary 5. Each of the following statements holds with probability at least 1−δ over the sample:\n•LetW be as in the Lp/L q norms example. F or allw∈W,\nL(w)≤ˆL(w) + 2Lℓ XW∗\n√\np−1\nn + Lℓ XW∗\n√\nlog(1/δ )\n2n\n•LetW be as in the L∞/L 1 norms example. F or allw∈W,\nL( ˆw)≤ˆL(w) + 2Lℓ XW 1\n√\n2 log(d)\nn + Lℓ XW 1\n√\nlog(1/δ )\n2n\nNg [2004] provides bounds for methods which use L1 regularization. These bounds are only stated\nas polynomial bounds, and, the methods used (covering number techniques from Pollard [1984] and\ncovering number bounds from Zhang [2002]) would provide rather loose bounds (the n dependence\nwould be n−1/4). In fact, even a more careful analysis via Dudley’s entropy integral using the\ncovering numbers from Zhang [2002] would result in a worse bound (with additional log n factors).\nThe above argument is sharp and rather direct.\n4.2 Margin Bounds\nIn this section we restrict ourselves to binary classiﬁcation where Y ={+1, −1}. Our prediction\nis given by sign(⟨w, x⟩). The zero-one loss function is given by ℓ(⟨w, x⟩ , y ) = 1[y⟨w, x⟩ ≤\n0]. Denote the fraction of the data having γ -margin mistakes by Kγ (f ) := |{i:yif (xi)<γ}|\nn . We\nnow demonstrate how to get improved margin bounds using the upper bounds for the Rademacher\ncomplexity derived in Section 3.\nBased on the Rademacher margin bound provided in the Introduction (see Theorem 2), we get the\nfollowing corollary which will directly imply the margin bounds we are aiming for. The bound for\nthe p = 2 case has been used to explain the performance of SVMs. Our bound essentially matches\nthe best known bound [Bartlett and Mendelson, 2002] which was an improvement over previous\nbounds [Bartlett and Shawe-Taylor, 1999] proved using fat-shattering dimension estimates. For the\nL∞/L 1 case, our bound improves the best known bound [Schapire et al., 1998] by removing a factor\nof√log n.\nCorollary 6. (Lp Margins) Each of the following statements holds with probability at least 1−δ\nover the sample:\n•LetW be as in the Lp/L q norms example. F or allγ > 0, w∈W,\nL(w)≤Kγ (w) + 4 XW∗\nγ\n√\np−1\nn +\n√\nlog(log2\n4XW ∗\nγ )\nn +\n√\nlog(1/δ )\n2n\n•LetW be as in the L∞/L 1 norms example. F or allγ > 0, w∈W,\nL(w)≤Kγ (w) + 4 XW 1\nγ\n√\n2 log(d)\nn +\n√\nlog(log2\n4XW 1\nγ )\nn +\n√\nlog(1/δ )\n2n\nThe following result improves the best known results of the same kind, [Langford et al., 2001, The-\norem 5] and [Zhang, 2002, Theorem 7], by removing a factor of √log n. These results themselves\nwere an improvement over previous results obtained using fat-shattering dimension estimates.\nCorollary 7. (Entropy Based Margins) Let X be such that for all x∈ X,∥x∥∞≤X. Consider\nthe classW ={w∈Rd : ∥w∥1≤W1}. Fix an arbitrary priorµ. We have that with probability\nat least 1−δ over the sample, for all margins γ > 0 and all weight vector w∈W,\nL(w)≤Kγ (w) + 8. 5 XW 1\nγ\n√\nentroµ(w) + 2. 5\nn +\n√\nlog(log2\n4XW 1\nγ )\nn +\n√\nlog(1/δ )\n2n\nwhere entroµ(w) := ∑\ni\n|wi|\n∥w∥1\nlog( |wi|\nµi∥w∥1\n)\nProof. Proof is provided in the appendix.\n4.3 PAC-Bayes Theorem\nWe now show that (a form of) the PAC Bayesian theorem [McAllester, 1999] is a consequence of\nTheorem 3. In the PAC Bayesian theorem, we have a set of hypothesis (possibly inﬁnite) C. We\nchoose some prior distribution over this hypothesis set say µ, and after observing the training data,\nwe choose any arbitrary posterior ν and the loss we are interested in is ℓ ν (x, y ) = Ec∼νℓ(c, x, y )\nthat is basically the expectation of the loss when hypothesis c∈Care drawn i.i.d. using distribution\nν . Note that in this section we are considering a more general form of the loss.\nThe key observation as that we can view ℓ ν (x) as the inner product ⟨dν (·), ℓ (·, x, y )⟩ between the\nmeasure dν (·) and the loss ℓ(·, x ). This leads to the following straightforward corollary.\nCorollary 8. (PAC-Bayes) F or a ﬁxed priorµ over the hypothesis setC, and any loss bounded by 1,\nwith probability at least 1−δ over the sample, simultaneously for all choice of posteriors ν overC\nwe have that,\nLν ≤ˆLν + 4. 5\n√\nmax{KL(ν∥µ), 2}\nn +\n√\nlog(1/δ )\n2n (8)\nProof. Proof is provided in the appendix.\nInterestingly, this result is an improvement over the original statement, in which the last term was√\nlog(n/δ )/n. Our bound removes this extra log(n) factor, so, in the regime where we ﬁx ν and\nexamine large n, this bound is sharper. We note that our goal was not to prove the PAC-Bayes\ntheorem, and we have made little attempt to optimize the constants.\n4.4 Covering Number Bounds\nIt is worth noting that using Sudakov’s minoration results we can obtain upper bound on the L2\n(and hence also L1) covering numbers using the Gaussian complexities. The following is a direct\ncorollary of the Sudakov minoration theorem for Gaussian complexities (Theorem 3.18, Page 80 of\nLedoux and Talagrand [1991]).\nCorollary 9. LetFW be the function class from Theorem 3. There exists a universal constant K > 0\nsuch that its L2 covering number is bounded as follows:\n∀ǫ >0 log( N2(FW , ǫ, n ))≤2K 2X 2W 2\n∗\nσǫ 2\nThis bound is sharper than those that could be derived from the N∞covering number bounds of\nZhang [2002].\n5 Discussion: Relations to Online, Regret Minimizing, Algorithms\nIn this section, we make a further assumption that loss ℓ(⟨w, x⟩ , y ) is convex in its ﬁrst argument.\nWe now show that in the online setting that the regret bounds for linear prediction closely match our\nrisk bounds. The algorithm we consider performs the update,\nwt+1 =∇F−1(∇F(wt)−η∇wℓ(⟨wt, xt⟩ , y t)) (9)\nThis algorithm captures both gradient updates, multiplicative updates, and updates based on the Lp\nnorms, through appropriate choices of F . See Shalev-Shwartz [2007] for discussion.\nFor the algorithm given by the above update, the following theorem is a bound on the cumulative\nregret. It is a corollary of Theorem 1 in Shalev-Shwartz and Singer [2006] (and also of Corollary 1\nin Shalev-Shwartz [2007]), applied to our linear case.\nCorollary 10. (Shalev-Shwartz and Singer [2006]) Let S be a closed convex set and let F : S→R\nbe σ -strongly convex w.r .t.∥·∥ ∗. Further , letX ={x: ∥x∥≤X}andW ={w∈S : F (w)≤\nW 2\n∗}. Then for the update given by Equation 9 if we start with w1 = argmin F (w), we have that\nfor all sequences{(xt, y t)}n\nt=1,\nn∑\nt=1\nℓ(⟨wt, xt⟩ , y t)−argmin\nw∈W\nn∑\nt=1\nℓ(⟨w, xt⟩ , y t)≤Lℓ XW∗\n√\n2n\nσ\nFor completeness, we provide a direct proof in the Appendix. Interestingly, the regret above is\nprecisely our complexity bounds (when Lℓ = 1 ). Also, our risk bounds are a factor of 2 worse,\nessentially due to the symmetrization step used in proving Theorem 1.\nReferences\nP . L. Bartlett and S. Mendelson. Rademacher and Gaussian complexities: Risk bounds and structural results.\nJournal of Machine Learning Research, 3:463–482, 2002.\nP . L. Bartlett and J. Shawe-Taylor. Generalization performance of support vector machines and other pattern\nclassiﬁers. In B. Sch ¨olkopf, C. J. C. Burges, and A. J. Smola, editors, Advances in Kernel Methods – Support\nV ector Learning, pages 43–54. MIT Press, 1999.\nN. Cesa-Bianchi and G. Lugosi. Prediction, learning, and games . Cambridge University Press, 2006.\nV . Koltchinskii and D. Panchenko. Empirical margin distributions and bounding the generalization error of\ncombined classiﬁers. Annals of Statistics , 30(1):1–50, 2002.\nJ. Langford and J. Shawe-Taylor. PAC-Bayes & margins. In Advances in Neural Information Processing\nSystems 15, pages 423–430, 2003.\nJ. Langford, M. Seeger, and Nimrod Megiddo. An improved predictive accuracy bound for averaging classiﬁers.\nIn Proceedings of the Eighteenth International Conference on Machine Learning, pages 290–297, 2001.\nM. Ledoux and M. Talagrand. Probability in Banach spaces: Isoperimetry and processes , volume 23 of Ergeb-\nnisse der Mathematik und ihrer Grenzgebiete (3) . Springer-V erlag, 1991.\nDavid A. McAllester. Simpliﬁed PAC-Bayesian margin bounds. In Proceedings of the Sixteenth Annual Con-\nference on Computational Learning Theory, pages 203–215, 2003.\nDavid A. McAllester. PAC-Bayesian model averaging. In Proceedings of the Twelfth Annual Conference on\nComputational Learning Theory, pages 164–170, 1999.\nRon Meir and Tong Zhang. Generalization error bounds for Bayesian mixture algorithms. Journal of Machine\nLearning Research, 4:839–860, 2003.\nA.Y . Ng. Feature selection,l1 vs. l2 regularization, and rotational invariance. In P roceedings of the Twenty-First\nInternational Conference on Machine Learning, 2004.\nDavid Pollard. Convergence of Stochastic Processes. Springer-V erlag, 1984.\nR.E. Schapire, Y . Freund, P . Bartlett, and W.S. Lee. Boosting the margin: A new explanation for the effective-\nness of voting methods. The Annals of Statistics , 26(5):1651–1686, October 1998.\nS. Shalev-Shwartz. Online Learning: Theory, Algorithms, and Applications . PhD thesis, The Hebrew Univer-\nsity, 2007.\nS. Shalev-Shwartz and Y . Singer. Convex repeated games and Fenchel duality. In Advances in Neural Informa-\ntion Processing Systems 20, 2006.\nM. Warmuth and A. K. Jagota. Continuous versus discrete-time non-linear gradient descent: Relative loss\nbounds and convergence. In Fifth International Symposium on Artiﬁcial Intelligence and Mathematics ,\n1997.\nT. Zhang. Covering number bounds of certain regularized linear function classes. Journal of Machine Learning\nResearch, 2:527–550, 2002.",
  "values": {
    "Explicability": "No",
    "Critiqability": "No",
    "Collective influence": "No",
    "Privacy": "No",
    "Beneficence": "No",
    "User influence": "No",
    "Fairness": "No",
    "Justice": "No",
    "Not socially biased": "No",
    "Respect for Persons": "No",
    "Autonomy (power to decide)": "No",
    "Interpretable (to users)": "No",
    "Respect for Law and public interest": "No",
    "Transparent (to users)": "No",
    "Deferral to humans": "No",
    "Non-maleficence": "No"
  }
}