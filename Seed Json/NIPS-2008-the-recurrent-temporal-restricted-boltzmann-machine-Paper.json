{
  "pdf": "NIPS-2008-the-recurrent-temporal-restricted-boltzmann-machine-Paper",
  "title": "The Recurrent Temporal Restricted Boltzmann Machine",
  "author": "Ilya Sutskever, Geoffrey E. Hinton, Graham W. Taylor",
  "paper_id": "NIPS-2008-the-recurrent-temporal-restricted-boltzmann-machine-Paper",
  "text": "The Recurrent Temporal Restricted Boltzmann\nM\nachine\nIlya Sutskever, Geoffrey Hinton, and Graham Taylor\nU\nniversity of Toronto\n{ilya, hinton, gwtaylor}@cs.utoronto.ca\nAbstract\nThe Temporal Restricted Boltzmann Machine (TRBM) is a probabilistic model for\nsequences that is able to successfully model (i.e., generate nice-looking samples\nof) several very high dimensional sequences, such as motion capture data and the\npixels of low resolution videos of balls bouncing in a box. The major disadvan-\ntage of the TRBM is that exact inference is extremely hard, since even computing\na Gibbs update for a single variable of the posterior is exponentially expensive.\nThis difﬁculty has necessitated the use of a heuristic inference procedure, that\nnonetheless was accurate enough for successful learning. In this paper we intro-\nduce the Recurrent TRBM, which is a very slight modiﬁcation of the TRBM for\nwhich exact inference is very easy and exact gradient learning is almost tractable.\nWe demonstrate that the RTRBM is better than an analogous TRBM at generating\nmotion capture and videos of bouncing balls.\n1 Introduction\nModeling sequences is an important problem since there is a vast amount of natural data, such as\nspeech and videos, that is inherently sequential. A good model for these data sources could be useful\nfor ﬁnding an abstract representation that is helpful for solving “natural” discrimination tasks (see\n[4] for an example of this approach for the non-sequential case). In addition, it could be also used\nfor predicting the future of a sequence from its past, be used as a prior for denoising tasks, and be\nused for other applications such as tracking objects in video. The Temporal Restricted Boltzmann\nMachine [14, 13] is a recently introduced probabilistic model that has the ability to accurately model\ncomplex probability distributions over high-dimensional sequences. It was shown to be able to\ngenerate realistic motion capture data [14], and low resolution videos of 2 balls bouncing in a box\n[13], as well as complete and denoise such sequences.\nAs a probabilistic model, the TRBM is a directed graphical model consisting of a sequence of Re-\nstricted Boltzmann Machines (RBMs) [3], where the state of one or more previous RBMs determines\nthe biases of the RBM in next timestep. This probabilistic formulation straightforwardly implies a\nlearning procedure where approximate inference is followed by learning. The learning consists of\nlearning a conditional RBM at each timestep, which is easily done with Contrastive Divergence\n(CD) [3]. Exact inference in TRBMs, on the other hand, is highly non-trivial, since computing even\na single Gibbs update requires computing the ratio of two RBM partition functions. The approx-\nimate inference procedure used in [13] was heuristic and was not even derived from a variational\nprinciple.\nIn this paper we introduce the Recurrent TRBM (RTRBM), which is a model that is very similar\nto the TRBM, and just as expressive. Despite the similarity, exact inference is very easy in the\nRTRBM and computing the gradient of the log likelihood is feasible (up to the error introduced\nby the use of Contrastive Divergence). We demonstrate that the RTRBM is able to generate more\nrealistic samples than an equivalent TRBM for the motion capture data and for the pixels of videos\nof bouncing balls. The RTRBM’s performance is better than the TRBM mainly because it learns to\nconvey more information through its hidden-to-hidden connections.\n2 Restricted Boltzmann Machines\nThe building block of the TRBM and the RTRBM is the Restricted Boltzmann Machine [3]. An\nRBM deﬁnes a probability distribution over pairs of vectors, V ∈{0, 1}NV and H∈{0, 1}NH (a\nshorthand for visible and hidden) by the equation\nP (v, h) = P (V = v, H = h) = exp( v⊤ bV + h⊤ bH + v⊤ W h)/Z (1)\nwhere bV is a vector of biases for the visible vectors, bH is a vector of biases for the hidden vectors,\nand W is the matrix of connection weights. The quantity Z = Z(bV , bH , W ) is the value of the\npartition function that ensures that Eq. 1 is a valid probability distribution. The RBM’s deﬁnition\nimplies that the conditional distributions P (H|v) and P (V|h) are factorial (i.e., all the compo-\nnents of H in P (H|v) are independent) and are given by P (H (j) = 1|v) = s(bH + W ⊤ v)(j) and\nP (V (i) = 1|h) = s(bV + W h)(i), where s(x)(j) = (1 + exp(−x(j)))−1 is the logistic function\nand x(j) is the jth component of the vector x. In general, we use i to index visible vectors V and j\nto index hidden vectors H. 1 The RBM can be slightly modiﬁed to allow the vector V to take real\nvalues; one way of achieving this is by the deﬁnition\nP (v, h) = exp(−∥v∥2/2 + v⊤ bV + h⊤ bH + v⊤ W h)/Z. (2)\nUsing this equation does not change the form of the gradients and the conditional distribution\nP (H|v). The only change it introduces is in the conditional distribution P (V|h), which is equal\nto a multivariate Gaussian with parameters N (bV + W h, I). See [18, 14] for more details and\ngeneralizations.\nThe gradient of the average log probability given a dataset S, L = 1 /|S|∑\nv∈S log P (v), has the\nfollowing simple form:\n∂L/∂W =\n⟨\nV· H ⊤ ⟩\nP (H|V) ˜P (V )−\n⟨\nV· H ⊤ ⟩\nP (H,V ) (3)\nwhere ˜P (V ) = 1 /|S|∑\nv∈S δv(V ) (here δx(X) is a distribution over real-valued vectors that is\nconcentrated at x), and⟨f (X)⟩P (X) is the expectation off (X) under the distribution P . Computing\nthe exact values of the expectations ⟨·⟩P (H,V ) is computationally intractable, and much work has\nbeen done on methods for computing approximate values for the expectations that are good enough\nfor practical learning and inference tasks (e.g., [16, 12, 19], including [15], which works well for\nthe RBM).\nWe will approximate the gradients with respect to the RBM’s parameters using the Contrastive\nDivergence [3] learning procedure, CDn, whose updates are computed by the following algorithm.\nAlgorithm 1 (CDn)\n1. Sample (v, h)∼P (H|V ) ˜P (V )\n2. Set ∆W to v· h⊤\n3. repeat n times: sample v∼P (V|h), then sample h∼P (H|v)\n4. Decrease ∆W by v· h⊤\nModels learned by CD 1 are often reasonable generative models of the data [3], but if learning is\ncontinued with CD25, the resulting generative models are much better [11]. The RBM also plays a\ncritical role in deep belief networks [4], [5], but we do not use this connection in this paper.\n3 The TRBM\nIt is easy to construct the TRBM with RBMs. The TRBM, as described in the introduction, is\na sequence of RBMs arranged in such a way that in any given timestep, the RBM’s biases de-\npend only on the state of the RBM in the previous timestep. In its simplest form, the TRBM can\n1W e use uppercase variables (as in P (H|v)) to denote distributions and lowercase variables (as in P (h|v))\nto denote the (real-valued) probability P (H = h|v).\nFigure 1: The graphical structure of a TRBM: a directed sequen ce of RBMs.\nbe viewed as a Hidden Markov Model (HMM) [9] with an exponentially large state space that\nhas an extremely compact parameterization of the transition and the emission probabilities. Let\nX tB\ntA = ( XtA , . . . , XtB ) denote a sequence of variables. The TRBM deﬁnes a probability distribu-\ntion P (V T\n1 = vT\n1 , HT\n1 = hT\n1 ) by the equation\nP (vT\n1 , hT\n1 ) =\nT∏\nt=2\nP (vt, ht|ht−1 )P0(v1, h1) (4)\nwhich is identical to the deﬁning equation of the HMM. The conditional distributionP (Vt, Ht|ht−1 )\nis that of an RBM, whose biases for Ht are a function of ht−1 . Speciﬁcally,\nP (vt, ht|ht−1 ) = exp\n(\nv⊤\nt bV + v⊤\nt W ht + h⊤\nt (bH + W ′ht−1 )\n)\n/Z(ht−1 ) (5)\nwhere bV , bH and W are as in Eq. 1, while W ′is the weight matrix of the connections from Ht−1\nto Ht, making bH + W ′ht−1 be the bias of RBM at time t. In this equation, V ∈{0, 1}NV and\nH∈{0, 1}NH ; it is easy to modify this deﬁnition to allowV to take real values as was done in Eq. 2.\nThe RBM’s partition function depends onht−1 , because the parameters (i.e., the biases) of the RBM\nat time t depend on the value of the random variable Ht−1 . Finally, the distribution P0 is deﬁned\nby an equation very similar to Eq. 5, except that the (undeﬁned) term W ′h0 is replaced by the\nterm binit, so the hidden units receive a special initial bias at P0; we will often write P (V1, H1|h0)\nfor P0(V1, H1) and W ′h0 for binit. It follows from these equations that the TRBM is a directed\ngraphical model that has an (undirected) RBM at each timestep (a related directed sequence of\nBoltzmann Machines has been considered in [7]).\nAs in most probabilistic models, the weight update is computed by solving the inference problem\nand computing the weight update as if the inferred variables were observed. fully-visible case. If\nthe hidden variables are observed, equation 4 implies that the gradient of the log likelihood with\nrespect to the TRBM’s parameters is ∑T\nt=1∇logP (vt, ht|ht−1 ), and each term, being the gradient\nof the log likelihood of an RBM, can be approximated using CD n. Thus the main computational\ndifﬁculty of learning TRBMs is in obtaining samples from a distribution approximating the posterior\nP (H T\n1|vT\n1 ).\nInference in a TRBM\nUnfortunately, the TRBM’s inference problem is harder than that of a typical undirected graphical\nmodel, because even computing the probability P (H (j)\nt = 1|everything else) involves evaluating\nthe exact ratio of two RBM partition functions, which can be seen from Eq. 5. This difﬁculty ne-\ncessitated the use of a heuristic inference procedure [13], which is based on the observation that the\ndistribution P (Ht|ht−1\n1 , vt\n1) = P (Ht|ht−1 , vt) is factorial by deﬁnition. This inference procedure\ndoes not do any kind of smoothing from the future and only does approximate ﬁltering from the past\nby sampling from the distribution ∏T\nt=1 P (Ht|H t−1\n1 , vt\n1) instead of the true posterior distribution∏T\nt=1 P (Ht|H t−1\n1 , vT\n1 ), which is easy because P (Ht|ht−1\n1 , vt\n1) is factorial. 2\n4 Recurrent TRBMs\nLet us start with notation. Consider an arbitrary factorial distribution P ′(H). The statement h∼\nP ′(H) means that h is sampled from the factorial distribution P ′(H), so each h(j) is set to 1 with\n2T his is a slightly simpliﬁed description of the inference procedure in [13].\nFigure 2: The graphical structure of the RTRBM, Q. The variables Ht are real valued while the\nvariables H ′\nt are binary. The conditional distribution Q(Vt, H′\nt|ht−1 ) is given by the equation\nQ(vt, h′\nt|ht−1 ) = exp\n(\nv⊤\nt W h′\nt + v⊤\nt bV + h′\nt(bH + W ′ht−1 )\n)\n/Z(ht−1 ), which is essentially the\nsame as the TRBM’s conditional distribution P from equation 5. We will always integrate out H ′\nt\nand will work directly with the distributionQ(Vt|ht−1 ). Notice that when V1 is observed, H ′\n1 cannot\naffect H1.\nprobability P ′(H (j) = 1), and 0 otherwise. In contrast, the statement h←P ′(H) means that each\nh(j) is set to the real value P ′(H (j) = 1) , so this is a “mean-ﬁeld” update [8, 17]. The symbol P\nstands for the distribution of some TRBM, while the symbolQ stands for the distribution deﬁned by\nan RTRBM. Note that the outcome of the operation·←P (Ht|vt, ht−1 ) is s(W vt + W ′ht−1 + bH ).\nAn RTRBM, Q(V T\n1 , HT\n1 ), is deﬁned by the equation\nQ(vT\n1 , hT\n1 ) =\nT∏\nt=2\nQ(vt|ht−1 )Q(ht|vt, ht−1 )· Q0(v1). Q0(h1|v1) (6)\nThe terms appearing in this equation will be deﬁned shortly.\nLet us contrast the generative process of the two models. To sample from a TRBM P , we need\nto perform a directed pass, sampling from each RBM on every timestep. One way of doing this is\ndescribed by the following algorithm.\nAlgorithm 2 (for sampling from the TRBM):\nfor 1≤t≤T :\n1. sample vt∼P (Vt|ht−1 )\n2. sample ht∼P (Ht|vt, ht−1 ) 3\nwhere step 1 requires sampling from the marginals of a Boltzmann Machine (by integrating outHt),\nwhich involves running a Markov chain.\nBy deﬁnition, RTRBMs and TRBMs are parameterized in the same way, so from now on we will\nassume that P and Q have identical parameters, which are W, W ′, bV , bH, and binit. The following\nalgorithm samples from the RTRBM Q under this assumption.\nAlgorithm 3 (for sampling from the RTRBM)\nfor 1≤t≤T :\n1. sample vt∼P (Vt|ht−1 )\n2. set ht←P (Ht|vt, ht−1 )\nWe can infer that Q(Vt|ht−1 ) = P (Vt|ht−1 ) because of step 1 in Algorithm 3, which is also con-\nsistent with the equation given in ﬁgure 2 where H ′\nt is integrated out. The only difference between\nAlgorithm 2 and Algorithm 3 is in step 2. The difference may seem small, since the operations\nht∼P (Ht|vt, ht−1 ) and ht←P (Ht|vt, ht−1 ) appear similar. However, this difference signiﬁ-\ncantly alters the inference and learning procedures of the RTRBM; in particular, it can already be\nseen that Ht are real-valued for the RTRBM.\n3W hen t = 1, P (Ht|vt, ht−1) stands for P0(H1|v1), and similarly for other conditional distributions. The\nsame convention is used in all algorithms.\n4.1 Inference in RTRBMs\nI\nnference in RTRBMs given vT\n1 is very easy, which might be surprising in light of its similarity to\nthe TRBM. The reason inference is easy is similar to the reason inference in square ICAs is easy [1]:\nThere is a unique and an easily computable value of the hidden variables that has a nonzero posterior\nprobability. Suppose, for example, that the value of V1 is v1, which means that v1 was produced at\nthe end of step 1 in Algorithm 3. Since step 2, the deterministic operation h1←P0(H1|v1), has\nbeen executed, the only valueh1 can take is the value assigned by the operation·←P0(H1|v1). Any\nother value for h1 is never produced by a generative process that outputs v1 and thus has posterior\nprobability 0. In addition, by executing this operation, we can recover h1. Thus, Q0(H1|v1) =\nδs(W v1+bH +binit)(H1). Note that H1’s value is completely independent of vT\n2 .\nOnce h1 is known, we can consider the generative process that produced v2. As before, since v2\nwas produced at the end of step 1, then the fact that step 2 has been executed implies that h2 can be\ncomputed by h2←P (H2|v2, h1) (recall that at this point h1 is known with absolute certainty). If\nthe same reasoning is repeated t times, then all of ht\n1 is uniquely determined and is easily computed\nwhen V t\n1 is known. There is no need for smoothing because Vt and Ht−1 inﬂuence Ht with such\nstrength that the knowledge of V T\nt+1 cannot alter the model’s belief about Ht. This is because\nQ(Ht|vt, ht−1 ) = δs(W vt+bH +W ′ht− 1)(Ht).\nThe resulting inference algorithm is simple:\nAlgorithm 4 (inference in RTRBMs)\nfor 1≤t≤T :\n1. ht←P (Ht|vt, ht−1 )\nLet h(v)T\n1 denote the output of the inference algorithm on input vT\n1 , in which case the posterior is\ndescribed by\nQ(H T\n1|vT\n1 ) = δh(v)T\n1\n(H T\n1 ). (7)\n4.2 Learning in RTRBMs\nLearning in RTRBMs may seem easy once inference is solved, since the main difﬁculty in learning\nTRBMs is the inference problem. However, the RTRBM does not allow EM-like learning because\nthe equation ∇logQ(vT\n1 ) =\n⟨\n∇logQ(vT\n1 , hT\n1 )\n⟩\nhT\n1 ∼Q( H T\n1 |vT\n1 ) is not meaningful. To be precise,\nthe gradient∇logQ(vT\n1 , hT\n1 ) is undeﬁned because δs(W ′ht− 1+bH +W T vt)(ht) is not, in general, a\ncontinuous function of W . Thus, the gradient has to be computed differently.\nNotice that the RTRBM’s log probability satisﬁes log Q(vT\n1 ) = ∑T\nt=1 log Q(vt|vt−1\n1 ), so we could\ntry computing the sum ∇∑T\nt=1 log Q(vt|vt−1\n1 ). The key observation that makes the computation\nfeasible is the equation\nQ(Vt|vt−1\n1 ) = Q(Vt|h(v)t−1 ) (8)\nwhere h(v)t−1 is the value computed by the RTRBM inference algorithm with inputsvt\n1. This equa-\ntion holds because Q(vt|vt−1\n1 ) =\n∫\nh′\nt− 1\nQ(vt|h′\nt−1 )Q(h′\nt−1|vt−1\n1 )dh′\nt−1 = Q(vt|h(v)t−1 ), as the\nposterior distribution Q(Ht−1|vt−1\n1 ) = δh(v)t− 1(Ht−1 ) is a point-mass at h(v)t−1 , which follows\nfrom Eq. 7.\nThe equality Q(Vt|vt−1\n1 ) = Q(Vt|h(v)t−1 ) allows us to deﬁne a recurrent neural network (RNN)\n[10] whose parameters are identical to those of the RTRBM, and whose cost function is equal to the\nlog likelihood of the RTRBM. This is useful because it is easy to compute gradients with respect to\nthe RNN’s parameters using the backpropagation through time algorithm [10]. The RNN has a pair\nof variables at each timestep,{(vt, rt)}T\nt=1, where vt are the input variables and rt are the RNN’s\nhidden variables (all of which are deterministic). The hiddens rT\n1 are computed by the equation\nrt = s(W vt + bH + W ′rt−1 ) (9)\nwhere W ′rt−1 is replaced with binit when t = 1 . This deﬁnition was chosen so that the equation\nrT\n1 = h(v)T\n1 would hold. The RNN attempts to probabilistically predict the next timestep from its\nhistory using the marginal distribution of the RBM Q(Vt+1|rt), so its objective function at time t is\ndeﬁned to be log Q(vt+1|rt), where Q depends on the RNN’s parameters in the same way it depends\non the RTRBM’s parameters (the two sets of parameters being id entical). This is a valid deﬁnition\nof an RNN whose cumulative objective for the sequence vT\n1 is\nO =\nT∑\nt=1\nlog Q(vt|rt−1 ) (10)\nwhere Q(v1|r0) = Q0(v1). But since rt as computed in equation 9 on inputvT\n1 is identical to h(v)t,\nthe equality log Q(vt|rt−1 ) = log Q(vt|vt−1\n1 ) holds. Substituting this identity into Eq. 10 yields\nO =\nT∑\nt=1\nlog Q(vt|rt−1 ) =\nT∑\nt=1\nlog Q(vt|vt−1\n1 ) = log Q(vT\n1 ) (11)\nwhich is the log probability of the corresponding RTRBM.\nThis means that∇O=∇log Q(vT\n1 ) can be computed with the backpropagation through time algo-\nrithm [10], where the contribution of the gradient from each timestep is computed with Contrastive\nDivergence.\n4.3 Details of the backpropagation through time algorithm\nThe backpropagation through time algorithm is identical to the usual backpropagation algorithm\nwhere the feedforward neural network is turned “on its side”. Speciﬁcally, the algorithm maintains\na term ∂O/∂r t which is computed from ∂O/∂r t+1 and ∂ log Q(vt+1|rt)/∂rt using the chain rule,\nby the equation\n∂O/∂r t = W ′⊤ (rt+1.(1−rt+1).∂O/∂r t+1) + W ′⊤∂ log Q(vt|rt−1 )/∂bH (12)\nwhere a.b denotes component-wise multiplication, the term rt.(1−rt) arises from the derivative of\nthe logistic function s′(x) = s(x).(1−s(x)), and ∂ log Q(vt+1|rt)/∂bH is computed by CD. Once\n∂O/∂r t is computed for all t, the gradients of the parameters can be computed using the following\nequations\n∂O\n∂W ′ =\nT∑\nt= 2\nrt−1 (rt.(1−rt).∂O/∂r t)⊤ (13)\n∂O\n∂W =\nT −1∑\nt= 1\nvt\n(\nW ′⊤ (rt+1.(1−rt+1).∂O/∂r t+1)\n) ⊤\n+\nT∑\nt=1\n∂ log Q(vt|rt−1 )/∂W (14)\nThe ﬁrst summation in Eq. 14 arises from the use ofW as weights for inference for computingrt and\nthe second summation arises from the use of W as RBM parameters for computing log Q(vt|rt−1 ).\nEach term of the form∂ log Q(vt+1|rt)/∂W is also computed with CD. Computing∂O/∂r t is done\nmost conveniently with a single backward pass through the sequence. As always, log Q(v1|r0) =\nQ0(v1). It is also seen that the gradient would be computed exactly if CD were to return the exact\ngradient of the RBM’s log probability.\n5 Experiments\nWe report the results of experiments comparing an RTRBM to a TRBM. The results in [14, 13] were\nobtained using TRBMs that had several delay-taps, which means that each hidden unit could directly\nobserve several previous timesteps. To demonstrate that the RTRBM learns to use the hidden units to\nstore information, we did not use delay-taps for the RTRBM nor the TRBM, which causes the results\nto be worse (but not much) than in [14, 13]. If delay-taps are allowed, then the results of [14, 13]\nshow that there is little beneﬁt from the hidden-to-hidden connections (which are W ′), making the\ncomparison between the RTRBM and the TRBM uninteresting.\nIn all experiments, the RTRBM and the TRBM had the same number of hidden units, their param-\neters were initialized in the same manner, and they were trained for the same number of weight\nupdates. When sampling from the TRBM, we would use the sampling procedure of the RTRBM\nusing the TRBM’s parameters to eliminate the additional noise from its hidden units. If this is not\ndone, the samples produced by the TRBM are signiﬁcantly worse. Unfortunately, the evaluation\nmetric is entirely qualitative since computing the log probability on a test set is infeasible for both\nthe TRBM and the RTRBM. We provide the code for our experiments in [URL].\nFigure 3: This ﬁgure shows the receptive ﬁelds of the ﬁrst 36 hi dden units of the RTRBM on the\nleft, and the corresponding hidden-to-hidden weights between these units on the right: theith row on\nthe right corresponds to the ith receptive ﬁeld on the left, when counted left-to-right. Hidden units\n18 and 19 exhibit unusually strong hidden-to-hidden connections; they are also the ones with the\nweakest visible-hidden connections, which effectively makes them belong to another hidden layer.\n5.1 Videos of bouncing balls\nWe used a dataset consisting of videos of 3 balls bouncing in a box. The videos are of length 100\nand of resolution 30×30. Each training example is synthetically generated, so no training sequence\nis seen twice by the model which means that overﬁtting is highly unlikely. The task is to learn to\ngenerate videos at the pixel level. This problem is high-dimensional, having 900 dimensions per\nframe, and the RTRBM and the TRBM are given no prior knowledge about the nature of the task\n(e.g., by convolutional weight matrices).\nBoth the RTRBM and the TRBM had 400 hidden units. Samples from these models are provided as\nvideos 1,2 (RTRBM) and videos 3,4 (TRBM). A sample training sequence is given in video 5. All\nthe samples can be found in [URL]. The real-values in the videos are the conditional probabilities\nof the pixels [13]. The RTRBM’s samples are noticeably better than the TRBM’s samples; a key\ndifference between these samples is that the balls produced by the TRBM moved in a random walk,\nwhile those produced by the RTRBM moved in a more persistent direction. An examination of the\nvisible to hidden connection weights of the RTRBM reveals a number of hidden units that are not\nconnected to visible units. These units have the most active hidden to hidden connections, which\nmust be used to propagate information through time. In particular, these units are the only units that\ndo not have a strong self connection (i.e.,W ′\ni,i is not large; see ﬁgure 3). No such separation of units\nis found in the TRBM and all its hidden units have large visible to hidden connections.\n5.2 Motion capture data\nWe used a dataset that represents human motion capture data by sequences of joint angle, transla-\ntions, and rotations of the base of the spine [14]. The total number of frames in the dataset set was\n3000, from which the model learned on subsequences of length 50. Each frame has 49 dimensions,\nand both models have 200 hidden units. The data is real-valued, so the TRBM and the RTRBM\nwere adapted to have Gaussian visible variables using equation 2. The samples produced by the\nRTRBM exhibit less sticking and foot-skate than those produced by the TRBM; samples from these\nmodels are provided as videos 6,7 (RTRBM) and videos 8,9 (TRBM); video 10 is a sample training\nsequence. Part of the Gaussian noise was removed in a manner described in [14] in both models.\n5.3 Details of the learning procedures\nEach problem was trained for 100,000 weight updates, with a momentum of 0.9, where the gradi-\nent was normalized by the length of the sequence for each gradient computation. The weights are\nupdated after computing the gradient on a single sequence. The learning starts with CD 10 for the\nﬁrst 1000 weight updates, which is then switched to CD25. The visible to hidden weights, W , were\ninitialized with static CD 5 (without using the (R)TRBM learning rules) on 30 sequences (which\nresulted in 30 weight updates) with learning rate of 0.01 and momentum 0.9. These weights were\nthen given to the (R)TRBM learning procedure, where the learning rate was linearly reduced to-\nwards 0. The weights W ′and the biases were initialized with a sample from spherical Gaussian of\nstandard-deviation 0.005. For the bouncing balls problem the initial learning rate was 0.01, and for\nthe motion capture data it was 0.005.\n6 Conclusions\nI\nn this paper we introduced the RTRBM, which is a probabilistic model as powerful as the intractable\nTRBM that has an exact inference and an almost exact learning procedure. The common disadvan-\ntage of the RTRBM is that it is a recurrent neural network, a type of model known to have difﬁculties\nlearning to use its hidden units to their full potential [2]. However, this disadvantage is common to\nmany other probabilistic models, and it can be partially alleviated using techniques such as the long\nshort term memory RNN [6].\nAcknowledgments\nThis research was partially supported by the Ontario Graduate Scholarship and by the Natural Coun-\ncil of Research and Engineering of Canada. The mocap data used in this project was obtained\nfrom http://people.csail.mit.edu/ehsu/work/sig05stf/. For Matlab playback\nof motion and generation of videos, we have adapted portions of Neil Lawrence’s motion capture\ntoolbox (http://www.dcs.shef.ac.uk/∼neil/mocap/).\nReferences\n[1] A.J. Bell and T.J. Sejnowski. An Information-Maximization Approach to Blind Separation and Blind\nDeconvolution. Neural Computation, 7(6):1129–1159, 1995.\n[2] Y . Bengio, P. Simard, and P. Frasconi. Learning long-term dependencies with gradient descent is difﬁcult.\nNeural Networks, IEEE Transactions on, 5(2):157–166, 1994.\n[3] G.E. Hinton. Training Products of Experts by Minimizing Contrastive Divergence. Neural Computation,\n14(8):1771–1800, 2002.\n[4] G.E. Hinton, S. Osindero, and Y .W. Teh. A Fast Learning Algorithm for Deep Belief Nets. Neural\nComputation, 18(7):1527–1554, 2006.\n[5] G.E. Hinton and R.R. Salakhutdinov. Reducing the Dimensionality of Data with Neural Networks. Sci-\nence, 313(5786):504–507, 2006.\n[6] S. Hochreiter and J. Schmidhuber. Long Short-Term Memory. Neural Computation, 9(8):1735–1780,\n1997.\n[7] S. Osindero and G. Hinton. Modeling image patches with a directed hierarchy of Markov random ﬁelds.\nAdvances Neural Information Processing Systems, 2008.\n[8] C. Peterson and J.R. Anderson. A mean ﬁeld theory learning algorithm for neural networks. Complex\nSystems, 1(5):995–1019, 1987.\n[9] L.R. Rabiner. A tutorial on hidden Markov models and selected applications inspeech recognition. Pro-\nceedings of the IEEE, 77(2):257–286, 1989.\n[10] D.E. Rumelhart, G.E. Hinton, and R.J. Williams. Learning representations by back-propagating errors.\nNature, 323(6088):533–536, 1986.\n[11] R. Salakhutdinov and I. Murray. On the quantitative analysis of deep belief networks. In Proceedings of\nthe International Conference on Machine Learning, volume 25, 2008.\n[12] D. Sontag and T. Jaakkola. New Outer Bounds on the Marginal Polytope.Advances in Neural Information\nProcessing Systems, 2008.\n[13] I. Sutskever and G.E. Hinton. Learning multilevel distributed representations for high-dimensional se-\nquences. Proceeding of the Eleventh International Conference on Artiﬁcial Intelligence and Statistics ,\npages 544–551, 2007.\n[14] G.W. Taylor, G.E. Hinton, and S. Roweis. Modeling human motion using binary latent variables. Ad-\nvances in Neural Information Processing Systems, 19:1345–1352, 2007.\n[15] T. Tieleman. Training restricted boltzmann machines using approximations to the likelihood gradient. In\nProceedings of the International Conference on Machine Learning, volume 25, 2008.\n[16] M.J. Wainwright, T.S. Jaakkola, and A.S. Willsky. A new class of upper bounds on the log partition\nfunction. IEEE Transactions on Information Theory, 51(7):2313–2335, 2005.\n[17] M.J. Wainwright and M.I. Jordan. Graphical models, exponential families, and variational inference. UC\nBerkeley, Dept. of Statistics, Technical Report, 649, 2003.\n[18] M. Welling, M. Rosen-Zvi, and G. Hinton. Exponential family harmoniums with an application to infor-\nmation retrieval. Advances in Neural Information Processing Systems, 17:1481–1488, 2005.\n[19] J.S. Yedidia, W.T. Freeman, and Y . Weiss. Understanding belief propagation and its generalizations.\nExploring Artiﬁcial Intelligence in the New Millennium, pages 239–236, 2003.",
  "values": {
    "Critiqability": "No",
    "Explicability": "No",
    "Non-maleficence": "No",
    "Beneficence": "No",
    "Deferral to humans": "No",
    "Collective influence": "No",
    "Interpretable (to users)": "No",
    "User influence": "No",
    "Respect for Law and public interest": "No",
    "Fairness": "No",
    "Not socially biased": "No",
    "Respect for Persons": "No",
    "Privacy": "No",
    "Autonomy (power to decide)": "No",
    "Justice": "No",
    "Transparent (to users)": "No"
  }
}