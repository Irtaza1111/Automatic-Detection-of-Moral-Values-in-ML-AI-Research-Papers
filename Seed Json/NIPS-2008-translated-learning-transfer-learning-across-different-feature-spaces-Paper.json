{
  "pdf": "NIPS-2008-translated-learning-transfer-learning-across-different-feature-spaces-Paper",
  "title": "Translated Learning: Transfer Learning across Different Feature Spaces",
  "author": "Wenyuan Dai, Yuqiang Chen, Gui-rong Xue, Qiang Yang, Yong Yu",
  "paper_id": "NIPS-2008-translated-learning-transfer-learning-across-different-feature-spaces-Paper",
  "text": "Translated Learning: Transfer Learning across\nD\nifferent Feature Spaces\n†W enyuan Dai, †Yuqiang Chen, †Gui-Rong Xue, ‡Qiang Yang and †Yong Yu\n†Shanghai Jiao Tong University\nShanghai 200240, China\n{dwyak,yuqiangchen,grxue,yyu}@apex.sjtu.edu.cn\n‡Hong Kong University of Science and Technology\nKowloon, Hong Kong\nqyang@cse.ust.hk\nAbstract\nThis paper investigates a new machine learning strategy called translated learn-\ning. Unlike many previous learning tasks, we focus on how to use labeled data\nfrom one feature space to enhance the classiﬁcation of other entirely different\nlearning spaces. For example, we might wish to use labeled text data to help learn\na model for classifying image data, when the labeled images are difﬁcult to ob-\ntain. An important aspect of translated learning is to build a “bridge” to link one\nfeature space (known as the “source space”) to another space (known as the “tar-\nget space”) through a translator in order to migrate the knowledge from source to\ntarget. The translated learning solution uses a language model to link the class\nlabels to the features in the source spaces, which in turn is translated to the fea-\ntures in the target spaces. Finally, this chain of linkages is completed by tracing\nback to the instances in the target spaces. We show that this path of linkage can\nbe modeled using a Markov chain and risk minimization. Through experiments\non the text-aided image classiﬁcation and cross-language classiﬁcation tasks, we\ndemonstrate that our translated learning framework can greatly outperform many\nstate-of-the-art baseline methods.\n1 Introduction\nTraditional machine learning relies on the availability of a large amount of labeled data to train a\nmodel in the same feature space. However, labeled data are often scarce and expensive to obtain. In\norder to save much labeling work, various machine learning strategies have been proposed, including\nsemi-supervised learning [13], transfer learning [3, 11, 10], self-taught learning [9], etc. One com-\nmonality among these strategies is they all require the training data and test data to be in the same\nfeature space. For example, if the training data are documents, then the classiﬁers cannot accept test\ndata from a video space. However, in practice, we often face the problem where the labeled data are\nscarce in its own feature space, whereas there are sufﬁcient labeled data in other feature spaces. For\nexample, there may be few labeled images available, but there are often plenty of labeled text docu-\nments on the Web (e.g., through the Open Directory Project, or ODP,http://www.dmoz.org/).\nAnother example is cross-language classiﬁcation where labeled documents in English are much\nmore than ones in some other languages such as Bangla, which has only 21 Web pages in the ODP.\nTherefore, it would be great if we could learn the knowledge across different feature spaces and to\nsave a substantial labeling effort.\nTo address the transferring of knowledge across different feature spaces, researchers have proposed\nmulti-view learning [2, 8, 7] in which each instance has multiple views in different feature spaces.\nDifferent from multi-view learning, in this paper, we focus on the situation where the training data\nare in a source feature space, and the test data are in a different target feature space, and that there\nis no correspondence between instances in these spaces. The source and target feature spaces can be\n(a) Supervised Learning\n (b) Semi-supervised Learning\n(c) Transfer Learning\n (d) Self-taught Learning\n(e) Multi-view Learning\nElephants\na\nre large\nand gray ...\nbig\nm\nammals\non earth...\nthick-\ns\nkinned,\n...\nmassive\nh\noofed\nmammal ...\n(f) Translated Learning\n Test Data\nF\nigure 1: An intuitive illustration to different kinds of learning strategies using classiﬁcation of\nimage elephants and rhinos as the example. The images in orange frames are labeled data, while the\nones without frames are unlabeled data.\nvery different, as in the case of text and images. To solve this novel learning problem, we develop\na novel framework named as translated learning, where training data and test data can be in totally\ndifferent feature spaces. A translator is needed to be exploited to link the different feature spaces.\nClearly, the translated learning framework is more general and difﬁcult than traditional learning\nproblems. Figure 1 presents an intuitive illustration of six different learning strategies, including\nsupervised learning, semi-supervised learning [13], transfer learning [10], self-taught learning [9],\nmulti-view learning [2], and ﬁnally, translated learning.\nAn intuitive idea for translated learning is to somehow translate all the training data into a target\nfeature space, where learning can be done within a single feature space. This idea has already been\ndemonstrated successful in several applications in cross-lingual text classiﬁcation [1]. However, for\nthe more general translated learning problem, this idea is hard to be realized, since machine trans-\nlation between different feature spaces is very difﬁcult to accomplish in many non-natural language\ncases, such as translating documents to images. Furthermore, while a text corpus can be exploited\nfor cross-langauge translation, for translated learning, the learning of the “feature-space translator”\nfrom available resources is a key issue.\nOur solution is to make the best use of available data that have both features of the source and target\ndomains in order to construct a translator. While these data may not be sufﬁcient in building a good\nclassiﬁer for the target domain, as we will demonstrate in our experimental study in the paper, by\nleveraging the available labeled data in the source domain, we can indeed build effective translators.\nAn example is to translate between the text and image feature spaces using the social tagging data\nfrom Web sites such as Flickr (http://www.flickr.com/).\nThe main contribution of our work is to combine the feature translation and the nearest neighbor\nlearning into a uniﬁed model by making use of a language model [5]. Intuitively, our model can be\nrepresented using a Markov chain c → y → x, where y represents the features of the data instances\nx. In translated learning, the training data xs are represented by the features ys in the source feature\nspace, while the test data xt are represented by the features yt in the target feature space. We model\nthe learning in the source space through a Markov chain c → ys → xs, which can be connected to\nanother Markov chain c → yt → xt in the target space. An important contribution of our work then\nis to show how to connect these two paths, so that the new chain c → ys → yt → xt, can be used\nto translate the knowledge from the source space to the target one, where the mapping ys → yt is\nacting as a feature-level translator. In our ﬁnal solution, which we callTLRisk, we exploit the risk\nminimization framework in [5] to model translated learning. Our framework can accept different\ndistance functions to measure the relevance between two models.\n2 Translated Learning Framework\n2.1 Problem Formulation\nWe ﬁrst deﬁne the translated learning problem formally. LetXs be the source instance space. In this\nspace, each instance xs ∈ X s is represented by a feature vector (y(1)\ns , . . . , y(ns)\ns ), where y(i)\ns ∈ Y s\nand Ys i s the source feature space. Let Xt be the target instance space, in which each instance\nxt ∈ X t is represented by a feature vector (y(1)\nt , . . . , y(nt)\nt ), where y(i)\nt ∈ Y t and Yt is the target\nfeature space. We have a labeled training data set Ls = {(x(i)\ns , c(i)\ns )}n\ni=1 in the source space, where\nx(i)\ns ∈ X s and c(i)\ns ∈ C = {1, . . . ,|C|} is the true class-label of x(i)\ns . We also have another labeled\ntraining data set Lt = {(x(i)\nt , c(i)\nt )}m\ni=1 in the target space, wherex(i)\nt ∈ X t and c(i)\nt ∈ C . Usually, m\nis assumed to be small, so that Lt is not enough to train a reliable prediction model. The unlabeled\ntest data set U is a set of k examples {x(i)\nu }k\ni=1, where x(i)\nu ∈ X t. Note that x(i)\ns is in a different\nfeature space from x(i)\nt and x(i)\nu . For example, x(i)\ns may be a text document, while x(i)\nt and x(i)\nu may\nbe visual images.\nTo link the two feature spaces, a feature translator p(yt|ys) ∝ φ(yt, ys) is constructed. However,\nfor ease of explanation, we ﬁrst assume that the translator φ is given, and will discuss the derivation\nof φ later in this section, based on co-occurrence data. We focus on our main objective in learning,\nwhich is to estimate a hypothesis ht : Xt ↦→ C to classify the instances x(i)\nu ∈ U as accurately as\npossible, by making use of the labeled training data L = Ls ∪ Lt and the translator φ.\n2.2 Risk Minimization Framework\nFirst, we formulate our objective in terms of how to minimize an expected risk function with respect\nto the labeled training data L = Ls ∪ Lt and the translator φ by extending the risk minimization\nframework in [5].\nIn this work, we use the risk function R(c, xt) to measure the the risk for classifying xt to the\ncategory c. Therefore, to predict the label for an instance xt, we need only to ﬁnd the class-label c\nwhich minimizes the risk function R(c, xt), so that\nht(xt) = arg min\nc∈C\nR(c, xt). (1)\nThe risk function R(c, xt) can be formulate as theexpected loss when c and xt are relevant; formally,\nR(c, xt) ≡ L(r = 1|c, xt) =\n∫\nΘC\n∫\nΘXt\nL(θC, θXt , r = 1)p(θC|c) p(θXt |xt) dθXt dθC. (2)\nHere, r = 1 represents the event of “relevant”, which means (in Equation (2)) “cand xt are relevant”,\nor “the label ofxt is c”. θC and θXt are the models with respect to classesC and target space instances\nXt respectively. ΘC and ΘXt are two corresponding model spaces involving all the possible models.\nNote that, in Equation (2),θC only depends on c and θXt only depends to xt. Thus, we use p(θC|c) to\nreplace p(θC|c, xt), and use p(θXt |xt) to replace p(θXt |c, xt). L(θC, θXt , r = 1) is the loss function\nwith respect to the event of θC and θXt being relevant. We next address the estimation of the risk\nfunction in Equation (2).\n2.3 Estimation\nThe risk function in Equation (2) is difﬁcult to estimate, since the sizes of ΘC and ΘXt can be\nexponential in general. Therefore, we have to use approximation for estimating the risk function\nfor efﬁciency. First of all, the loss function L(θC, θXt , r = 1) can be formulated using distance\nfunctions between the two models θC and θXt, so that L(θC, θXt , r = 1) = α∆(θC, θXt ), where\n∆(θC, θXt ) is the distance (or dissimilarity) function, e.g. the Kullback-Leibler divergence. Replac-\ning L(θC, θXt, r = 1) with ∆(θC, θXt ), the risk function is reformulated as\nR(c, xt) ∝\n∫\nΘC\n∫\nΘXt\n∆(θC, θXt )p(θC|c) p(θXt|xt) dθXt dθC. (3)\nSince the sizes of ΘC and ΘXt are exponential in general, we cannot calculate Equation (3) straight-\nforwardly. In this paper, we approximate the risk function by its value at the posterior mode:\nR(c, xt) ≈ ∆(ˆθc, ˆθxt )p(ˆθc|c)p(ˆθxt |xt) ∝ ∆(ˆθc, ˆθxt )p(ˆθc|c), (4)\nwhere ˆθc = arg maxθC p(θC|c), and ˆθxt = arg maxθXt p(θXt|xt).\nIn Equation (4), p(ˆθc|c) is the prior probability of ˆθc with respect to the target class c. This prior can\nbe used to balance the inﬂuence of different classes in the class-imbalance case. When we assume\nthere is no prior difference among all the classes, the risk function can be rewritten into\nAlgorithm 1 R isk Minimization Algorithm for Translated Learning: (TLRisk)\nInput: L abeled training data L in the source space, unlabeled test data U in the target space, a\ntranslator φ to link the two feature spaces Ys and Yt and a dissimilarity function ∆(·, ·).\nOutput: The prediction label ht(xt) for each xt ∈ U .\nProcedureTLRisk train\n1\n: for each c ∈ C do\n2: Estimate the model ˆθc based on Equation (6).\n3: end for\nProcedureTLRisk test\n1\n: for each xt ∈ U do\n2: Estimate the model ˆθxt based on Equation (7).\n3: Predict the label ht(xt) for xt based on Equations (1) and (5).\n4: end for\nR(c, x t) ∝ ∆(ˆθc, ˆθxt ), (5)\nwhere ∆(ˆθc, ˆθxt ) denotes the dissimilarity between two modelsˆθc and ˆθxt. To achieve this objective,\nas in [5], we formulate these two models in the target feature space Yt; speciﬁcally, if we use KL\ndivergence as the distance function, ∆(ˆθc, ˆθxt ) can be measured by KL(p(Yt|ˆθc)||p(Yt|ˆθxt )).\nOur estimation is based on the Markov chain assumption where ˆθc → c → ys → yt → xt → ˆθxt\nand ˆθc → c → yt → xt → ˆθxt, so that\np(yt|ˆθc) =\n∫\nYs\n∑\nc′∈C\np(yt|ys)p(ys|c′)p(c′|ˆθc) dys + λ\n∑\nc′∈C\np(yt|c′)p(c′|ˆθc), (6)\nwhere p(yt|ys) can be estimated using the translator φ; p(ys|c′) can be estimated based on the\nstatistical observations in the labeled text data set Ls in the source feature space Ys; p(yt|c′) can be\nestimated based on Lt in the target feature space Yt; p(c′|ˆθc) can be calculated as: p(c′|ˆθc) = 1 if\nc = c′, and otherwise, p(c′|ˆθc) = 0 ; and λ is a trade-off parameter which controls the inﬂuence of\ntarget space labeled data Lt.\nFor another model p(Yt|ˆθxt ), it can be estimated by\np(yt|ˆθxt) =\n∫\nXt\np(yt|x′\nt)p(x′\nt|ˆθxt ) dx′\nt, (7)\nwhere p(yt|x′\nt) can be estimated using the feature extractor in the target feature space Yt, and\np(x′\nt|ˆθxt ) can be calculated as p(x′\nt|ˆθxt) = 1 if x′\nt = xt; otherwise p(x′\nt|ˆθxt ) = 0 .\nIntegrating Equations (1), (5), (6) and (7), our translated learning framework is summarized as\nalgorithmTLRisk, an abbreviation for Translated Learning via Risk Minimization, which is shown\nin Algorithm 1.\nConsidering the computational cost of Algorithm 1, due to the Markov chain assumption, our al-\ngorithmTLRisk can be implemented using dynamic programming. Therefore, in the worst case,\nthe time complexity of TLRisk is O(|C||Yt| + |Yt||Ys|) in training, and O(|C||Yt|) for predicting\nan instance. In practice, the data are quite sparse, and good feature mappings (or translator) should\nalso be sparse, otherwise it will consist of many ambiguous cases. Therefore,TLRisk can perform\nmuch faster than the worst cases generally, and the computational cost of TLRisk is linear in the\nnon-zero occurrences in feature mappings.\n2.4 Translator φ\nWe now explain in particular how to build the translator φ(yt, ys) ∝ p(yt|ys) to connect two dif-\nferent feature spaces. As mentioned before, to estimate the translator p(yt|ys), we need some co-\noccurrence data across the two feature spaces: source and target. Formally, we need co-occurrence\ndata in the form of p(yt, ys), p(yt, xs), p(xt, ys), or p(xt, xs). In cross-language problems, dictio-\nnaries can be considered as data in the form of p(yt, ys) (feature-level co-occurrence). On the Web,\nDA TA SET\nDA TA SIZE\nDA TA SET\nDA TA SIZE\nDOCUMENTS IMAGES DOCUMENTS IMAGES\n+ − + − + − + −\nhorse vs coin 1610 1345 270 123 dog vs canoe 1084 1047 102 103\nkayak vs bear 1045 885 102 101 greyhound vs cd 380 362 94 102\nelectric-guitar vs snake 335 326 122 112 stained-glass vs microwave 331 267 99 107\ncake vs binoculars 265 320 104 216 rainbow vs sheet-music 261 256 102 84\nlaptop vs sword 210 203 128 102 tomato vs llama 175 172 102 119\nbonsai vs comet 166 164 122 120 frog vs saddle 150 148 115 110\nTable 1: The description for each data set. Here, h orse vs coin indicates all the positive in-\nstances are about horse while all the negative instances are about coin. “+” means positive\ninstance; “−” means negative instances.\nsocial annotations on images (e.g. Flickr, images associated with keywords) and search-engine re-\nsults in response to queries are examples for correlational data in the forms ofp(yt, xs) and p(xt, ys)\n(feature-instance co-occurrence). Moreover, multi-view data (e.g. Web pages including both text\nand pictures) is an example for data in the form of p(xt, xs) (instance-level co-occurrence). Where\nthere is a pool of such co-occurrence data available, we can build the translator φ for estimating the\nMarkov chains in the previous subsections.\nIn particular, to estimate the translator φ, at ﬁrst, the feature-instance co-occurrence data (p( yt, xs)\nor p(xt, ys)) can be used to estimate the probabilities for feature-level co-occurrence p(yt, ys);\nformally, p(yt, ys) =\n∫\nXs\np(yt, xs)p(ys|xs) d xs and p(yt, ys) =\n∫\nXt\np(xt, ys)p(yt|xt) d xt. The\ninstance-level co-occurrence data can also be converted to feature-level co-occurrence; formally,\np(yt, ys) =\n∫\nXt\n∫\nXs\np(xt, xs)p(ys|xs)p(yt|xt) dxsdxt. Here, p(ys|xs) and p(yt|xt) are two feature\nextractors in Ys and Yt. Using the feature-level co-occurrence probability p(yt, ys), we can estimate\nthe translator as p(yt|ys) = p(yt, ys)/\n∫\nYt\np(y′\nt, ys)dy′\nt.\n3 Evaluation: Text-aided Image Classiﬁcation\nIn this section, we apply our frameworkTLRisk to a text-aided image classiﬁcation problem, which\nuses binary labeled text documents as auxiliary data to enhance the image classiﬁcation. This prob-\nlem is derived from the application where a user or a group of users may have expressed preferences\nover some text documents, and we wish to translate these preferences to images for the same group\nof users. We will show the effectiveness of TLRisk on text-aided image classiﬁcation. Our ob-\njective is to demonstrate that even with a small amount of labeled image training data, we can still\nbuild a high-quality translated learning solution for image classiﬁcation by leveraging the text doc-\numents, even if the co-occurrence data themselves are not sufﬁcient when directly used for training\na classiﬁcation model in the target space.\n3.1 Data Sets\nThe data sets of Caltech-256 1 and Open Directory Project (ODP, http://www.dmoz.org/)\nwere used in our evaluation, as the image and text corpora. Our ODP collection was crawled during\nAugust 2006, and involves 1,271,106 English Web pages. We generated 12 binary text-to-image\nclassiﬁcation tasks from the above corpora. The description for each data set is presented in Table\n1. The ﬁrst column presents the name of each data set, e.g. horse vs coin indicates all the\npositive instances are about horse while all the negative instances are about coin. We collected\nthe corresponding documents from ODP for each category. However, due to space limitation, we\nomit the detailed ODP directory information with respect to each data set here. In the table, we\nalso listed the data sizes for each task, including documents and images. Generally, the number of\ndocuments is much larger than the number of images.\nFor data preprocessing, the SIFT descriptor [6] was used to ﬁnd and describe the interesting points\nin the images, and then clustered the extracted interest points into 800 clusters to obtain the code-\nbook. Based on the code-book, each image can be converted to a corresponding feature vector. For\ntext documents, we ﬁrst extracted and stemmed all the tokens from the ODP Web pages, and then\ninformation gain [12] was used to select the most important features for further learning process.\nWe collected the co-occurrence data from a commercial image search engine during April 2008.\nThe collected data are in the form of feature-instance co-occurrence p(ys, xt), so that we have to\nconvert them to feature-level co-occurrence p(ys, yt) as discussed in Section 2.4.\n1h ttp://www.vision.caltech.edu/Image Datasets/Caltech256/\n12 4 8 16 32\n0.20\n0.25\n0.30\n0.35\n0.40\n0.15\nnumber of labeled images per category\nError Rate\nCosine\n \n \nImage Only\nSearch+Image\nTLRisk\nLowerbound\n(a)\n12 4 8 16 32\n0.20\n0.25\n0.30\n0.35\n0.40\n0.15\nnumber of labeled images per category\nError Rate\nKullback−Leibler Divergence\n \n \nImage Only\nSearch+Image\nTLRisk\nLowerbound\n(b)\n12 4 8 16 32\n0.20\n0.25\n0.30\n0.35\n0.40\n0.15\nnumber of labeled images per category\nError Rate\nPearson’s Correlation Coefficient\n \n \nImage Only\nSearch+Image\nTLRisk\nLowerbound\n(c)\nF\nigure 2: The average error rates over 12 data sets for text-aided image classiﬁcation with different\nnumber of labeled images Lt.\n0.0625 0.25 1 4 16\n0.20\n0.25\n0.30\n0.15\n0.35\nλ (in log scale)\nError Rate\nCosine\n \n \naverage over 12 data sets\n(a)\n0.0625 0.25 1 4 160.15\n0.20\n0.25\n0.30\n0.35\nλ (in log scale)\nError Rate\nKullback−Liebler Divergence\n \n \naverage over 12 data sets\n(b)\n0.0625 0.25 1 4 160.15\n0.20\n0.25\n0.30\n0.35\nλ (in log scale)\nError Rate\nPearson’s Correlation Coefficient\n \n \naverage over 12 data sets\n(c)\nF\nigure 3: The average error rates over 12 data sets for text-aided image classiﬁcation with different\ntrade-off λ.\n3.2 Evaluation Methods\nFew existing research works addressed the text-aided image classiﬁcation problem, so that for the\nbaseline methods in our experiments, we ﬁrst simply used the labeled data Lt as the training data in\nthe target space to train a classiﬁcation model; we refer to this model asImage Only. The second\nbaseline is to use the category name (in this case, there are two names for binary classiﬁcation\nproblems) to search for training images and then to train classiﬁers together with labeled images in\nLt; we refer to this model asSearch+Image.\nOur frameworkTLRisk was evaluated under three different dissimilarity functions: (1) Kullback-\nLeibler divergence (named KL):\n∫\nYt\np(yt|θC) log p(yt|θC)\np(yt|θXt ) dyt; (2) Negative of cosine function\n(namedNCOS): −\n∫\nYt p(yt|θC)p(yt|θXt )dyt\n√ ∫\nYt\np2(yt|θC)\ndyt\n√ ∫\nYt\np2(yt|θXt )\ndyt\n; (3) Negative of the Pearson’s correlation co-\nefﬁcient (namedNPCC): − cov(p(Yt|θC),p(Yt|θXt ))√\nvar(p(Yt|θC) )var(p(Yt|θXt )).\nWe also evaluated the lower bound of the error rate with respect to each data set. To estimate the\nlower bound, we conducted a 5-fold cross-validation on the test dataU. Note that this strategy, which\nis referred to asLowerbound, is unavailable in our problem setting, since it uses a large amount of\nlabeled data in the target space. In our experiments, this lower bound is used just for reference. We\nalso note that on some data sets, the performance ofLowerbound may be slightly worse than that\nof TLRisk, because Lowerbound was trained based on images in Caltech-256, while TLRisk\nwas based on the co-occurrence data. These models used different supervisory knowledge.\n3.3 Experimental Results\nThe experimental results were evaluated in terms of error rates, and are shown in Figure 2. On\none hand, from the table, we can see that our framework TLRisk greatly outperforms the baseline\nmethods Image Only and Search+Image, no matter which dissimilarity function is applied.\nOn the other hand, compared with Lowerbound, TLRisk also shows comparable performance.\nIt indicates that our framework TLRisk can effectively learn knowledge across different feature\nspaces in the case of text-to-image classiﬁcation.\nMoreover, when the number of target space labeled images decreases, the performance of Image\nOnly declines rapidly, while the performances of Search+Image and TLRisk stay very sta-\nDA TA SET EN GLISH GE RMAN\nLO CATION SI ZE LO CATION SI ZE\n1 Top: Sport: Ballsport 2000 Top: World: Deutsch: Sport: Ballsport 128\nTop: Computers: Internet 2000 Top: World: Deutsch: Computer: Internet 126\n2 Top: Arts: Architecture: Building Types 1259 Top: World: Deutsch: Kultur: Architektur: Geb ¨a udetypen 71\nTop: Home: Cooking: Recipe Collections 475 Top: World: Deutsch: Zuhause: Kochen: Rezeptesammlungen 72\n3 Top: Science: Agriculture 1886 Top: World: Deutsch: Wissenschaft: Agrarwissenschaften 71\nTop: Society: Crime 1843 Top: World: Deutsch: Gesellschaft: Kriminalit ¨a t 69\n4 Top: Sports: Skating: Roller Skating 926 Top: World: Deutsch: Sport: Rollsport 70\nTop: Health: Public Health and Safety 2361 Top: World: Deutsch: Gesundheit: Public Health 71\n5 Top: Recreation: Outdoors: Hunting 2919 Top: World: Deutsch: Freizeit: Outdoor: Jagd 70\nTop: Society: Holidays 2258 Top: World: Deutsch: Gesellschaft: Fest ´u nd Feiertage 72\nTable 2: The description for each cross-language classiﬁcat ion data set.\nble. This indicates that TLRisk is not quite sensitive to the size of Lt; in other words, TLRisk\nhas good robustness. We also want to note that, sometimes TLRisk performs slightly better than\nLowerbound. This is not a mistake, because these two methods use different supervisory knowl-\nedge: Lowerbound is based on images in the Caltech-256 corpus; TLRisk is based on the co-\noccurrence data. In these experiments,Lowerbound is just for reference.\nInTLRisk, a parameter to tune is the trade off parameter λ (refer to Equation (6)). Figure 3 shows\nthe average error rate curves on all the 12 data sets, when λ gradually changes from 2−5 to 25.\nIn this experiment, we ﬁxed the number of target training images per category to one, and set the\nthreshold K (which is the number of images to collect for each text keyword, when collecting the\nco-occurrence data) to 40. From the ﬁgure, we can see that, on one hand, whenλ is very large, which\nmeans the classiﬁcation model mainly builds on the target space training imagesLt, the performance\nis rather poor. On the other hand, when λ is small such that the classiﬁcation model relies more on\nthe auxiliary text training data Ls, the classiﬁcation performance is relatively stable. Therefore, we\nsuggest to set the trade-off parameter λ to a small value, and in these experiments, all the λs are set\nto 1, based on Figure 3.\n4 Evaluation: Cross-language Classiﬁcation\nIn this section, we apply our framework TLRisk to another scenario, the cross-language classiﬁ-\ncation. We focused on English-to-German classiﬁcation, where English documents are used as the\nsource data to help classify German documents, which are target data.\nIn these experiments, we collected the documents from corresponding categories from ODP English\npages and ODP German pages, and generated ﬁve cross-language classiﬁcation tasks, as shown in\nTable 2. For the co-occurrence data, we used the English-German dictionary from the Internet Dic-\ntionary Project2 (IDP). The dictionary data are in the form of feature-level co-occurrence p(yt, ys).\nWe note that while most cross-language classiﬁcation works rely on machine translation [1], our\nassumption is that the machine translation is unavailable and we rely on dictionary only.\nWe evaluatedTLRisk with the negative of cosine (namedNCOS) as the dissimilarity function. Our\nframeworkTLRisk was compared to classiﬁcation using only very few German labeled documents\nas a baseline, called German Labels Only. We also present the lower bound of error rates by\nperforming 5-fold cross-validation on the test data U, which we refer to as Lowerbound. The\nperformances of the evaluated methods are presented in Table 3. In this experiment, we have only\nsixteen German labeled documents in each category. The error rates in Table 3 were evaluated\nby averaging the results of 20 random repeats. From the ﬁgure, we can see that TLRisk always\nshows marked improvements compared with the baseline method German Labels Only, al-\nthough there are still gaps between TLRisk and the ideal case Lowerbound. This indicates our\nalgorithmTLRisk is effective on the cross-language classiﬁcation problem.\nDA TA SET 1 2 3 4 5\nGerman Labels Only 0.2 46 ± 0.061 0.1 33 ± 0.037 0.3 01 ± 0.067 0.2 57 ± 0.053 0.2 77 ± 0.068\nTLRisk 0.1 91 ± 0.045 0.1 22 ± 0.043 0.2 53 ± 0.062 0.2 47 ± 0.059 0.1 83 ± 0.072\nLowerbound 0.1 70 ± 0.000 0.1 16 ± 0.000 0.1 57 ± 0.000 0.1 76 ± 0.000 0.1 66 ± 0.000\nTable 3: The average error rate and variance on each data set, g iven by all the evaluation methods,\nfor English-to-German cross-language classiﬁcation.\nWe have empirically tuned the trade-off parameter λ. Similar to the results of the text-aided image\nclassiﬁcation experiments, when λ is small, the performance of TLRisk is better and stable. In\n2h ttp://www.ilovelanguages.com/idp/index.html\nthese experiments, we set λ t o 2−4. However, due to space limitation, we cannot present the curves\nfor λ tuning here.\n5 Related Work\nWe review several prior works related to our work. To solve the label sparsity problem, researchers\nproposed several learning strategies, e.g. semi-supervised learning [13] and transfer learning [3,\n11, 10, 9, 4]. Transfer learning mainly focuses on training and testing processes being in different\nscenarios, e.g. multi-task learning [3], learning with auxiliary data sources [11], learning from\nirrelevant categories [10], and self-taught learning [9, 4]. The translated learning proposed in this\npaper can be considered as an instance of general transfer learning; that is, transfer learning from\ndata in different feature spaces.\nMulti-view learning addresses learning across different feature spaces. Co-training [2] established\nthe foundation of multi-view learning, in which the classiﬁers in two views learn from each other\nto enhance the learning process. Nigam and Ghani [8] proposed co-EM to apply EM algorithm to\neach view, and interchange probabilistic labels between different views. Co-EMT [7] is an active\nlearning multi-view learning algorithm, and has shown more robustness empirically. However, as\ndiscussed before, multi-view learning requires that each instance should contain two views, while in\ntranslated learning, this requirement is relaxed. Translated learning can accept training data in one\nview and test data in another view.\n6 Conclusions\nIn this paper, we proposed a translated learning framework for classifying target data using data\nfrom another feature space. We have shown that in translated learning, even though we have very\nlittle labeled data in the target space, if we can ﬁnd a bridge to link the two spaces through feature\ntranslation, we can achieve good performance by leveraging the knowledge from the source data.\nWe formally formulated our translated learning framework using risk minimization, and presented\nan approximation method for model estimation. In our experiments, we have demonstrated how this\ncan be done effectively through the co-occurrence data in TLRisk. The experimental results on\nthe text-aided image classiﬁcation and the cross-language classiﬁcation show that our algorithm can\ngreatly outperform the state-of-the-art baseline methods.\nAcknowledgement We thank the anonymous reviewers for their greatly helpful comments.\nWenyuan Dai and Gui-Rong Xue are supported by the grants from National Natural Science Foun-\ndation of China (NO. 60873211) and the MSRA-SJTU joint lab project “Transfer Learning and its\nApplication on the Web”. Qiang Yang thanks the support of Hong Kong CERG Project 621307.\nReferences\n[1] N. Bel, C. Koster, and M. Villegas. Cross-lingual text categorization. In ECDL, 2003.\n[2] A. Blum and T. Mitchell. Combining labeled and unlabeled data with co-training. In COLT, 1998.\n[3] R. Caruana. Multitask learning. Machine Learning, 28(1):41–75, 1997.\n[4] W. Dai, Q. Yang, G.-R. Xue, and Y . Yu. Self-taught clustering. InICML, 2008.\n[5] J. Lafferty and C. Zhai. Document language models, query models, and risk minimization for information\nretrieval. In SIGIR, 2001.\n[6] D. Lowe. Distinctive image features from scale-invariant keypoints. International Journal of Computer\nVision, 60(2):91–110, 2004.\n[7] I. Muslea, S. Minton, and C. Knoblock. Active + semi-supervised learning = robust multi-view learning.\nIn ICML, 2002.\n[8] K. Nigam and R. Ghani. Analyzing the effectiveness and applicability of co-training. In CIKM, 2000.\n[9] R. Raina, A. Battle, H. Lee, B. Packer, and A. Ng. Self-taught learning: transfer learning from unlabeled\ndata. In ICML, 2007.\n[10] R. Raina, A. Ng, and D. Koller. Constructing informative priors using transfer learning. In ICML, 2006.\n[11] P. Wu and T. Dietterich. Improving svm accuracy by training on auxiliary data sources. In ICML, 2004.\n[12] Y . Yang and J. Pedersen. A comparative study on feature selection in text categorization. InICML, 1997.\n[13] X. Zhu. Semi-supervised learning literature survey. Technical Report 1530, University of Wisconsin-\nMadison, 2007.",
  "values": {
    "Respect for Law and public interest": "Yes",
    "Non-maleficence": "Yes",
    "Autonomy (power to decide)": "Yes",
    "Respect for Persons": "Yes",
    "Interpretable (to users)": "Yes",
    "Transparent (to users)": "Yes",
    "Privacy": "Yes",
    "Critiqability": "Yes",
    "Not socially biased": "Yes",
    "Fairness": "Yes",
    "Explicability": "Yes",
    "Beneficence": "Yes",
    "Deferral to humans": "Yes",
    "User influence": "Yes",
    "Justice": "Yes",
    "Collective influence": "Yes"
  }
}