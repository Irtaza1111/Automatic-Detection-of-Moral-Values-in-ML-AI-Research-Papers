{
  "pdf": "NIPS-2009-kernel-methods-for-deep-learning-Paper",
  "title": "Kernel Methods for Deep Learning",
  "author": "Youngmin Cho, Lawrence K. Saul",
  "paper_id": "NIPS-2009-kernel-methods-for-deep-learning-Paper",
  "text": "Kernel Methods for Deep Learning\nYoungmin Cho and Lawrence K. Saul\nDepartment of Computer Science and Engineering\nUniversity of California, San Diego\n9500 Gilman Drive, Mail Code 0404\nLa Jolla, CA 92093-0404\n{yoc002,saul}@cs.ucsd.edu\nAbstract\nWe introduce a new family of positive-deﬁnite kernel functions that mimic the\ncomputation in large, multilayer neural nets. These kernel functions can be used\nin shallow architectures, such as support vector machines (SVMs), or in deep\nkernel-based architectures that we call multilayer kernel machines (MKMs). We\nevaluate SVMs and MKMs with these kernel functions on problems designed to\nillustrate the advantages of deep architectures. On several problems, we obtain\nbetter results than previous, leading benchmarks from both SVMs with Gaussian\nkernels as well as deep belief nets.\n1 Introduction\nRecent work in machine learning has highlighted the circumstances that appear to favor deep archi-\ntectures, such as multilayer neural nets, over shallow architectures, such as support vector machines\n(SVMs) [1]. Deep architectures learn complex mappings by transforming their inputs through mul-\ntiple layers of nonlinear processing [2]. Researchers have advanced several motivations for deep\narchitectures: the wide range of functions that can be parameterized by composing weakly non-\nlinear transformations, the appeal of hierarchical distributed representations, and the potential for\ncombining unsupervised and supervised methods. Experiments have also shown the beneﬁts of\ndeep learning in several interesting applications [3, 4, 5].\nMany issues surround the ongoing debate over deep versus shallow architectures [1, 6]. Deep ar-\nchitectures are generally more difﬁcult to train than shallow ones. They involve difﬁcult nonlinear\noptimizations and many heuristics. The challenges of deep learning explain the early and continued\nappeal of SVMs, which learn nonlinear classiﬁers via the “kernel trick”. Unlike deep architectures,\nSVMs are trained by solving a simple problem in quadratic programming. However, SVMs cannot\nseemingly beneﬁt from the advantages of deep learning.\nLike many, we are intrigued by the successes of deep architectures yet drawn to the elegance of ker-\nnel methods. In this paper, we explore the possibility of deep learning in kernel machines. Though\nwe share a similar motivation as previous authors [7], our approach is very different. Our paper\nmakes two main contributions. First, we develop a new family of kernel functions that mimic the\ncomputation in large neural nets. Second, using these kernel functions, we show how to train multi-\nlayer kernel machines (MKMs) that beneﬁt from many advantages of deep learning.\nThe organization of this paper is as follows. In section 2, we describe a new family of kernel\nfunctions and experiment with their use in SVMs. Our results on SVMs are interesting in their own\nright; they also foreshadow certain trends that we observe (and certain choices that we make) for the\nMKMs introduced in section 3. In this section, we describe a kernel-based architecture with multiple\nlayers of nonlinear transformation. The different layers are trained using a simple combination of\nsupervised and unsupervised methods. Finally, we conclude in section 4 by evaluating the strengths\nand weaknesses of our approach.\n1\n2 Arc-cosine kernels\nIn this section, we develop a new family of kernel functions for computing the similarity of vector\ninputs x, y∈ℜ d. As shorthand, let Θ(z) = 1\n2(1 + sign(z)) denote the Heaviside step function. We\ndeﬁne the nth order arc-cosine kernel function via the integral representation:\nkn(x, y) = 2\n∫\ndw e− ∥w∥2\n2\n(2π)d/2 Θ(w· x) Θ(w· y) (w· x)n (w· y)n (1)\nThe integral representation makes it straightforward to show that these kernel functions are positive-\nsemideﬁnite. The kernel function in eq. (1) has interesting connections to neural computation [8]\nthat we explore further in sections 2.2–2.3. However, we begin by elucidating its basic properties.\n2.1 Basic properties\nWe show how to evaluate the integral in eq. (1) analytically in the appendix. The ﬁnal result is most\neasily expressed in terms of the angle θ between the inputs:\nθ = cos−1\n( x· y\n∥x∥∥y∥\n)\n. (2)\nThe integral in eq. (1) has a simple, trivial dependence on the magnitudes of the inputs x and y, but\na complex, interesting dependence on the angle between them. In particular, we can write:\nkn(x, y) = 1\nπ∥x∥n∥y∥nJn(θ) (3)\nwhere all the angular dependence is captured by the family of functions Jn(θ). Evaluating the\nintegral in the appendix, we show that this angular dependence is given by:\nJn(θ) = ( −1)n(sin θ)2n+1\n( 1\nsin θ\n∂\n∂θ\n)n(π− θ\nsin θ\n)\n. (4)\nFor n = 0, this expression reduces to the supplement of the angle between the inputs. However, for\nn >0, the angular dependence is more complicated. The ﬁrst few expressions are:\nJ0(θ) = π− θ (5)\nJ1(θ) = sin θ + (π− θ) cosθ (6)\nJ2(θ) = 3 sin θ cos θ + (π− θ)(1 + 2 cos2 θ) (7)\nWe describe eq. (3) as an arc-cosine kernel because for n = 0 , it takes the simple form\nk0(x, y) = 1− 1\nπ cos−1 x·y\n∥x∥∥y∥. In fact, the zeroth and ﬁrst order kernels in this family are strongly\nmotivated by previous work in neural computation. We explore these connections in the next section.\nArc-cosine kernels have other intriguing properties. From the magnitude dependence in eq. (3),\nwe observe the following: (i) the n = 0 arc-cosine kernel maps inputs x to the unit hypersphere\nin feature space, with k0(x, x) = 1 ; (ii) the n = 1 arc-cosine kernel preserves the norm of inputs,\nwith k1(x, x) =∥x∥2; (iii) higher order (n >1) arc-cosine kernels expand the dynamic range of the\ninputs, with kn(x, x)∼∥ x∥2n. Properties (i)–(iii) are shared respectively by radial basis function\n(RBF), linear, and polynomial kernels. Interestingly, though, the n = 1 arc-cosine kernel is highly\nnonlinear, also satisfying k1(x,−x) = 0 for all inputs x. As a practical matter, we note that arc-\ncosine kernels do not have any continuous tuning parameters (such as the kernel width in RBF\nkernels), which can be laborious to set by cross-validation.\n2.2 Computation in single-layer threshold networks\nConsider the single-layer network shown in Fig. 1 (left) whose weights Wij connect the jth input\nunit to the ith output unit. The network maps inputs x to outputs f(x) by applying an elementwise\nnonlinearity to the matrix-vector product of the inputs and the weight matrix: f(x) = g(Wx). The\nnonlinearity is described by the network’s so-called activation function. Here we consider the family\nof one-sided polynomial activation functionsgn(z) = Θ(z)zn illustrated in the right panel of Fig. 1.\n2\nf2 f3 fi\nx1 x2 xj\n. . . \n. . . \nf1 fm\nxd\nW\n. . . \n. . . \n−1 0 1\n0\n0.5\n1\nStep (n=0)\n−1 0 1\n0\n0.5\n1\nRamp (n=1)\n−1 0 1\n0\n0.5\n1\nQuarter−pipe (n=2)\nFigure 1: Single layer network and activation functions\nFor n=0 , the activation function is a step function, and the network is an array of perceptrons. For\nn= 1, the activation function is a ramp function (or rectiﬁcation nonlinearity [9]), and the mapping\nf(x) is piecewise linear. More generally, the nonlinear (non-polynomial) behavior of these networks\nis induced by thresholding on weighted sums. We refer to networks with these activation functions\nas single-layer threshold networks of degree n.\nComputation in these networks is closely connected to computation with the arc-cosine kernel func-\ntion in eq. (1). To see the connection, consider how inner products are transformed by the mapping\nin single-layer threshold networks. As notation, let the vector wi denote ith row of the weight\nmatrix W. Then we can express the inner product between different outputs of the network as:\nf(x)· f(y) =\nm∑\ni=1\nΘ(wi· x)Θ(wi· y)(wi· x)n(wi· y)n, (8)\nwhere m is the number of output units. The connection with the arc-cosine kernel function emerges\nin the limit of very large networks [10, 8]. Imagine that the network has an inﬁnite number of\noutput units, and that the weights Wij are Gaussian distributed with zero mean and unit vari-\nance. In this limit, we see that eq. (8) reduces to eq. (1) up to a trivial multiplicative factor:\nlimm→∞ 2\nmf(x)· f(y) = kn(x, y). Thus the arc-cosine kernel function in eq. (1) can be viewed\nas the inner product between feature vectors derived from the mapping of an inﬁnite single-layer\nthreshold network [8].\nMany researchers have noted the general connection between kernel machines and neural networks\nwith one layer of hidden units [1]. The n = 0 arc-cosine kernel in eq. (1) can also be derived from\nan earlier result obtained in the context of Gaussian processes [8]. However, we are unaware of any\nprevious theoretical or empirical work on the general family of these kernels for degrees n≥0.\nArc-cosine kernels differ from polynomial and RBF kernels in one especially interesting respect.\nAs highlighted by the integral representation in eq. (1), arc-cosine kernels induce feature spaces\nthat mimic the sparse, nonnegative, distributed representations of single-layer threshold networks.\nPolynomial and RBF kernels do not encode their inputs in this way. In particular, the feature vector\ninduced by polynomial kernels is neither sparse nor nonnegative, while the feature vector induced\nby RBF kernels resembles the localized output of a soft vector quantizer. Further implications of\nthis difference are explored in the next section.\n2.3 Computation in multilayer threshold networks\nA kernel function can be viewed as inducing a nonlinear mapping from inputs x to fea-\nture vectors Φ(x). The kernel computes the inner product in the induced feature space:\nk(x, y) = Φ(x)·Φ(y). In this section, we consider how to compose the nonlinear mappings in-\nduced by kernel functions. Speciﬁcally, we show how to derive new kernel functions\nk(ℓ)(x, y) = Φ(Φ(...Φ  \nℓ times\n(x)))· Φ(Φ(...Φ  \nℓ times\n(y))) (9)\nwhich compute the inner product after ℓ successive applications of the nonlinear mappingΦ(·). Our\nmotivation is the following: intuitively, if the base kernel function k(x, y) = Φ(x)· Φ(y) mimics\nthe computation in a single-layer network, then the iterated mapping in eq. (9) should mimic the\ncomputation in a multilayer network.\n3\n22\n24\n26\nDBN−3\nSVM−RBF\nTest error rate (%)\n1   2    3   4    5   6\n       Step (n=0)\n1   2    3   4    5   6\n      Ramp (n=1)\n1   2    3   4    5   6\nQuarter−pipe (n=2)\n22\n24\n26\nDBN!3\nSVM!RBF\nTest error rate (%)\n1   2    3   4    5   6       Step (n=0) 1   2    3   4    5   6      Ramp (n=1) 1   2    3   4    5   6Quarter!pipe (n=2)\nFigure “: Left: examples from the rectangles-image data set: Right: classiﬁcation error rates on the\ntest set: SVMs with arc cosine kernels have error rates from 22.36–25.64%: Results are shown for\nkernels of varying degree ﬁn2 and levels of recursion ﬁℓ2: The best previous results are 24.04% for\nSVMs with RBF kernels and 22.50% for deep belief nets [“]: See text for details:\n17\n18\n19\n20\n21\nDBN!3\nSVM!RBF\nTest error rate (%)\n1   2    3   4    5   6       Step (n=0) 1   2    3   4    5   6      Ramp (n=1) 1   2    3   4    5   6Quarter!pipe (n=2)\nFigure -: Left: examples from the convex data set: Right: classiﬁcation error rates on the test set:\nSVMs with arc cosine kernels have error rates from 17.15–20.51%: Results are shown for kernels\nof varying degree ﬁn2 and levels of recursion ﬁℓ2: The best previous results are 19.13% for SVMs\nwith RBF kernels and 18.63% for deep belief nets [“]: See text for details:\n“;;; training examples as a validation set to choose the margin penalty parameter; after choosing\nthis parameter by cross.validation0we then retrained each SVM using all the training examples: For\nreference0we also report the best results obtained previously from three layer deep belief nets ﬁDBN.\n-2 and SVMs with RBF kernels ﬁSVM.RBF2: These references are representative of the current\nstate.of.the.art for deep and shallow architectures on these data sets:\nThe right panels of ﬁgures “ and - show the test set error rates from arc cosine kernels of varying\ndegree ﬁn2 and levels of recursion ﬁℓ2: We experimented with kernels of degree n = 0, 1 and “0\ncorresponding to single layer threshold networks with “step”0“ramp”0and “quarter.pipe” activation\nfunctions: We also experimented with the multilayer kernels described in section “:-0 composed\nfrom one to six levels of recursion: Overall0 the ﬁgures show that on these two data sets0 many\ndifferent arc cosine kernels outperform the best results previously reported for SVMs with RBF\nkernels and deep belief nets: We give more details on these experiments below: At a high level0\nthough0we note that SVMs with arc cosine kernels are very straightforward to train; unlike SVMs\nwith RBF kernels0they do not require tuning a kernel width parameter0and unlike deep belief nets0\nthey do not require solving a difﬁcult nonlinear optimization or searching over possible architectures:\nIn our experiments0 we quickly discovered that the multilayer kernels only performed well when\nn = 1 kernels were used at higher ﬁℓ > 12 levels in the recursion: Figs: “ and - therefore show only\nthese sets of results; in particular0each group of bars shows the test error rates when a particular\nkernel ﬁofdegree n = 0, 1, 22 was used at the ﬁrst layer of nonlinearity0while the n = 1 kernel was\nused at successive layers: We do not have a formal explanation for this effect: However0recall that\nonly the n = 1 arc cosine kernel preserves the norm of its inputs: the n = 0 kernel maps all inputs\nonto a unit hypersphere in feature space0 while higher.order ﬁn > 12 kernels may induce feature\nspaces with severely distorted dynamic ranges: Therefore0we hypothesize that only n = 1 arc cosine\nkernels preserve sufﬁcient information about the magnitude of their inputs to work effectively in\ncomposition with other kernels:\nFinally0the results on both data sets reveal an interesting trend: the multilayer arc cosine kernels\noften perform better than their single layer counterparts: Though SVMs are shallow architectures0\n(\nFigure 2: Left: examples from the rectangles-image data set. Right: classiﬁcation error rates on the\ntest set. SVMs with arc-cosine kernels have error rates from 22.36–25.64%. Results are shown for\nkernels of varying degree ( n) and levels of recursion ( ℓ). The best previous results are 24.04% for\nSVMs with RBF kernels and 22.50% for deep belief nets [11]. See text for details.\nWe ﬁrst examine the results of this procedure for widely used kernels. Here we ﬁnd that the iterated\nmapping in eq. (9) does not yield particularly interesting results. Consider the two-fold composition\nthat maps x to Φ(Φ(x)). For linear kernels k(x, y) = x· y, the composition is trivial: we obtain\nthe identity map Φ(Φ(x)) = Φ(x) = x. For homogeneous polynomial kernels k(x, y) = (x· y)d,\nthe composition yields:\nΦ(Φ(x))· Φ(Φ(y)) = (Φ(x)· Φ(y))d = ((x· y)d)d = (x· y)d2\n. (10)\nThe above result is not especially interesting: the kernel implied by this composition is also polyno-\nmial, just of higher degree ( d2 versus d) than the one from which it was constructed. Likewise, for\nRBF kernels k(x, y) = e−λ∥x−y∥2\n, the composition yields:\nΦ(Φ(x))· Φ(Φ(y)) = e−λ∥Φ(x)−Φ(y)∥2\n= e−2λ(1−k(x,y)). (11)\nThough non-trivial, eq. (11) does not represent a particularly interesting computation. Recall that\nRBF kernels mimic the computation of soft vector quantizers, with k(x, y)≪ 1 when∥x−y∥ is\nlarge compared to the kernel width. It is hard to see how the iterated mapping Φ(Φ(x)) would\ngenerate a qualitatively different representation than the original mapping Φ(x).\nNext we consider the ℓ-fold composition in eq. (9) for arc-cosine kernel functions. We state the\nresult in the form of a recursion. The base case is given by eq. (3) for kernels of depth ℓ = 1 and\ndegree n. The inductive step is given by:\nk(l+1)\nn (x, y) = 1\nπ\n[\nk(l)\nn (x, x) k(l)\nn (y, y)\n]n/2\nJn\n(\nθ(ℓ)\nn\n)\n, (12)\nwhere θ(ℓ)\nn is the angle between the images of x and y in the feature space induced by the ℓ-fold\ncomposition. In particular, we can write:\nθ(ℓ)\nn = cos−1\n(\nk(ℓ)\nn (x, y)\n[\nk(ℓ)\nn (x, x) k(ℓ)\nn (y, y)\n]−1/2)\n. (13)\nThe recursion in eq. (12) is simple to compute in practice. The resulting kernels mimic the com-\nputations in large multilayer threshold networks. Above, for simplicity, we have assumed that the\narc-cosine kernels have the same degree n at every level (or layer) ℓ of the recursion. We can also\nuse kernels of different degrees at different layers. In the next section, we experiment with SVMs\nwhose kernel functions are constructed in this way.\n2.4 Experiments on binary classiﬁcation\nWe evaluated SVMs with arc-cosine kernels on two challenging data sets of 28× 28 grayscale pixel\nimages. These data sets were speciﬁcally constructed to compare deep architectures and kernel\nmachines [11]. In the ﬁrst data set, known as rectangles-image, each image contains an occluding\nrectangle, and the task is to determine whether the width of the rectangle exceeds its height; ex-\namples are shown in Fig. 2 (left). In the second data set, known as convex, each image contains a\nwhite region, and the task is to determine whether the white region is convex; examples are shown\n4\n22\n24\n26\nDBN!3\nSVM!RBF\nTest error rate (%)\n1   2    3   4    5   6       Step (n=0) 1   2    3   4    5   6      Ramp (n=1) 1   2    3   4    5   6Quarter!pipe (n=2)\nFigure “: Left: examples from the rectangles-image data set: Right: classiﬁcation error rates on the\ntest set: SVMs with arc cosine kernels have error rates from 22.36–25.64%: Results are shown for\nkernels of varying degree ﬁn2 and levels of recursion ﬁℓ2: The best previous results are 24.04% for\nSVMs with RBF kernels and 22.50% for deep belief nets [“]: See text for details:\n17\n18\n19\n20\n21\nDBN!3\nSVM!RBF\nTest error rate (%)\n1   2    3   4    5   6       Step (n=0) 1   2    3   4    5   6      Ramp (n=1) 1   2    3   4    5   6Quarter!pipe (n=2)\nFigure -: Left: examples from the convex data set: Right: classiﬁcation error rates on the test set:\nSVMs with arc cosine kernels have error rates from 17.15–20.51%: Results are shown for kernels\nof varying degree ﬁn2 and levels of recursion ﬁℓ2: The best previous results are 19.13% for SVMs\nwith RBF kernels and 18.63% for deep belief nets [“]: See text for details:\n“;;; training examples as a validation set to choose the margin penalty parameter; after choosing\nthis parameter by cross.validation0we then retrained each SVM using all the training examples: For\nreference0we also report the best results obtained previously from three layer deep belief nets ﬁDBN.\n-2 and SVMs with RBF kernels ﬁSVM.RBF2: These references are representative of the current\nstate.of.the.art for deep and shallow architectures on these data sets:\nThe right panels of ﬁgures “ and - show the test set error rates from arc cosine kernels of varying\ndegree ﬁn2 and levels of recursion ﬁℓ2: We experimented with kernels of degree n = 0, 1 and “0\ncorresponding to single layer threshold networks with “step”0“ramp”0and “quarter.pipe” activation\nfunctions: We also experimented with the multilayer kernels described in section “:-0 composed\nfrom one to six levels of recursion: Overall0 the ﬁgures show that on these two data sets0 many\ndifferent arc cosine kernels outperform the best results previously reported for SVMs with RBF\nkernels and deep belief nets: We give more details on these experiments below: At a high level0\nthough0we note that SVMs with arc cosine kernels are very straightforward to train; unlike SVMs\nwith RBF kernels0they do not require tuning a kernel width parameter0and unlike deep belief nets0\nthey do not require solving a difﬁcult nonlinear optimization or searching over possible architectures:\nIn our experiments0 we quickly discovered that the multilayer kernels only performed well when\nn = 1 kernels were used at higher ﬁℓ > 12 levels in the recursion: Figs: “ and - therefore show only\nthese sets of results; in particular0each group of bars shows the test error rates when a particular\nkernel ﬁofdegree n = 0, 1, 22 was used at the ﬁrst layer of nonlinearity0while the n = 1 kernel was\nused at successive layers: We do not have a formal explanation for this effect: However0recall that\nonly the n = 1 arc cosine kernel preserves the norm of its inputs: the n = 0 kernel maps all inputs\nonto a unit hypersphere in feature space0 while higher.order ﬁn > 12 kernels may induce feature\nspaces with severely distorted dynamic ranges: Therefore0we hypothesize that only n = 1 arc cosine\nkernels preserve sufﬁcient information about the magnitude of their inputs to work effectively in\ncomposition with other kernels:\nFinally0the results on both data sets reveal an interesting trend: the multilayer arc cosine kernels\noften perform better than their single layer counterparts: Though SVMs are shallow architectures0\n(\n17\n18\n19\n20\n21\nDBN−3\nSVM−RBF\nTest error rate (%)\n1   2    3   4    5   6\n       Step (n=0)\n1   2    3   4    5   6\n      Ramp (n=1)\n1   2    3   4    5   6\nQuarter−pipe (n=2)\nFigure 3: Left: examples from the convex data set. Right: classiﬁcation error rates on the test set.\nSVMs with arc-cosine kernels have error rates from 17.15–20.51%. Results are shown for kernels\nof varying degree ( n) and levels of recursion ( ℓ). The best previous results are 19.13% for SVMs\nwith RBF kernels and 18.63% for deep belief nets [11]. See text for details.\nin Fig. 3 (left). The rectangles-image data set has 12000 training examples, while the convex data\nset has 8000 training examples; both data sets have 50000 test examples. These data sets have\nbeen extensively benchmarked by previous authors [11]. Our experiments in binary classiﬁcation\nfocused on these data sets because in previously reported benchmarks, they exhibited the biggest\nperformance gap between deep architectures (e.g., deep belief nets) and traditional SVMs.\nWe followed the same experimental methodology as previous authors [11]. SVMs were trained using\nlibSVM (version 2.88) [12], a publicly available software package. For each SVM, we used the last\n2000 training examples as a validation set to choose the margin penalty parameter; after choosing\nthis parameter by cross-validation, we then retrained each SVM using all the training examples.\nFor reference, we also report the best results obtained previously from three-layer deep belief nets\n(DBN-3) and SVMs with RBF kernels (SVM-RBF). These references appear to be representative of\nthe current state-of-the-art for deep and shallow architectures on these data sets.\nFigures 2 and 3 show the test set error rates from arc-cosine kernels of varying degree (n) and levels\nof recursion (ℓ). We experimented with kernels of degree n = 0, 1 and 2, corresponding to thresh-\nold networks with “step”, “ramp”, and “quarter-pipe” activation functions. We also experimented\nwith the multilayer kernels described in section 2.3, composed from one to six levels of recursion.\nOverall, the ﬁgures show that many SVMs with arc-cosine kernels outperform traditional SVMs,\nand a certain number also outperform deep belief nets. In addition to their solid performance, we\nnote that SVMs with arc-cosine kernels are very straightforward to train; unlike SVMs with RBF\nkernels, they do not require tuning a kernel width parameter, and unlike deep belief nets, they do not\nrequire solving a difﬁcult nonlinear optimization or searching over possible architectures.\nOur experiments with multilayer kernels revealed that these SVMs only performed well when arc-\ncosine kernels of degree n = 1 were used at higher ( ℓ > 1) levels in the recursion. Figs. 2 and\n3 therefore show only these sets of results; in particular, each group of bars shows the test error\nrates when a particular kernel (of degree n = 0, 1, 2) was used at the ﬁrst layer of nonlinearity,\nwhile the n = 1 kernel was used at successive layers. We hypothesize that only n = 1 arc-cosine\nkernels preserve sufﬁcient information about the magnitude of their inputs to work effectively in\ncomposition with other kernels. Recall that only the n = 1 arc-cosine kernel preserves the norm of\nits inputs: the n = 0 kernel maps all inputs onto a unit hypersphere in feature space, while higher-\norder (n >1) kernels induce feature spaces with different dynamic ranges.\nFinally, the results on both data sets reveal an interesting trend: the multilayer arc-cosine kernels\noften perform better than their single-layer counterparts. Though SVMs are (inherently) shallow\narchitectures, this trend suggests that for these problems in binary classiﬁcation, arc-cosine kernels\nmay be yielding some of the advantages typically associated with deep architectures.\n3 Deep learning\nIn this section, we explore how to use kernel methods in deep architectures [7]. We show how to train\ndeep kernel-based architectures by a simple combination of supervised and unsupervised methods.\nUsing the arc-cosine kernels in the previous section, these multilayer kernel machines (MKMs)\nperform very competitively on multiclass data sets designed to foil shallow architectures [11].\n5\n3.1 Multilayer kernel machines\nWe explored how to train MKMs in stages that involve kernel PCA [13] and feature selection [14] at\nintermediate hidden layers and large-margin nearest neighbor classiﬁcation [15] at the ﬁnal output\nlayer. Speciﬁcally, for ℓ-layer MKMs, we considered the following training procedure:\n1. Prune uninformative features from the input space.\n2. Repeat ℓ times:\n(a) Compute principal components in the feature space induced by a nonlinear kernel.\n(b) Prune uninformative components from the feature space.\n3. Learn a Mahalanobis distance metric for nearest neighbor classiﬁcation.\nThe individual steps in this procedure are well-established methods; only their combination is new.\nWhile many other approaches are worth investigating, our positive results from the above procedure\nprovide a ﬁrst proof-of-concept. We discuss each of these steps in greater detail below.\nKernel PCA. Deep learning in MKMs is achieved by iterative applications of kernel PCA [13]. This\nuse of kernel PCA was suggested over a decade ago [16] and more recently inspired by the pre-\ntraining of deep belief nets by unsupervised methods. In MKMs, the outputs (or features) from\nkernel PCA at one layer are the inputs to kernel PCA at the next layer. However, we do not strictly\ntransmit each layer’s top principal components to the next layer; some components are discarded if\nthey are deemed uninformative. While any nonlinear kernel can be used for the layerwise PCA in\nMKMs, arc-cosine kernels are natural choices to mimic the computations in large neural nets.\nFeature selection. The layers in MKMs are trained by interleaving a supervised method for feature\nselection with the unsupervised method of kernel PCA. The feature selection is used to prune away\nuninformative features at each layer in the MKM (including the zeroth layer which stores the raw\ninputs). Intuitively, this feature selection helps to focus the unsupervised learning in MKMs on\nstatistics of the inputs that actually contain information about the class labels. We prune features\nat each layer by a simple two-step procedure that ﬁrst ranks them by estimates of their mutual\ninformation, then truncates them using cross-validation. More speciﬁcally, in the ﬁrst step, we\ndiscretize each real-valued feature and construct class-conditional and marginal histograms of its\ndiscretized values; then, using these histograms, we estimate each feature’s mutual information with\nthe class label and sort the features in order of these estimates [14]. In the second step, considering\nonly the ﬁrst w features in this ordering, we compute the error rates of a basic kNN classiﬁer using\nEuclidean distances in feature space. We compute these error rates on a held-out set of validation\nexamples for many values of k and w and record the optimal values for each layer. The optimal w\ndetermines the number of informative features passed onto the next layer; this is essentially the\nwidth of the layer. In practice, we varied k from 1 to 15 and w from 10 to 300; though exhaustive,\nthis cross-validation can be done quickly and efﬁciently by careful bookkeeping. Note that this\nprocedure determines the architecture of the network in a greedy, layer-by-layer fashion.\nDistance metric learning. Test examples in MKMs are classiﬁed by a variant of kNN classiﬁcation\non the outputs of the ﬁnal layer. Speciﬁcally, we use large margin nearest neighbor (LMNN) clas-\nsiﬁcation [15] to learn a Mahalanobis distance metric for these outputs, though other methods are\nequally viable [17]. The use of LMNN is inspired by the supervised ﬁne-tuning of weights in the\ntraining of deep architectures [18]. In MKMs, however, this supervised training only occurs at the\nﬁnal layer (which underscores the importance of feature selection in earlier layers). LMNN learns a\ndistance metric by solving a problem in semideﬁnite programming; one advantage of LMNN is that\nthe required optimization is convex. Test examples are classiﬁed by the energy-based decision rule\nfor LMNN [15], which was itself inspired by earlier work on multilayer neural nets [19].\n3.2 Experiments on multiway classiﬁcation\nWe evaluated MKMs on the two multiclass data sets from previous benchmarks [11] that exhibited\nthe largest performance gap between deep and shallow architectures. The data sets were created from\nthe MNIST data set [20] of 28× 28 grayscale handwritten digits. The mnist-back-rand data set was\ngenerated by ﬁlling the image background by random pixel values, while themnist-back-image data\nset was generated by ﬁlling the image background with random image patches; examples are shown\nin Figs. 4 and 5. Each data set contains 12000 and 50000 training and test examples, respectively.\n6\n5\n6\n7\n8\nDBN−3\nTest error rate (%)\n 0  1    2    3    4   5\n      Step (n=0)\n 1    2    3    4   5\n     Ramp (n=1)\n            1    2\nQuarter−pipe (n=2)\n 1    2\n  RBF\n22\n24\n26\nDBN!3\nSVM!RBF\nTest error rate (%)\n1   2    3   4    5   6\n       Step (n=0)\n1   2    3   4    5   6\n      Ramp (n=1)\n1   2    3   4    5   6\nQuarter!pipe (n=2)\nFigure“:Left:examplesfromtherectangles-ima gedataset:Right:classiﬁcationerrorratesonthe\ntestset:SVMswitharccosinekernelshaveerrorratesfrom22.36–25.64%:Resultsareshownfor\nkernelsofvaryingdegreeﬁn2andlevelsofrecursion ﬁℓ2:Thebestpreviousresultsare24.04%for\nSVMswithRBFkernelsand22.50%fordeepbeliefnets[“]:Seetextfordetails:\n17\n18\n19\n20\n21\nDBN!3\nSVM!RBF\nTest error rate (%)\n1   2    3   4    5   6\n       Step (n=0)\n1   2    3   4    5   6\n      Ramp (n=1)\n1   2    3   4    5   6\nQuarter!pipe (n=2)\nFigure-:Left:examples fromtheconvexdataset:Right:classiﬁcationerrorratesonthetestset:\nSVMswitharccosinekernelshaveerrorratesfrom17.15–20.51%:Resultsareshownforkernels\nofvaryingdegreeﬁn2andlevelsofrecursion ﬁℓ2:Thebestpreviousresultsare19.13%forSVMs\nwithRBFkernelsand18.63%fordeepbeliefnets[“]:Seetextfordetails:\n“;;; trainingexamples asavalidation settochoosethemarginpenaltyparameter; afterchoosing\nthisparameter bycross.validation0wethenretrainedeachSVMusingallthetrainingexamples: For\nreference0wealsoreportthebestresultsobtainedpreviouslyfromthreelayerdeepbeliefnetsﬁDBN.\n-2andSVMswithRBFkernelsﬁSVM.RBF2:Thesereferences arerepresentativeofthecurrent\nstate.of.the.art fordeepandshallowarchitectures onthesedatasets:\nTherightpanelsofﬁgures“and-showthetestseterrorratesfromarccosinekernelsofvarying\ndegreeﬁn2andlevelsofrecursion ﬁℓ2:Weexperimented withkernelsofdegreen=0,1and“0\ncorresponding tosinglelayerthresholdnetworkswith“step”0“ramp”0and“quarter.pipe”activation\nfunctions: Wealsoexperimented withthemultilayerkernelsdescribed insection“:-0composed\nfromonetosixlevelsofrecursion: Overall0theﬁguresshowthatonthesetwodatasets0many\ndifferentarccosinekernelsoutperform thebestresultspreviouslyreported forSVMswithRBF\nkernelsanddeepbeliefnets:Wegivemoredetailsontheseexperimentsbelow:Atahighlevel0\nthough0wenotethatSVMswitharccosinekernelsareverystraightforw ardtotrain;unlikeSVMs\nwithRBFkernels0theydonotrequiretuningakernelwidthparameter0andunlikedeepbeliefnets0\ntheydonotrequiresolvingadifﬁcultnonlinearoptimization orsearchingoverpossiblearchitectures:\nInourexperiments0 wequicklydiscoveredthatthemultilayer kernelsonlyperformed wellwhen\nn=1kernelswereusedathigherﬁℓ>12levelsintherecursion: Figs:“and-therefore showonly\nthesesetsofresults;inparticular0eachgroupofbarsshowsthetesterrorrateswhenaparticular\nkernelﬁofdegreen=0,1,22wasusedattheﬁrstlayerofnonlinearity0whilethen=1kernelwas\nusedatsuccessivelayers:Wedonothaveaformalexplanation forthiseffect:However0recallthat\nonlythen=1arccosinekernelpreservesthenormofitsinputs:then=0kernelmapsallinputs\nontoaunithypersphere infeaturespace0whilehigher.orderﬁn>12kernelsmayinducefeature\nspaceswithseverelydistorteddynamicranges:Therefore0wehypothesize thatonlyn=1arccosine\nkernelspreservesufﬁcientinformation aboutthemagnitude oftheirinputstoworkeffectivelyin\ncomposition withotherkernels:\nFinally0theresultsonbothdatasetsrevealaninteresting trend:themultilayer arccosinekernels\noftenperformbetterthantheirsinglelayercounterparts:ThoughSVMsareshallowarchitectures0\n(\n17\n18\n19\n20\n21\nDBN!3\nSVM!RBF\nTest error rate (%)\n1   2    3   4    5   6\n       Step (n=0)\n1   2    3   4    5   6\n      Ramp (n=1)\n1   2    3   4    5   6\nQuarter!pipe (n=2)\nFigure 4: Left: examples from the mnist-back-rand data set. Right: classiﬁcation error rates on the\ntest set for MKMs with different kernels and numbers of layers ℓ. MKMs with arc-cosine kernel\nhave error rates from6.36–7.52%. The best previous results are14.58% for SVMs with RBF kernels\nand 6.73% for deep belief nets [11].\n22\n24\n26\nDBN!3\nSVM!RBF\nTest error rate (%)\n1   2    3   4    5   6\n       Step (n=0)\n1   2    3   4    5   6\n      Ramp (n=1)\n1   2    3   4    5   6\nQuarter!pipe (n=2)\nFigure“:Left:examplesfromtherectangles-ima gedataset:Right:classiﬁcationerrorratesonthe\ntestset:SVMswitharccosinekernelshaveerrorratesfrom22.36–25.64%:Resultsareshownfor\nkernelsofvaryingdegreeﬁn2andlevelsofrecursion ﬁℓ2:Thebestpreviousresultsare24.04%for\nSVMswithRBFkernelsand22.50%fordeepbeliefnets[“]:Seetextfordetails:\n17\n18\n19\n20\n21\nDBN!3\nSVM!RBF\nTest error rate (%)\n1   2    3   4    5   6\n       Step (n=0)\n1   2    3   4    5   6\n      Ramp (n=1)\n1   2    3   4    5   6\nQuarter!pipe (n=2)\nFigure-:Left:examples fromtheconvexdataset:Right:classiﬁcationerrorratesonthetestset:\nSVMswitharccosinekernelshaveerrorratesfrom17.15–20.51%:Resultsareshownforkernels\nofvaryingdegreeﬁn2andlevelsofrecursion ﬁℓ2:Thebestpreviousresultsare19.13%forSVMs\nwithRBFkernelsand18.63%fordeepbeliefnets[“]:Seetextfordetails:\n“;;; trainingexamples asavalidation settochoosethemarginpenaltyparameter; afterchoosing\nthisparameter bycross.validation0wethenretrainedeachSVMusingallthetrainingexamples: For\nreference0wealsoreportthebestresultsobtainedpreviouslyfromthreelayerdeepbeliefnetsﬁDBN.\n-2andSVMswithRBFkernelsﬁSVM.RBF2:Thesereferences arerepresentativeofthecurrent\nstate.of.the.art fordeepandshallowarchitectures onthesedatasets:\nTherightpanelsofﬁgures“and-showthetestseterrorratesfromarccosinekernelsofvarying\ndegreeﬁn2andlevelsofrecursion ﬁℓ2:Weexperimented withkernelsofdegreen=0,1and“0\ncorresponding tosinglelayerthresholdnetworkswith“step”0“ramp”0and“quarter.pipe”activation\nfunctions: Wealsoexperimented withthemultilayerkernelsdescribed insection“:-0composed\nfromonetosixlevelsofrecursion: Overall0theﬁguresshowthatonthesetwodatasets0many\ndifferentarccosinekernelsoutperform thebestresultspreviouslyreported forSVMswithRBF\nkernelsanddeepbeliefnets:Wegivemoredetailsontheseexperimentsbelow:Atahighlevel0\nthough0wenotethatSVMswitharccosinekernelsareverystraightforw ardtotrain;unlikeSVMs\nwithRBFkernels0theydonotrequiretuningakernelwidthparameter0andunlikedeepbeliefnets0\ntheydonotrequiresolvingadifﬁcultnonlinearoptimization orsearchingoverpossiblearchitectures:\nInourexperiments0 wequicklydiscoveredthatthemultilayer kernelsonlyperformed wellwhen\nn=1kernelswereusedathigherﬁℓ>12levelsintherecursion: Figs:“and-therefore showonly\nthesesetsofresults;inparticular0eachgroupofbarsshowsthetesterrorrateswhenaparticular\nkernelﬁofdegreen=0,1,22wasusedattheﬁrstlayerofnonlinearity0whilethen=1kernelwas\nusedatsuccessivelayers:Wedonothaveaformalexplanation forthiseffect:However0recallthat\nonlythen=1arccosinekernelpreservesthenormofitsinputs:then=0kernelmapsallinputs\nontoaunithypersphere infeaturespace0whilehigher.orderﬁn>12kernelsmayinducefeature\nspaceswithseverelydistorteddynamicranges:Therefore0wehypothesize thatonlyn=1arccosine\nkernelspreservesufﬁcientinformation aboutthemagnitude oftheirinputstoworkeffectivelyin\ncomposition withotherkernels:\nFinally0theresultsonbothdatasetsrevealaninteresting trend:themultilayer arccosinekernels\noftenperformbetterthantheirsinglelayercounterparts:ThoughSVMsareshallowarchitectures0\n(\n17\n18\n19\n20\n21\nDBN!3\nSVM!RBF\nTest error rate (%)\n1   2    3   4    5   6\n       Step (n=0)\n1   2    3   4    5   6\n      Ramp (n=1)\n1   2    3   4    5   6\nQuarter!pipe (n=2)\n15\n20\n25\n30\nDBN−3\nSVM−RBF\nTest error rate (%)\n 0  1    2    3    4   5\n      Step (n=0)\n 1    2    3    4   5\n     Ramp (n=1)\n            1    2\nQuarter−pipe (n=2)\n 1    2\n  RBF\nFigure 5: Left: examples from the mnist-back-image data set. Right: classiﬁcation error rates on the\ntest set for MKMs with different kernels and numbers of layers ℓ. MKMs with arc-cosine kernel\nhave error rates from 18.43–29.79%. The best previous results are 22.61% for SVMs with RBF\nkernels and 16.31% for deep belief nets [11].\nWe trained MKMs with arc-cosine kernels and RBF kernels in each layer. For each data set, we\ninitially withheld the last 2000 training examples as a validation set. Performance on this validation\nset was used to determine each MKM’s architecture, as described in the previous section, and also\nto set the kernel width in RBF kernels, following the same methodology as earlier studies [11].\nOnce these parameters were set by cross-validation, we re-inserted the validation examples into the\ntraining set and used all 12000 training examples for feature selection and distance metric learning.\nFor kernel PCA, we were limited by memory requirements to processing only 6000 out of 12000\ntraining examples. We chose these 6000 examples randomly, but repeated each experiment ﬁve\ntimes to obtain a measure of average performance. The results we report for each MKM are the\naverage performance over these ﬁve runs.\nThe right panels of Figs. 4 and 5 show the test set error rates of MKMs with different kernels and\nnumbers of layers ℓ. For reference, we also show the best previously reported results [11] using\ntraditional SVMs (with RBF kernels) and deep belief nets (with three layers). MKMs perform sig-\nniﬁcantly better than shallow architectures such as SVMs with RBF kernels or LMNN with feature\nselection (reported as the case ℓ = 0). Compared to deep belief nets, the leading MKMs obtain\nslightly lower error rates on one data set and slightly higher error rates on another.\nWe can describe the architecture of an MKM by the number of selected features at each layer (in-\ncluding the input layer). The number of features essentially corresponds to the number of units in\neach layer of a neural net. For the mnist-back-rand data set, the best MKM used an n=1 arc-cosine\nkernel and 300-90-105-136-126-240 features at each layer. For the mnist-back-image data set, the\nbest MKM used an n=0 arc-cosine kernel and 300-50-130-240-160-150 features at each layer.\nMKMs worked best with arc-cosine kernels of degree n = 0 and n = 1. The kernel of degree n = 2\nperformed less well in MKMs, perhaps because multiple iterations of kernel PCA distorted the\ndynamic range of the inputs (which in turn seemed to complicate the training for LMNN). MKMs\nwith RBF kernels were difﬁcult to train due to the sensitive dependence on kernel width parameters.\nIt was extremely time-consuming to cross-validate the kernel width at each layer of the MKM. We\nonly obtained meaningful results for one and two-layer MKMs with RBF kernels.\n7\nWe brieﬂy summarize many results that we lack space to report in full. We also experimented\non multiclass data sets using SVMs with single and multi-layer arc-cosine kernels, as described in\nsection 2. For multiclass problems, these SVMs compared poorly to deep architectures (both DBNs\nand MKMs), presumably because they had no unsupervised training that shared information across\nexamples from all different classes. In further experiments on MKMs, we attempted to evaluate the\nindividual contributions to performance from feature selection and LMNN classiﬁcation. Feature\nselection helped signiﬁcantly on the mnist-back-image data set, but only slightly on the mnist-back-\nrandom data set. Finally, LMNN classiﬁcation in the output layer yielded consistent improvements\nover basic kNN classiﬁcation provided that we used the energy-based decision rule [15].\n4 Discussion\nIn this paper, we have developed a new family of kernel functions that mimic the computation in\nlarge, multilayer neural nets. On challenging data sets, we have obtained results that outperform pre-\nvious SVMs and compare favorably to deep belief nets. More signiﬁcantly, our experiments validate\nthe basic intuitions behind deep learning in the altogether different context of kernel-based archi-\ntectures. A similar validation was provided by recent work on kernel methods for semi-supervised\nembedding [7]. We hope that our results inspire more work on kernel methods for deep learning.\nThere are many possible directions for future work. For SVMs, we are currently experimenting with\narc-cosine kernel functions of fractional and (even negative) degree n. For MKMs, we are hoping\nto explore better schemes for feature selection [21, 22] and kernel selection [23]. Also, it would be\ndesirable to incorporate prior knowledge, such as the invariances modeled by convolutional neural\nnets [24, 4], though it is not obvious how to do so. These issues and others are left for future work.\nA Derivation of kernel function\nIn this appendix, we show how to evaluate the multidimensional integral in eq. (1) for the arc-cosine\nkernel. Let θ denote the angle between the inputsx and y. Without loss of generality, we can takex\nto lie along the w1 axis and y to lie in the w1w2-plane. Integrating out the orthogonal coordinates\nof the weight vector w, we obtain the result in eq. (3) where Jn(θ) is the remaining integral:\nJn(θ) =\n∫\ndw1 dw2 e− 1\n2 (w2\n1+w2\n2) Θ(w1) Θ(w1 cos θ + w2 sin θ) wn\n1 (w1 cos θ + w2 sin θ)n. (14)\nChanging variables to u= w1 and v = w1 cos θ+w2 sin θ, we simplify the domain of integration to\nthe ﬁrst quadrant of the uv-plane:\nJn(θ) = 1\nsin θ\n∫ ∞\n0\ndu\n∫ ∞\n0\ndv e−(u2+v2−2uv cos θ)/(2 sin2 θ) unvn. (15)\nThe prefactor of (sin θ)−1 in eq. (15) is due to the Jacobian. To simplify the integral further, we\nadopt polar coordinates u = r cos( ψ\n2 + π\n4 ) and v = r sin( ψ\n2 + π\n4 ). Then, integrating out the radius\ncoordinate r, we obtain:\nJn(θ) = n! (sinθ)2n+1\n∫ π\n2\n0\ndψ cosn ψ\n(1− cos θ cos ψ)n+1 . (16)\nTo evaluate eq. (16), we ﬁrst consider the special casen=0. The following result can be derived by\ncontour integration in the complex plane [25]:\n∫ π/2\n0\ndψ\n1− cos θ cos ψ = π− θ\nsin θ . (17)\nSubstituting eq. (17) into our expression for the angular part of the kernel function in eq. (16), we\nrecover our earlier claim thatJ0(θ) = π− θ. Related integrals for the special case n = 0 can also be\nfound in earlier work [8].For the case n >0, the integral in eq. (16) can be performed by the method\nof differentiating under the integral sign. In particular, we note that:\n∫ π\n2\n0\ndψ cosn ψ\n(1− cos θ cos ψ)n+1 = 1\nn!\n∂n\n∂(cos θ)n\n∫ π/2\n0\ndψ\n1− cos θ cos ψ . (18)\nSubstituting eq. (18) into eq. (16), then appealing to the previous result in eq. (17), we recover the\nexpression for Jn(θ) in eq. (4).\n8\nReferences\n[1] Y . Bengio and Y . LeCun.Scaling learning algorithms towards AI . MIT Press, 2007.\n[2] G.E. Hinton, S. Osindero, and Y .W. Teh. A fast learning algorithm for deep belief nets. Neural Compu-\ntation, 18(7):1527–1554, 2006.\n[3] G.E. Hinton and R. Salakhutdinov. Reducing the dimensionality of data with neural networks. Science,\n313(5786):504–507, July 2006.\n[4] M.A. Ranzato, F.J. Huang, Y .L. Boureau, and Y . LeCun. Unsupervised learning of invariant feature\nhierarchies with applications to object recognition. In Proceedings of the 2007 IEEE Conference on\nComputer Vision and Pattern Recognition (CVPR-07), pages 1–8, 2007.\n[5] R. Collobert and J. Weston. A uniﬁed architecture for natural language processing: deep neural net-\nworks with multitask learning. In Proceedings of the 25th International Conference on Machine Learning\n(ICML-08), pages 160–167, 2008.\n[6] Y . Bengio. Learning deep architectures for AI. F oundations and Trends in Machine Learning, to appear,\n2009.\n[7] J. Weston, F. Ratle, and R. Collobert. Deep learning via semi-supervised embedding. In Proceedings of\nthe 25th International Conference on Machine Learning (ICML-08) , pages 1168–1175, 2008.\n[8] C.K.I. Williams. Computation with inﬁnite neural networks. Neural Computation , 10(5):1203–1216,\n1998.\n[9] R.H.R. Hahnloser, H.S. Seung, and J.J. Slotine. Permitted and forbidden sets in symmetric threshold-\nlinear networks. Neural Computation, 15(3):621–638, 2003.\n[10] R.M. Neal. Bayesian Learning for Neural Networks . Springer-Verlag New York, Inc., 1996.\n[11] H. Larochelle, D. Erhan, A. Courville, J. Bergstra, and Y . Bengio. An empirical evaluation of deep archi-\ntectures on problems with many factors of variation. In Proceedings of the 24th International Conference\non Machine Learning (ICML-07) , pages 473–480, 2007.\n[12] C.C. Chang and C.J. Lin. LIBSVM: a library for support vector machines , 2001. Software available at\nhttp://www.csie.ntu.edu.tw/˜cjlin/libsvm.\n[13] B. Sch ¨olkopf, A. Smola, and K. M ¨uller. Nonlinear component analysis as a kernel eigenvalue problem.\nNeural Computation, 10(5):1299–1319, 1998.\n[14] I. Guyon and A. Elisseeff. An introduction to variable and feature selection. Journal of Machine Learning\nResearch, 3:1157–1182, 2003.\n[15] K.Q. Weinberger and L.K. Saul. Distance metric learning for large margin nearest neighbor classiﬁcation.\nJournal of Machine Learning Research, 10:207–244, 2009.\n[16] B. Sch ¨olkopf, A. J. Smola, and K.-R. M ¨uller. Nonlinear component analysis as a kernel eigenvalue\nproblem. Technical Report 44, Max-Planck-Institut f¨ur biologische Kybernetik, 1996.\n[17] J. Goldberger, S. Roweis, G.E. Hinton, and R. Salakhutdinov. Neighbourhood components analysis. In\nL.K. Saul, Y . Weiss, and L. Bottou, editors,Advances in Neural Information Processing Systems 17, pages\n513–520. MIT Press, 2005.\n[18] Y . Bengio, P. Lamblin, D. Popovici, and H. Larochelle. Greedy layer-wise training of deep networks. In\nB. Sch¨olkopf, J. Platt, and T. Hoffman, editors, Advances in Neural Information Processing Systems 19 ,\npages 153–160. MIT Press, 2007.\n[19] S. Chopra, R. Hadsell, and Y . LeCun. Learning a similarity metric discriminatively, with application to\nface veriﬁcation. In Proceedings of the 2005 IEEE Conference on Computer Vision and Pattern Recogni-\ntion (CVPR-05), pages 539–546, 2005.\n[20] Y . LeCun and C. Cortes. The MNIST database of handwritten digits. http://yann.lecun.com/\nexdb/mnist/.\n[21] M. Tipping. Sparse kernel principal component analysis. In Advances in Neural Information Processing\nSystems 13. MIT Press, 2001.\n[22] A.J. Smola, O.L. Mangasarian, and B. Sch ¨olkopf. Sparse kernel feature analysis. Technical Report 99-04,\nUniversity of Wisconsin, Data Mining Institute, Madison, 1999.\n[23] G. Lanckriet, N. Cristianini, P. Bartlett, L.E. Ghaoui, and M.I. Jordan. Learning the kernel matrix with\nsemideﬁnite programming. Journal of Machine Learning Research, 5:27–72, 2004.\n[24] Y . LeCun, B. Boser, J.S. Denker, D. Henderson, R.E. Howard, W. Hubbard, and L.D. Jackel. Backpropa-\ngation applied to handwritten zip code recognition. Neural Computation, 1(4):541–551, 1989.\n[25] G.F. Carrier, M. Krook, and C.E. Pearson. Functions of a Complex V ariable: Theory and Technique .\nSociety for Industrial and Applied Mathematics, 2005.\n9",
  "values": {
    "Respect for Persons": "No",
    "Privacy": "No",
    "Critiqability": "No",
    "Respect for Law and public interest": "No",
    "Beneficence": "No",
    "Interpretable (to users)": "No",
    "Transparent (to users)": "No",
    "Explicability": "No",
    "User influence": "No",
    "Fairness": "No",
    "Non-maleficence": "No",
    "Justice": "No",
    "Autonomy (power to decide)": "No",
    "Not socially biased": "No",
    "Collective influence": "No",
    "Deferral to humans": "No"
  }
}