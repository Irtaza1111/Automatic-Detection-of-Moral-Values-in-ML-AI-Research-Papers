{
  "pdf": "NIPS-2009-learning-non-linear-combinations-of-kernels-Paper",
  "title": "Learning Non-Linear Combinations of Kernels",
  "author": "Corinna Cortes, Mehryar Mohri, Afshin Rostamizadeh",
  "paper_id": "NIPS-2009-learning-non-linear-combinations-of-kernels-Paper",
  "text": "Learning Non-Linear Combinations of Kernels\nCorinna Cortes\nGoogle Research\n76 Ninth Ave\nNew York, NY 10011\ncorinna@google.com\nMehryar Mohri\nCourant Institute and Google\n251 Mercer Street\nNew York, NY 10012\nmohri@cims.nyu.edu\nAfshin Rostamizadeh\nCourant Institute and Google\n251 Mercer Street\nNew York, NY 10012\nrostami@cs.nyu.edu\nAbstract\nThis paper studies the general problem of learning kernels based on a polynomial\ncombination of base kernels. We analyze this problem in the c ase of regression\nand the kernel ridge regression algorithm. We examine the corresponding learning\nkernel optimization problem, show how that minimax problemcan be reduced to a\nsimpler minimization problem, and prove that the global solution of this problem\nalways lies on the boundary. We give a projection-based grad ient descent algo-\nrithm for solving the optimization problem, shown empirically to converge in few\niterations. Finally, we report the results of extensive exp eriments with this algo-\nrithm using several publicly available datasets demonstrating the effectiveness of\nour technique.\n1 Introduction\nLearning algorithms based on kernels have been used with much success in a variety of tasks [17,19].\nClassiﬁcation algorithms such as support vector machines ( SVMs) [6, 10], regression algorithms,\ne.g., kernel ridge regression and support vector regression (SVR) [16, 22], and general dimensional-\nity reduction algorithms such as kernel PCA (KPCA) [18] all beneﬁt from kernel methods. Positive\ndeﬁnite symmetric (PDS) kernel functions implicitly speci fy an inner product in a high-dimension\nHilbert space where large-margin solutions are sought. So l ong as the kernel function used is PDS,\nconvergence of the training algorithm is guaranteed.\nHowever, in the typical use of these kernel method algorithms, the choice of the PDS kernel, which\nis crucial to improved performance, is left to the user. A les s demanding alternative is to require\nthe user to instead specify a family of kernels and to use the training data to select the most suitable\nkernel out of that family. This is commonly referred to as the problem of learning kernels.\nThere is a large recent body of literature addressing variou s aspects of this problem, including de-\nriving efﬁcient solutions to the optimization problems it generates and providing a better theoretical\nanalysis of the problem both in classiﬁcation and regression [1, 8, 9, 11, 13, 15, 21]. With the excep-\ntion of a few publications considering inﬁnite-dimensional kernel families such as hyperkernels [14]\nor general convex classes of kernels [2], the great majorityof analyses and algorithmic results focus\non learning ﬁnite linear combinations of base kernels as originally considered by [1 2]. However,\ndespite the substantial progress made in the theoretical un derstanding and the design of efﬁcient\nalgorithms for the problem of learning such linear combinations of kernels, no method seems to re-\nliably give improvements over baseline methods. For example, the learned linear combination does\nnot consistently outperform either the uniform combination of base kernels or simply the best single\nbase kernel (see, for example, UCI dataset experiments in [9 , 12], see also NIPS 2008 workshop).\nThis suggests exploring other non-linear families of kernels to obtain consistent and signiﬁcant\nperformance improvements.\nNon-linear combinations of kernels have been recently cons idered by [23]. However, here too,\nexperimental results have not demonstrated a consistent pe rformance improvement for the general\n1\nlearning task. Another method, hierarchical multiple learning [3], considers learning a linear combi-\nnation of an exponential number of linear kernels, which can be efﬁciently represented as a product\nof sums. Thus, this method can also be classiﬁed as learning a non-linear combination of kernels.\nHowever, in [3] the base kernels are restricted to concatenation kernels, where the base kernels\napply to disjoint subspaces. For this approach the authors p rovide an effective and efﬁcient algo-\nrithm and some performance improvement is actually observed for regression problems in very high\ndimensions.\nThis paper studies the general problem of learning kernels b ased on a polynomial combination of\nbase kernels. We analyze that problem in the case of regressi on using the kernel ridge regression\n(KRR) algorithm. We show how to simplify its optimization pr oblem from a minimax problem\nto a simpler minimization problem and prove that the global s olution of the optimization problem\nalways lies on the boundary. We give a projection-based gradient descent algorithm for solving this\nminimization problem that is shown empirically to convergein few iterations. Furthermore, we give\na necessary and sufﬁcient condition for this algorithm to reach a global optimum. Finally, we report\nthe results of extensive experiments with this algorithm us ing several publicly available datasets\ndemonstrating the effectiveness of our technique.\nThe paper is structured as follows. In Section 2, we introduc e the non-linear family of kernels\nconsidered. Section 3 discusses the learning problem, form ulates the optimization problem, and\npresents our solution. In Section 4, we study the performanc e of our algorithm for learning non-\nlinear combinations of kernels in regression (NKRR) on several publicly available datasets.\n2 Kernel Family\nThis section introduces and discusses the family of kernels we consider for our learning kernel\nproblem. Let K1, . . . , K p be a ﬁnite set of kernels that we combine to deﬁne more complexkernels.\nWe refer to these kernels as base kernels. In much of the previous work on learning kernels, the\nfamily of kernels considered is that of linear or convex comb inations of some base kernels. Here,\nwe consider polynomial combinations of higher degree d≥1 of the base kernels with non-negative\ncoefﬁcients of the form:\nKµ =\n∑\n0≤ k1+···+kp≤ d, k i≥ 0, i ∈[0,p ]\nµk1···kpK k1\n1 ··· K kp\np , µ k1···kp≥0. (1)\nAny kernel function Kµ of this form is PDS since products and sums of PDS kernels are P DS [4].\nNote that Kµ is in fact a linear combination of the PDS kernels K k1\n1 ··· K kp\np . However, the number\nof coefﬁcients µk1···kp is in O(pd), which may be too large for a reliable estimation from a sampl e\nof size m. Instead, we can assume that for some subset I of all p-tuples (k1, . . . , k p), µk1···kp can\nbe written as a product of non-negative coefﬁcients µ1, . . . , µ p: µk1···kp = µk1\n1 ··· µkp\np . Then, the\ngeneral form of the polynomial combinations we consider becomes\nK =\n∑\n(k1,...,k p)∈I\nµk1\n1 ··· µkp\np K k1\n1 ··· K kp\np +\n∑\n(k1,...,k p)∈J\nµk1···kp K k1\n1 ··· K kp\np , (2)\nwhere J denotes the complement of the subset I. The total number of free parameters is then\nreduced to p+|J|. The choice of the set I and its size depends on the sample size m and possible\nprior knowledge about relevant kernel combinations. The se cond sum of equation (2) deﬁning our\ngeneral family of kernels represents a linear combination o f PDS kernels. In the following, we\nfocus on kernels that have the form of the ﬁrst sum and that are thus non-linear in the parameters\nµ1, . . . , µ p. More speciﬁcally, we consider kernels Kµ deﬁned by\nKµ =\n∑\nk1+···+kp=d\nµk1\n1 ··· µkp\np K k1\n1 ··· K kp\np , (3)\nwhere µ = (µ1, . . . , µ p)⊤∈Rp. For the ease of presentation, our analysis is given for the case d = 2,\nwhere the quadratic kernel can be given the following simpler expression:\nKµ =\np∑\nk,l =1\nµkµl KkKl . (4)\nBut, the extension to higher-degree polynomials is straigh tforward and our experiments include\nresults for degrees d up to 4.\n2\n3 Algorithm for Learning Non-Linear Kernel Combinations\n3.1 Optimization Problem\nWe consider a standard regression problem where the learner receives a training sample of size\nm, S = ((x1, y 1), . . . , (xm, y m))∈(X×Y )m, where X is the input space and Y ∈R the label\nspace. The family of hypotheses Hµ out of which the learner selects a hypothesis is the reproducing\nkernel Hilbert space (RKHS) associated to a PDS kernel funct ion Kµ : X×X→R as deﬁned in\nthe previous section. Unlike standard kernel-based regres sion algorithms however, here, both the\nparameter vector µ deﬁning the kernel Kµ and the hypothesis are learned using the training sample\nS.\nThe learning kernel algorithm we consider is derived from ke rnel ridge regression (KRR). Let y =\n[y1, . . . , y m]⊤∈Rm denote the vector of training labels and let Kµ denote the Gram matrix of the\nkernel Kµ for the sample S: [Kµ]i,j = Kµ(xi, x j), for all i, j ∈[1, m ]. The standard KRR dual\noptimization algorithm for a ﬁxed kernel matrix Kµ is given in terms of the Lagrange multipliers\nα∈Rm by [16]:\nmax\nα∈Rm\n−α⊤ (Kµ + λ I)α + 2α⊤ y (5)\nThe related problem of learning the kernel Kµ concomitantly can be formulated as the following\nmin-max optimization problem [9]:\nmin\nµ∈M\nmax\nα∈Rm\n−α⊤ (Kµ + λ I)α + 2α⊤ y, (6)\nwhereM is a positive, bounded, and convex set. The positivity of µ ensures that Kµ is positive\nsemi-deﬁnite (PSD) and its boundedness forms a regularizat ion controlling the norm of µ.1 Two\nnatural choices for the setM are the norm-1 and norm-2 bounded sets,\nM1 ={µ|µ⪰ 0∧∥µ−µ0∥1≤Λ} (7)\nM2 ={µ|µ⪰ 0∧ ∥µ−µ0∥2≤Λ}. (8)\nThese deﬁnitions include an offset parameter µ0 for the weights µ. Some natural choices for µ0\nare: µ0 = 0, or µ0/ ∥µ0∥ = 1. Note that here, since the objective function is not linear i n µ, the\nnorm-1-type regularization may not lead to a sparse solution.\n3.2 Algorithm Formulation\nFor learning linear combinations of kernels, a typical tech nique consists of applying the minimax\ntheorem to permute the min and max operators, which can lead to optimization problems com-\nputationally more efﬁcient to solve [8, 12]. However, in the non-linear case we are studying, this\ntechnique is unfortunately not applicable.\nInstead, our method for learning non-linear kernels and solving the min-max problem in equation (6)\nconsists of ﬁrst directly solving the inner maximization pr oblem. In the case of KRR for any ﬁxed\nµ the optimum is given by\nα = (Kµ + λ I)− 1y. (9)\nPlugging the optimal expression of α in the min-max optimization yields the following equivalen t\nminimization in terms of µ only:\nmin\nµ∈M\nF (µ) = y⊤ (Kµ + λ I)− 1y. (10)\nWe refer to this optimization as the NKRR problem. Although t he original min-max problem has\nbeen reduced to a simpler minimization problem, the functio n F is not convex in general as illus-\ntrated by Figure 1. For small values of µ, concave regions are observed. Thus, standard interior-\npoint or gradient methods are not guaranteed to be successful at ﬁnding a global optimum.\nIn the following, we give an analysis which shows that under certain conditions it is however possible\nto guarantee the convergence of a gradient-descent type algorithm to a global minimum.\nAlgorithm 1 illustrates a general gradient descent algorit hm for the norm-2 bounded setting which\nprojects µ back to the feasible setM2 after each gradient step (projecting toM1 is very similar).\n1To clarify the difference between similar acronyms, a PDS function corresponds to a PSD matrix [4].\n3\n0\n0.5\n1\n0\n0.5\n1\n195\n200\n205\n210\nµ2µ1\nF(µ1,µ2)\n0\n0.5\n1 0\n0.5\n1\n20\n20.5\n21\nµ2µ1\nF(µ1,µ2)\n0\n0.5\n1 0\n0.5\n1\n2.06\n2.07\n2.08\n2.09\nµ2µ1\nF(µ1,µ2)\nFigure 1: Example plots for F deﬁned over two linear base kernels generated from the ﬁrst t wo\nfeatures of the sonar dataset. From left to right λ = 1, 10, 100. For larger values of λ it is clear that\nthere are in fact concave regions of the function near 0.\nAlgorithm 1 Projection-based Gradient Descent Algorithm\nInput: µinit∈M2, η∈[0, 1], ǫ > 0, Kk, k∈[1, p ]\nµ′←µinit\nrepeat\nµ←µ′\nµ′←−η∇F (µ) + µ\n∀k, µ ′\nk←max(0, µ ′\nk)\nnormalize µ′, s.t.∥µ′−µ0∥ = Λ\nuntil∥µ′−µ∥ < ǫ\nIn Algorithm 1 we have ﬁxed the step size η , however this can be adjusted at each iteration via\na line-search. Furthermore, as shown later, the thresholdi ng step that forces µ′to be positive is\nunnecessary since∇F is never positive.\nNote that Algorithm 1 is simpler than the wrapper method proposed by [20]. Because of the closed\nform expression (10), we do not alternate between solving fo r the dual variables and performing a\ngradient step in the kernel parameters. We only need to optimize with respect to the kernel parame-\nters.\n3.3 Algorithm Properties\nWe ﬁrst explicitly calculate the gradient of the objective function for the optimization problem (10).\nIn what follows,◦denotes the Hadamard (pointwise) product between matrices.\nProposition 1. For any k∈[1, p ], the partial derivative of F : µ→y⊤ (Kµ + λ I)− 1y with respect\nto µi is given by\n∂F\n∂µ k\n=−2α⊤ Ukα, (11)\nwhere Uk =\n( ∑p\nr=1(µrKr)◦Kk\n)\n.\nProof. In view of the identity∇M Tr(y⊤ M− 1y) =−M− 1⊤\nyy⊤ M− 1⊤\n, we can write:\n∂F\n∂µ k\n= Tr\n[ ∂ y⊤ (Kµ + λ I)− 1y\n∂ (Kµ + λ I)\n∂ (Kµ + λ I)\n∂µ k\n]\n=−Tr\n[\n(Kµ + λ I)− 1yy⊤ (Kµ + λ I)− 1 ∂ (Kµ + λ I)\n∂µ k\n]\n=−Tr\n[\n(Kµ + λ I)− 1yy⊤ (Kµ + λ I)− 1\n(\n2\np∑\nr=1\n(µrKr)◦Kk\n) ]\n=−2y⊤ (Kµ + λ I)− 1\n( p∑\nr=1\n(µrKr)◦Kk\n)\n(Kµ + λ I)− 1y =−2α⊤ Ukα.\n4\nMatrix Uk just deﬁned in proposition 1 is always PSD, thus ∂F\n∂µ k\n≤0 for all i∈[1, p ] and∇F≤0.\nAs already mentioned, this fact obliterates the thresholdi ng step in Algorithm 1. We now provide\nguarantees for convergence to a global optimum. We shall assume that λ is strictly positive: λ > 0.\nProposition 2. Any stationary point µ⋆ of the function F : µ→y⊤ (Kµ + λ I)− 1y necessarily\nmaximizes F :\nF (µ⋆ ) = max\nµ\nF (µ) =∥y∥2\nλ . (12)\nProof. In view of the expression of the gradient given by Proposition 1, at any point µ⋆ ,\nµ⋆ ⊤∇F (µ⋆ ) = α⊤\np∑\ni=1\nµ⋆\nkUkα = α⊤ Kµ⋆ α. (13)\nBy deﬁnition, if µ⋆ is a stationary point, ∇F (µ⋆ ) = 0 , which implies µ⋆ ⊤∇F (µ⋆ ) = 0 . Thus,\nα⊤ Kµ⋆ α = 0, which implies Kµ⋆ α = 0, that is\nKµ⋆ (Kµ⋆ + λ I)− 1y = 0⇔(Kµ⋆ + λ I−λ I)(Kµ⋆ + λ I)− 1y = 0 (14)\n⇔y−λ (Kµ⋆ + λ I)− 1y = 0 (15)\n⇔(Kµ⋆ + λ I)− 1y = y\nλ . (16)\nThus, for any such stationary point µ⋆ , F (µ⋆ ) = y⊤ (Kµ⋆ + λ I)− 1y = y⊤y\nλ , which is clearly a\nmaximum.\nWe next show that there cannot be an interior stationary point, and thus any local minimum strictly\nwithin the feasible set, unless the function is constant.\nProposition 3. If any point µ⋆ > 0 is a stationary point of F : µ→y⊤ (Kµ + λ I)− 1y, then the\nfunction is necessarily constant.\nProof. Assume that µ⋆ > 0 is a stationary point, then, by Proposition 2, F (µ⋆ ) = y⊤ (Kµ⋆ +\nλ I)− 1y = y⊤y\nλ , which implies thaty is an eigenvector of(Kµ⋆ +λ I)− 1 with eigenvalueλ − 1. Equiv-\nalently, y is an eigenvector of Kµ⋆ + λ I with eigenvalue λ , which is equivalent to y∈null(Kµ⋆ ).\nThus,\ny⊤ Kµ⋆ y =\np∑\nk,l =1\nµkµl\nm∑\nr,s =1\nyrysKk(xr, x s)Kl(xr, x s)\n  \n(∗)\n= 0. (17)\nSince the product of PDS functions is also PDS, (*) must be non -negative. Furthermore, since by\nassumption µi > 0 for all i∈[1, p ], it must be the case that the term (*) is equal to zero. Thus,\nequation 17 is equal to zero for all µ and the function F is equal to the constant∥y∥2/λ .\nThe previous propositions are sufﬁcient to show that the gradient descent algorithm will not become\nstuck at a local minimum while searching the interior of a con vex setM and, furthermore, they\nindicate that the optimum is found at the boundary.\nThe following proposition gives a necessary and sufﬁcient c ondition for the convexity of F on a\nconvex region C. If the boundary region deﬁned by ∥µ−µ0∥ = Λ is contained in this convex\nregion, then Algorithm 1 is guaranteed to converge to a globa l optimum. Let u∈Rp represent an\narbitrary direction of µ in C. We simplify the analysis of convexity in the following deri vation by\nseparating the terms that depend on Kµ and those depending on Ku, which arise when showing\nthe positive semi-deﬁniteness of the Hessian, i.e. u⊤∇2F u⪰ 0. We denote by ⊗the Kronecker\nproduct of two matrices.\nProposition 4. The function F : µ→y⊤ (Kµ + λ I)− 1y is convex over the convex set C iff the\nfollowing condition holds for all µ∈C and all u:\n⟨M, N−˜1⟩F≥0, (18)\n5\nData m p lin. base lin. ℓ 1 lin. ℓ 2 quad. base quad. ℓ 1 quad. ℓ 2\nParkinsons 194 21 . 70±. 03 . 70±. 04 . 70±. 03 . 65±. 03 . 66±. 03 . 64±. 03\nIono 351 34 . 82±. 03 . 81±. 04 . 81±. 03 . 62±. 05 . 62±. 05 . 60±. 05\nSonar 208 60 . 90±. 02 . 92±. 03 . 90±. 04 . 84±. 03 . 80±. 04 . 80±. 04\nBreast 683 9 . 70±. 02 . 71±. 02 . 70±. 02 . 70±. 02 . 70±. 01 . 70±. 01\nTable 1: The square-root of the mean squared error is reported for each method and several datasets.\nwhere M =\n(\n1⊗vec(αα⊤ )⊤ )\n◦(Ku⊗Ku), N = 4\n(\n1⊗vec(V)⊤ )\n◦(Kµ⊗Kµ), and ˜1 is the\nmatrix with zero-one entries constructed to select the terms [M]ijkl where i = k and j = l, i.e. it is\nnon-zero only in the (i, j )th coordinate of the (i, j )th m×m block.\nProof. For any u∈Rp the expression of the Hessian of F at the point µ∈C can be derived from\nthat of its gradient and shown to be\nu⊤ (∇2F )u = 4α⊤ (Kµ◦Ku)V(Kµ◦Ku)α−α⊤ (Ku◦Ku)α. (19)\nExpanding each term, we obtain:\nα⊤ (Kµ◦Ku)V(Kµ◦Ku)α =\nm∑\ni,j =1\nα iα j\nm∑\nk,l =1\n[Kµ]ik[Ku]ik[V]kl[Kµ]ik[Kµ]lj (20)\n=\nm∑\ni,j,k,l =1\n(α iα j[Ku]ik[Ku]lj )([V]kl[Kµ]ik[Kµ]lj ) (21)\nand α⊤ (Ku◦Ku)α = ∑m\ni,j =1 α iα j[Ku]ij[Ku]ij. Let 1∈Rm2\ndeﬁne the column vector of all\nones and let vec(A) denote the vectorization of a matrixA by stacking its columns. Let the matrices\nM and N be deﬁned as in the statement of the proposition. Then, [M]ijkl = ( α iα j[Ku]ik[Ku]lj )\nand [N]ijkl = [ V]kl[Kµ]ik[Kµ]lj. Then, in view of the deﬁnition of ˜\n1, the terms of equation (19)\ncan be represented with the Frobenius inner product,\nu⊤ (∇2F )u =⟨M, N⟩F−⟨M, ˜1⟩F =⟨M, N−˜1⟩F .\nFor any µ∈Rp, let Kµ = ∑\ni µiKi and let V = (Kµ + λ I)− 1. We now show that the condition\nof Proposition 4 is satisﬁed for convex regions for which Λ, and therefore µ, is sufﬁciently large, in\nthe case where Ku and Kµ are diagonal. In that case, M, N and V are diagonal as well and the\ncondition of Proposition 4 can be rewritten as follows:\n∑\ni,j\n[Ku]ii[Ku]jj α iα j(4[Kµ]ii[Kµ]jj Vij−1i=j)≥0. (22)\nUsing the fact that V is diagonal, this inequality we can be further simpliﬁed\nm∑\ni=1\n[Ku]2\nii α 2\ni (4[Kµ]2\niiVii−1)≥0. (23)\nA sufﬁcient condition for this inequality to hold is that each term (4[Kµ]2\niiVii−1) be non-negative,\nor equivalently that 4K2\nµV−I⪰ 0, that is Kµ⪰\n√\nλ\n3 I. Therefore, it sufﬁces to select µ such that\nmini\n∑p\nk=1 µk[Kk]ii≥\n√\nλ/ 3.\n4 Empirical Results\nTo test the advantage of learning non-linear kernel combina tions, we carried out a number of ex-\nperiments on publicly available datasets. The datasets are chosen to demonstrate the effectiveness\nof the algorithm under a number of conditions. For general pe rformance improvement, we chose a\nnumber of UCI datasets frequently used in kernel learning experiments, e.g., [7,12,15]. For learning\nwith thousands of kernels, we chose the sentiment analysis d ataset of Blitzer et. al [5]. Finally, for\nlearning with higher-order polynomials, we selected datasets with large number of examples such as\nkin-8nm from the Delve repository. The experiments were run on a 2.33 GHz Intel Xeon Processor\nwith 2GB of RAM.\n6\n0 1000 2000 3000 40001.4\n1.45\n1.5\n1.55\n1.6\n1.65\n1.7\nKitchen\nRMSE\n# bigrams\n \n \nL2 reg. Baseline L1 reg.\n0 1000 2000 3000 4000 50001.4\n1.45\n1.5\n1.55\n1.6\n1.65\n1.7\nElectronics\nRMSE\n# bigrams\n \n \nL2 reg. Baseline L1 reg.\nFigure 2: The performance of baseline and learned quadratic kernels (plus or minus one standard\ndeviation) versus the number of bigrams (and kernels) used.\n4.1 UCI Datasets\nWe ﬁrst analyzed the performance of the kernels learned as qu adratic combinations. For each\ndataset, features were scaled to lie in the interval[0, 1]. Then, both labels and features were centered.\nIn the case of classiﬁcation dataset, the labels were set to ±1 and the RMSE was reported. We as-\nsociated a base kernel to each feature, which computes the product of this feature between different\nexamples. We compared both linear and quadratic combinatio ns, each with a baseline (uniform),\nnorm-1-regularized and norm-2-regularized weighting using µ0 = 1 corresponding to the weights of\nthe baseline kernel. The parameters λ and Λ were selected via 10-fold cross validation and the error\nreported was based on 30 random 50/50 splits of the entire dataset into training and test sets. For the\ngradient descent algorithm, we started with η = 1 and reduced it by a factor of 0. 8 if the step was\nfound to be too large, i.e., the difference∥µ′−µ∥ increased. Convergence was typically obtained\nin less than 25 steps, each requiring a fraction of a second (∼0. 05 seconds).\nThe results, which are presented in Table 1, are in line with p revious ones reported for learning\nkernels on these datasets [7,8,12,15]. They indicate that learning quadratic combination kernels can\nsometimes offer improvements and that it clearly does not degrade with respect to the performance\nof the baseline kernel. The learned quadratic combination performs well, particularly on tasks where\nthe number of features was large compared to the number of poi nts. This suggests that the learned\nkernel is better regularized than the plain quadratic kerne l and can be advantageous is scenarios\nwhere over-ﬁtting is an issue.\n4.2 Text Based Dataset\nWe next analyzed a text-based task where features are freque nt word n-grams. Each base kernel\ncomputes the product between the counts of a particular n-gram for the given pair of points. Such\nkernels have a direct connection to count-based rational ke rnels, as described in [8]. We used the\nsentiment analysis dataset of Blitzer et. al [5]. This datas et contains text-based user reviews found\nfor products onamazon.com. Each text review is associated with a 0-5 star rating. The product re-\nviews fall into two categories: electronics and kitchen-wares, each with 2,000 data-points. The data\nwas not centered in this case since we wished to preserve the sparsity, which offers the advantage of\nsigniﬁcantly more efﬁcient computations. A constant feature was included to act as an offset.\nFor each domain, the parameters λ and Λ were chosen via 10-fold cross validation on 1,000 points.\nOnce these parameters were ﬁxed, the performance of each alg orithm was evaluated using 20 ran-\ndom 50/ 50 splits of the entire 2,000 points into training and test sets . We used the performance of\nthe uniformly weighted quadratic combination kernel as a ba seline, and showed the improvement\nwhen learning the kernel with norm-1 or norm-2 regularizati on using µ0 = 1 corresponding to the\nweights of the baseline kernel. As shown by Figure 2, the lear ned kernels signiﬁcantly improved\nover the baseline quadratic kernel in both the kitchen and el ectronics categories. For this case too,\nthe number of features was large in comparison with the number of points. Using 900 training points\nand about 3,600 bigrams, and thus kernels, each iteration of the algorithm took approximately 25\n7\n0 20 40 60 80 100\n0.10\n0.15\n0.20\n0.25\nTraining data subsampling factor\nMSE\nKRR, with (dashed) and without (solid) learning\n1st degree\n2nd degree\n3rd degree\n4th degree\nFigure 3: Performance on the kin-8nm dataset. For all polynomials, we compared un-weighted,\nstandard KRR (solid lines) with norm-2 regularized kernel l earning (dashed lines). For 4th degree\npolynomials we observed a clear performance improvement, especially for medium amount of train-\ning data (subsampling factor of 10-50). Standard deviations were typically in the order 0. 005, so the\nresults were statistically signiﬁcant.\nseconds to compute with our Matlab implementation. When usi ng norm-2 regularization, the algo-\nrithm generally converges in under 30 iterations, while the norm-1 regularization requires an even\nfewer number of iterations, typically less than 5.\n4.3 Higher-order Polynomials\nWe ﬁnally investigated the performance of higher-order non -linear combinations. For these exper-\niments, we used the kin-8nm dataset from the Delve repository. This dataset has 20,000 e xamples\nwith 8 input features. Here too, we used polynomial kernels o ver the features, but this time we\nexperimented with polynomials with degrees as high as 4. Aga in, we made the assumption that all\ncoefﬁcients of µ are in the form of products of µis (see Section 2), thus only 8 kernel parameters\nneeded to be estimated.\nWe split the data into 10,000 examples for training and 10,00 0 examples for testing, and, to inves-\ntigate the effect of the sample size on learning kernels, sub sampled the training data so that only a\nfraction from 1 to 100 was used. The parameters λ and Λ were determined by 10-fold cross vali-\ndation on the training data, and results are reported on the t est data, see Figure 3. We used norm-2\nregularization with µ0 = 1 and compare our results with those of uniformly weighted KRR.\nFor lower degree polynomials, the performance was essentia lly the same, but for 4th degree poly-\nnomials we observed a signiﬁcant performance improvement of learning kernels over the uniformly\nweighted KRR, especially for a medium amount of training data (subsampling factor of 10-50). For\nthe sake of readability, the standard deviations are not ind icated in the plot. They were typically in\nthe order of 0.005, so the results were statistically signiﬁ cant. This result corroborates the ﬁnding\non the UCI dataset, that learning kernels is better regulari zed than plain unweighted KRR and can\nbe advantageous is scenarios where overﬁtting is an issue.\n5 Conclusion\nWe presented an analysis of the problem of learning polynomial combinations of kernels in regres-\nsion. This extends learning kernel ideas and helps explore k ernel combinations leading to better\nperformance. We proved that the global solution of the optim ization problem always lies on the\nboundary and gave a simple projection-based gradient descent algorithm shown empirically to con-\nverge in few iterations. We also gave a necessary and sufﬁcie nt condition for that algorithm to\nconverge to a global optimum. Finally, we reported the resul ts of several experiments on publicly\navailable datasets demonstrating the beneﬁts of learning polynomial combinations of kernels. We are\nwell aware that this constitutes only a preliminary study and that a better analysis of the optimization\nproblem and solution should be further investigated. We hop e that the performance improvements\nreported will further motivate such analyses.\n8\nReferences\n[1] A. Argyriou, R. Hauser, C. Micchelli, and M. Pontil. A DC-programming algorithm for kernel\nselection. In International Conference on Machine Learning, 2006.\n[2] A. Argyriou, C. Micchelli, and M. Pontil. Learning conve x combinations of continuously\nparameterized basic kernels. In Conference on Learning Theory, 2005.\n[3] F. Bach. Exploring large feature spaces with hierarchical multiple kernel learning. InAdvances\nin Neural Information Processing Systems, 2008.\n[4] C. Berg, J. P. R. Christensen, and P. Ressel. Harmonic Analysis on Semigroups . Springer-\nVerlag: Berlin-New York, 1984.\n[5] J. Blitzer, M. Dredze, and F. Pereira. Biographies, Bollywood, Boom-boxes and Blenders: Do-\nmain Adaptation for Sentiment Classiﬁcation. In Association for Computational Linguistics ,\n2007.\n[6] B. Boser, I. Guyon, and V . Vapnik. A training algorithm fo r optimal margin classiﬁers. In\nConference on Learning Theory, 1992.\n[7] O. Chapelle, V . Vapnik, O. Bousquet, and S. Mukherjee. Ch oosing multiple parameters for\nsupport vector machines. Machine Learning, 46(1-3), 2002.\n[8] C. Cortes, M. Mohri, and A. Rostamizadeh. Learning seque nce kernels. In Machine Learning\nfor Signal Processing, 2008.\n[9] C. Cortes, M. Mohri, and A. Rostamizadeh. L2 regularization for learning kernels. In Uncer-\ntainty in Artiﬁcial Intelligence, 2009.\n[10] C. Cortes and V . Vapnik. Support-Vector Networks.Machine Learning, 20(3), 1995.\n[11] T. Jebara. Multi-task feature and kernel selection for SVMs. In International Conference on\nMachine Learning, 2004.\n[12] G. Lanckriet, N. Cristianini, P. Bartlett, L. E. Ghaoui , and M. Jordan. Learning the kernel\nmatrix with semideﬁnite programming. Journal of Machine Learning Research, 5, 2004.\n[13] C. Micchelli and M. Pontil. Learning the kernel function via regularization.Journal of Machine\nLearning Research, 6, 2005.\n[14] C. S. Ong, A. Smola, and R. Williamson. Learning the kern el with hyperkernels. Journal of\nMachine Learning Research, 6, 2005.\n[15] A. Rakotomamonjy, F. Bach, Y . Grandvalet, and S. Canu. S implemkl. Journal of Machine\nLearning Research, 9, 2008.\n[16] C. Saunders, A. Gammerman, and V . V ovk. Ridge Regressio n Learning Algorithm in Dual\nVariables. In International Conference on Machine Learning, 1998.\n[17] B. Sch¨ olkopf and A. Smola.Learning with Kernels. MIT Press: Cambridge, MA, 2002.\n[18] B. Scholkopf, A. Smola, and K. Muller. Nonlinear compon ent analysis as a kernel eigenvalue\nproblem. Neural computation, 10(5), 1998.\n[19] J. Shawe-Taylor and N. Cristianini. Kernel Methods for Pattern Analysis. Cambridge Univer-\nsity Press, 2004.\n[20] S. Sonnenburg, G. R¨ atsch, C. Sch¨ afer, and B. Sch¨ olkopf. Large scale multiple kernel learning.\nJournal of Machine Learning Research, 7, 2006.\n[21] N. Srebro and S. Ben-David. Learning bounds for support vector machines with learned ker-\nnels. In Conference on Learning Theory, 2006.\n[22] V . N. Vapnik.Statistical Learning Theory. Wiley-Interscience, New York, 1998.\n[23] M. Varma and B. R. Babu. More generality in efﬁcient mult iple kernel learning. In Interna-\ntional Conference on Machine Learning, 2009.\n9",
  "values": {
    "User influence": "No",
    "Interpretable (to users)": "No",
    "Critiqability": "No",
    "Explicability": "No",
    "Transparent (to users)": "No",
    "Collective influence": "No",
    "Non-maleficence": "No",
    "Deferral to humans": "No",
    "Not socially biased": "No",
    "Beneficence": "No",
    "Autonomy (power to decide)": "No",
    "Privacy": "No",
    "Fairness": "No",
    "Respect for Persons": "No",
    "Justice": "No",
    "Respect for Law and public interest": "No"
  }
}