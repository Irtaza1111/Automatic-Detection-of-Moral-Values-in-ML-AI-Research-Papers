{
  "pdf": "NIPS-2009-multi-label-prediction-via-compressed-sensing-Paper",
  "title": "Multi-Label Prediction via Compressed Sensing",
  "author": "John Langford, Tong Zhang, Daniel J. Hsu, Sham M. Kakade",
  "paper_id": "NIPS-2009-multi-label-prediction-via-compressed-sensing-Paper",
  "text": "Multi-Label Prediction via Compressed Sensing\nDaniel Hsu\nU\nC San Diego\ndjhsu@cs.ucsd.edu\nSham M. Kakade\nTTI-Chicago\nsham@tti-c.org\nJohn Langford\nYahoo! Research\njl@hunch.net\nTong Zhang\nRutgers University\ntongz@rci.rutgers.edu\nAbstract\nWe consider multi-label prediction problems with large output spaces under the\nassumption of output sparsity – that the target (label) vectors have small support.\nWe develop a general theory for a variant of the popular error correcting output\ncode scheme, using ideas from compressed sensing for exploiting this sparsity.\nThe method can be regarded as a simple reduction from multi-label regression\nproblems to binary regression problems. We show that the number of subprob-\nlems need only be logarithmic in the total number of possible labels, making this\napproach radically more efﬁcient than others. We also state and prove robustness\nguarantees for this method in the form of regret transform bounds (in general),\nand also provide a more detailed analysis for the linear prediction setting.\n1 Introduction\nSuppose we have a large database of images, and we want to learn to predict who or what is in any\ngiven one. A standard approach to this task is to collect a sample of these images x along with\ncorresponding labels y = ( y1, . . . , y d)∈{0, 1}d, where yi = 1 if and only if person or object i\nis depicted in image x, and then feed the labeled sample to a multi-label learning algorithm. Here,\nd is the total number of entities depicted in the entire database. When d is very large (e.g. 103,\n104), the simple one-against-all approach of learning a single predictor for each entity can become\nprohibitively expensive, both at training and testing time.\nOur motivation for the present work comes from the observation that although the output (label)\nspace may be very high dimensional, the actual labels are often sparse. That is, in each image, only\na small number of entities may be present and there may only be a small amount of ambiguity in\nwho or what they are. In this work, we consider how this sparsity in the output space, or output\nsparsity, eases the burden of large-scale multi-label learning.\nExploiting output sparsity. A subtle but critical point that distinguishes output sparsity from more\ncommon notions of sparsity (say, in feature or weight vectors) is that we are interested in the sparsity\nof E[y|x] rather than y. In general, E[y|x] may be sparse while the actual outcome y may not (e.g. if\nthere is much unbiased noise); and, vice versa, y may be sparse with probability one but E[y|x] may\nhave large support (e.g. if there is little distinction between several labels).\nConventional linear algebra suggests that we must predict d parameters in order to ﬁnd the value of\nthe d-dimensional vector E[y|x] for each x. A crucial observation – central to the area of compressed\nsensing [1] – is that methods exist to recoverE[y|x] from just O(k log d) measurements when E[y|x]\nis k-sparse. This is the basis of our approach.\n1\nOur contributions. W e show how to apply algorithms for compressed sensing to the output coding\napproach [2]. At a high level, the output coding approach creates a collection of subproblems of\nthe form “Is the label in this subset or its complement?”, solves these problems, and then uses their\nsolution to predict the ﬁnal label.\nThe role of compressed sensing in our application is distinct from its more conventional uses in data\ncompression. Although we do employ a sensing matrix to compress training data, we ultimately\nare not interested in recovering data explicitly compressed this way. Rather, we learn to predict\ncompressed label vectors, and then use sparse reconstruction algorithms to recover uncompressed\nlabels from these predictions. Thus we are interested in reconstruction accuracy of predictions,\naveraged over the data distribution.\nThe main contributions of this work are:\n1. A formal application of compressed sensing to prediction problems with output sparsity.\n2. An efﬁcient output coding method, in which the number of required predictions is only\nlogarithmic in the number of labels d, making it applicable to very large-scale problems.\n3. Robustness guarantees, in the form of regret transform bounds (in general) and a further\ndetailed analysis for the linear prediction setting.\nPrior work. The ubiquity of multi-label prediction problems in domains ranging from multiple ob-\nject recognition in computer vision to automatic keyword tagging for content databases has spurred\nthe development of numerous general methods for the task. Perhaps the most straightforward ap-\nproach is the well-known one-against-all reduction [3], but this can be too expensive when the num-\nber of possible labels is large (especially if applied to the power set of the label space [4]). When\nstructure can be imposed on the label space (e.g. class hierarchy), efﬁcient learning and prediction\nmethods are often possible [5, 6, 7, 8, 9]. Here, we focus on a different type of structure, namely\noutput sparsity, which is not addressed in previous work. Moreover, our method is general enough to\ntake advantage of structured notions of sparsity (e.g. group sparsity) when available [10]. Recently,\nheuristics have been proposed for discovering structure in large output spaces that empirically offer\nsome degree of efﬁciency [11].\nAs previously mentioned, our work is most closely related to the class of output coding method\nfor multi-class prediction, which was ﬁrst introduced and shown to be useful experimentally in [2].\nRelative to this work, we expand the scope of the approach to multi-label prediction and provide\nbounds on regret and error which guide the design of codes. The loss based decoding approach [12]\nsuggests decoding so as to minimize loss. However, it does not provide signiﬁcant guidance in the\nchoice of encoding method, or the feedback between encoding and decoding which we analyze here.\nThe output coding approach is inconsistent when classiﬁers are used and the underlying problems\nbeing encoded are noisy. This is proved and analyzed in [13], where it is also shown that using a\nHadamard code creates a robust consistent predictor when reduced to binary regression. Compared\nto this method, our approach achieves the same robustness guarantees up to a constant factor, but\nrequires training and evaluating exponentially (in d) fewer predictors.\nOur algorithms rely on several methods from compressed sensing, which we detail where used.\n2 Preliminaries\nLetX be an arbitrary input space andY⊂Rd be a d-dimensional output (label) space. We assume\nthe data source is deﬁned by a ﬁxed but unknown distribution over X×Y. Our goal is to learn a\npredictor F :X→Ywith low expected ℓ 2\n2-error Ex∥F (x)−E[y|x]∥2\n2 (the sum of mean-squared-\nerrors over all labels) using a set of n training data{(xi, y i)}n\ni=1.\nWe focus on the regime in which the output space is very high-dimensional (d very large), but for\nany given x ∈ X, the expected value E[y|x] of the corresponding label y ∈ Yhas only a few\nnon-zero entries. A vector is k-sparse if it has at most k non-zero entries.\n2\n3 Learning and Prediction\n3\n.1 Learning to Predict Compressed Labels\nLet A : Rd→Rm be a linear compression function, where m≤d (but hopefully m≪d). We use\nA to compress (i.e. reduce the dimension of) the labelsY, and learn a predictor H :X→A(Y) of\nthese compressed labels. Since A is linear, we simply represent A∈Rm×d as a matrix.\nSpeciﬁcally, given a sample{(xi, y i)}n\ni=1, we form a compressed sample {(xi, Ay i)}n\ni=1 and then\nlearn a predictor H of E[Ay|x] with the objective of minimizing theℓ 2\n2-error Ex∥H(x)−E[Ay|x]∥2\n2.\n3.2 Predicting Sparse Labels\nTo obtain a predictor F of E[y|x], we compose the predictor H of E[Ay|x] (learned using the com-\npressed sample) with a reconstruction algorithm R : Rm→Rd. The algorithm R maps predictions\nof compressed labels h∈Rm to predictions of labels y∈Yin the original output space. These\nalgorithms typically aim to ﬁnd a sparse vector y such that Ay closely approximates h.\nRecent developments in the area of compressed sensing have produced a spate of reconstruction\nalgorithms with strong performance guarantees when the compression function A satisﬁes certain\nproperties. We abstract out the relevant aspects of these guarantees in the following deﬁnition.\nDeﬁnition. An algorithm R is a valid reconstruction algorithm for a family of compression functions\n(Ak⊂⋃\nm≥1 Rm×d : k∈N) and sparsity error sperr : N×Rd→R, if there exists a function\nf : N→N and constants C1, C 2 ∈R such that: on input k∈N, A∈Ak with m rows, and\nh∈Rm, the algorithm R(k, A, h) returns an f (k)-sparse vector ˆy satisfying\n∥ˆy−y∥2\n2 ≤C1·∥ h−Ay∥2\n2 + C2· sperr(k, y )\nfor all y∈Rd. The function f is the output sparsity of R and the constants C1 and C2 are the regret\nfactors.\nInformally, if the predicted compressed label H(x) is close to E[Ay|x] = AE[y|x], then the sparse\nvector ˆy returned by the reconstruction algorithm should be close to E[y|x]; this latter distance\n∥ˆy−E[y|x]∥2\n2 should degrade gracefully in terms of the accuracy ofH(x) and the sparsity of E[y|x].\nMoreover, the algorithm should be agnostic about the sparsity of E[y|x] (and thus the sparsity error\nsperr(k, E[y|x])), as well as the “measurement noise” (the prediction error ∥H(x)−E[Ay|x]∥2).\nThis is a subtle condition and precludes certain reconstruction algorithm (e.g. Basis Pursuit [14])\nthat require the user to supply a bound on the measurement noise. However, the condition is needed\nin our application, as such bounds on the prediction error (for each x) are not generally known\nbeforehand.\nWe make a few additional remarks on the deﬁnition.\n1. The minimum number of rows of matrices A∈Ak may in general depend on k (as well as\nthe ambient dimension d). In the next section, we show how to construct suchA with close\nto the optimal number of rows.\n2. The sparsity error sperr(k, y ) should measure how poorly y∈Rd is approximated by a\nk-sparse vector.\n3. A reasonable output sparsity f (k) for sparsity level k should not be much more than k,\ne.g. f (k) = O(k).\nConcrete examples of valid reconstruction algorithms (along with the associatedAk, sperr, etc.) are\ngiven in the next section.\n4 Algorithms\nOur prescribed recipe is summarized in Algorithms 1 and 2. We give some examples of compression\nfunctions and reconstruction algorithms in the following subsections.\n3\nAlgorithm 1 T raining algorithm\nparameters s parsity level k, compression\nfunction A∈Ak with m rows, regression\nlearning algorithm L\ninput training data S⊂X×Rd\nfor i = 1, . . . , m do\nhi←L({(x, (Ay)i) : ( x, y )∈S})\nend for\noutput regressors H = [h1, . . . , h m]\nAlgorithm 2 P rediction algorithm\nparameters s parsity level k, compression\nfunction A∈Ak with m rows, valid re-\nconstruction algorithm R forAk\ninput regressors H = [ h1, . . . , h m], test\npoint x∈X\noutput ˆy = ⃗R(k, A, [h1(x), . . . , h m(x)])\nFigure 1: Training and prediction algorithms.\n4\n.1 Compression Functions\nSeveral valid reconstruction algorithms are known for compression matrices that satisfy arestricted\nisometry property.\nDeﬁnition. A matrix A∈Rm×d satisﬁes the (k, δ )-restricted isometry property ((k, δ )-RIP), δ ∈\n(0, 1), if (1−δ )∥x∥2\n2≤∥Ax∥2\n2≤(1 + δ )∥x∥2\n2 for all k-sparse x∈Rd.\nWhile some explicit constructions of (k, δ )-RIP matrices are known (e.g. [15]), the best guarantees\nare obtained when the matrix is chosen randomly from an appropriate distribution, such as one of\nthe following [16, 17].\n•All entries i.i.d. Gaussian N (0, 1/m) , with m = O(k log(d/k )).\n•All entries i.i.d. Bernoulli B(1/2) over{±1/√\nm}, with m = O(k log(d/k )).\n•m randomly chosen rows of the d×d Hadamard matrix over {±1/√m}, with m =\nO(k log5 d).\nThe hidden constants in the big-O notation depend inversely on δ and the probability of failure.\nA striking feature of these constructions is the very mild dependence ofm on the ambient dimension\nd. This translates to a signiﬁcant savings in the number of learning problems one has to solve after\nemploying our reduction.\nSome reconstruction algorithms require a stronger guarantee of bounded coherence µ(A) ≤\nO(1/k ), where µ(A) deﬁned as\nµ(A) = max\n1≤i<j ≤d\n|(A⊤ A)i,j|/\n√\n|(A⊤ A)i ,i||(A⊤ A)j,j|\nIt is easy to check that the Gaussian, Bernoulli, and Hadamard-based random matrices given\nabove have coherence bounded by O(\n√\n(log d)/ m ) with high probability. Thus, one can take\nm = O(k2 log d) to guarantee 1/k coherence. This is a factor k worse than what was needed\nfor (k, δ )-RIP, but the dependence on d is still small.\n4.2 Reconstruction Algorithms\nIn this section, we give some examples of valid reconstruction algorithms. Each of these algorithm\nis valid with respect to the sparsity error given by\nsperr(k, y ) = ∥y−y(1:k)∥2\n2 + 1\nk∥y−y( 1:k)∥2\n1\nwhere y(1:k) is the best k-sparse approximation of y (i.e. the vector with just the k largest (in mag-\nnitude) coefﬁcients of y).\nThe following theorem relates reconstruction quality to approximate sparse regression, giving a\nsufﬁcient condition for any algorithm to be valid for RIP matrices.\n4\nAlgorithm 3 P rediction algorithm with R = OMP\nparameters s parsity level k, compression function A = [a1|. . . |ad]∈Ak with m rows,\ninput regressors H = [h1, . . . , h m], test point x∈X\nh←[h1(x), . . . , h m(x)]⊤ (predict compressed label vector)\nˆy←⃗0, J←∅,r←h\nfor i = 1, . . . , 2k do\nj∗←arg maxj|r⊤ aj|/∥a j∥2 (column of A most correlated with residual r)\nJ←J∪{j∗}(add j∗ to set of selected columns)\nˆyJ←(AJ )†h, ˆyJ c←⃗0 (least-squares restricted to columns in J)\nr←h−Aˆy (update residual)\nend for\noutput ˆy\nFigure 2: Prediction algorithm specialized with Orthogonal Matching Pursuit.\nTheorem 1. LetAk ={(k + f (k), δ )-RIP matrices}for some function f : N→N, and let A∈Ak\nhave m rows. If for any h∈Rm, a reconstruction algorithm R returns an f (k)-sparse solution\nˆy = R(k, A, h) satisfying\n∥Aˆy−h∥2\n2≤inf\ny∈Rd\nC∥Ay(1:k)−h∥2\n2,\nthen it is a valid reconstruction algorithm forAk and sperr given above, with output sparsity f and\nregret factors C1 = 2(1 +\n√\nC)2/( 1−δ ) and C2 = 4(1 + (1 +\n√\nC)/( 1 −δ ))2.\nProofs are deferred to Appendix B.\nIterative and greedy algorithms. Orthogonal Matching Pursuit (OMP) [18], FoBa [19], and\nCoSaMP [20] are examples of iterative or greedy reconstruction algorithms. OMP is a greedy\nforward selection method that repeatedly selects a new column of A to use in ﬁtting h (see Al-\ngorithm 3). FoBa is similar, except it also incorporates backward steps to un-select columns that are\nlater discovered to be unnecessary. CoSaMP is also similar to OMP, but instead selects larger sets\nof columns in each iteration.\nFoBa and CoSaMP are valid reconstruction algorithms for RIP matrices ((8 k, 0. 1)-RIP and\n(4k, 0. 1)-RIP, respectively) and have linear output sparsity (8k and 2k). These guarantees are ap-\nparent from the cited references. For OMP, we give the following guarantee.\nTheorem 2. If µ(A)≤0. 1/k , then after f (k) = 2 k steps of OMP , the algorithm returnsˆy satisfying\n∥Aˆy−h∥2\n2≤23∥Ay(1:k)−h∥2\n2 ∀y∈Rd.\nThis theorem, combined with Theorem 1, implies that OMP is valid for matrices A with µ(A)≤\n0. 1/k and has output sparsity f (k) = 2 k.\nℓ 1 algorithms. Basis Pursuit (BP) [14] and its variants are based on ﬁnding the minimum ℓ 1-norm\nsolution to a linear system. While the basic form of BP is ill-suited for our application (it requires\nthe user to supply the amount of measurement error∥Ay−h∥2), its more advanced path-following\nor multi-stage variants may be valid [21].\n5 Analysis\n5.1 General Robustness Guarantees\nWe now state our main regret transform bound, which follows immediately from the deﬁnition of a\nvalid reconstruction algorithm and linearity of expectation.\nTheorem 3 (Regret Transform). Let R be a valid reconstruction algorithm for{Ak : k∈N}and\nsperr : N×Rd→R. Then there exists some constants C1 and C2 such that the following holds.\n5\nPick any k∈N, A∈ Ak with m rows, and H :X→Rm. Let F :X→Rd be the composition of\nR(k, A, ·) and H, i.e. F (x) = R(k, A, H (x)). Then\nEx∥F (x)−E[y|x]∥2\n2 ≤C1· Ex∥H(x)−E[Ay|x]∥2\n2 + C2· sperr(k, E[y|x]).\nThe simplicity of this theorem is a consequence of the careful composition of the learned predictors\nwith the reconstruction algorithm meeting the formal speciﬁcations described above.\nIn order compare this regret bound with the bounds afforded by Sensitive Error Correcting Output\nCodes (SECOC) [13], we need to relateEx∥H(x)−E[Ay|x]∥2\n2 to the average scaled mean-squared-\nerror over all induced regression problems; the error is scaled by the maximum difference Li =\nmaxy∈Y (Ay)i−miny(Ay)i between induced labels:\n¯r = 1\nm\nm∑\ni= 1\nEx\n( H(x)i−E[(Ay)i|x]\nLi\n) 2\n.\nI\nn k-sparse multi-label problems, we haveY ={y∈{0, 1}d :∥y∥0≤k}. In these terms, SECOC\ncan be tuned to yield Ex∥F (x)−E[y|x]∥2\n2 ≤4k2· ¯r for general k.\nFor now, ignore the sparsity error. For simplicity, let A∈Rm×d with entries chosen i.i.d. from the\nBernoulli B(1/2) distribution over{±1/√\nm}, where m = O(k log d). Then for any k-sparse y,\nwe have∥Ay∥∞ ≤k/ √m, and thus Li≤2k/ √m f or each i. This gives the bound\nC1· Ex∥H(x)−E[Ay|x]∥2\n2 ≤4C1· k2· ¯r,\nwhich is within a constant factor of the guarantee afforded by SECOC. Note that our reduction\ninduces exponentially (in d) fewer subproblems than SECOC.\nNow we consider the sparsity error. In the extreme case m = d, E[y|x] is allowed to be fully\ndense (k = d) and sperr(k, E[y|x]) = 0 . When m = O(k log d) < d, we potentially incur an\nextra penalty in sperr(k, E[y|x]), which relates how far E[y|x] is from being k-sparse. For example,\nsuppose E[y|x] has small ℓ p norm for 0≤p < 2. Then even if E[y|x] has full support, the penalty\nwill decrease polynomially in k≈m/ log d.\n5.2 Linear Prediction\nA danger of using generic reductions is that one might create a problem instance that is even harder\nto solve than the original problem. This is an oft cited issue with using output codes for multi-\nclass problems. In the case of linear prediction, however, the danger is mitigated, as we now show.\nSuppose, for instance, there is a perfect linear predictor of E[y|x], i.e. E[y|x] = B⊤ x for some\nB∈Rp×d (hereX = Rp). Then it is easy to see that H = BA⊤ is a perfect linear predictor of\nE[Ay|x]:\nH ⊤ x = AB⊤ x = AE[y|x] = E[Ay|x].\nThe following theorem generalizes this observation to imperfect linear predictors for certain well-\nbehaved A.\nTheorem 4. SupposeX⊂Rp. Let B∈Rp×d be a linear function with\nEx\nB⊤ x−E[y|x]\n\n2\n2 = ǫ.\nLet A∈Rm×d have entries drawn i.i.d. from N (0, 1/m) , and let H = BA⊤ . Then with high\nprobability (over the choice of A),\nEx∥H ⊤ x−AE[y|x]∥2\n2 ≤\n(\n1 + O(1/√\nm)\n)\nǫ .\nRemark 5. Similar guarantees can be proven for the Bernoulli-based matrices. Note thatd does not\nappear in the bound, which is in contrast to the expected spectral norm ofA: roughly 1+O(\n√\nd/m ).\nT\nheorem 4 implies that the errors of any linear predictor are not magniﬁed much by the compres-\nsion function. So a good linear predictor for the original problem implies an almost-as-good linear\npredictor for the induced problem. Using this theorem together with known results about linear\nprediction [22], it is straightforward to derive sample complexity bounds for achieving a given error\nrelative to that of the best linear predictor in some class. The bound will depend polynomially in k\nbut only logarithmically in d. This is cosmetically similar to learning bounds for feature-efﬁcient\nalgorithms (e.g. [23, 22]) which are concerned with sparsity in the weight vector, rather than in the\noutput.\n6\n6 Experimental Validation\nW\ne conducted an empirical assessment of our proposed reduction on two labeled data sets with large\nlabel spaces. These experiments demonstrate the feasibility of our method – a sanity check that the\nreduction does in fact preserve learnability – and compare different compression and reconstruction\noptions.\n6.1 Data\nImage data. 1 The ﬁrst data set was collected by the ESP Game [24], an online game in which\nplayers ultimately provide word tags for a diverse set of web images.\nThe set contains nearly 68000 images, with about 22000 unique labels. We retained just the 1000\nmost frequent labels: the least frequent of these occurs 39 times in the data, and the most frequent\noccurs about 12000 times. Each image contains about four labels on average. We used half of the\ndata for training and half for testing.\nWe represented each image as a bag-of-features vector in a manner similar to [25]. Speciﬁcally, we\nidentiﬁed 1024 representative SURF features points [26] from 10×10 gray-scale patches chosen\nrandomly from the training images; this partitions the space of image patches (represented with\nSURF features) into V oronoi cells. We then built a histogram for each image, counting the number\nof patches that fall in each cell.\nText data.2 The second data set was collected by Tsoumakas et al. [11] from del.icio.us, a\nsocial bookmarking service in which users assign descriptive textual tags to web pages.\nThe set contains about 16000 labeled web page and 983 unique labels. The least frequent label\noccurs 21 times and the most frequent occurs almost 6500 times. Each web page is assigned 19\nlabels on average. Again, we used half the data for training and half for testing.\nEach web page is represented as a boolean bag-of-words vector, with the vocabulary chosen using a\ncombination of frequency thresholding and χ 2 feature ranking. See [11] for details.\nEach binary label vector (in both data sets) indicates the labels of the corresponding data point.\n6.2 Output Sparsity\nWe ﬁrst performed a bit of exploratory data analysis to get a sense of how sparse the target in our\ndata is. We computed the least-squares linear regressor ˆB∈Rp×d on the training data (without any\noutput coding) and predicted the label probabilities ˆp(x) = ˆB⊤ x on the test data (clipping values\nto the range [0, 1]). Using ˆp(x) as a surrogate for the actual target E[y|x], we examined the relative\nℓ 2\n2 error of ˆp and its best k-sparse approximation ǫ(k, ˆp(x)) = ∑d\ni=k+1 ˆp(i)(x)2/∥ ˆp(x)∥2\n2, where\nˆp(1)(x)≥. . . ≥ˆp(d)(x).\nExamining Exǫ(k, ˆp(x)) as a function of k, we saw that in both the image and text data, the fall-\noff with k is eventually super-polynomial, but we are interested in the behavior for small k where it\nappears polynomial k−r for some r. Around k = 10, we estimated an exponent of0. 50 for the image\ndata and 0. 55 for the text data. This is somewhat below the standard of what is considered sparse\n(e.g. vectors with small ℓ 1-norm show k−1 decay). Thus, we expect the reconstruction algorithms\nwill have to contend with the sparsity error of the target.\n6.3 Procedure\nWe used least-squares linear regression as our base learning algorithm, with no regularization on the\nimage data and with ℓ 2-regularization with the text data (λ = 0 . 01) for numerical stability. We did\nnot attempt any parameter tuning.\n1h ttp://hunch.net/∼learning/ESP-ImageSet.tar.gz\n2http://mlkd.csd.auth.gr/multilabel.html\n7\nThe compression functions we used were generated by selectin g m random rows of the 1024×1024\nHadamard matrix, for m∈{100, 200, 300, 400}. We also experimented with Gaussian matrices;\nthese yielded similar but uniformly worse results.\nWe tested the greedy and iterative reconstruction algorithms described earlier (OMP, FoBa, and\nCoSaMP) as well as a path-following version of Lasso based on LARS [21]. Each algorithm was\nused to recover a k-sparse label vector ˆyk from the predicted compressed label H(x), for k =\n1, . . . , 10. We measured the ℓ 2\n2 distance∥ˆyk−y∥2\n2 of the prediction to the true test label y. In\naddition, we measured the precision of the predicted support at various values of k using the 10-\nsparse label prediction. That is, we ordered the coefﬁcients of each 10-sparse label prediction ˆy10\nby magnitude, and measured the precision of predicting the ﬁrst k coordinates|supp(ˆy10\n(1:k))∩\nsupp(y)|/k . Actually, for k≥6, we used ˆy2k instead of ˆy10.\nWe used correlation decoding (CD) as a baseline method, as it is a standard decoding method for\nECOC approaches. CD predicts using the top k coordinates in A⊤ H(x), ordered by magnitude. For\nmean-squared-error comparisons, we used the least-squares approximation of H(x) using these k\ncolumns of A. Note that CD is not a valid reconstruction algorithm when m < d.\n6.4 Results\nAs expected, the performance of the reduction, using any reconstruction algorithm, improves as the\nnumber of induced subproblems m is increased (see ﬁgures in Appendix A) When m is small and\nA̸∈AK, the reconstruction algorithm cannot reliably choose k≥K coordinates, so its perfor-\nmance may degrade after this point by over-ﬁtting. But when the compression function A is inAK\nfor a sufﬁciently large K, then the squared-error decreases as the output sparsity k increases up to\nK. Note the fact that precision-at-k decreases as k increases is expected, as fewer data will have at\nleast k correct labels.\nAll of the reconstruction algorithms at least match or out-performed the baseline on the mean-\nsquared-error criterion, except when m = 100 . When A has few rows, (1) A∈AK only for very\nsmall K, and (2) many of its columns will have signiﬁcant correlation. In this case, when choosing\nk > K columns, it is better to choose correlated columns to avoid over-ﬁtting. Both OMP and\nFoBa explicitly avoid this and thus do not fare well; but CoSaMP, Lasso, and CD do allow selecting\ncorrelated columns and thus perform better in this regime.\nThe results for precision-at-k are similar to that of mean-squared-error, except that choosing corre-\nlated columns does not necessarily help in the small m regime. This is because the extra correlated\ncolumns need not correspond to accurate label coordinates.\nIn summary, the experiments demonstrate the feasibility and robustness of our reduction method for\ntwo natural multi-label prediction tasks. They show that predictions of relatively few compressed\nlabels are sufﬁcient to recover an accurate sparse label vector, and as our theory suggests, the ro-\nbustness of the reconstruction algorithms is a key factor in their success.\nAcknowledgments\nWe thank Andy Cotter for help processing the image features for the ESP Game data. This work\nwas completed while the ﬁrst author was an intern at TTI-C in 2008.\nReferences\n[1] David Donoho. Compressed sensing. IEEE Trans. Info. Theory, 52(4):1289–1306, 2006.\n[2] T. Dietterich and G. Bakiri. Solving multiclass learning problems via error-correcting output codes.\nJournal of Artiﬁcial Intelligence Research, 2:263–286, 1995.\n[3] R. Rifkin and A. Klautau. In defense of one-vs-all classiﬁcation. Journal of Machine Learning Research,\n5:101–141, 2004.\n[4] M. Boutell, J. Luo, X. Shen, and C. Brown. Learning multi-label scene classiﬁcation.Pattern Recognition,\n37(9):1757–1771, 2004.\n[5] A. Clare and R.D. King. Knowledge discovery in multi-label phenotype data. In European Conference\non Principles of Data Mining and Knowledge Discovery, 2001.\n8\n[6] B. Taskar, C. Guestrin, and D. Koller. Max-margin markov network s. In NIPS, 2003.\n[7] N. Cesa-Bianchi, C. Gentile, and L. Zaniboni. Incremental algorithms for hierarchical classiﬁcation.\nJournal of Machine Learning Research, 7:31–54, 2006.\n[8] I. Tsochantaridis, T. Hofmann, T. Joachims, and Y . Altun. Support vector machine learning for interde-\npendent and structured output spaces. In ICML, 2004.\n[9] J. Rousu, C. Saunders, S. Szedmak, and J. Shawe-Taylor. Kernel-based learning of hierarchical multilabel\nclassiﬁcation models. Journal of Machine Learning Research, 7:1601–1626, 2006.\n[10] J. Huang, T. Zhang, and D. Metaxax. Learning with structured sparsity. In ICML, 2009.\n[11] G. Tsoumakas, I. Katakis, and I. Vlahavas. Effective and efﬁcient multilabel classiﬁcation in domains\nwith large number of labels. In Proc. ECML/PKDD 2008 Workshop on Mining Multidimensional Data,\n2008.\n[12] Erin Allwein, Robert Schapire, and Yoram Singer. Reducing multiclass to binary: A unifying approach\nfor margin classiﬁers. Journal of Machine Learning Research, 1:113–141, 2000.\n[13] J. Langford and A. Beygelzimer. Sensitive error correcting output codes. InProc. Conference on Learning\nTheory, 2005.\n[14] Emmanuel Cand `es, Justin Romberg, and Terrence Tao. Stable signal recovery from incomplete and\ninaccurate measurements. Comm. Pure Appl. Math., 59:1207–122, 2006.\n[15] R. DeV ore. Deterministic constructions of compressed sensing matrices. J. of Complexity, 23:918–925,\n2007.\n[16] Shahar Mendelson, Alain Pajor, and Nicole Tomczak-Jaegermann. Uniform uncertainty principle for\nBernoulli and subgaussian ensembles. Constructive Approximation, 28(3):277–289, 2008.\n[17] M. Rudelson and R. Vershynin. Sparse reconstruction by convex relaxation: Fourier and Gaussian mea-\nsurements. In Proc. Conference on Information Sciences and Systems, 2006.\n[18] S. Mallat and Z. Zhang. Matching pursuits with time-frequency dictionaries. IEEE Transactions on Signal\nProcessing, 41(12):3397–3415, 1993.\n[19] Tong Zhang. Adaptive forward-backward greedy algorithm for sparse learning with linear models. In\nProc. Neural Information Processing Systems, 2008.\n[20] D. Needell and J.A. Tropp. CoSaMP: Iterative signal recovery from incomplete and inaccurate samples.\nApplied and Computational Harmonic Analysis, 2007.\n[21] Bradley Efron, Trevor Hastie, Iain Johnstone, and Robert Tibshirani. Least angle regression. Annals of\nStatistics, 32(2):407–499, 2004.\n[22] Sham M. Kakade, Karthik Sridharan, and Ambuj Tewari. On the complexity of linear prediction: Risk\nbounds, margin bounds, and regularization. In Proc. Neural Information Processing Systems, 2008.\n[23] Andrew Ng. Feature selection, l1 vs. l2 regularization, and rotational invariance. In ICML, 2004.\n[24] Luis von Ahn and Laura Dabbish. Labeling images with a computer game. In Proc. ACM Conference on\nHuman Factors in Computing Systems, 2004.\n[25] Marcin Marszałek, Cordelia Schmid, Hedi Harzallah, and Joost van de Weijer. Learning object repre-\nsentations for visual object class recognition. In Visual Recognition Challange Workshop, in conjunction\nwith ICCV, 2007.\n[26] Herbert Bay, Andreas Ess, Tinne Tuytelaars, and Luc Van Gool. SURF: Speeded up robust features.\nComputer Vision and Image Understanding, 110(3):346–359, 2008.\n[27] David Donoho, Michael Elad, and Vladimir Temlyakov. Stable recovery of sparse overcomplete repre-\nsentations in the presence of noise. IEEE Trans. Info. Theory, 52(1):6–18, 2006.\n[28] Sanjoy Dasgupta. Learning Probability Distributions. PhD thesis, University of California, 2000.\n9",
  "values": {
    "Critiqability": "No",
    "Explicability": "No",
    "Respect for Persons": "No",
    "Collective influence": "No",
    "Beneficence": "No",
    "Privacy": "No",
    "User influence": "No",
    "Non-maleficence": "No",
    "Fairness": "No",
    "Justice": "No",
    "Not socially biased": "No",
    "Deferral to humans": "No",
    "Respect for Law and public interest": "No",
    "Autonomy (power to decide)": "No",
    "Interpretable (to users)": "No",
    "Transparent (to users)": "No"
  }
}