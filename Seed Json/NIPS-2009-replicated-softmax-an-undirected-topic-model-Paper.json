{
  "pdf": "NIPS-2009-replicated-softmax-an-undirected-topic-model-Paper",
  "title": "Replicated Softmax: an Undirected Topic Model",
  "author": "Geoffrey E. Hinton, Russ R. Salakhutdinov",
  "paper_id": "NIPS-2009-replicated-softmax-an-undirected-topic-model-Paper",
  "text": "Replicated Softmax: an Undirected Topic Model\nRuslan Salakhutdinov\nBrain and Cognitive Sciences and CSAIL\nMassachusetts Institute of Technology\nrsalakhu@mit.edu\nGeoffrey Hinton\nDepartment of Computer Science\nUniversity of Toronto\nhinton@cs.toronto.edu\nAbstract\nWe introduce a two-layer undirected graphical model, calle d a “Replicated Soft-\nmax”, that can be used to model and automatically extract low-dimensional latent\nsemantic representations from a large unstructured collec tion of documents. We\npresent efﬁcient learning and inference algorithms for this model, and show how a\nMonte-Carlo based method, Annealed Importance Sampling, c an be used to pro-\nduce an accurate estimate of the log-probability the model a ssigns to test data.\nThis allows us to demonstrate that the proposed model is able to generalize much\nbetter compared to Latent Dirichlet Allocation in terms of both the log-probability\nof held-out documents and the retrieval accuracy.\n1 Introduction\nProbabilistic topic models [2, 9, 6] are often used to analyz e and extract semantic topics from large\ntext collections. Many of the existing topic models are based on the assumption that each document\nis represented as a mixture of topics, where each topic deﬁnes a probability distribution over words.\nThe mixing proportions of the topics are document speciﬁc, b ut the probability distribution over\nwords, deﬁned by each topic, is the same across all documents.\nAll these models can be viewed as graphical models in which la tent topic variables have directed\nconnections to observed variables that represent words in a document. One major drawback is that\nexact inference in these models is intractable, so one has to resort to slow or inaccurate approxima-\ntions to compute the posterior distribution over topics. A s econd major drawback, that is shared by\nall mixture models, is that these models can never make predictions for words that are sharper than\nthe distributions predicted by any of the individual topics . They are unable to capture the essential\nidea of distributed representations which is that the distributions predicted by individual active fea-\ntures get multiplied together (and renormalized) to give the distribution predicted by a whole set of\nactive features. This allows individual features to be fairly general but their intersection to be much\nmore precise. For example, distributed representations al low the topics “government”, ”maﬁa” and\n”playboy” to combine to give very high probability to a word “ Berlusconi” that is not predicted\nnearly as strongly by each topic alone.\nTo date, there has been very little work on developing topic models using undirected graphical mod-\nels. Several authors [4, 17] used two-layer undirected graphical models, called Restricted Boltzmann\nMachines (RBMs), in which word-count vectors are modeled as a Poisson distribution. While these\nmodels are able to produce distributed representations of the input and perform well in terms of re-\ntrieval accuracy, they are unable to properly deal with documents of different lengths, which makes\nlearning very unstable and hard. This is perhaps the main rea son why these potentially powerful\nmodels have not found their application in practice. Direct ed models, on the other hand, can eas-\nily handle unobserved words (by simply ignoring them), whic h allows them to easily deal with\ndifferent-sized documents. For undirected models margina lizing over unobserved variables is gen-\nerally a non-trivial operation, which makes learning far mo re difﬁcult. Recently, [13] attempted to\nﬁx this problem by proposing a Constrained Poisson model that would ensure that the mean Poisson\n1\nrates across all words sum up to the length of the document. While the parameter learning has been\nshown to be stable, the introduced model no longer deﬁnes a proper probability distribution over the\nword counts.\nIn the next section we introduce a “Replicated Softmax” model. The model can be efﬁciently trained\nusing Contrastive Divergence, it has a better way of dealingwith documents of different lengths, and\ncomputing the posterior distribution over the latent topic values is easy. We will also demonstrate\nthat the proposed model is able to generalize much better com pared to a popular Bayesian mixture\nmodel, Latent Dirichlet Allocation (LDA) [2], in terms of bo th the log-probability on previously\nunseen documents and the retrieval accuracy.\n2 Replicated Softmax: A Generative Model of Word Counts\nConsider modeling discrete visible units v using a restricted Boltzmann machine, that has a two-\nlayer architecture as shown in Fig. 1. Let v ∈ { 1, ..., K}D, where K is the dictionary size and D\nis the document size, and let h ∈ { 0, 1}F be binary stochastic hidden topic features. Let V be a\nK × D observed binary matrix with vk\ni = 1 if visible unit i takes on kth value. We deﬁne the energy\nof the state {V, h} as follows:\nE(V, h) = −\nD∑\ni=1\nF∑\nj=1\nK∑\nk=1\nW k\nijhjvk\ni −\nD∑\ni=1\nK∑\nk=1\nvk\ni bk\ni −\nF∑\nj=1\nhjaj, (1)\nwhere {W, a, b} are the model parameters: W k\nij is a symmetric interaction term between visible\nunit i that takes on value k, and hidden feature j, bk\ni is the bias of unit i that takes on value k, and aj\nis the bias of hidden feature j (see Fig. 1). The probability that the model assigns to a visible binary\nmatrix V is:\nP (V) = 1\nZ\n∑\nh\nexp (−E(V, h)), Z =\n∑\nV\n∑\nh\nexp (−E(V, h)), (2)\nwhere Z is known as the partition function or normalizing constant. The conditional distributions\nare given by softmax and logistic functions:\np(vk\ni = 1|h) =\nexp (bk\ni + ∑F\nj=1 hjW k\nij)\n∑K\nq=1 exp\n(\nbq\ni + ∑F\nj=1 hjW q\nij\n) (3)\np(hj = 1|V) = σ\n(\naj +\nD∑\ni=1\nK∑\nk=1\nvk\ni W k\nij\n)\n, (4)\nwhere σ(x) = 1 /(1 + exp(−x)) is the logistic function.\nNow suppose that for each document we create a separate RBM with as many softmax units as there\nare words in the document. Assuming we can ignore the order ofthe words, all of these softmax units\ncan share the same set of weights, connecting them to binary h idden units. Consider a document\nthat contains D words. In this case, we deﬁne the energy of the state {V, h} to be:\nE(V, h) = −\nF∑\nj=1\nK∑\nk=1\nW k\nj hj ˆvk −\nK∑\nk=1\nˆvkbk − D\nF∑\nj=1\nhjaj, (5)\nwhere ˆvk = ∑D\ni=1 vk\ni denotes the count for the kth word. Observe that the bias terms of the hidden\nunits are scaled up by the length of the document. This scalin g is crucial and allows hidden topic\nunits to behave sensibly when dealing with documents of different lengths.\nGiven a collection of N documents {Vn}N\nn=1, the derivative of the log-likelihood with respect to\nparameters W takes the form:\n1\nN\nN∑\nn=1\n∂ log P (Vn)\n∂W k\nj\n= EPdata\n[\nˆvkhj\n]\n− EPModel\n[\nˆvkhj\n]\n,\nwhere E Pdata [·] denotes an expectation with respect to the data distributio n Pdata(h, V) =\np(h|V)Pdata(V), with Pdata(V) = 1\nN\n∑\nn δ(V − Vn) representing the empirical distribution,\n2\nW1\nW1 W2\nW2\nh\nv\nW1\nW1\nW1\nW2\nW2\nW2\nW1 W2\nLatent Topics\nObserved Softmax Visibles\nLatent Topics\nMultinomial Visible\nFigure 1: Replicated Softmax model. The top layer represents a vector h of stochastic, binary topic features\nand and the bottom layer represents softmax visible units v. All visible units share the same set of weights,\nconnecting them to binary hidden units. Left: The model for a document containing two and three words.\nRight: A different interpretation of the Replicated Softmax model , in which D softmax units with identical\nweights are replaced by a single multinomial unit which is sampled D times.\nand EPModel [·] is an expectation with respect to the distribution deﬁned by the model. Exact maxi-\nmum likelihood learning in this model is intractable becaus e exact computation of the expectation\nEPModel [·] takes time that is exponential in min{D, F }, i.e the number of visible or hidden units. To\navoid computing this expectation, learning is done by following an approximation to the gradient of\na different objective function, called the “Contrastive Divergence” (CD) ([7]):\n∆W k\nj = α\n(\nEPdata\n[\nˆvkhj\n]\n− EPT\n[\nˆvkhj\n])\n, (6)\nwhere α is the learning rate and PT represents a distribution deﬁned by running the Gibbs chain ,\ninitialized at the data, for T full steps. The special bipartite structure of RBM’s allows for quite an\nefﬁcient Gibbs sampler that alternates between sampling the states of the hidden units independently\ngiven the states of the visible units, and vise versa (see Eqs. 3, 4). Setting T = ∞ recovers maximum\nlikelihood learning.\nThe weights can now be shared by the whole family of different -sized RBM’s that are created for\ndocuments of different lengths (see Fig. 1). We call this the“Replicated Softmax” model. A pleasing\nproperty of this model is that computing the approximate gradients of the CD objective (Eq. 6) for a\ndocument that contains 100 words is computationally not much more expensive than computing the\ngradients for a document that contains only one word. A key ob servation is that using D softmax\nunits with identical weights is equivalent to having a singl e multinomial unit which is sampled D\ntimes, as shown in Fig. 1, right panel. If instead of sampling , we use real-valued softmax proba-\nbilities multiplied by D, we exactly recover the learning algorithm of a Constrained Poisson model\n[13], except for the scaling of the hidden biases with D.\n3 Evaluating Replicated Softmax as a Generative Model\nAssessing the generalization performance of probabilisti c topic models plays an important role in\nmodel selection. Much of the existing literature, particul arly for undirected topic models [4, 17],\nuses extremely indirect performance measures, such as information retrieval or document classiﬁca-\ntion. More broadly, however, the ability of the model to gene ralize can be evaluated by computing\nthe probability that the model assigns to the previously unseen documents, which is independent of\nany speciﬁc application.\nFor undirected models, computing the probability of held-out documents exactly is intractable, since\ncomputing the global normalization constant requires enum eration over an exponential number of\nterms. Evaluating the same probability for directed topic m odels is also difﬁcult, because there are\nan exponential number of possible topic assignments for the words.\nRecently, [14] showed that a Monte Carlo based method, Annealed Importance Sampling (AIS) [12],\ncan be used to efﬁciently estimate the partition function of an RBM. We also ﬁnd AIS attractive\nbecause it not only provides a good estimate of the partition function in a reasonable amount of\ncomputer time, but it can also just as easily be used to estimate the probability of held-out documents\nfor directed topic models, including Latent Dirichlet Allo cation (for details see [16]). This will\nallow us to properly measure and compare generalization cap abilities of Replicated Softmax and\n3\nAlgorithm 1 Annealed Importance Sampling (AIS) run.\n1: Initialize 0 = β0 < β 1 < ... < β S = 1.\n2: Sample V1 from p0.\n3: for s = 1 : S − 1 do\n4: Sample Vs+1 given Vs using Ts(Vs+1 ← Vs).\n5: end for\n6: Set wAIS = QS\ns=1 p∗\ns(Vs)/p∗\ns−1(Vs).\nLDA models. We now show how AIS can be used to estimate the partition function of a Replicated\nSoftmax model.\n3.1 Annealed Importance Sampling\nSuppose we have two distributions: pA(x) = p∗\nA(x)/ZA and pB(x) = p∗\nB(x)/ZB. Typically\npA(x) is deﬁned to be some simple proposal distribution with known ZA, whereas pB represents\nour complex target distribution of interest. One way of estimating the ratio of normalizing constants\nis to use a simple importance sampling method:\nZB\nZA\n=\n∑\nx\np∗\nB(x)\np∗\nA(x) pA(x) = EpA\n[ p∗\nB(x)\np∗\nA(x)\n]\n≈ 1\nN\nN∑\ni=1\np∗\nB(x(i))\np∗\nA(x(i)) , (7)\nwhere x(i) ∼ pA. However, if the pA and pB are not close enough, the estimator will be very poor.\nIn high-dimensional spaces, the variance of the importancesampling estimator will be very large, or\npossibly inﬁnite, unless pA is a near-perfect approximation to pB.\nAnnealed Importance Sampling can be viewed as simple import ance sampling deﬁned on a much\nhigher dimensional state space. It uses many auxiliary variables in order to make the proposal distri-\nbution pA be closer to the target distribution pB. AIS starts by deﬁning a sequence of intermediate\nprobability distributions: p0, ..., pS, with p0 = pA and pS = pB. One general way to deﬁne this\nsequence is to set:\npk(x) ∝ p∗\nA(x)1− βk p∗\nB(x)βk , (8)\nwith “inverse temperatures”0 = β0 < β 1 < ... < β K = 1 chosen by the user. For each intermediate\ndistribution, a Markov chain transition operator Tk(x′; x) that leaves pk(x) invariant must also be\ndeﬁned.\nUsing the special bipartite structure of RBM’s, we can devise a better AIS scheme [14] for estimating\nthe model’s partition function. Let us consider a Replicate d Softmax model with D words. Using\nEq. 5, the joint distribution over {V, h} is deﬁned as1:\np(V, h) = 1\nZ exp\n\n\nF∑\nj=1\nK∑\nk=1\nW k\nj hj ˆvk\n\n , (9)\nwhere ˆvk = ∑D\ni=1 vk\ni denotes the count for the kth word. By explicitly summing out the latent topic\nunits h we can easily evaluate an unnormalized probability p∗(V). The sequence of intermediate\ndistributions, parameterized by β, can now be deﬁned as follows:\nps(V) = 1\nZs\np∗(V) = 1\nZs\n∑\nh\np∗\ns(V, h) = 1\nZs\nF∏\nj=1\n(\n1 + exp\n(\nβs\nK∑\nk=1\nW k\nj ˆvk\n))\n. (10)\nNote that for s = 0 , we have βs = 0 , and so p0 represents a uniform distribution, whose partition\nfunction evaluates to Z0 = 2 F , where F is the number of hidden units. Similarly, when s = S, we\nhave βs = 1, and so pS represents the distribution deﬁned by the Replicated Softmax model. For the\nintermediate values of s, we will have some interpolation between uniform and target distributions.\nUsing Eqs. 3, 4, it is also straightforward to derive an efﬁcient Gibbs transition operator that leaves\nps(V) invariant.\n1We have omitted the bias terms for clarity of presentation\n4\nA single run of AIS procedure is summarized in Algorithm 1. It starts by ﬁrst sampling from a sim-\nple uniform distribution p0(V) and then applying a series of transition operators T1, T2, . . . , TS− 1\nthat “move” the sample through the intermediate distributions ps(V) towards the target distribution\npS(V). Note that there is no need to compute the normalizing consta nts of any intermediate distri-\nbutions. After performing M runs of AIS, the importance weights w(i)\nAIS can be used to obtain an\nunbiased estimate of our model’s partition function ZS:\nZS\nZ0\n≈ 1\nM\nM∑\ni=1\nw(i)\nAIS, (11)\nwhere Z0 = 2 F . Observe that the Markov transition operators do not necessarily need to be ergodic.\nIn particular, if we were to choose dumb transition operator s that do nothing, Ts(V′ ← V) =\nδ(V′− V) for all s, we simply recover the simple importance sampling procedure of Eq. 7.\nWhen evaluating the probability of a collection of several documents, we need to perform a separate\nAIS run per document, if those documents are of different len gths. This is because each different-\nsized document can be represented as a separate RBM that has its own global normalizing constant.\n4 Experimental Results\nIn this section we present experimental results on three thr ee text datasets: NIPS proceedings pa-\npers, 20-newsgroups, and Reuters Corpus V olume I (RCV1-v2)[10], and report generalization per-\nformance of Replicated Softmax and LDA models.\n4.1 Description of Datasets\nThe NIPS proceedings papers 2 contains 1740 NIPS papers. We used the ﬁrst 1690 documents as\ntraining data and the remaining 50 documents as test. The dat aset was already preprocessed, where\neach document was represented as a vector containing 13,649 word counts.\nThe 20-newsgroups corpus contains 18,845 postings taken fr om the Usenet newsgroup collection.\nThe corpus is partitioned fairly evenly into 20 different ne wsgroups, each corresponding to a sepa-\nrate topic.3 The data was split by date into 11,314 training and 7,531 testarticles, so the training and\ntest sets were separated in time. We further preprocessed the data by removing common stopwords,\nstemming, and then only considering the 2000 most frequent words in the training dataset. As a re-\nsult, each posting was represented as a vector containing 2000 word counts. No other preprocessing\nwas done.\nThe Reuters Corpus V olume I is an archive of 804,414 newswire stories4 that have been manually\ncategorized into 103 topics. The topic classes form a tree wh ich is typically of depth 3. For this\ndataset, we deﬁne the relevance of one document to another tobe the fraction of the topic labels that\nagree on the two paths from the root to the two documents. The data was randomly split into 794,414\ntraining and 10,000 test articles. The available data was al ready in the preprocessed format, where\ncommon stopwords were removed and all documents were stemmed. We again only considered the\n10,000 most frequent words in the training dataset.\nFor all datasets, each word count wi was replaced by log(1 + wi), rounded to the nearest integer,\nwhich slightly improved retrieval performance of both models. Table 1 shows description of all three\ndatasets.\n4.2 Details of Training\nFor the Replicated Softmax model, to speed-up learning, we s ubdivided datasets into minibatches,\neach containing 100 training cases, and updated the paramet ers after each minibatch. Learning\nwas carried out using Contrastive Divergence by starting wi th one full Gibbs step and gradually\nincreaing to ﬁve steps during the course of training, as desc ribed in [14]. For all three datasets, the\ntotal number of parameter updates was set to 100,000, which t ook several hours to train. For the\n2Available at http://psiexp.ss.uci.edu/research/programs data/toolbox.htm.\n3Available at http://people.csail.mit.edu/jrennie/20Newsgroups (20news-bydate.tar.gz).\n4Available at http://trec.nist.gov/data/reuters/reuters.html\n5\nData set Number of docs K ¯D St. Dev. Avg. Test perplexity per word (in nats)\nTrain Test LDA-50 LDA-200 R. Soft-50 Unigram\nNIPS 1,690 50 13,649 98.0 245.3 3576 3391 3405 4385\n20-news 11,314 7,531 2,000 51.8 70.8 1091 1058 953 1335\nReuters 794,414 10,000 10,000 94.6 69.3 1437 1142 988 2208\nTable 1: Results for LDA using 50 and 200 topics, and Replaced Softmax model that uses 50 topics. K is\nthe vocabulary size, ¯D is the mean document length, St. Dev. is the estimated standa rd deviation in document\nlength.\n2500 3000 3500 4000 4500 5000\n2500\n3000\n3500\n4000\n4500\n5000\nReplicated Softmax\nLDA\n600 800 1000 1200 1400 1600\n600\n800\n1000\n1200\n1400\n1600\nReplicated Softmax\nLDA\n0 500 1000 1500 2000 25000\n500\n1000\n1500\n2000\n2500\nReplicated Softmax\nLDA\nNIPS Proceedings 20-newsgroups Reuters\nFigure 2: The average test perplexity scores for each of the 50 held-ou t documents under the learned 50-\ndimensional Replicated Softmax and LDA that uses 50 topics.\nLDA model, we used the Gibbs sampling implementation of the M atlab Topic Modeling Toolbox5\n[5]. The hyperparameters were optimized using stochastic E M as described by [15]. For the 20-\nnewsgroups and NIPS datasets, the number of Gibbs updates wa s set to 100,000. For the large\nReuters dataset, it was set to 10,000, which took several days to train.\n4.3 Assessing Topic Models as Generative Models\nFor each of the three datasets, we estimated the log-probability for 50 held-out documents.6 For both\nthe Replicated Softmax and LDA models we used 10,000 inversetemperatures βs, spaced uniformly\nfrom 0 to 1. For each held-out document, the estimates were av eraged over 100 AIS runs. The\naverage test perplexity per word was then estimated as exp\n(\n−1/N\n∑N\nn=1 1/Dn log p(vn)\n)\n, where\nN is the total number of documents, Dn and vn are the total number of words and the observed\nword-count vector for a document n.\nTable 1 shows that for all three datasets the 50-dimensionalReplicated Softmax consistently outper-\nforms the LDA with 50-topics. For the NIPS dataset, the undirected model achieves the average test\nperplexity of 3405, improving upon LDA’s perplexity of 3576. The LDA with 200 topics performed\nmuch better on this dataset compared to the LDA-50, but its pe rformance only slightly improved\nupon the 50-dimensional Replicated Softmax model. For the 20-newsgroups dataset, even with 200\ntopics, the LDA could not match the perplexity of the Replicated Softmax model with 50 topic units.\nThe difference in performance is particularly striking forthe large Reuters dataset, whose vocabulary\nsize is 10,000. LDA achieves an average test perplexity of 14 37, substantially reducing it from\n2208, achieved by a simple smoothed unigram model. The Repli cated Softmax further reduces the\nperplexity down to 986, which is comparable in magnitude to the improvement produced by the LDA\nover the unigram model. LDA with 200 topics does improve upon LDA-50, achieving a perplexity\nof 1142. However, its performance is still considerably wor se than that of the Replicated Softmax\nmodel.\n5The code is available at http://psiexp.ss.uci.edu/research/programs data/toolbox.htm\n6For the 20-newsgroups and Reuters datasets, the 50 held-out documents were randomly sampled from the\ntest sets.\n6\n0.02    0.1     0.4     1.6     6.4     25.6    100 \n10\n20\n30\n40\n50\n60\nRecall (%) \nPrecision (%)\nReplicated \nSoftmax 50−D\nLDA 50−D\n0.001     0.006     0.051     0.4        1.6       6.4       25.6      100 \n10\n20\n30\n40\n50\nRecall (%) \nPrecision (%)\nReplicated \nSoftmax 50−D\nLDA 50−D\n20-newsgroups Reuters\nFigure 3: Precision-Recall curves for the 20-newsgroups and Reuters datasets, when a query document from\nthe test set is used to retrieve similar documents from the tr aining corpus. Results are averaged over all 7,531\n(for 20-newsgroups) and 10,000 (for Reuters) possible queries.\nFigure 2 further shows three scatter plots of the average test perplexity per document. Observe that\nfor almost all test documents, the Replicated Softmax achie ves a better perplexity compared to the\ncorresponding LDA model. For the Reuters dataset, as expected, there are many documents that are\nmodeled much better by the undirected model than an LDA. Clearly, the Replicated Softmax is able\nto generalize much better.\n4.4 Document Retrieval\nWe used 20-newsgroup and Reuters datasets to evaluate modelperformance on a document retrieval\ntask. To decide whether a retrieved document is relevant to the query document, we simply check if\nthey have the same class label. This is the only time that the class labels are used. For the Replicated\nSoftmax, the mapping from a word-count vector to the values o f the latent topic features is fast,\nrequiring only a single matrix multiplication followed by a componentwise sigmoid non-linearity.\nFor the LDA, we used 1000 Gibbs sweeps per test document in order to get an approximate posterior\nover the topics. Figure 3 shows that when we use the cosine of the angle between two topic vectors to\nmeasure their similarity, the Replicated Softmax signiﬁcantly outperforms LDA, particularly when\nretrieving the top few documents.\n5 Conclusions and Extensions\nWe have presented a simple two-layer undirected topic model that be used to model and automati-\ncally extract distributed semantic representations from large collections of text corpora. The model\ncan be viewed as a family of different-sized RBM’s that shareparameters. The proposed model have\nseveral key advantages: the learning is easy and stable, it can model documents of different lengths,\nand computing the posterior distribution over the latent to pic values is easy. Furthermore, using\nstochastic gradient descent, scaling up learning to billio ns of documents would not be particularly\ndifﬁcult. This is in contrast to directed topic models, where most of the existing inference algorithms\nare designed to be run in a batch mode. Therefore one would hav e to make further approximations,\nfor example by using particle ﬁltering [3]. We have also demo nstrated that the proposed model is\nable to generalize much better than LDA in terms of both the log-probability on held-out documents\nand the retrieval accuracy.\nIn this paper we have only considered the simplest possible topic model, but the proposed model can\nbe extended in several ways. For example, similar to supervi sed LDA [1], the proposed Replicated\nSoftmax can be easily extended to modeling the joint the dist ribution over words and a document\nlabel, as shown in Fig. 4, left panel. Recently, [11] introdu ced a Dirichlet-multinomial regression\nmodel, where a prior on the document-speciﬁc topic distribu tions was modeled as a function of\nobserved metadata of the document. Similarly, we can deﬁne a conditional Replicated Softmax\nmodel, where the observed document-speciﬁc metadata, suchas author, references, etc., can be used\n7\nLatent Topics\nMultinomial Visible\nLabel\nLatent Topics\nMultinomial Visible\nMetadata\nFigure 4: Left: A Replicated Softmax model that models the joint distributi on of words and document label.\nRight: Conditional Replicated Softmax model where the observed do cument-speciﬁc metadata affects binary\nstates of the hidden topic units.\nto inﬂuence the states of the latent topic units, as shown in Fig. 4, right panel. Finally, as argued by\n[13], a single layer of binary features may not the best way to capture the complex structure in the\ncount data. Once the Replicated Softmax has been trained, we can add more layers to create a Deep\nBelief Network [8], which could potentially produce a better generative model and further improve\nretrieval accuracy.\nAcknowledgments\nThis research was supported by NSERC, CFI, and CIFAR.\nReferences\n[1] D. Blei and J. McAuliffe. Supervised topic models. In NIPS, 2007.\n[2] D. Blei, A. Ng, and M. Jordan. Latent dirichlet allocatio n. Journal of Machine Learning Research ,\n3:993–1022, 2003.\n[3] K. Canini, L. Shi, and T. Grifﬁths. Online inference of topics with latent Dirichlet allocation. In Proceed-\nings of the International Conference on Artiﬁcial Intelligence and Statistics, volume 5, 2009.\n[4] P. Gehler, A. Holub, and M. Welling. The Rate Adapting Poi sson (RAP) model for information retrieval\nand object recognition. In Proceedings of the 23rd International Conference on Machine Learning, 2006.\n[5] T. Grifﬁths and M. Steyvers. Finding scientiﬁc topics. I n Proceedings of the National Academy of\nSciences, volume 101, pages 5228–5235, 2004.\n[6] Thomas Grifﬁths and Mark Steyvers. Finding scientiﬁc to pics. PNAS, 101(suppl. 1), 2004.\n[7] G. Hinton. Training products of experts by minimizing co ntrastive divergence. Neural Computation,\n14(8):1711–1800, 2002.\n[8] G. Hinton, S. Osindero, and Y . W. Teh. A fast learning algorithm for deep belief nets.Neural Computation,\n18(7):1527–1554, 2006.\n[9] T. Hofmann. Probabilistic latent semantic analysis. In Proceedings of the 15th Conference on Uncertainty\nin AI, pages 289–296, San Fransisco, California, 1999. Morgan Kaufmann.\n[10] D. Lewis, Y . Yang, T. Rose, and F. Li. RCV1: A new benchmar k collection for text categorization\nresearch. Journal of Machine Learning Research, 5:361–397, 2004.\n[11] D. Mimno and A. McCallum. Topic models conditioned on ar bitrary features with dirichlet-multinomial\nregression. In UAI, pages 411–418, 2008.\n[12] R. Neal. Annealed importance sampling. Statistics and Computing, 11:125–139, 2001.\n[13] R. Salakhutdinov and G. Hinton. Semantic Hashing. In SIGIR workshop on Information Retrieval and\napplications of Graphical Models, 2007.\n[14] R. Salakhutdinov and I. Murray. On the quantitative ana lysis of deep belief networks. In Proceedings of\nthe International Conference on Machine Learning, volume 25, pages 872 – 879, 2008.\n[15] H. Wallach. Topic modeling: beyond bag-of-words. In ICML, volume 148, pages 977–984, 2006.\n[16] H. Wallach, I. Murray, R. Salakhutdinov, and D. Mimno. E valuation methods for topic models. In\nProceedings of the 26th International Conference on Machine Learning (ICML 2009), 2009.\n[17] E. Xing, R. Yan, and A. Hauptmann. Mining associated tex t and images with dual-wing harmoniums. In\nProceedings of the 21st Conference on Uncertainty in Artiﬁcial Intelligence (UAI-2005), 2005.\n8",
  "values": {
    "Interpretable (to users)": "No",
    "Respect for Law and public interest": "No",
    "Respect for Persons": "No",
    "Justice": "No",
    "Transparent (to users)": "No",
    "Not socially biased": "No",
    "Privacy": "No",
    "Non-maleficence": "No",
    "Fairness": "No",
    "Autonomy (power to decide)": "No",
    "User influence": "No",
    "Beneficence": "No",
    "Collective influence": "No",
    "Deferral to humans": "No",
    "Explicability": "No",
    "Critiqability": "No"
  }
}