{
  "pdf": "NIPS-2009-rethinking-lda-why-priors-matter-Paper",
  "title": "Rethinking LDA: Why Priors Matter",
  "author": "Andrew McCallum, David M. Mimno, Hanna M. Wallach",
  "paper_id": "NIPS-2009-rethinking-lda-why-priors-matter-Paper",
  "text": "Rethinking LDA: Why Priors Matter\nHanna M. Wallach David Mimno Andrew McCallum\nDepartment of Computer Science\nUniversity of Massachusetts Amherst\nAmherst, MA 01003\n{wallach,mimno,mccallum}@cs.umass.edu\nAbstract\nImplementations of topic models typically use symmetric Dirichlet priors with\nﬁxed concentration parameters, with the implicit assumption that such “smoothing\nparameters” have little practical effect. In this paper, we explore several classes\nof structured priors for topic models. We ﬁnd that an asymmetric Dirichlet prior\nover the document–topic distributions has substantial advantages over a symmet-\nric prior, while an asymmetric prior over the topic–word distributions provides no\nreal beneﬁt. Approximation of this prior structure through simple, efﬁcient hy-\nperparameter optimization steps is sufﬁcient to achieve these performance gains.\nThe prior structure we advocate substantially increases the robustness of topic\nmodels to variations in the number of topics and to the highly skewed word fre-\nquency distributions common in natural language. Since this prior structure can be\nimplemented using efﬁcient algorithms that add negligible cost beyond standard\ninference techniques, we recommend it as a new standard for topic modeling.\n1 Introduction\nTopic models such as latent Dirichlet allocation (LDA) [3] have been recognized as useful tools for\nanalyzing large, unstructured collections of documents. There is a signiﬁcant body of work apply-\ning LDA to an wide variety of tasks including analysis of news articles [14], study of the history of\nscientiﬁc ideas [2, 9], topic-based search interfaces 1 and navigation tools for digital libraries [12].\nIn practice, users of topic models are typically faced with two immediate problems: First, extremely\ncommon words tend to dominate all topics. Second, there is relatively little guidance available on\nhow to set T , the number of topics, or studies regarding the effects of using a suboptimal setting\nforT . Standard practice is to remove “stop words” before modeling using a manually constructed,\ncorpus-speciﬁc stop word list and to optimize T by either analyzing probabilities of held-out doc-\numents or resorting to a more complicated nonparametric model. Additionally, there has been rel-\natively little work in the machine learning literature on the structure of the prior distributions used\nin LDA: most researchers simply use symmetric Dirichlet priors with heuristically set concentration\nparameters. Asuncion et al. [1] recently advocated inferring the concentration parameters of these\nsymmetric Dirichlets from data, but to date there has been no rigorous scientiﬁc study of the priors\nused in LDA—from the choice of prior (symmetric versus asymmetric Dirichlets) to the treatment\nof hyperparameters (optimize versus integrate out)—and the effects of these modeling choices on\nthe probability of held-out documents and, more importantly, the quality of inferred topics. In this\npaper, we demonstrate that practical implementation issues (handling stop words, setting the number\nof topics) and theoretical issues involving the structure of Dirichlet priors are intimately related.\nWe start by exploring the effects of classes of hierarchically structured Dirichlet priors over the\ndocument–topic distributions and topic–word distributions in LDA. Using MCMC simulations, we\nﬁnd that using an asymmetric, hierarchical Dirichlet prior over the document–topic distributions and\n1http://rexa.info/\n1\na symmetric Dirichlet prior over the topic–word distributions results in signiﬁcantly better model\nperformance, measured both in terms of the probability of held-out documents and in the quality\nof inferred topics. Although this hierarchical Bayesian treatment of LDA produces good results,\nit is computationally intensive. We therefore demonstrate that optimizing the hyperparameters of\nasymmetric, nonhierarchical Dirichlets as part of an iterative inference algorithm results in similar\nperformance to the full Bayesian model while adding negligible computational cost beyond standard\ninference techniques. Finally, we show that using optimized Dirichlet hyperparameters results in\ndramatically improved consistency in topic usage as T is increased. By decreasing the sensitivity\nof the model to the number of topics, hyperparameter optimization results in robust, data-driven\nmodels with substantially less model complexity and computational cost than nonparametric models.\nSince the priors we advocate (an asymmetric Dirichlet over the document–topic distributions and a\nsymmetric Dirichlet over the topic–word distributions) have signiﬁcant modeling beneﬁts and can\nbe implemented using highly efﬁcient algorithms, we recommend them as a new standard for LDA.\n2 Latent Dirichlet Allocation\nLDA is a generative topic model for documents W ={w(1),w(2),..., w(D)}. A “topic” t is a\ndiscrete distribution over words with probability vector φt. A Dirichlet prior is placed over Φ =\n{φ1,... φT}. In almost all previous work on LDA, this prior is assumed to be symmetric (i.e., the\nbase measure is ﬁxed to a uniform distribution over words) with concentration parameterβ:\nP (Φ) =∏\nt Dir (φt;βu) =∏\nt\nΓ(β)∏\nw Γ( β\nW )\n∏\nwφ\nβ\nW−1\nw|t δ\n(∑\nwφw|t− 1\n)\n. (1)\nEach document, indexed by d, has a document-speciﬁc distribution over topicsθd. The prior over\nΘ ={θ1,... θD} is also assumed to be a symmetric Dirichlet, this time with concentration param-\neterα. The tokens in every document w(d) ={w(d)\nn }Nd\nn=1 are associated with corresponding topic\nassignments z(d) ={z(d)\nn }Nd\nn=1, drawn i.i.d. from the document-speciﬁc distribution over topics,\nwhile the tokens are drawn i.i.d. from the topics’ distributions over words Φ ={φ1,..., φT}:\nP (z(d)|θd) =∏\nnθz(d)\nn |d and P (w(d)|z(d), Φ) =∏\nnφw(d)\nn |z(d)\nn\n. (2)\nDirichlet–multinomial conjugacy allows Θ and Φ to be marginalized out.\nFor real-world data, documentsW are observed, while the corresponding topic assignments Z are\nunobserved. Variational methods [3, 16] and MCMC methods [7] are both effective at inferring the\nlatent topic assignmentsZ. Asuncion et al. [1] demonstrated that the choice of inference method\nhas negligible effect on the probability of held-out documents or inferred topics. We use MCMC\nmethods throughout this paper—speciﬁcally Gibbs sampling [5]—since the internal structure of\nhierarchical Dirichlet priors are typically inferred using a Gibbs sampling algorithm, which can be\neasily interleaved with Gibbs updates for Z givenW. The latter is accomplished by sequentially\nresampling each topic assignment z(d)\nn from its conditional posterior givenW,αu,βu andZ\\d,n\n(the current topic assignments for all tokens other than the token at positionn in documentd):\nP (z(d)\nn |W,Z\\d,n,αu,βu)∝P (w(d)\nn |z(d)\nn ,W\\d,n,Z\\d,n,βu)P (z(d)\nn |Z\\d,n,αu)\n∝\nN\\d,n\nw(d)\nn |z(d)\nn\n+ β\nW\nN\\d,n\nz(d)\nn\n+β\nN\\d,n\nz(d)\nn |d + α\nT\nNd− 1 +α, (3)\nwhere sub- or super-script “\\d,n ” denotes a quantity excluding data from positionn in documentd.\n3 Priors for LDA\nThe previous section outlined LDA as it is most commonly used—namely with symmetric Dirich-\nlet priors over Θ and Φ with ﬁxed concentration parameters α and β, respectively. The simplest\nway to vary this choice of prior for either Θ or Φ is to infer the relevant concentration parameter\nfrom data, either by computing a MAP estimate [1] or by using an MCMC algorithm such as slice\nsampling [13]. A broad Gamma distribution is an appropriate choice of prior for bothα andβ.\n2\nu\nα\nφt\nθd\nzn\nwn\nβ\nu T\nD\nN\n(a)\nu\nα\nφt\nθd\nzn\nwn\nβ\nu n\nβ′\nT\nD\nN (b)\nt|d t ′|d t |d t |d\nγ1 =t γ2 =t′ γ3 =t\nm (c)\nu\nα\nm α′\nφt\nθd\nzn\nwn\nβ\nu T\nD\nN\n(d)\nu\nα\nm α′\nφt\nθd\nzn\nwn\nβ\nu n\nβ′\nT\nD\nN (e)\nt|d t ′|d t |d t |d\nγ1 =t γ2 =t′ γ3 =t\nt′|d′ t′|d′ t′|d′ t′|d′\nγ1 =t′ γ2 =t′\nγ1 =t γ2 =t′ γ3 =t′\nu (f)\nFigure 1: (a)-(e): LDA with (a) symmetric Dirichlet priors over Θ and Φ, (b) a symmetric Dirichlet prior over\nΘ and an asymmetric Dirichlet prior overΦ, (d) an asymmetric Dirichlet prior overΘ and a symmetric Dirichlet\nprior over Φ, (e) asymmetric Dirichlet priors over Θ and Φ. (c) Generating {z(d)\nn }4\nn=1 = (t,t ′,t,t ) from the\nasymmetric, predictive distribution for document d; (f) generating{z(d)\nn }4\nn=1 = (t,t ′,t,t ) and{z(d′)\nn }4\nn=1 =\n(t′,t ′,t ′,t ′) from the asymmetric, hierarchical predictive distributions for documentsd andd′, respectively.\nAlternatively, the uniform base measures in the Dirichlet priors over Θ and Φ can be replaced with\nnonuniform base measuresm andn, respectively. Throughout this section we use the prior over Θ\nas a running example, however the same construction and arguments also apply to the prior over Φ.\nIn section 3.1, we describe the effects on the document-speciﬁc conditional posterior distributions,\nor predictive distributions, of replacingu with a ﬁxed asymmetric (i.e., nonuniform) base measure\nm. In section 3.2, we then treat m as unknown, and take a fully Bayesian approach, giving m a\nDirichlet prior (with a uniform base measure and concentration parameterα′) and integrating it out.\n3.1 Asymmetric Dirichlet Priors\nIf Θ is given an asymmetric Dirichlet prior with concentration parameter α and an known (nonuni-\nform) base measurem, the predictive probability of topict occurring in documentd givenZ is\nP (z(d)\nNd+1 =t|Z,αm) =\n∫\ndθdP (t|θd)P (θd|Z,αm) = Nt|d +αmt\nNd +α . (4)\nIf topict does not occur inz(d), thenNt|d will be zero, and the probability of generatingz(d)\nNd+1 =t\nwill be mt. In other words, under an asymmetric prior, Nt|d is smoothed with a topic-speciﬁc\nquantityαmt. Consequently, different topics can be a priori more or less probable in all documents.\nOne way of describing the process of generating from (4) is to say that generating a topic assignment\nz(d)\nn is equivalent to setting the value ofz(d)\nn to the the value of some document-speciﬁc draw from\nm. While this interpretation provides little beneﬁt in the case of ﬁxedm, it is useful for describing\nthe effects of marginalizing over m on the predictive distributions (see section 3.2). Figure 1c\ndepicts the process of drawing{z(d)\nn }4\nn=1 using this interpretation. When drawingz(d)\n1 , there are no\nexisting document-speciﬁc draws fromm, so a new draw γ1 must be generated, and z(d)\n1 assigned\nthe value of this draw (t in ﬁgure 1c). Next, z(d)\n2 is drawn by either selecting γ1, with probability\nproportional to the number of topic assignments that have been previously “matched” to γ1, or a\nnew draw fromm, with probability proportional toα. In ﬁgure 1c, a new draw is selected, soγ2 is\ndrawn fromm andz(d)\n2 assigned its value, in this caset′. The next topic assignment is drawn in the\nsame way: existing draws γ1 andγ2 are selected with probabilities proportional to the numbers of\ntopic assignments to which they have previously been matched, while with probability proportional\ntoα, z(d)\n3 is matched to a new draw from m. In ﬁgure 1c, γ1 is selected and z(d)\n3 is assigned the\nvalue of γ1. In general, the probability of a new topic assignment being assigned the value of an\nexisting document-speciﬁc drawγi fromm is proportional toN (i)\nd , the number of topic assignments\n3\npreviously matched toγi. The predictive probability of topict in documentd is therefore\nP (z(d)\nNd+1 =t|Z,αm) =\n∑I\ni=1N (i)\nd δ (γi−t) +αmt\nNd +α , (5)\nwhereI is the current number of draws from m for document d. Since every topic assignment is\nmatched to a draw fromm,∑I\ni=1N (i)\nd δ (γi−t) =Nt|d. Consequently, (4) and (5) are equivalent.\n3.2 Integrating out m\nIn practice, the base measure m is not ﬁxed a priori and must therefore be treated as an unknown\nquantity. We take a fully Bayesian approach, and givem a symmetric Dirichlet prior with concentra-\ntion parameterα′ (as shown in ﬁgures 1d and 1e). This prior overm induces a hierarchical Dirichlet\nprior over Θ. Furthermore, Dirichlet–multinomial conjugacy then allowsm to be integrated out.\nGivingm a symmetric Dirichlet prior and integrating it out has the effect of replacingm in (5) with\na “global” P ´olya conditional distribution, shared by the document-speciﬁc predictive distributions.\nFigure 1f depicts the process of drawing eight topic assignments—four for document d and four\nfor document d′. As before, when a topic assignment is drawn from the predictive distribution\nfor document d, it is assigned the value of an existing (document-speciﬁc) internal draw γi with\nprobability proportional to the number of topic assignments previously matched to that draw, and to\nthe value of a new drawγi′ with probability proportional toα. However, sincem has been integrated\nout, the new draw must be obtained from the “global” distribution. At this level, γi′ treated as if\nit were a topic assignment, and assigned the value of an existing global drawγj with probability\nproportional to the number of document-level draws previously matched to γj, and to a new global\ndraw, fromu, with probability proportional toα′. Since the internal draws at the document level are\ntreated as topic assignments the global level, there is a path from every topic assignment to u, via\nthe internal draws. The predictive probability of topict in documentd givenZ is now\nP (z(d)\nNd+1 =t|Z,α,α′u) =\n∫\ndmP (z(d)\nNd+1 =t|Z,αm)P (m|Z,α′u)\n=\nNt|d +α\nˆNt + α′\nT∑\nt ˆNt +α′\nNd +α , (6)\nwhereI andJ are the current numbers of document-level and global internal draws, respectively,\nNt|d =∑I\ni=1N (i)\nd δ (γi−t) as before and ˆNt =∑J\nj=1N (j)δ (γj−t). The quantity N (j) is the\ntotal number of document-level internal draws matched to global internal drawγj. Since some topic\nassignments will be matched to existing document-level draws, ∑\nd δ (Nt|d > 0)≤ ˆNt ≤ Nt,\nwhere∑\nd δ (Nt|d > 0) is the number of unique documents inZ in which topict occurs.\nAn important property of (6) is that if concentration parameter α′ is large relative to ∑\nt ˆNt, then\ncounts ˆNt and∑\nt ˆNt are effectively ignored. In other words, as α′→∞ the hierarchical, asym-\nmetric Dirichlet prior approaches a symmetric Dirichlet prior with concentration parameterα.\nFor any givenZ for real-world documents W, the internal draws and the paths from Z tou are\nunknown. Only the value of each topic assignment is known, and hence Nt|d for each topic t and\ndocument d. In order to compute the conditional posterior distribution for each topic assignment\n(needed to resampleZ) it is necessary to infer ˆNt for each topict. These values can be inferred by\nGibbs sampling the paths fromZ tou [4, 15]. Resampling the paths fromZ tou can be interleaved\nwith resamplingZ itself. Removing z(d)\nn =t from the model prior to resampling its value consists\nof decrementingNt|d and removing its current path to u. Similarly, adding a newly sampled value\nz(d)\nn =t′ into the model consists of incrementingNt′|d and sampling a new path fromz(d)\nn tou.\n4 Comparing Priors for LDA\nTo investigate the effects of the priors over Θ and Φ, we compared the four combinations of sym-\nmetric and asymmetric Dirichlets shown in ﬁgure 1: symmetric priors over both Θ and Φ (denoted\n4\n0 1000 3000 5000\n−760000 −720000 −680000\n50 topics\nIteration\nLog Probability\n(a)\nα\nFrequency\n3.5 4.5 5.5\n050 150\nα'\nFrequency\n0 50 100 150\n050 150\nβ\nFrequency\n60708090\n0 200 400\nlog β'\nFrequency\n10 30 50 70\n0 4080 140 (b)\n-6.90-6.85-6.80-6.75-6.70-6.65\nPatent abstracts\npatents[, 1]\n-9.28-9.27-9.26-9.25-9.24-9.23-9.22\nNYT\nnyt[, 1]\n-8.33 -8.32 -8.31 -8.30 -8.29 -8.28\n20News (c)\nFigure 2: (a) logP (W,Z| Ω) (patent abstracts) for SS, SA, AS and AS, computed every 20 iterations and\naveraged over 5 Gibbs sampling runs. AS (red) and AA (black) perform similarly and converge to higher\nvalues of logP (W,Z| Ω) than SS (blue) and SA (green). (b) Histograms of 4000 (iterations 1000-5000)\nconcentration parameter values for AA (patent abstracts). Note the log scale forβ′: the prior over Φ approaches\na symmetric Dirichlet, making AA equivalent to AS. (c)logP (W,Z| Ω) for all three data sets atT = 50. AS\nis consistently better than SS. SA is poor (not shown). AA is capable of matching AS, but does not always.\nData set D ¯Nd N W Stop\nPatent abstracts 1016 101.87 103499 6068 yes\n20 Newsgroups 540 148.17 80012 14492 no\nNYT articles 1768 270.06 477465 41961 no\nTable 1: Data set statistics.D is the number of documents, ¯Nd is the mean document length,N is the number\nof tokens,W is the vocabulary size. “Stop” indicates whether stop words were present (yes) or not (no).\nSS), a symmetric prior over Θ and an asymmetric prior over Φ (denoted SA), an asymmetric prior\nover Θ and a symmetric prior over Φ (denoted AS), and asymmetric priors over both Θ and Φ (de-\nnoted AA). Each combination was used to model three collections of documents: patent abstracts\nabout carbon nanotechnology, New York Times articles, and 20 Newsgroups postings. Due to the\ncomputationally intensive nature of the fully Bayesian inference procedure, only a subset of each\ncollection was used (see table 1). In order to stress each combination of priors with respect to skewed\ndistributions over word frequencies, stop words were not removed from the patent abstracts.\nThe four models (SS, SA, AS, AA) were implemented in Java, with integrated-out base measures,\nwhere appropriate. Each model was run with T ∈{ 25, 50, 75, 100} for ﬁve runs of 5000 Gibbs\nsampling iterations, using different random initializations. The concentration parameters for each\nmodel (denoted by Ω) were given broad Gamma priors and inferred using slice sampling [13].\nDuring inference, logP (W,Z| Ω) was recorded every twenty iterations. These values, averaged\nover the ﬁve runs forT = 50, are shown in ﬁgure 2a. (Results for other values of T are similar.)\nThere are two distinct patterns: models with an asymmetric prior over Θ (AS and AA; red and\nblack, respectively) perform very similarly, while models with a symmetric prior over Θ (SS and\nSA; blue and green, respectively) also perform similarly, with signiﬁcantly worse performance than\nAS and AA. Results for all three data sets are summarized in ﬁgure 2c, with the log probability\ndivided by the number of tokens in the collection. SA performs extremely poorly on NYT and 20\nNewsgroups, and is not therefore shown. AS consistently achieves better likelihood than SS. The\nfully asymmetric model, AA, is inconsistent, matching AS on the patents and 20 Newsgroups but\ndoing poorly on NYT. This is most likely due to the fact that although AA can match AS, it has\nmany more degrees of freedom and therefore a much larger space of possibilities to explore.\nWe also calculated the probability of held-out documents using the “left-to-right” evaluation method\ndescribed by Wallach et al. [17]. These results are shown in ﬁgure 3a, and exhibit a similar pattern\nto the results in ﬁgure 2a—the best-performing models are those with an asymmetric priors overΘ.\nWe can gain intuition about the similarity between AS and AA by examining the values of the sam-\npled concentration parameters. As explained in section 3.2, asα′ orβ′ grows large relative to∑\nt ˆNt\nor∑\nw ˆNw, an asymmetric Dirichlet prior approaches a symmetric Dirichlet with concentration pa-\nrameter α or β. Histograms of 4000 concentration parameter values (from iterations 1000-4000)\nfrom the ﬁve Gibbs runs of AA with T = 50 are shown in ﬁgure 2b. The values for α,α′ andβ\n5\n-6.18-6.16-6.14-6.12-6.10\nHeld-out probability\nTopics\nNats / token\n25 50 75 100\n(a)\n0.080  a ﬁeld emission an electron the\n0.080  a the carbon and gas to an\n0.080  the of a to and about at\n0.080  of a surface the with in contact\n0.080  the a and to is of liquid\n0.895  the a of to and is in\n0.187  carbon nanotubes nanotube catalyst\n0.043  sub is c or and n sup\n0.061  fullerene compound fullerenes\n0.044  material particles coating inorganic\n0.042  a ﬁeld the emission and carbon is\n0.042  the carbon catalyst a nanotubes \n0.042  a the of substrate to material on\n0.042  carbon single wall the nanotubes\n0.042  the a probe tip and of to\n1.300  the a of to and is in\n0.257  and are of for in as such\n0.135  a carbon material as structure nanotube\n0.065  diameter swnt about nm than ﬁber swnts\n0.029  compositions polymers polymer containAsymmetric \nα\nSymmetric \nα\nAsymmetric βSymmetric β (b)\nFigure 3: (a) Log probability of held-out documents (patent abstracts). These results mirror those in ﬁgure 2a.\nAS (red) and AA (black) again perform similarly, while SS (blue) and SA (green) are also similar, but exhibit\nmuch worse performance. (b) αmt values and the most probable words for topics obtained with T = 50. For\neach model, topics were ranked according to usage and the topics at ranks 1, 5, 10, 20 and 30 are shown. AS\nand AA are robust to skewed word frequency distributions and tend to sequester stop words in their own topics.\nare all relatively small, while the values forβ′ are extremely large, with a median around exp 30. In\nother words, given the values ofβ′, the prior overΦ is effectively a symmetric prior overΦ with con-\ncentration parameterβ. These results demonstrate that even when the model can use an asymmetric\nprior over Φ, a symmetric prior gives better performance. We therefore advocate using model AS.\nIt is worth noting the robustness of AS to stop words. Unlike SS and SA, AS effectively sequesters\nstop words in a small number of more frequently used topics. The remaining topics are relatively\nunaffected by stop words. Creating corpus-speciﬁc stop word lists is seen as an unpleasant but nec-\nessary chore in topic modeling. Also, for many specialized corpora, once standard stop words have\nbeen removed, there are still other words that occur with very high probability, such as “model,”\n“data,” and “results” in machine learning literature, but are not technically stop words. If LDA\ncannot handle such words in an appropriate fashion then they must be treated as stop words and re-\nmoved, despite the fact that they play meaningful semantic roles. The robustness of AS to stop words\nhas implications for HMM-LDA [8] which models stop words using a hidden Markov model and\n“content” words using LDA, at considerable computational cost. AS achieves the same robustness\nto stop words much more efﬁciently. Although there is empirical evidence that topic models that\nuse asymmetric Dirichlet priors with optimized hyperparameters, such as Pachinko allocation [10]\nand Wallach’s topic-based language model [18], are robust to the presence of extremely common\nwords, these studies did not establish whether the robustness was a function of a more complicated\nmodel structure or if careful consideration of hyperparameters alone was sufﬁcient. We demonstrate\nthat AS is capable of learning meaningful topics even with no stop word removal. For efﬁciency,\nwe do not necessarily advocate doing away with stop word lists entirely, but we argue that using\nan asymmetric prior over Θ allows practitioners to use a standard, conservative list of determiners,\nprepositions and conjunctions that is applicable to any document collection in a given language,\nrather than hand-curated corpus-speciﬁc lists that risk removing common but meaningful terms.\n5 Efﬁciency: Optimizing rather than Integrating Out\nInference in the full Bayesian formulation of AS is expensive because of the additional complexity\nin sampling the paths from Z tou and maintaining hierarchical data structures. It is possible to\nretain the theoretical and practical advantages of using AS without sacriﬁcing the advantages of\nsimple, efﬁcient models by directly optimizingm, rather than integrating it out. The concentration\nparametersα andβ may also be optimized (along withm forα and by itself forβ). In this section,\nwe therefore compare the fully Bayesian version of AS with optimized AS, using SS as a baseline.\nWallach [19] compared several methods for jointly the maximum likelihood concentration parameter\nand asymmetric base measure for a Dirichlet–multinomial model. We use the most efﬁcient of\nthese methods. The advantage of optimizing m is considerable: although it is likely that further\noptimizations would reduce the difference, 5000 Gibbs sampling iterations (including sampling α,\n6\nPatents NYT 20 NG\nASO -6.65 ± 0.04 -9.24 ± 0.01 -8.27 ± 0.01\nAS -6.62 ± 0.03 -9.23 ± 0.01 -8.28 ± 0.01\nSS -6.91 ± 0.01 -9.26 ± 0.01 -8.31 ± 0.01\n25 50 75 100\nASO -6.18 -6.12 -6.12 -6.08\nAS -6.15 -6.13 -6.11 -6.10\nSS -6.18 -6.18 -6.16 -6.13\nTable 2: logP (W,Z| Ω)/N forT = 50 (left) and logP (Wtest|W,Z, Ω)/N test for varying values of T\n(right) for the patent abstracts. AS and ASO (optimized hyperparameters) consistently outperform SS except\nfor ASO withT = 25. Differences between AS and ASO are inconsistent and within standard deviations.\nASO AS SS\nASO 4.37 ± 0.08 4.34 ± 0.09 5.43 ± 0.05\nAS — 4.18 ± 0.09 5.39 ± 0.06\nSS — — 5.93 ± 0.03\nASO AS SS\nASO 3.36 ± 0.03 3.43 ± 0.05 3.50 ± 0.07\nAS — 3.36 ± 0.02 3.56 ± 0.07\nSS — — 3.49 ± 0.04\nTable 3: Average VI distances between multiple runs of each model withT = 50 on (left) patent abstracts and\n(right) 20 newsgroups. ASO partitions are approximately as similar to AS partitions as they are to other ASO\npartitions. ASO and AS partitions are both are further from SS partitions, which tend to be more dispersed.\nα′ andβ) for the patent abstracts using fully Bayesian AS withT = 25 took over four hours, while\n5000 Gibbs sampling iterations (including hyperparameter optimization) took under 30 minutes.\nIn order to establish that optimizing m is a good approximation to integrating it out, we computed\nlogP (W,Z| Ω) and the log probability of held-out documents for fully Bayesian AS, optimized\nAS (denoted ASO) and as a baseline SS (see table 2). AS and ASO consistently outperformed SS,\nexcept for ASO whenT = 25. Since twenty-ﬁve is a very small number of topics, this is not a cause\nfor concern. Differences between AS and ASO are inconsistent and within standard deviations.\nFrom a point of view of log probabilities, ASO therefore provides a good approximation to AS.\nWe can also compare topic assignments. Any set of topic assignments can be thought of as partition\nof the corresponding tokens into T topics. In order to measure similarity between two sets of topic\nassignmentsZ andZ′ forW, we can compute the distance between these partitions usingvariation\nof information (VI) [11, 6] (see suppl. mat. for a deﬁnition of VI for topic models). VI has several\nattractive properties: it is a proper distance metric, it is invariant to permutations of the topic labels,\nand it can be computed in O (N +TT′) time, i.e., time that is linear in the number of tokens and\nthe product of the numbers of topics inZ andZ′. For each model (AS, ASO and SS), we calculated\nthe average VI distance between all 10 unique pairs of topic assignments from the 5 Gibbs runs for\nthat model, giving a measure of within-model consistency. We also calculated the between-model\nVI distance for each pair of models, averaged over all 25 unique pairs of topic assignments for that\npair. Table 3 indicates that ASO partitions are approximately as similar to AS partitions as they are\nto other ASO partitions. ASO and AS partitions are both further away from SS partitions, which\ntend to be more dispersed. These results conﬁrm that ASO is indeed a good approximation to AS.\n6 Effect on Selecting the Number of Topics\nSelecting the number of topics T is one of the most problematic modeling choices in ﬁnite topic\nmodeling. Not only is there no clear method for choosingT (other than evaluating the probability of\nheld-out data for various values ofT ), but degree to which LDA is robust to a poor setting ofT is not\nwell-understood. Although nonparametric models provide an alternative, they lose the substantial\ncomputational efﬁciency advantages of ﬁnite models. We explore whether the combination of priors\nadvocated in the previous sections (model AS) can improve the stability of LDA to different values of\nT , while retaining the static memory management and simple inference algorithms of ﬁnite models.\nIdeally, if LDA has sufﬁcient topics to modelW well, the assignments of tokens to topics should be\nrelatively invariant to an increase inT —i.e., the additional topics should be seldom used. For exam-\nple, if ten topics is sufﬁcient to accurately model the data, then increasing the number of topics to\ntwenty shouldn’t signiﬁcantly affect inferred topic assignments. If this is the case, then using large\nT should not have a signiﬁcant impact on eitherZ or the speed of inference, especially as recently-\nintroduced sparse sampling methods allow models with large T to be trained efﬁciently [20]. Fig-\nure 4a shows the average VI distance between topic assignments (for the patent abstracts) inferred\nby models withT = 25 and models withT∈{ 50, 75, 100}. AS and AA, the bottom two lines, are\n7\n4.0 4.5 5.0 5.5 6.0 6.5\nClustering distance from T=25\nTopics\nVariation of Information\n50 75 100\n(a)\n50 topics75 topics100 topics\nAS prior\n0.0 0.2 0.4 0.6 0.8 1.0\n50 topics75 topics100 topics\nSS prior\n0.0 0.2 0.4 0.6 0.8 1.0 (b)\nFigure 4: (a) Topic consistency measured by average VI distance from models withT = 25. As T increases,\nAS (red) and AA (black) produce Zs that stay signiﬁcantly closer to those obtained with T = 25 than SA\n(green) and SS (blue). (b) Assignments of tokens (patent abstracts) allocated to the largest topic in a 25 topic\nmodel, asT increases. For AS, the topic is relatively intact, even at T = 100: 80% of tokens assigned to the\ntopic atT = 25 are assigned to seven topics. For SS, the topic has been subdivided across many more topics.\nmuch more stable (smaller average VI distances) than SS and SA at 50 topics and remain so as T\nincreases: even at 100 topics, AS has a smaller VI distance to a 25 topic model than SS at 50 topics.\nFigure 4b provides intuition for this difference: for AS, the tokens assigned to the largest topic at\nT = 25 remain within a small number of topics asT is increased, while for SS, topic usage is more\nuniform and increasing T causes the tokens to be divided among many more topics. These results\nsuggest that for AS, new topics effectively “nibble away” at existing topics, rather than splitting\nthem more uniformly. We therefore argue that the risk of using too many topics is lower than the\nrisk of using too few, and that practitioners should be comfortable using larger values ofT .\n7 Discussion\nThe previous sections demonstrated that AS results in the best performance over AA, SA and SS,\nmeasured in several ways. However, it is worth examining why this combination of priors results\nin superior performance. The primary assumption underlying topic modeling is that a topic should\ncapture semantically-related word co-occurrences. Topics must also be distinct in order to convey\ninformation: knowing only a few co-occurring words should be sufﬁcient to resolve semantic ambi-\nguities. A priori, we therefore do not expect that a particular topic’s distribution over words will be\nlike that of any other topic. An asymmetric prior over Φ is therefore a bad idea: the base measure\nwill reﬂect corpus-wide word usage statistics, and a priori, all topics will exhibit those statistics too.\nA symmetric prior over Φ only makes a prior statement (determined by the concentration param-\neter β) about whether topics will have more sparse or more uniform distributions over words, so\nthe topics are free to be as distinct and specialized as is necessary. However, it is still necessary to\naccount for power-law word usage. A natural way of doing this is to expect that certain groups of\nwords will occur more frequently than others in every document in a given corpus. For example, the\nwords “model,” “data,” and “algorithm” are likely to appear in every paper published in a machine\nlearning conference. These assumptions lead naturally to the combination of priors that we have\nempirically identiﬁed as superior: an asymmetric Dirichlet prior over Θ that serves to share com-\nmonalities across documents and a symmetric Dirichlet prior over Φ that serves to avoid conﬂicts\nbetween topics. Since these priors can be implemented using efﬁcient algorithms that add negligible\ncost beyond standard inference techniques, we recommend them as a new standard for LDA.\n8 Acknowledgments\nThis work was supported in part by the Center for Intelligent Information Retrieval, in part by\nCIA, NSA and NSF under NSF grant number IIS-0326249, and in part by subcontract number\nB582467 from Lawrence Livermore National Security, LLC under prime contract number DE-\nAC52-07NA27344 from DOE/NNSA. Any opinions, ﬁndings and conclusions or recommendations\nexpressed in this material are the authors’ and do not necessarily reﬂect those of the sponsor.\n8\nReferences\n[1] A. Asuncion, M. Welling, P. Smyth, and Y . W. Teh. On smoothing and inference for topic\nmodels. In Proceedings of the 25th Conference on Uncertainty in Artiﬁcial Intelligence, 2009.\n[2] D. Blei and J. Lafferty. A correlated topic model of Science. Annals of Applied Statistics ,\n1(1):17–35, 2007.\n[3] D. M. Blei, A. Y . Ng, and M. I. Jordan. Latent Dirichlet allocation. Journal of Machine\nLearning Research, 3:993–1022, January 2003.\n[4] P. J. Cowans. Probabilistic Document Modelling. PhD thesis, University of Cambridge, 2006.\n[5] S. Geman and D. Geman. Stochastic relaxation, Gibbs distributions, and the Bayesian restora-\ntion of images. IEEE Transaction on Pattern Analysis and Machine Intelligence 6 , pages\n721–741, 1984.\n[6] S. Goldwater and T. L. Grifﬁths. A fully Bayesian approach to unsupervised part-of-speech\ntagging. In Association for Computational Linguistics, 2007.\n[7] T. L. Grifﬁths and M. Steyvers. Finding scientiﬁc topics.Proceedings of the National Academy\nof Sciences, 101(suppl. 1):5228–5235, 2004.\n[8] T. L. Grifﬁths, M. Steyvers, D. M. Blei, and J. B. Tenenbaum. Integrating topics and syntax.\nIn L. K. Saul, Y . Weiss, and L. Bottou, editors, Advances in Neural Information Processing\nSystems 17, pages 536–544. The MIT Press, 2005.\n[9] D. Hall, D. Jurafsky, and C. D. Manning. Studying the history of ideas using topic models. In\nProceedings of EMNLP 2008, pages 363–371.\n[10] W. Li and A. McCallum. Mixtures of hierarchical topics with pachinko allocation. In Proceed-\nings of the 24th International Conference on Machine learning, pages 633–640, 2007.\n[11] M. Meil ˘a. Comparing clusterings by the variation of information. In Conference on Learning\nTheory, 2003.\n[12] D. Mimno and A. McCallum. Organizing the OCA: Learning faceted subjects from a library\nof digital books. In Proceedings of the 7th ACM/IEEE joint conference on Digital libraries ,\npages 376–385, Vancouver, BC, Canada, 2007.\n[13] R. M. Neal. Slice sampling. Annals of Statistics, 31:705–767, 2003.\n[14] D. Newman, C. Chemudugunta, P. Smyth, and M. Steyvers. Analyzing entities and topics in\nnews articles using statistical topic models. In Intelligence and Security Informatics, Lecture\nNotes in Computer Science. 2006.\n[15] Y . W. Teh, M. I. Jordan, M. J. Beal, and D. M. Blei. Hierarchical Dirichlet processes. Journal\nof the American Statistical Association, 101:1566–1581, 2006.\n[16] Y . W. Teh, D. Newman, and M. Welling. A collapsed variational Bayesian inference algorithm\nfor latent Dirichlet allocation. In Advances in Neural Information Processing Systems 18 ,\n2006.\n[17] H. Wallach, I. Murray, R. Salakhutdinov, and D. Mimno. Evaluation methods for topic models.\nIn Proceedings of the 26th Interational Conference on Machine Learning, 2009.\n[18] H. M. Wallach. Topic modeling: Beyond bag-of-words. In Proceedings of the 23rd Interna-\ntional Conference on Machine Learning, pages 977–984, Pittsburgh, Pennsylvania, 2006.\n[19] H. M. Wallach. Structured Topic Models for Language. Ph.D. thesis, University of Cambridge,\n2008.\n[20] L. Yao, D. Mimno, and A. McCallum. Efﬁcient methods for topic model inference on stream-\ning document collections. In Proceedings of KDD 2009, 2009.\n9",
  "values": {
    "Fairness": "No",
    "User influence": "No",
    "Non-maleficence": "No",
    "Beneficence": "No",
    "Collective influence": "No",
    "Interpretable (to users)": "No",
    "Respect for Persons": "No",
    "Respect for Law and public interest": "No",
    "Not socially biased": "No",
    "Autonomy (power to decide)": "No",
    "Transparent (to users)": "No",
    "Critiqability": "No",
    "Justice": "No",
    "Privacy": "No",
    "Explicability": "No",
    "Deferral to humans": "No"
  }
}