{
  "pdf": "NIPS-2010-guaranteed-rank-minimization-via-singular-value-projection-Paper",
  "title": "Guaranteed Rank Minimization via Singular Value Projection",
  "author": "Prateek Jain, Raghu Meka, Inderjit S. Dhillon",
  "paper_id": "NIPS-2010-guaranteed-rank-minimization-via-singular-value-projection-Paper",
  "text": "Guaranteed Rank Minimization via Singular Value\nProjection\nPrateek Jain\nMicrosoft Research Bangalore\nBangalore, India\nprajain@microsoft.com\nRaghu Meka\nUT Austin Dept. of Computer Sciences\nAustin, TX, USA\nraghu@cs.utexas.edu\nInderjit Dhillon\nUT Austin Dept. of Computer Sciences\nAustin, TX, USA\ninderjit@cs.utexas.edu\nAbstract\nMinimizing the rank of a matrix subject to afﬁne constraints is a fundamental\nproblem with many important applications in machine learning and statistics. In\nthis paper we propose a simple and fast algorithm SVP (Singular V alue Projec-\ntion) for rank minimization under afﬁne constraints ( ARMP) and show that SVP\nrecovers the minimum rank solution for afﬁne constraints that satisfy a restricted\nisometry property (RIP). Our method guarantees geometric convergence rate even\nin the presence of noise and requires strictly weaker assumptions on the RIP con-\nstants than the existing methods. We also introduce a Newton-step for our SVP\nframework to speed-up the convergence with substantial empirical gains. Next,\nwe address a practically important application of ARMP - the problem of low-\nrank matrix completion, for which the deﬁning afﬁne constraints do not directly\nobey RIP , hence the guarantees of SVP do not hold. However, we provide partial\nprogress towards a proof of exact recovery for our algorithm by showing a more\nrestricted isometry property and observe empirically that our algorithm recovers\nlow-rank incoherent matrices from an almost optimal number of uniformly sam-\npled entries. We also demonstrate empirically that our algorithms outperform ex-\nisting methods, such as those of [5, 18, 14], for ARMP and the matrix completion\nproblem by an order of magnitude and are also more robust to noise and sampling\nschemes. In particular, results show that our SVP-Newton method is signiﬁcantly\nrobust to noise and performs impressively on a more realistic power-law sampling\nscheme for the matrix completion problem.\n1 Introduction\nIn this paper we study the general afﬁne rank minimization problem (ARMP),\nmin rank(X) s.t A(X) = b, X ∈ Rm×n, b ∈ Rd, (ARMP)\nwhere A is an afﬁne transformation from Rm×n to Rd.\nThe afﬁne rank minimization problem above is of considerable practical interest and many important\nmachine learning problems such as matrix completion, low-dimensional metric embedding, low-\nrank kernel learning can be viewed as instances of the above problem. Unfortunately, ARMP is\nNP-hard in general and is also NP-hard to approximate ([22]).\nUntil recently, most known methods for ARMP were heuristic in nature with few known rigorous\nguarantees. In a recent breakthrough, Recht et al. [24] gave the ﬁrst nontrivial results for the\n1\nproblem obtaining guaranteed rank minimization for afﬁne transformations A that satisfy a restricted\nisometry property (RIP). Deﬁne the isometry constant of A, δk to be the smallest number such that\nfor all X ∈ Rm×n of rank at most k,\n(1 − δk)∥X∥2\nF ≤ ∥A(X)∥2\n2 ≤ (1 + δk)∥X∥2\nF . (1)\nThe above RIP condition is a direct generalization of the RIP condition used in the compressive\nsensing context. Moreover, RIP holds for many important practical applications of ARMP such\nas image compression, linear time-invariant systems. In particular, Recht et al. show that for most\nnatural families of random measurements, RIP is satisﬁed even for only O(nk log n) measurements.\nAlso, Recht et al. show that for ARMP with isometry constant δ5k < 1/10, the minimum rank\nsolution can be recovered by the minimum trace-norm solution.\nIn this paper we propose a simple and efﬁcient algorithm SVP (Singular V alue Projection) based\non the projected gradient algorithm. We present a simple analysis showing that SVP recovers the\nminimum rank solution for noisy afﬁne constraints that satisfy RIP and prove the following guar-\nantees. (Independent of our work, Goldfarb and Ma [12] proposed an algorithm similar to SVP.\nHowever, their analysis and formulation is different from ours. They also require stronger isometry\nassumptions, δ3k < 1/\n√\n30, than our analysis.)\nTheorem 1.1 Suppose the isometry constant of A satisﬁes δ2k < 1/3 and let b = A(X ∗) for a\nrank-k matrix X ∗. Then, SVP (Algorithm 1) with step-size ηt = 1 /(1 + δ2k) converges to X ∗.\nFurthermore, SVP outputs a matrix X of rank at most k such that ∥A(X) − b∥2\n2 ≤ ϵ and ∥X −\nX ∗∥2\nF ≤ ϵ/(1 − δ2k) in at most\n⌈\n1\nlog((1−δ2k)/2δ2k) log ∥b∥2\n2ϵ\n⌉\niterations.\nTheorem 1.2 (Main) Suppose the isometry constant of A satisﬁes δ2k < 1/3 and let b = A(X ∗)+e\nfor a rank k matrix X ∗ and an error vector e ∈ Rd. Then, SVP with step-size ηt = 1 /(1 + δ2k)\noutputs a matrix X of rank at most k such that ∥A(X) − b∥2\n2 ≤ C∥e∥2 + ϵ and ∥X − X ∗∥2\nF ≤\nC∥e∥2+ϵ\n1−δ2k\n, ϵ ≥ 0, in at most\n⌈\n1\nlog(1/D) log ∥b∥2\n2(C∥e∥2+ϵ)\n⌉\niterations for universal constants C, D.\nAs our SVP algorithm is based on projected gradient descent, it behaves as a ﬁrst order methods\nand may require a relatively large number of iterations to achieve high accuracy, even after iden-\ntifying the correct row and column subspaces. To this end, we introduce a Newton-type step in\nour framework ( SVP-Newton) rather than using a simple gradient-descent step. Guarantees sim-\nilar to Theorems 1.1, 1.2 follow easily for SVP-Newton using the proofs for SVP. In practice,\nSVP-Newton performs better than SVP in terms of accuracy and number of iterations.\nWe next consider an important application of ARMP: the low-rank matrix completion problem\n(MCP)— given a small number of entries from an unknown low-rank matrix, the task is to complete\nthe missing entries. Note that RIP does not hold directly for this problem. Recently, Candes and\nRecht [6], Candes and Tao [7] and Keshavan et al. [14] gave the ﬁrst theoretical guarantees for the\nproblem obtaining exact recovery from an almost optimal number of uniformly sampled entries.\nWhile RIP does not hold for MCP , we show that a similar property holds for incoherent matrices\n[6]. Given our reﬁned RIP and a hypothesis bounding the incoherence of the iterates arising in SVP,\nan analysis similar to that of Theorem 1.1 immediately implies that SVP optimally solves MCP .\nWe provide strong empirical evidence for our hypothesis and show that that both of our algorithms\nrecover a low-rank matrix from an almost optimal number of uniformly sampled entries.\nIn summary, our main contributions are:\n• Motivated by [11], we propose a projected gradient based algorithm, SVP, for ARMP and show\nthat our method recovers the optimal rank solution when the afﬁne constraints satisfy RIP. To the\nbest of our knowledge, our isometry constant requirements are least stringent: we only require\nδ2k < 1/3 as opposed to δ5k < 1/10 by Recht et al., δ3k < 1/4\n√\n3 by Lee and Bresler [18] and\nδ4k < 0.04 by Lee and Bresler [17].\n• We introduce a Newton-type step in the SVP method which is useful if high precision is criti-\ncally. SVP-Newton has similar guarantees to that of SVP, is more stable and has better empirical\nperformance in terms of accuracy. For instance, on the Movie-lens dataset [1] and rank k = 3 ,\nSVP-Newton achieves an RMSE of 0.89, while SVT method [5] achieves an RMSE of 0.98.\n• As observed in [23], most trace-norm based methods perform poorly for matrix completion when\nentries are sampled from more realistic power-law distributions. Our method SVP-Newton is\nrelatively robust to sampling techniques and performs signiﬁcantly better than the methods of\n[5, 14, 23] even for power-law distributed samples.\n2\n• We show that the afﬁne constraints in the low-rank matrix completion problem satisfy a weaker\nrestricted isometry property and as supported by empirical evidence, conjecture that SVP (as\nwell as SVP-Newton) recovers the underlying matrix from an almost optimal number of uni-\nformly random samples.\n• We evaluate our method on a variety of synthetic and real-world datasets and show that our\nmethods consistently outperform, both in accuracy and time, various existing methods [5, 14].\n2 Method\nIn this section, we ﬁrst introduce our Singular V alue Projection ( SVP) algorithm for ARMP and\npresent a proof of its optimality for afﬁne constraints satisfying RIP (1). We then specialize our\nalgorithm for the problem of matrix completion and prove a more restricted isometry property for\nthe same. Finally, we introduce a Newton-type step in our SVP algorithm and prove its convergence.\n2.1 Singular Value Decomposition (SVP)\nConsider the following more robust formulation of ARMP (RARMP),\nmin\nX\nψ(X) = 1\n2 ∥A(X) − b∥2\n2 s.t X ∈ C (k) = {X : rank(X) ≤ k}. (RARMP)\nThe hardness of the above problem mainly comes from the non-convexity of the set of low-rank\nmatrices C(k). However, the Euclidean projection onto C(k) can be computed efﬁciently using\nsingular value decomposition (SVD). Our algorithm uses this observation along with the projected\ngradient method for efﬁciently minimizing the objective function speciﬁed in (RARMP).\nLet Pk : Rm×n → Rm×n denote the orthogonal projection on to the set C(k). That is, Pk(X) =\nargminY {∥Y − X∥F : Y ∈ C (k)}. It is well known that Pk(X) can be computed efﬁciently by\ncomputing the top k singular values and vectors of X.\nIn SVP, a candidate solution to ARMP is computed iteratively by starting from the all-zero ma-\ntrix and adapting the classical projected gradient descent update as follows (note that ∇ψ(X) =\nAT (A(X) − b)):\nX t+1 ← P k\n(\nX t − ηt∇ψ(X t)\n)\n= Pk\n(\nX t − ηtAT (A(X t) − b)\n)\n. (1)\nFigure 1 presents SVP in more detail. Note that the iterates X t are always low-rank, facilitating\nfaster computation of the SVD. See Section 3 for a more detailed discussion of computational issues.\nAlgorithm 1 Singular V alue Projection (SVP) Algorithm\nRequire: A, b, tolerance ε, ηt for t = 0, 1, 2, . . .\n1: Initialize: X 0 = 0 and t = 0\n2: repeat\n3: Y t+1 ← X t − ηtAT (A(X t) − b)\n4: Compute top k singular vectors of Y t+1: Uk, Σk, Vk\n5: X t+1 ← UkΣkV T\nk\n6: t ← t + 1\n7: until ∥A(X t+1) − b∥2\n2 ≤ ε\nAnalysis for Constraints Satisfying RIP\nTheorem 1.1 shows that SVP converges to an ϵ-approximate solution of RARMP in O(log ∥b∥2\nϵ )\nsteps. Theorem 1.2 shows a similar result for the noisy case. The theorems follow from the following\nlemma that bounds the objective function after each iteration.\nLemma 2.1 Let X ∗ be an optimal solution of (RARMP) and let X t be the iterate obtained by SVP\nat t-th iteration. Then, ψ(X t+1) ≤ ψ(X ∗) + δ2k\n(1−δ2k) ∥A(X ∗ − X t)∥2\n2, where δ2k is the rank 2k\nisometry constant of A.\nThe lemma follows from elementary linear algebra, optimality of SVD (Eckart-Y oung theorem) and\ntwo simple applications of RIP. We refer to the supplementary material (Appendix A) for a detailed\nproof. We now prove Theorem 1.1. Theorem 1.2 can also be proved similarly; see supplementary\nmaterial (Appendix A) for a detailed proof.\nProof of Theorem 1.1 Using Lemma 2.1 and the fact that ψ(X ∗) = 0 , it follows that\nψ(X t+1) ≤ δ2k\n(1 − δ2k) ∥A(X ∗ − X t)∥2\n2 = 2δ2k\n(1 − δ2k) ψ(X t).\n3\nAlso, note that for δ2k < 1/3, 2δ2k\n(1−δ2k) < 1. Hence, ψ(X τ ) ≤ ϵ where τ =⌈\n1\nlog((1−δ2k)/2δ2k) log ψ(X 0)\nϵ\n⌉\n. Further, using RIP for the rank at most 2k matrix X τ − X ∗ we\nget: ∥X τ − X ∗∥ ≤ ψ(X τ )/(1 − δ2k) ≤ ϵ/(1 − δ2k). Now, the SVP algorithm is initialized using\nX 0 = 0, i.e., ψ(X 0) = ∥b∥2\n2 . Hence, τ =\n⌈\n1\nlog((1−δ2k)/2δ2k) log ∥b∥2\n2ϵ\n⌉\n.\n2.2 Matrix Completion\nWe ﬁrst describe the low-rank matrix completion problem formally. For Ω ⊆ [m] × [n], let PΩ :\nRm×n → Rm×n denote the projection onto the index set Ω. That is, (PΩ(X))ij = Xij for (i, j) ∈\nΩ and (PΩ(X))ij = 0 otherwise. Then, the low-rank matrix completion problem ( MCP) can be\nformulated as follows,\nmin\nX\nrank(X) s.t PΩ(X) = PΩ(X ∗), X ∈ Rm×n. (MCP)\nObserve that MCP is a special case of ARMP, so we can apply SVP for matrix completion. We\nuse step-size ηt = 1/(1 + δ)p, where p is the density of sampled entries and δ is a parameter which\nwe will explain later in this section. Using the given step-size and update (1), we get the following\nupdate for matrix-completion:\nX t+1 ← P k\n(\nX t − 1\n(1 + δ)p (PΩ(X t) − PΩ(X ∗))\n)\n. (2)\nAlthough matrix completion is a special case of ARMP, the afﬁne constraints that deﬁne MCP, PΩ,\ndo not satisfy RIP in general. Thus Theorems 1.1, 1.2 above and the results of Recht et al. [24] do\nnot directly apply to MCP. However, we show that the matrix completion afﬁne constraints satisfy\nRIP for low-rank incoherent matrices.\nDeﬁnition 2.1 (Incoherence) A matrix X ∈ Rm×n with singular value decomposition X =\nU ΣV T is µ-incoherent if maxi,j |Uij| ≤\n√µ√m , maxi,j |Vij| ≤\n√µ√n .\nThe above notion of incoherence is similar to that introduced by Candes and Recht [6] and also used\nby [7, 14]. Intuitively, high incoherence (i.e., µ is small) implies that the non-zero entries of X\nare not concentrated in a small number of entries. Hence, a random sampling of the matrix should\nprovide enough global information to satisfy RIP.\nUsing the above deﬁnition, we prove the following reﬁned restricted isometry property.\nTheorem 2.2 There exists a constant C ≥ 0 such that the following holds for all 0 < δ < 1,\nµ ≥ 1, n ≥ m ≥ 3: F or Ω ⊆ [m] × [n] chosen according to the Bernoulli model with density\np ≥ Cµ 2k2 log n/δ2m, with probability at least 1−exp(−n log n), the following restricted isometry\nproperty holds for all µ-incoherent matrices X of rank at most k:\n(1 − δ)p ∥X∥2\nF ≤ ∥P Ω(X)∥2\nF ≤ (1 + δ)p ∥X∥2\nF . (3)\nRoughly, our proof combines a Chernoff bound estimate for ∥PΩ(X)∥2\nF with a union bound over\nlow-rank incoherent matrices. A proof sketch is presented in Section 2.2.1.\nGiven the above reﬁned RIP, if the iterates arising in SVP are shown to be incoherent, the arguments\nof Theorem 1.1 can be used to show that SVP achieves exact recovery for low-rank incoherent\nmatrices from uniformly sampled entries. As supported by empirical evidence, we hypothesize that\nthe iterates X t arising in SVP remain incoherent when the underlying matrix X ∗ is incoherent.\nFigure 1 (d) plots the maximum incoherence maxt µ(X t) = √n maxt,i,j |U t\nij|, where U t are the\nleft singular vectors of the intermediate iterates X t computed by SVP. The ﬁgure clearly shows\nthat the incoherence µ(X t) of the iterates is bounded by a constant independent of the matrix size\nn and density p throughout the execution of SVP. Figure 2 (c) plots the threshold sampling density\np beyond which matrix completion for randomly generated matrices is solved exactly by SVP for\nﬁxed k and varying matrix sizes n. Note that the density threshold matches the optimal information-\ntheoretic bound [14] of Θ(k log n/n).\nMotivated by Theorem 2.2 and supported by empirical evidence (Figures 2 (c), (d)) we hypothesize\nthat SVP achieves exact recovery from an almost optimal number of samples for incoherent matrices.\nConjecture 2.3 Fix µ, k and δ ≤ 1/3. Then, there exists a constant C such that for a µ-\nincoherent matrix X ∗ of rank at most k and Ω sampled from the Bernoulli model with density\np = Ω µ,k((log n)/m), SVP with step-size ηt = 1 /(1 + δ)p converges to X ∗ with high probability.\nMoreover ,SVP outputs a matrix X of rank at most k such that ∥PΩ(X) − PΩ(X ∗)∥2\nF ≤ ϵ after\nOµ,k\n(⌈\nlog\n( 1\nϵ\n)⌉)\niterations.\n4\n2.2.1 RIP for Matrix Completion on Incoherent Matrices\nWe now prove the restricted isometry property of Theorem 2.2 for the afﬁne constraints that result\nfrom the projection operator PΩ. To prove Theorem 2.2 we ﬁrst show the theorem for a discrete\ncollection of matrices using Chernoff type large-deviation bounds and use standard quantization\narguments to generalize to the continuous case. We ﬁrst introduce some notation and provide useful\nlemmas for our main proof 1. First, we introduce the notion of α-regularity.\nDeﬁnition 2.2 A matrix X ∈ Rm×n is α-regular if maxi,j |Xij| ≤ α√mn · ∥X∥F .\nLemma 2.4 below relates the notion of regularity to incoherence and Lemma 2.5 proves (3) for a\nﬁxed regular matrix when the samples Ω are selected independently.\nLemma 2.4 Let X ∈ Rm×n be a µ-incoherent matrix of rank at most k. Then X is µ\n√\nk-regular .\nLemma 2.5 Fix a α-regular X ∈ Rm×n and 0 < δ < 1. Then, for Ω ⊆ [m] × [n] chosen according\nto the Bernoulli model, with each pair (i, j) ∈ Ω chosen independently with probability p,\nPr\n[ \f\f∥PΩ(X)∥2\nF − p∥X∥2\nF\n\f\f ≥ δp∥X∥2\nF\n]\n≤ 2 exp\n(\n− δ2pmn\n3 α2\n)\n.\nWhile the above lemma shows Equation (3) for a ﬁxed rank k, µ-incoherent X (i.e., (µ\n√\nk)-regular\nX using Lemma 2.4), we need to show Equation (3) for all such rank k incoherent matrices. To\nhandle this problem, we discretize the space of low-rank incoherent matrices so as to be able to\nuse the above lemma and a union bound. We now show the existence of a small set of matrices\nS(µ, ϵ) ⊆ Rm×n such that every low-rank µ-incoherent matrix is close to an appropriately regular\nmatrix from the set S(µ, ϵ).\nLemma 2.6 F or all0 < ϵ < 1/2, µ ≥ 1, m, n ≥ 3 and k ≥ 1, there exists a set S(µ, ϵ) ⊆ Rm×n\nwith |S(µ, ϵ)| ≤ (mnk/ϵ)3 (m+n)k such that the following holds. F or any µ-incoherent X ∈ Rm×n\nof rank k with ∥X∥2 = 1, there exists Y ∈ S(µ, ϵ) s.t. ∥Y − X∥F < ϵ and Y is (4µ\n√\nk)-regular .\nWe now prove Theorem 2.2 by combining Lemmas 2.5, 2.6 and applying a union bound. We present\na sketch of the proof but defer the details to the supplementary material (Appendix B).\nProof Sketch of Theorem 2.2 Let S′(µ, ϵ) = {Y : Y ∈ S(µ, ϵ), Y is 4µ\n√\nk-regular}, where\nS(µ, ϵ) is as in Lemma 2.6 for ϵ = δ/9mnk. Let m ≤ n. Then, by Lemma 2.5 and union bound,\nfor any Y ∈ S′(µ, ϵ),\nPr\n[ \f\f∥PΩ(Y )∥2\nF − p∥Y ∥2\nF\n\f\f ≥ δp∥Y ∥2\nF\n]\n≤ 2(mnk/ϵ)3(m+n)k exp\n( −δ2pmn\n16µ2k\n)\n≤ exp(C1nk log n)·exp\n( −δ2pmn\n16µ2k\n)\n,\nwhere C1 ≥ 0 is a constant independent of m, n, k. Thus, if p > Cµ 2k2 log n/δ2m, where C =\n16(C1 + 1), with probability at least 1 − exp(−n log n), the following holds\n∀Y ∈ S′(µ, ϵ), |∥PΩ(Y )∥2\nF − p∥Y ∥2\nF | ≤ δp∥Y ∥2\nF . (4)\nAs the statement of the theorem is invariant under scaling, it is enough to show the statement for all\nµ-incoherent matrices X of rank at most k and ∥X∥2 = 1. Fix such a X and suppose that (4) holds.\nNow, by Lemma 2.6 there exists Y ∈ S′(µ, ϵ) such that ∥Y − X∥F ≤ ϵ. Moreover,\n∥Y ∥2\nF ≤ (∥X∥F + ϵ)2 ≤ ∥X∥2\nF + 2ϵ∥X∥F + ϵ2 ≤ ∥X∥2\nF + 3ϵk.\nProceeding similarly, we can show that\n|∥X∥2\nF − ∥Y ∥2\nF | ≤ 3ϵk, |∥PΩ(Y )∥2\nF − ∥PΩ(X)∥2\nF | ≤ 3ϵk. (5)\nCombining inequalities (4), (5) above, with probability at least 1 − exp(−n log n) we have,\n|∥PΩ(X)∥2\nF − p∥X∥2\nF | ≤ |∥P Ω(X)∥2\nF − ∥PΩ(Y )∥2\nF | + p |∥X∥2\nF − ∥Y ∥2\nF | + |∥PΩ(Y )∥2\nF − p∥Y ∥2\nF | ≤ 2δp∥X∥2\nF .\nThe theorem follows using the above inequality.\n2.3 SVP-Newton\nIn this section we introduce a Newton-type step in our SVP method to speed up its convergence.\nRecall that each iteration of SVP (Equation (1)) takes a step along the gradient of the objective\nfunction and then projects the iterate to the set of low rank matrices using SVD. Now, the top k\nsingular vectors (Uk, Vk) of Y t+1 = X t −ηtAT (A(X t)−b) determine the range-space and column-\nspace of the next iterate in SVP. Then, Σk is given by Σk = Diag(U T\nk (X t −ηtAT (A(X t)−b))Vk).\n1Detailed proofs of all the lemmas in this section are provided in Appendix B of the supplementary material.\n5\nHence, Σk can be seen as a product of gradient-descent step for a quadratic objective function, i.e.,\nΣk = argmin S ψ(UkSV T\nk ). This leads us to the following variant of SVP we call SVP-Newton:2\nCompute top k-singular vectors Uk, Vk of Y t+1 = X t − ηtAT (A(X t) − b)\nX t+1 = UkΣkVk, Σk = argmin\nS\nΨ(UkSV T\nk ) = argmin\nS\n∥A(UkΣkV T\nk ) − b∥2.\nNote that as A is an afﬁne transformation, Σk can be computed by solving a least squares problem\non k ×k variables. Also, for a single iteration, given the same starting point, SVP-Newton decreases\nthe objective function more than SVP. This observation along with straightforward modiﬁcations of\nthe proofs of Theorems 1.1, 1.2 show that similar guarantees hold for SVP-Newton as well3.\nNote that the least squares problem for computing Σk has k2 variables. This makes SVP-Newton\ncomputationally expensive for problems with large rank, particularly for situations with a large\nnumber of constraints as is the case for matrix completion. To overcome this issue, we also consider\nthe alternative where we restrict Σk to be a diagonal matrix, leading to the update\nΣk = argmin\nS,s.t.,Sij =0 for i̸=j\n∥A(UkSV T\nk ) − b∥2 (6)\nWe call the above method SVP-NewtonD (for SVP-Newton Diagonal). As for SVP-Newton, guar-\nantees similar to SVP follow for SVP-NewtonD by observing that for each iteration, SVP-NewtonD\ndecreases the objective function more than SVP.\n3 Related Work and Computational Issues\nThe general rank minimization problem with afﬁne constraints is NP-hard and is also NP-hard to\napproximate [22]. Most methods for ARMP either relax the rank constraint to a convex function\nsuch as the trace-norm [8], [9], or assume a factorization and optimize the resulting non-convex\nproblem by alternating minimization [4, 3, 15].\nThe results of Recht et al. [24] were later extended to noisy measurements and isometry constants\nup to δ3k < 1/4\n√\n3 by Fazel et al. [10] and Lee and Bresler [18]. However, even the best existing\noptimization algorithms for the trace-norm relaxation are relatively inefﬁcient in practice. Recently,\nLee and Bresler [17] proposed an algorithm (ADMiRA) motivated by the orthogonal matching pur-\nsuit line of work in compressed sensing and show that for afﬁne constraints with isometry constant\nδ4k ≤ 0.04, their algorithm recovers the optimal solution. However, their method is not very efﬁ-\ncient for large datasets and when the rank of the optimal solution is relatively large.\nFor the matrix-completion problem until the recent works of [6], [7] and [14], there were few meth-\nods with rigorous guarantees. The alternating least squares minimization heuristic and its variants\n[3, 15] perform the best in practice, but are notoriously hard to analyze. Candes and Recht [6],\nCandes and Tao [7] show that if X ∗ is µ-incoherent and the known entries are sampled uniformly\nat random with |Ω| ≥ C(µ) k2n log2 n, ﬁnding the minimum trace-norm solution recovers the min-\nimum rank solution. Keshavan et.al obtained similar results independently for exact recovery from\nuniformly sampled Ω with |Ω| ≥ C(µ, k) n log n.\nMinimizing the trace-norm of a matrix subject to afﬁne constraints can be cast as a semi-deﬁnite\nprogram (SDP). However, algorithms for semi-deﬁnite programming, as used by most methods for\nminimizing trace-norm, are prohibitively expensive even for moderately large datasets. Recently,\na variety of methods based mostly on iterative soft-thresholding have been proposed to solve the\ntrace-norm minimization problem more efﬁciently. For instance, Cai et al. [5] proposed a Singular\nV alue Thresholding (SVT) algorithm which is based on Uzawa’s algorithm [2]. A related approach\nbased on linearized Bregman iterations was proposed by Ma et al. [20], Toh and Y un [25], while Ji\nand Y e [13] use Nesterov’s gradient descent methods for optimizing the trace-norm.\nWhile the soft-thresholding based methods for trace-norm minimization are signiﬁcantly faster than\nSDP based approaches, they suffer from slow convergence (see Figure 2 (d)). Also, noisy measure-\nments pose considerable computational challenges for trace-norm optimization as the rank of the\nintermediate iterates can become very large (see Figure 3(b)).\n2We call our method SVP-Newton as the Newton method when applied to a quadratic objective function\nleads to the exact solution by solving the resulting least squares problem.\n3As a side note, we can show a stronger result for SVP-Newton when applied to the special case of\ncompressed-sensing, i.e., when the matrix X is restricted to be diagonal. Speciﬁcally, we can show that under\ncertain assumptions SVP-Newton converges to the optimal solution in O(log k), improving upon the result of\nMaleki [21]. We give the precise statement of the theorem and proof in the supplementary material.\n6\n40 60 80 100 120 140 16010\n0\n10\n2\n10\n4\nn (Size of Matrix)\n \n \nARMP: Random Instances\nSVP\nSVT\n600 800 1000 1200 1400 16000\n2\n4\n6\n8\n10\n12\nARMP: MIT Logo\nNumber of Constraints\nError (Frobenius Norm)\n \n \nSVP\nSVT\n1000 2000 3000 4000 50000.02\n0.04\n0.06\n0.08\n0.1\nn (Size of the matrix)\nSVP Density Threshold\n \n \nk = 10, threshold p\nk=10, Cklog(n)/n\n1000 2000 3000 4000 50003.5\n4\n4.5\n5\n5.5\nIncoherence (SVP)\nn (Size of the Matrix)\nµ\n \n \np=.05\np=.15\np=.25\np=.35\n(a) (b) (c) (d)\nFigure 1: (a) Time taken by SVP and SVT for random instances of the Afﬁne Rank Minimization\nProblem (ARMP) with optimal rank k = 5. (b) Reconstruction error for the MIT logo. (c) Empirical\nestimates of the sampling density threshold required for exact matrix completion by SVP (here\nC = 1 .28). Note that the empirical bounds match the information theoretically optimal bound\nΘ(k log n/n). (d) Maximum incoherence maxt µ(X t) over the iterates of SVP for varying densities\np and sizes n. Note that the incoherence is bounded by a constant, supporting Conjecture 2.3.\n1000 2000 3000 4000 5000\n−1\n0\n1\n2\n3\nn (Size of Matrix)\n \n \nSVP−NewtonD\nSVP\nSVT\nALS\nADMiRA\nOPT\n1000 2000 3000 4000 50000\n1\n2\n3\n4x 10\n−3\nn (Size of Matrix)\nRMSE\n \n \nSVP−NewtonD\nSVP\nSVT\nALS\nADMiRA\nOPT\n2 4 6 8 1010\n0\n10\n1\n10\n2\n10\n3\nk (Rank of Matrix)\nTime Taken (secs)\n \n \nSVP−NewtonD\nSVP\nSVT\nALS\nADMiRA\nOPT\n1000 2000 3000 4000 50000\n50\n100\n150\n200\nn (Size of Matrix)\nNumber of Iterations\n \n \nSVP−NewtonD\nSVP\nSVT\n(a) (b) (c) (d)\nFigure 2: (a), (b) Running time (on log scale) and RMSE of various methods for matrix completion\nproblem with sampling density p = .1 and optimal rank k = 2 . (c) Running time ( on log scale ) of\nvarious methods for matrix completion with sampling density p = .1 and n = 1000. (d) Number of\niterations needed to get RMSE 0.001.\nFor the case of matrix completion, SVP has an important property facilitating fast computation\nof the main update in equation (2); each iteration of SVP involves computing the singular value\ndecomposition (SVD) of the matrix Y = X t + PΩ(X t − X ∗), where X t is a matrix of rank at\nmost k whose SVD is known and PΩ(X t − X ∗) is a sparse matrix. Thus, matrix-vector products\nof the form Y v can be computed in time O((m + n)k + |Ω|). This facilitates the use of fast SVD\ncomputing packages such as PROPACK [16] and ARPACK [19] that only require subroutines for\ncomputing matrix-vector products.\n4 Experimental Results\nIn this section, we empirically evaluate our methods for the afﬁne rank minimization problem and\nlow-rank matrix completion. For both problems we present empirical results on synthetic as well\nas real-world datasets. For ARMP we compare our method against the trace-norm based singular\nvalue thresholding (SVT) method [5]. Note that although Cai et al. present the SVT algorithm in the\ncontext of MCP , it can be easily adapted for ARMP. For MCP we compare against SVT, ADMiRA\n[17], the OptSpace (OPT) method of Keshavan et al. [14], and regularized alternating least squares\nminimization (ALS). We use our own implementation of SVT for ARMP and ALS, while for matrix\ncompletion we use the code provided by the respective authors for SVT, ADMiRA and OPT. We\nreport results averaged over 20 runs. All the methods are implemented in Matlab and use mex ﬁles.\n4.1 Afﬁne Rank Minimization\nWe ﬁrst compare our method against SVT on random instances of ARMP. We generate random\nmatrices X ∈ Rn×n of different sizes n and ﬁxed rank k = 5 . We then generate d = 6 kn random\nafﬁne constraint matrices Ai and compute b = A(X). Figure 1(a) compares the computational time\nrequired by SVP and SVT (in log-scale) for achieving a relative error (∥A(X) − b∥2/∥b∥2) of 10−3,\nand shows that our method requires many fewer iterations and is signiﬁcantly faster than SVT.\nNext we evaluate our method for the problem of matrix reconstruction from random measurements.\nAs in Recht et al. [24], we use the MIT logo as the test image for reconstruction. The MIT logo\nwe use is a 38 × 73 image and has rank four. For reconstruction, we generate random measurement\nmatrices Ai and measure bi = T r(AiX). We let both SVP and SVT converge and then compute the\nreconstruction error for the original image. Figure 1 (b) shows that our method incurs signiﬁcantly\nsmaller reconstruction error than SVT for the same number of measurements.\nMatrix Completion: Synthetic Datasets (Uniform Sampling)\nWe now evaluate our method against various matrix completion methods for random low-rank ma-\n7\nk SVP-NewtonD SVP ALS SVT\n2 0.90 1.15 0.88 1.06\n3 0.89 1.14 0.87 0.98\n5 0.89 1.09 0.86 0.95\n7 0.89 1.08 0.86 0.93\n10 0.90 1.07 0.87 0.91\n12 0.91 1.08 0.88 0.90\n(a)\n1000 2000 3000 4000 500010\n0\n10\n1\n10\n2\n10\n3\nn (Size of Matrix)\nTime Taken (secs)\n \n \nSVP−NewtonD\nSVP\nSVT\nALS\n500 1000 1500 20000\n0.5\n1\n1.5\n2\n2.5\nn (Size of Matrix)\nRMSE\n \n \nICMC\nALS\nSVT\nSVP\nSVP NewtonD\n500 1000 1500 20000\n1\n2\n3\n4\nn (Size of Matrix)\nRMSE\n \n \nICMC\nALS\nSVT\nSVP\nSVP NewtonD (b) (c) (d)\nFigure 3: (a): RMSE incurred by various methods for matrix completion with different rank ( k)\nsolutions on Movie-Lens Dataset. (b): Time( on log scale ) required by various methods for matrix\ncompletion with p = .1, k = 2 and 10% Gaussian noise. Note that all the four methods achieve\nsimilar RMSE. (c): RMSE incurred by various methods for matrix completion with p = 0.1, k = 10\nwhen the sampling distribution follows Power-law distribution (Chung-Lu-Vu Model). (d): RMSE\nincurred for the same problem setting as plot (c) but with added Gaussian noise.\ntrices and uniform samples. We generate a random rank k matrix X ∈ Rn×n and generate random\nBernoulli samples with probability p. Figure 2 (a) compares the time required by various methods\n(in log-scale) to obtain a root mean square error (RMSE) of 10−3 on the sampled entries for ﬁxed\nk = 2 . Clearly, SVP is substantially faster than the other methods. Next, we evaluate our method\nfor increasing k. Figure 2 (b) compares the overall RMSE obtained by various methods. Note that\nSVP-Newton is signiﬁcantly more accurate than both SVP and SVT. Figure 2 (c) compares the time\nrequired by various methods to obtain a root mean square error (RMSE) of 10−3 on the sampled\nentries for ﬁxed n = 1000 and increasing k. Note that our algorithms scale well with increasing k\nand are faster than other methods. Next, we analyze reasons for better performance of our methods.\nTo this end, we plot the number of iterations required by our methods as compared to SVT (Fig-\nure 2 (d)). Note that even though each iteration of SVT is almost as expensive as our methods’, our\nmethods converge in signiﬁcantly fewer iterations.\nFinally, we study the behavior of our method in presence of noise. For this experiment, we generate\nrandom matrices of different size and add approximately 10% Gaussian noise. Figure 2 (c) plots\ntime required by various methods as n increases from 1000 to 5000. Note that SVT is particularly\nsensitive to noise. One of the reason for this is that due to noise, the rank of the intermediate iterates\narising in SVT can be fairly large.\nMatrix Completion: Synthetic Dataset (Power-law Sampling) We now evaluate our methods\nagainst existing matrix-completion methods under more realistic power-law distributed samples.\nAs before, we generate a random rank- k = 10 matrix X ∈ Rn×n and sample the entries of X\nusing a graph generated using Chung-Lu-Vu model with power-law distributed degrees (see [23])\nfor details. Figure 3 (c) plots the RMSE obtained by various methods for varying n and ﬁxed\nsampling density p = 0 .1. Note that SVP-NewtonD performs signiﬁcantly better than SVT as well\nas SVP . Figure 3 (d) plots the RMSE obtained by various methods when each sampled entry is\ncorrupted with around 1% Gaussian noise. Note that here again SVP-NewtonD performs similar to\nALS and is signiﬁcantly better than the other methods including the ICMC method [23] which is\nspecially designed for power-law sampling but is quite sensitive to noise.\nMatrix Completion: Movie-Lens Dataset\nFinally, we evaluate our method on the Movie-Lens dataset [1], which contains 1 million ratings for\n3900 movies by 6040 users. Figure 3 (a) shows the RMSE obtained by each method with varying k.\nFor SVP and SVP-Newton, we ﬁx step size to be η = 1/p\n√\n(t), where t is the number of iterations.\nFor SVT, we ﬁx δ = .2p using cross-validation. Since, rank cannot be ﬁxed in SVT, we try various\nvalues for the parameter τ to obtain the desired rank solution. Note that SVP-Newton incurs a\nRMSE of 0.89 for k = 3. In contrast, SVT achieves a RMSE of 0.98 for the same rank. We remark\nthat SVT was able to achieve RMSE up to 0.89 but required rank 17 solution and was signiﬁcantly\nslower in convergence because many intermediate iterates had large rank (up to around 150). We\nattribute the relatively poor performance of SVP and SVT as compared with ALS and SVP-Newton\nto the fact that the ratings matrix is not sampled uniformly, thus violating the crucial assumption of\nuniformly distributed samples.\nAcknowledgements: This research was supported in part by NSF grant CCF-0728879.\n8\nReferences\n[1] Movie lens dataset. Public dataset. URL http://www.grouplens.org/taxonomy/term/14.\n[2] K. Arrow, L. Hurwicz, and H. Uzawa. Studies in Linear and Nonlinear Programming. Stanford University\nPress, Stanford, 1958.\n[3] Robert Bell and Y ehuda Koren. Scalable collaborative ﬁltering with jointly derived neighborhood inter-\npolation weights. In ICDM, pages 43–52, 2007. doi: 10.1109/ICDM.2007.90.\n[4] Matthew Brand. Fast online SVD revisions for lightweight recommender systems. In SIAM International\nConference on Data Mining , 2003.\n[5] Jian-Feng Cai, Emmanuel J. Cand `es, and Zuowei Shen. A singular value thresholding algorithm for\nmatrix completion. SIAM Journal on Optimization , 20(4):1956–1982, 2010.\n[6] Emmanuel J. Cand`es and Benjamin Recht. Exact matrix completion via convex optimization. F oundations\nof Computational Mathematics, 9(6):717–772, December 2009.\n[7] Emmanuel J. Cand `es and Terence Tao. The power of convex relaxation: Near-optimal matrix completion.\nIEEE Trans. Inform. Theory, 56(5):2053–2080, 2009.\n[8] M. Fazel, H. Hindi, and S. Boyd. A rank minimization heuristic with application to minimum order\nsystem approximation. In American Control Conference, Arlington, Virginia, 2001.\n[9] M. Fazel, H. Hindi, and S. Boyd. Log-det heuristic for matrix rank minimization with applications to\nHankel and Euclidean distance matrices. In American Control Conference, 2003.\n[10] M. Fazel, E. Candes, B. Recht, and P . Parrilo. Compressed sensing and robust recovery of low rank\nmatrices. In Signals, Systems and Computers, 2008 42nd Asilomar Conference on , pages 1043–1047,\nOct. 2008. doi: 10.1109/ACSSC.2008.5074571.\n[11] Rahul Garg and Rohit Khandekar. Gradient descent with sparsiﬁcation: an iterative algorithm for sparse\nrecovery with restricted isometry property. In ICML, 2009.\n[12] Donald Goldfarb and Shiqian Ma. Convergence of ﬁxed point continuation algorithms for matrix rank\nminimization, 2009. Submitted.\n[13] Shuiwang Ji and Jieping Y e. An accelerated gradient method for trace norm minimization. In ICML,\n2009.\n[14] Raghunandan H. Keshavan, Sewoong Oh, and Andrea Montanari. Matrix completion from a few entries.\nIn ISIT’09: Proceedings of the 2009 IEEE international conference on Symposium on Information Theory,\npages 324–328, Piscataway, NJ, USA, 2009. IEEE Press. ISBN 978-1-4244-4312-3.\n[15] Y ehuda Koren. Factorization meets the neighborhood: a multifaceted collaborative ﬁltering model. In\nKDD, pages 426–434, 2008. doi: 10.1145/1401890.1401944.\n[16] R.M. Larsen. Propack: a software for large and sparse SVD calculations. Available online. URL http:\n//sun.stanford.edu/rmunk/PROPACK/.\n[17] Kiryung Lee and Y oram Bresler. Admira: Atomic decomposition for minimum rank approximation, 2009.\n[18] Kiryung Lee and Y oram Bresler. Guaranteed minimum rank approximation from linear observations by\nnuclear norm minimization with an ellipsoidal constraint, 2009.\n[19] Richard B. Lehoucq, Danny C. Sorensen, and Chao Y ang. ARPACK Users’ Guide: Solution of Large-\nScale Eigenvalue Problems with Implicitly Restarted Arnoldi Methods . SIAM, Philadelphia, 1998.\n[20] S. Ma, D. Goldfarb, and L. Chen. Fixed point and bregman iterative methods for matrix rank minimiza-\ntion. To appear , Mathematical Programming Series A, 2010.\n[21] Arian Maleki. Coherence analysis of iterative thresholding algorithms. CoRR, abs/0904.1193, 2009.\n[22] Raghu Meka, Prateek Jain, Constantine Caramanis, and Inderjit S. Dhillon. Rank minimization via online\nlearning. In ICML, pages 656–663, 2008. doi: 10.1145/1390156.1390239.\n[23] Raghu Meka, Prateek Jain, and Inderjit S. Dhillon. Matrix completion from power-law distributed sam-\nples. In NIPS, 2009.\n[24] Benjamin Recht, Maryam Fazel, and Pablo A. Parrilo. Guaranteed minimum-rank solutions of linear\nmatrix equations via nuclear norm minimization, 2007. To appear in SIAM Review.\n[25] K.C. Toh and S. Y un. An accelerated proximal gradient algorithm for nuclear norm regularized least\nsquares problems. Preprint, 2009. URL http://www.math.nus.edu.sg/˜matys/apg.pdf.\n9",
  "values": {
    "Interpretable (to users)": "No",
    "Transparent (to users)": "No",
    "Autonomy (power to decide)": "No",
    "Critiqability": "No",
    "Respect for Law and public interest": "No",
    "Explicability": "No",
    "Non-maleficence": "No",
    "Not socially biased": "No",
    "User influence": "No",
    "Deferral to humans": "No",
    "Privacy": "No",
    "Respect for Persons": "No",
    "Beneficence": "No",
    "Collective influence": "No",
    "Fairness": "No",
    "Justice": "No"
  }
}