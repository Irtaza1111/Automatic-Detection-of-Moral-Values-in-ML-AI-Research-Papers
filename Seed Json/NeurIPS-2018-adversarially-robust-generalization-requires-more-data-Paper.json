{
  "pdf": "NeurIPS-2018-adversarially-robust-generalization-requires-more-data-Paper",
  "title": "NeurIPS-2018-adversarially-robust-generalization-requires-more-data-Paper",
  "author": "Unknown",
  "paper_id": "NeurIPS-2018-adversarially-robust-generalization-requires-more-data-Paper",
  "text": "Adversarially Robust Generalization\nRequires More Data\nLudwig Schmidt\nUC Berkeley\nludwig@berkeley.edu\nShibani Santurkar\nMIT\nshibani@mit.edu\nDimitris Tsipras\nMIT\ntsipras@mit.edu\nKunal Talwar\nGoogle Brain\nkunal@google.com\nAleksander M ˛ adry\nMIT\nmadry@mit.edu\nAbstract\nMachine learning models are often susceptible to adversarial perturbations of their\ninputs. Even small perturbations can cause state-of-the-art classiﬁers with high\n“standard” accuracy to produce an incorrect prediction with high conﬁdence. To\nbetter understand this phenomenon, we study adversarially robust learning from the\nviewpoint of generalization. We show that already in a simple natural data model,\nthe sample complexity of robust learning can be signiﬁcantly larger than that of\n“standard” learning. This gap is information theoretic and holds irrespective of the\ntraining algorithm or the model family. We complement our theoretical results with\nexperiments on popular image classiﬁcation datasets and show that a similar gap\nexists here as well. We postulate that the difﬁculty of training robust classiﬁers\nstems, at least partially, from this inherently larger sample complexity.\n1 Introduction\nModern machine learning models achieve high accuracy on a broad range of datasets, yet can easily\nbe misled by small perturbations of their input. While such perturbations are often simple noise to a\nhuman or even imperceptible, they cause state-of-the-art models to misclassify their input with high\nconﬁdence. This phenomenon has ﬁrst been studied in the context of secure machine learning for\nspam ﬁlters and malware classiﬁcation [7, 16, 35]. More recently, researchers have demonstrated\nthe phenomenon under the name of adversarial examples in image classiﬁcation [21, 51], question\nanswering [28], voice recognition [12, 13, 49, 62], and other domains (for instance, see [ 2, 4, 14,\n22, 25, 26, 32, 60]). Overall, the existence of such adversarial examples raises concerns about the\nrobustness of current classiﬁers. As we increasingly deploy machine learning systems in safety- and\nsecurity-critical environments, it is crucial to understand the robustness properties of our models in\nmore detail.\nA growing body of work is exploring this robustness question from the security perspective by\nproposing attacks (methods for crafting adversarial examples) and defenses (methods for making\nclassiﬁers robust to such perturbations). Often, the focus is on deep neural networks, e.g., see [11, 24,\n36, 37, 41, 47, 53, 59]. While there has been success with robust classiﬁers on simple datasets [31,\n36, 44, 48], more complicated datasets still exhibit a large gap between “standard” and robust\naccuracy [3, 11]. An implicit assumption underlying most of this work is that the same training\ndataset that enables good standard accuracy also sufﬁces to train a robust model. However, it is\nunclear if this assumption is valid.\n32nd Conference on Neural Information Processing Systems (NeurIPS 2018), Montréal, Canada.\nSo far, the generalization aspects of adversarially robust classiﬁcation have not been thoroughly\ninvestigated. Since adversarial robustness is a learning problem, the statistical perspective is of\nintegral importance. A key observation is that adversarial examples are not at odds with the standard\nnotion of generalization as long as they occupy only a small total measure under the data distribution.\nSo to achieve adversarial robustness, a classiﬁer must generalize in a stronger sense. We currently do\nnot have a good understanding of how such a stronger notion of generalization compares to standard\n“benign” generalization, i.e., without an adversary.\nIn this work, we address this gap and explore the statistical foundations of adversarially robust\ngeneralization. We focus on sample complexity as a natural starting point since it underlies the core\nquestion of when it is possible to learn an adversarially robust classiﬁer. Concretely, we pose the\nfollowing question:\nHow does the sample complexity of standard generalization compare to that of\nadversarially robust generalization?\nPut differently, we ask if a dataset that allows for learning a good classiﬁer also sufﬁces for learning a\nrobust one. To study this question, we analyze robust generalization in two distributional models. By\nfocusing on speciﬁc distributions, we can establish information-theoretic lower bounds and describe\nthe exact sample complexity requirements for generalization. We ﬁnd that even for a simple data\ndistribution such as a mixture of two class-conditional Gaussians, the sample complexity of robust\ngeneralization is signiﬁcantly larger than that of standard generalization. Our lower bound holds for\nany model and learning algorithm. Hence no amount of algorithmic ingenuity is able to overcome\nthis limitation.\nIn spite of this negative result, simple datasets such as MNIST have recently seen signiﬁcant progress\nin terms of adversarial robustness [31, 36, 44, 48]. The most robust models achieve accuracy around\n90% against largeℓ∞-perturbations. To better understand this discrepancy with our ﬁrst theoretical\nresult, we also study a second distributional model with binary features. This binary data model\nhas the same standard generalization behavior as the previous Gaussian model. Moreover, it also\nsuffers from a signiﬁcantly increased sample complexity whenever one employslinear classiﬁers\nto achieve adversarially robust generalization. Nevertheless, a slightly non-linear classiﬁer that\nutilizes thresholding turns out to recover the smaller sample complexity of standard generalization.\nSince MNIST is a mostly binary dataset, our result provides evidence thatℓ∞-robustness on MNIST\nis signiﬁcantly easier than on other datasets. Moreover, our results show that distributions with\nsimilar sample complexity for standard generalization can still exhibit considerably different sample\ncomplexity for robust generalization.\nTo complement our theoretical results, we conduct a range of experiments on MNIST, CIFAR10,\nand SVHN. By subsampling the datasets at various rates, we study the impact of sample size\non adversarial robustness. When plotted as a function of training set size, our results show that\nthe standard accuracy on SVHN indeed plateaus well before the adversarial accuracy reaches its\nmaximum. On MNIST, explicitly adding thresholding to the model during training signiﬁcantly\nreduces the sample complexity, similar to our upper bound in the binary data model. On CIFAR10,\nthe situation is more nuanced because there are no known approaches that achieve more than 50%\naccuracy even against a mild adversary. But as we show below, there is clear evidence for overﬁtting\nin the current state-of-the-art methods.\nOverall, our results suggest that current approaches may be unable to attain higher adversarial\naccuracy on datasets such as CIFAR10 for a fundamental reason: the dataset may not be large\nenough to train a standard convolutional network robustly. Moreover, our lower bounds illustrate\nthat the existence of adversarial examples should not necessarily be seen as a shortcoming of speciﬁc\nclassiﬁcation methods. Already in a simple data model, adversarial examples provably occur for\nany learning approach, even when the classiﬁer already achieves high standard accuracy. So while\nvulnerability to adversarialℓ∞-perturbations might seem counter-intuitive at ﬁrst, in some regimes it\nis an unavoidable consequence of working in a statistical setting.\n1.1 A motivating example: Overﬁtting on CIFAR10\nBefore we describe our main results, we brieﬂy highlight the importance of generalization for\nadversarial robustness via two experiments on MNIST and CIFAR10. In both cases, our goal is to\nlearn a classiﬁer that achieves good test accuracy even underℓ∞-bounded perturbations. We follow\n2\n0 20000 40000 60000\nTraining Steps\n0\n20\n40\n60\n80\n100Accuracy\nMNIST\nAdversarial train\nAdversarial test\nStandard test\n0 20000 40000 60000 80000\nTraining Steps\n0\n20\n40\n60\n80\n100\nCIFAR10\nAdversarial train\nAdversarial test\nStandard test\nFigure 1: Classiﬁcation accuracies for robust optimization on MNIST and CIFAR10. In both cases,\nwe trained standard convolutional networks to be robust toℓ∞-perturbations of the input. On MNIST,\nthe robust test error closely tracks the corresponding training error and the model achieves high robust\naccuracy. On CIFAR10, the model still achieves a good natural (non-adversarial) test error, but there\nis a signiﬁcant generalization gap for the robust accuracy. This phenomenon motivates our study of\nadversarially robust generalization.\nthe standard robust optimization approach [6, 36, 54] – also known as adversarial training [21, 51] –\nand (approximately) solve the saddle point problem\nmin\nθ E\nx\n[\nmax\n∥x′−x∥∞≤ε\nloss(θ,x′)\n]\nvia stochastic gradient descent over the model parametersθ. We utilize projected gradient descent\nfor the inner maximization problem over allowed perturbations of magnitudeε (see [36] for details).\nFigure 1 displays the training curves for three quantities: (i) adversarial training error, (ii) adversarial\ntest error, and (iii) standard test error.\nThe results show that on MNIST, robust optimization is able to learn a model with around 90%\nadversarial accuracy and a relatively small gap between training and test error. However, CIFAR10\noffers a different picture. Here, the model (a wide residual network [61]) is still able to fully ﬁt the\ntraining set even against an adversary, but the generalization gap is signiﬁcantly larger. The model\nonly achieves 47% adversarial test accuracy, which is about 50% lower than its training accuracy.1\nMoreover, the standard test accuracy is about 87%, so the failure of generalization indeed primarily\noccurs in the context of adversarial robustness. This failure may be surprising particularly since\nproperly tuned convolutional networks rarely overﬁt much on standard vision datasets.\n1.2 Outline of the paper\nIn the next section, we describe our main theoretical results at a high level. Section 3 complements\nthese results with experiments. We discuss related works in Section 4 and conclude in Section 5. Due\nto space constraints, a longer discussion of related work, several open questions, and all proofs are\ndeferred to the appendix in the supplementary material.\n2 Theoretical Results\nOur theoretical results concern statistical aspects of adversarially robust classiﬁcation. In order to\nunderstand how properties of data affect the number of samples needed for robust generalization, we\nstudy two concrete distributional models.\nWhile our two data models are clearly much simpler than the image datasets currently being used in\nthe experimental work onℓ∞-robustness, we believe that the simplicity of our models is a strength in\nthis context. The fact that we can establish a separation between standard and robust generalization\nalready in our Gaussian data model is evidence that the existence of adversarial examples for neural\n1We remark that this accuracy is still currently the best published robust accuracy on CIFAR10 [ 3]. For\ninstance, contemporary approaches to architecture tuning do not yield better robust accuracies [15].\n3\nnetworks should not come as a surprise. The same phenomenon (i.e., classiﬁers with just enough\nsamples for high standard accuracy necessarily being vulnerable toℓ∞- attacks) already occurs in\nmuch simpler settings such as a mixture of two Gaussians. Note that more complicated distributional\nsetups that can “simulate” the Gaussian model directly inherit our lower bounds.\nIn addition, conclusions from our simple models also transfer to real datasets. As we describe\nin the subsection on the Bernoulli model, the beneﬁts of the thresholding layer predicted by our\ntheoretical analysis do indeed appear in experiments on MNIST as well. Since multiple defenses\nagainst adversarial examples have been primarily evaluated on MNIST [31, 44, 48], it is important to\nnote thatℓ∞-robustness on MNIST is a particularly easy case: adding a simple thresholding layer\ndirectly yields nearly state-of-the-art robustness against moderately strong adversaries (ε = 0.1),\nwithout any further changes to the model architecture or training algorithm.\n2.1 The Gaussian model\nOur ﬁrst data model is a mixture of two spherical Gaussians with one component per class.\nDeﬁnition 1 (Gaussian model). Letθ⋆∈ Rd be the per-class mean vector and let σ > 0 be the\nvariance parameter. Then the (θ⋆,σ )-Gaussian model is deﬁned by the following distribution over\n(x,y )∈ Rd×{± 1}: First, draw a labely∈{± 1} uniformly at random. Then sample the data point\nx∈ Rd fromN (y·θ⋆,σ 2I).\nWhile not explicitly speciﬁed in the deﬁnition, we will use the Gaussian model in the regime where\nthe norm of the vectorθ⋆ is approximately\n√\nd. Hence the main free parameter for controlling the\ndifﬁculty of the classiﬁcation task is the varianceσ2, which controls the amount of overlap between\nthe two classes.\nTo contrast the notions of “standard” and “robust” generalization, we brieﬂy recap a standard deﬁnition\nof classiﬁcation error.\nDeﬁnition 2 (Classiﬁcation error). LetP : Rd×{±1}→ R be a distribution. Then the classiﬁcation\nerrorβ of a classiﬁerf : Rd→{± 1} is deﬁned asβ = P(x,y)∼P[f(x)̸=y].\nNext, we deﬁne our main quantity of interest, which is an adversarially robust counterpart of the\nabove classiﬁcation error. Instead of counting misclassiﬁcations under the data distribution, we allow\na bounded worst-case perturbation before passing the perturbed sample to the classiﬁer.\nDeﬁnition 3 (Robust classiﬁcation error). LetP : Rd×{± 1} →R be a distribution and let\nB : Rd→ P(Rd) be a perturbation set.2 Then theB-robust classiﬁcation errorβ of a classiﬁer\nf : Rd→{± 1} is deﬁned asβ = P(x,y)∼P[∃x′∈B (x) : f(x′)̸=y].\nSinceℓ∞-perturbations have recently received a signiﬁcant amount of attention, we focus on ro-\nbustness toℓ∞-bounded adversaries in our work. For this purpose, we deﬁne the perturbation set\nBε\n∞(x) ={x′∈ Rd|∥x′−x∥∞≤ε}. To simplify notation, we refer to robustness with respect to\nthis set also asℓε\n∞-robustness. As we remark in the discussion section, understanding generalization\nfor other measures of robustness (ℓ2, rotations, etc.) is an important direction for future work.\nStandard generalization. The Gaussian model has one parameter for controlling the difﬁculty of\nlearning a good classiﬁer. In order to simplify the following bounds, we study a regime where it is\npossible to achieve good standard classiﬁcation error from a single sample.3 As we will see later,\nthis also allows us to calibrate our two data models to have comparable standard sample complexity.\nConcretely, we prove the following theorem, which is a direct consequence of Gaussian concentration.\nNote that in this theorem we use a linear classiﬁer: for a vectorw, the linear classiﬁerfw : Rd→\n{±1} is deﬁned asfw(x) = sgn(⟨w,x⟩).\nTheorem 4. Let (x,y ) be drawn from a (θ⋆,σ )-Gaussian model with∥θ⋆∥2 =\n√\nd andσ≤ c·d\n1/4\nwherec is a universal constant. Let ˆw∈ Rd be the vector ˆw =y·x. Then with high probability, the\nlinear classiﬁerf ˆw has classiﬁcation error at most 1%.\n2We write P(Rd) to denote the power set of Rd, i.e., the set of subsets of Rd.\n3We remark that it is also possible to study a more general setting where standard generalization requires a\nlarger number of samples.\n4\nTo minimize the number of parameters in our bounds, we have set the error probability to 1%.\nBy tuning the model parameters appropriately, it is possible to achieve a vanishingly small error\nprobability from a single sample (see Corollary 19 in Appendix D.1).\nRobust generalization. As we just demonstrated, we can easily achieve standard generalization\nfrom only a single sample in our Gaussian model. We now show that achieving a low ℓ∞-robust\nclassiﬁcation error requires signiﬁcantly more samples. To this end, we begin with a natural strength-\nening of Theorem 4 and prove that the (class-weighted) sample mean can also be a robust classiﬁer\n(given sufﬁcient data).\nTheorem 5. Let (x1,y 1),..., (xn,yn) be drawn i.i.d. from a (θ⋆,σ )-Gaussian model with∥θ⋆∥2 =√\nd andσ≤c1d\n1/4. Let ˆw∈ Rd be the weighted mean vector ˆw = 1\nn\n∑n\ni=1yixi. Then with high\nprobability, the linear classiﬁerf ˆw hasℓε\n∞-robust classiﬁcation error at most 1% if\nn ≥\n{1 for ε≤ 1\n4d−1/4\nc2ε2√\nd for 1\n4d−1/4≤ ε≤ 1\n4\n.\nWe refer the reader to Corollary 22 in Appendix D.1 for the details. As before, c1 andc2 are two\nuniversal constants. Overall, the theorem shows that it is possible to learn an ℓε\n∞-robust classiﬁer\nin the Gaussian model as long asε is bounded by a small constant and we have a large number of\nsamples.\nNext, we show that this signiﬁcantly increased sample complexity is necessary. Our main theorem\nestablishes a lower bound for all learning algorithms, which we formalize as functions from data\nsamples to binary classiﬁers. In particular, the lower bound applies not only to learning linear\nclassiﬁers.\nTheorem 6. Letgn be any learning algorithm, i.e., a function fromn samples to a binary classiﬁer\nfn. Moreover, letσ =c1·d\n1/4, letε≥ 0, and letθ∈ Rd be drawn fromN (0,I ). We also drawn\nsamples from the (θ,σ )-Gaussian model. Then the expectedℓε\n∞-robust classiﬁcation error offn is at\nleast (1− 1/d) 1\n2 if\nn ≤ c2\nε2√\nd\nlogd .\nThe proof of the theorem can be found in Corollary 23 (Appendix D.2). It is worth noting that the\nclassiﬁcation error1/2 in the lower bound is tight. A classiﬁer that always outputs a ﬁxed prediction\ntrivially achieves perfect robustness on one of the two classes and hence robust accuracy 1/2.\nComparing Theorems 5 and 6, we see that the sample complexityn required for robust generalization\nis bounded as\ncε2√\nd\nlogd ≤ n ≤ c′ε2√\nd.\nHence the lower bound is nearly tight in our regime of interest. When the perturbation has constant\nℓ∞-norm, the sample complexity of robust generalization is larger than that of standard generalization\nby\n√\nd, i.e., polynomial in the problem dimension. This shows that for high-dimensional problems,\nadversarial robustness can provably require a signiﬁcantly larger number of samples.\nFinally, we remark that our lower bound applies also to a more restricted adversary. Our proof uses\nonly a single adversarial perturbation per class. As a result, the lower bound provides transferable ad-\nversarial examples and applies to worst-case distribution shifts without a classiﬁer-adaptive adversary.\nWe refer the reader to Section 5 for a more detailed discussion.\n2.2 The Bernoulli model\nAs mentioned in the introduction, simpler datasets such as MNIST have recently seen signiﬁcant\nprogress in terms of ℓ∞-robustness. We now investigate a possible mechanism underlying these\nadvances. To this end, we study a second distributional model that highlights how the data distribution\ncan signiﬁcantly affect the achievable robustness. The second data model is deﬁned on the hypercube\n{±1}d, and the two classes are represented by opposite vertices of that hypercube. When sampling a\ndatapoint for a given class, we ﬂip each bit of the corresponding class vertex with a certain probability.\nThis data model is inspired by the MNIST dataset because MNIST images are close to binary (many\npixels are almost fully black or white).\n5\nDeﬁnition 7 (Bernoulli model). Letθ⋆∈{± 1}d be the per-class mean vector and letτ >0 be the\nclass bias parameter. Then the (θ⋆,τ )-Bernoulli model is deﬁned by the following distribution over\n(x,y )∈{± 1}d×{± 1}: First, draw a labely∈{± 1} uniformly at random from its domain. Then\nsample the data pointx∈{± 1}d by sampling each coordinatexi from the distribution\nxi =\n{ y·θ⋆\ni with probability 1/2 +τ\n−y·θ⋆\ni with probability 1/2−τ .\nAs in the previous subsection, the model has one parameter for controlling the difﬁculty of learning.\nA small value ofτ makes the samples less correlated with their respective class vectors and hence\nleads to a harder classiﬁcation problem. Note that both the Gaussian and the Bernoulli model are\ndeﬁned by simple sub-Gaussian distributions. Nevertheless, we will see that they differ signiﬁcantly\nin terms of robust sample complexity.\nStandard generalization. As in the Gaussian model, we ﬁrst calibrate the distribution so that we\ncan learn a classiﬁer with goodstandard accuracy from a single sample.4 The following theorem is a\ndirect consequence of the fact that bounded random variables exhibit sub-Gaussian concentration.\nTheorem 8. Let (x,y ) be drawn from a (θ⋆,τ )-Bernoulli model with τ ≥ c·d−1/4 wherec is a\nuniversal constant. Let ˆw∈ Rd be the vector ˆw = y·x. Then with high probability, the linear\nclassiﬁerf ˆw has classiﬁcation error at most 1%.\nTo simplify the bound, we have set the error probability to be 1% as in the Gaussian model. We refer\nthe reader to Corollary 28 in Appendix F.1 for the proof.\nRobust generalization. Next, we investigate the sample complexity of robust generalization in\nour Bernoulli model. For linear classiﬁers, a small robust classiﬁcation error again requires a large\nnumber of samples:\nTheorem 9. Letgn be a linear classiﬁer learning algorithm, i.e., a function from n samples to a\nlinear classiﬁerfn. Suppose that we chooseθ⋆ uniformly at random from{±1}d and drawn samples\nfrom the (θ⋆,τ )-Bernoulli model withτ =c1·d−1/4. Moreover, letε< 3τ and 0<γ < 1/2. Then\nthe expectedℓε\n∞-robust classiﬁcation error offn is at least 1\n2−γ if\nn ≤ c2\nε2γ2d\nlogd/γ\n.\nWe defer the proof to Appendix F.2. At ﬁrst, the lower bound for linear classiﬁers might suggest that\nℓ∞-robustness requires an inherently larger sample complexity here as well. However, in contrast\nto the Gaussian model, non-linear classiﬁers can achieve a signiﬁcantly improved robustness. In\nparticular, consider the following thresholding operationT : Rd→ Rd which is deﬁned element-wise\nas\nT (x)i =\n{+1 ifxi≥ 0\n−1 otherwise.\nIt is easy to see that forε< 1, the thresholding operator undoes the action of anyℓ∞-bounded adver-\nsary, i.e., we haveT (Bε\n∞(x)) ={x} for anyx∈{± 1}d. Hence we can combine the thresholding\noperator with the classiﬁer learned from a single sample to get the following upper bound.\nTheorem 10. Let (x,y ) be drawn from a (θ⋆,τ )-Bernoulli model withτ ≥ c·d−1/4 wherec is a\nuniversal constant. Let ˆw∈ Rd be the vector ˆw = yx. Then with high probability, the classiﬁer\nf ˆw◦T hasℓε\n∞-robust classiﬁcation error at most 1% for anyε< 1.\nThis theorem shows a stark contrast to the Gaussian case. Although both models have similar sample\ncomplexity for standard generalization, there is a\n√\nd gap between theℓ∞-robust sample complexity\nfor the Bernoulli and Gaussian models. This discrepancy provides evidence that robust generalization\nrequires a more nuanced understanding of the data distribution than standard generalization.\n4To be precise, the two distributions have comparable sample complexity for standard generalization in the\nregime whereσ≈τ −1.\n6\n103 104\nTraining Set Size\n0\n20\n40\n60\n80\n100Test Accuracy (%)\nMNIST\ntest=0\ntest=0.1\ntest=0.2\ntest=0.3\n103 104\nTraining Set Size\n0\n20\n40\n60\n80\n100Test Accuracy (%)\nCIFAR-10\ntest=0\ntest=2\ntest=4\ntest=8\n103 104 105\nTraining Set Size\n0\n20\n40\n60\n80\n100Test Accuracy (%)\nSVHN\ntest=0\ntest=1\ntest=2\ntest=4\nFigure 2: Adversarially robust generalization performance as a function of training data size for\nℓ∞ adversaries on the MNIST, CIFAR-10 and SVHN datasets. For each choice of training set size\nandεtest, we plot the best performance achieved over εtrain and network capacity. This clearly\nshows that achieving a certain level of adversarially robust generalization requires signiﬁcantly more\nsamples than achieving the same level of standard generalization.\nIn isolation, the thresholding step might seem speciﬁc to the Bernoulli model studied here. However,\nour experiments in Section 3 show that an explicit thresholding layer also signiﬁcantly improves the\nsample complexity of training a robust neural network on MNIST. We conjecture that the effectiveness\nof thresholding is behind many of the successful defenses against adversarial examples on MNIST\n(for instance, see Appendix C in [36]).\n3 Experiments\nWe complement our theoretical results by performing experiments on multiple common datasets. We\nconsider standard convolutional neural networks and train models on datasets of varying complexity.\nSpeciﬁcally, we study the MNIST [34], CIFAR-10 [33], and SVHN [40] datasets. We use a simple\nconvolutional architecture for MNIST, a standard ResNet model [23] for CIFAR-10, and a wider\nResNet [61] for SVHN. We perform robust optimization to train our classiﬁers on perturbations\ngenerated by projected gradient descent. Appendix G provides additional details for our experiments.\nEmpirical sample complexity evaluation. We study how the generalization performance of adver-\nsarially robust networks varies with the size of the training dataset. To do so, we train networks with\na speciﬁcℓ∞ adversary while reducing the size of the training set. The training subsets are produced\nby randomly sub-sampling the complete dataset in a class-balanced fashion. When increasing the\nnumber of samples, we ensure that each dataset is a superset of the previous one.\nWe evaluate the robustness of each trained network to perturbations of varying magnitude (εtest). For\neach choice of training set size N and ﬁxed attackεtest, we select the best performance achieved\nacross all hyperparameters settings (training perturbations εtrain and model size). On all three\ndatasets, we observed that the best standard accuracy is usually achieved for the standard trained\nnetwork, while the best adversarial accuracy for almost all values ofεtest was achieved when training\nwith the largestεtrain. We maximize over the hyperparameter settings since we are not interested in\nthe performance of a speciﬁc model, but rather in the inherent generalization properties of the dataset\nindependently of the classiﬁer used. Figure 2 shows the results of these experiments.\nThe plots demonstrate the need for more data to achieve adversarially robust generalization. For any\nﬁxed test set accuracy, the number of samples needed is signiﬁcantly higher for robust generalization.\nIn the SVHN experiments (where we have sufﬁcient training samples to observe plateauing behavior),\nthe standard accuracy reaches its maximum with signiﬁcantly fewer samples than the adversarial\naccuracy. We report more details of our experiments in Section H of the supplementary material.\nThresholding experiments. Motivated by our theoretical study of the Bernoulli model, we investi-\ngate whether thresholding can also improve the sample complexity of robust generalization against\nanℓ∞ adversary on MNIST.\n7\n103 104\nTraining Set Size\n0\n20\n40\n60\n80\n100Test Accuracy (%)\nStandard Training\n103 104\nTraining Set Size\n0\n20\n40\n60\n80\n100Test Accuracy (%)\nStandard Training + Thresholding\n103 104\nTraining Set Size\n0\n20\n40\n60\n80\n100Test Accuracy (%)\nAdversarial Training\n103 104\nTraining Set Size\n0\n20\n40\n60\n80\n100Test Accuracy (%)\nAdversarial Training + Thresholding\ntest=0\ntest=0.1\ntest=0.2\ntest=0.3\nFigure 3: Adversarial robustness to ℓ∞ attacks on the MNIST dataset for a simple convolution\nnetwork [36] with and without explicit thresholding ﬁlters. For each training set size choice and\nεtest, we report the best test set accuracy achieved over choice of thresholding ﬁlters andεtrain. We\nobserve that introducing thresholding ﬁlters signiﬁcantly reduces the number of samples needed to\nachieve good adversarial generalization.\nWe repeat the above sample complexity experiments with networks where thresholding ﬁlters are\nexplicitly encoded in the model. Here, we replace the ﬁrst convolutional layer with a ﬁxed thresholding\nlayer consisting of two channels, ReLU(x−εfilter ) and ReLU(1−x−εfilter ), wherex is the input\nimage. Figure 3 shows results for networks trained with such a thresholding layer. For standard\ntrained networks, we use a value ofεfilter = 0.1 for the thresholding ﬁlters, whereas for adversarially\ntrained networks we setεfilter =εtrain. For each data subset size and test perturbationεtest, we plot\nthe best test accuracy achieved over networks trained with different thresholding ﬁlters, i.e., different\nvalues ofε. We separately show the effect of explicit thresholding in such networks when they are\ntrained adversarially using PGD.\nAs predicted by our theory, the networks achieve good adversarially robust generalization with\nsigniﬁcantly fewer samples when thresholding ﬁlters are added. Further, note that adding a simple\nthresholding layer directly yields nearly state-of-the-art robustness against moderately strong adver-\nsaries (ε = 0.1), without any other modiﬁcations to the model architecture or training algorithm. It\nis also worth noting that the thresholding ﬁlters could have been learned by the original network\narchitecture, and that this modiﬁcation only decreases the capacity of the model. Our ﬁndings\nemphasize network architecture as a crucial factor for learning adversarially robust networks from a\nlimited number of samples.\nWe also experimented with thresholding ﬁlters on the CIFAR10 dataset, but did not observe any\nsigniﬁcant difference from the standard architecture. This agrees with our theoretical understanding\nthat thresholding helps primarily in the case of (approximately) binary datasets.\n4 Related Work\nDue to the large body of work on adversarial robustness, we focus on related papers that also provide\ntheoretical explanations for adversarial examples. We defer a detailed discussion of related work to\nAppedix A and discuss here the works most closely related to ours.\nWang et al. [55] study the adversarial robustness of nearest neighbor classiﬁers. In contrast to our\nwork, the authors give theoretical guarantees for a speciﬁc classiﬁcation algorithm, and do not\nsee a separation in sample complexity between robust and regular generalization. Recent work by\nGilmer et al. [20] explores a speciﬁc distribution where robust learning is empirically difﬁcult with\noverparametrized neural networks. The main phenomenon is that even a small natural error rate\non their dataset translates to a large adversarial error rate. Our results give a more nuanced picture\nthat involves the sample complexity required for generalization. In our data models, it is possible to\nachieve an error rate that is essentially zero by using a very small number of samples, whereas the\nadversarial error rate is still large unless we have seen a lot of samples.\nThe work of Xu et al. [58] establishes a connection between robust optimization and regularization\nfor linear classiﬁcation. In particular, they show that robustness to a speciﬁc perturbation set is exactly\nequivalent to the standard support vector machine. Subsequent work by Xu and Mannor [57] builds\na deeper connection between robustness and generalization. They prove that for a certain notion\nof robustness, robust algorithms generalize. Moreover, they show that robustness is a necessary\n8\ncondition for generalization in an asymptotic sense. Bellet and Habrard [5] gives similar results\nfor metric learning. However, these results do no imply sample complexity bounds since they are\nasymptotic. Our results stand in stark contrast: we show that generalization can, in simple models, be\nsigniﬁcantly easier than robustness when sample complexity enters the picture.\nFawzi et al. [18] relate the robustness of linear and non-linear classiﬁers to adversarial and\n(semi-) random perturbations. Their work studies the setting where the classiﬁer is ﬁxed and does not\nencompass the learning task. Fawzi et al. [19] give provable lower bounds for adversarial robustness\nin models where robust classiﬁers do not exist. In contrast, we are interested in a setting where robust\nclassiﬁers exist, but need many samples to learn. Papernot et al.[43] discuss adversarial robustness at\nthe population level. We defer a more detailed discussion of these works to Appendix A.\nThere is also a long line of work in machine learning on exploring the connection between various no-\ntions of margin and generalization, e.g., see [46] and references therein. In this setting, the ℓp margin,\ni.e., how robustly classiﬁable the data is forℓ∗\np-bounded classiﬁers, enables dimension-independent\ncontrol of the sample complexity. However, the sample complexity in concrete distributional models\ncan often be signiﬁcantly smaller than what the margin implies.\n5 Discussion and Conclusions\nThe vulnerability of neural networks to adversarial perturbations has recently been a source of much\ndiscussion and is still poorly understood. Different works have argued that this vulnerability stems\nfrom their discontinuous nature [ 51], their linear nature [ 21], or is a result of high-dimensional\ngeometry and independent of the model class [20]. Our work gives a more nuanced picture. We show\nthat for a natural data distribution (the Gaussian model), the model class we train does not matter and\na standard linear classiﬁer achieves optimal robustness. However, robustness also strongly depends on\nproperties of the underlying data distribution. For other data models (such as MNIST or the Bernoulli\nmodel), our results demonstrate that non-linearities are indispensable to learn from few samples. This\ndichotomy provides evidence that defenses against adversarial examples need to be tailored to the\nspeciﬁc dataset (even for the same type of perturbations) and hence may be more complicated than a\nsingle, broad approach. Understanding the interactions between robustness, classiﬁer model, and\ndata distribution from the perspective of generalization is an important direction for future work. We\nrefer the reader to Section B in the appendix for concrete questions in this direction.\nThe focus of our paper is on adversarial perturbations in a setting where the test distribution (before\nthe adversary’s action) is the same as the training distribution. While this is a natural scenario from a\nsecurity point of view, other setups can be more relevant in different robustness contexts. For instance,\nwe may want a classiﬁer that is robust to small changes between the training and test distribution.\nThis can be formalized as the classiﬁcation accuracy on unperturbed examples coming from an\nadversarially modiﬁed distribution. Here, the power of the adversary is limited by how much the\ntest distribution can be modiﬁed, and the adversary is not allowed to perturb individual samples\ncoming from the modiﬁed test distribution. Interestingly, our lower bound for the Gaussian model\nalso applies to such worst-case distributional shifts. In particular, if the adversary is allowed to shift\nthe meanθ⋆ by a vector inBε\n∞, our proof sketched in Section C transfers to the distribution shift\nsetting. Since the lower bound relies only on a single universal perturbation, this perturbation can\nalso be applied directly to the mean vector.\nWhat do our results mean for robust classiﬁcation of real images? Our Gaussian lower bound implies\nthat if an algorithm works for all (or most) settings of the unknown parameter θ⋆, then achieving\nstrongℓ∞-robustness requires a sample complexity increase that is polynomial in the dimension.\nThere are a few different ways this lower bound could be bypassed. It is conceivable that the noise\nscaleσ is signiﬁcantly smaller for real image datasets, making robust classiﬁcation easier. Even if that\nwas not the case, a good algorithm could work for the parametersθ⋆ that correspond to real datasets\nwhile not working for most other parameters. To accomplish this, the algorithm would implicitly\nor explicitly have prior information about the correctθ⋆. While some prior information is already\nincorporated in the model architectures (e.g., convolutional and pooling layers), the conventional\nwisdom usually is not to bias the neural network with our priors. Our work suggests that there are\ntrade-offs with robustness here and that adding more prior information could help to learn more\nrobust classiﬁers.\n9\nAcknowledgements\nDuring this research project, Ludwig Schmidt was supported by a Google PhD fellowship and\na Microsoft Research fellowship at the Simons Institute for the Theory of Computing. Ludwig\nwas also an intern in the Google Brain team. Shibani Santurkar is supported by the National\nScience Foundation (NSF) under grants IIS-1447786, IIS-1607189, and CCF-1563880, and the Intel\nCorporation. Dimitris Tsipras was supported in part by the NSF grant CCF-1553428 and the NSF\nFrontier grant CNS-1413920. Aleksander M ˛ adry was supported in part by an Alfred P. Sloan Research\nFellowship, a Google Research Award, and the NSF grants CCF-1553428 and CNS-1815221.\nReferences\n[1] Tensor ﬂow models repository. https://www.tensorflow.org/tutorials/layers, 2017.\n[2] Anurag Arnab, Ondrej Miksik, and Philip H. S. Torr. On the robustness of semantic segmentation\nmodels to adversarial attacks. In Conference on Computer Vision and Pattern Recognition\n(CVPR), 2018. URL http://arxiv.org/abs/1711.09856.\n[3] Anish Athalye, Nicholas Carlini, and David Wagner. Obfuscated gradients give a false sense\nof security: Circumventing defenses to adversarial examples. In International Conference on\nMachine Learning (ICML), 2018. URL https://arxiv.org/abs/1802.00420.\n[4] Vahid Behzadan and Arslan Munir. Vulnerability of deep reinforcement learning to policy\ninduction attacks. In International Conference on Machine Learning and Data Mining (MLDM),\n2017. URL https://arxiv.org/abs/1701.04143.\n[5] Aurélien Bellet and Amaury Habrard. Robustness and generalization for metric learning.\nNeurocomputing, 2015. URL https://arxiv.org/abs/1209.1086.\n[6] Aharon Ben-Tal, Laurent El Ghaoui, and Arkadi Nemirovski. Robust optimization. Princeton\nUniversity Press, 2009.\n[7] Battista Biggio and Fabio Roli. Wild patterns: Ten years after the rise of adversarial machine\nlearning. Pattern Recognition, 2018. URL https://arxiv.org/abs/1712.03141.\n[8] Stéphane Boucheron, Gábor Lugosi, and Pascal Massart. Concentration Inequalities: A\nNonasymptotic Theory of Independence. Oxford University Press, 2013.\n[9] Nader H. Bshouty, Nadav Eiron, and Eyal Kushilevitz. PAC learning with nasty noise. In\nAlgorithmic Learning Theory (ALT), 1999. URL https://link.springer.com/chapter/\n10.1007/3-540-46769-6_17 .\n[10] Nicholas Carlini and David Wagner. Defensive distillation is not robust to adversarial examples.\narXiv, 2016.\n[11] Nicholas Carlini and David Wagner. Towards evaluating the robustness of neural networks. In\nSymposium on Security and Privacy (SP), 2016. URL http://arxiv.org/abs/1608.04644.\n[12] Nicholas Carlini and David Wagner. Audio adversarial examples: Targeted attacks on speech-\nto-text. In Security and Privacy Workshops (SPW), 2018. URL https://arxiv.org/abs/\n1801.01944.\n[13] Nicholas Carlini, Pratyush Mishra, Tavish Vaidya, Yuankai Zhang, Micah Sherr, Clay\nShields, David Wagner, and Wenchao Zhou. Hidden voice commands. In USENIX Security\nSymposium, 2016. URL https://www.usenix.org/conference/usenixsecurity16/\ntechnical-sessions/presentation/carlini.\n[14] Moustapha M Cisse, Yossi Adi, Natalia Neverova, and Joseph Keshet. Houdini: Fooling\ndeep structured visual and speech recognition models with adversarial examples. In Neural\nInformation Processing Systems (NIPS), 2017. URL https://arxiv.org/abs/1707.05373.\n[15] Ekin D. Cubuk Cubuk, Barret Zoph, Samuel S. Schoenholz, and Quoc V . Le. Intriguing\nproperties of adversarial examples. arXiv, 2017. URL https://arxiv.org/abs/1711.\n02846.\n10\n[16] Nilesh Dalvi, Pedro Domingos, Mausam, Sumit Sanghai, and Deepak Verma. Adversarial\nclassiﬁcation. In International Conference on Knowledge Discovery and Data Mining (KDD),\n2004. URL http://doi.acm.org/10.1145/1014052.1014066.\n[17] Logan Engstrom, Brandon Tran, Dimitris Tsipras, Ludwig Schmidt, and Aleksander Madry.\nA rotation and a translation sufﬁce: Fooling CNNs with simple transformations. arXiv, 2017.\nURL https://arxiv.org/abs/1712.02779.\n[18] Alhussein Fawzi, Seyed-Mohsen Moosavi-Dezfooli, and Pascal Frossard. Robustness of\nclassiﬁers: from adversarial to random noise. InNeural Information Processing Systems (NIPS),\n2016. URL https://arxiv.org/abs/1608.08967.\n[19] Alhussein Fawzi, Hamza Fawzi, and Omar Fawzi. Adversarial vulnerability for any classiﬁer.\narXiv, 2018. URL https://arxiv.org/abs/1802.08686.\n[20] Justin Gilmer, Luke Metz, Fartash Faghri, Samuel S. Schoenholz, Maithra Raghu, Martin\nWattenberg, and Ian Goodfellow. Adversarial spheres. arXiv, 2018. URL https://arxiv.\norg/abs/1801.02774.\n[21] Ian J. Goodfellow, Jonathon Shlens, and Christian Szegedy. Explaining and harnessing adver-\nsarial examples. arXiv, 2014. URL http://arxiv.org/abs/1412.6572.\n[22] Kathrin Grosse, Nicolas Papernot, Praveen Manoharan, Michael Backes, and Patrick D.\nMcDaniel. Adversarial perturbations against deep neural networks for malware classiﬁca-\ntion. In European Symposium on Research in Computer Security (ESORICS) , 2016. URL\nhttp://arxiv.org/abs/1606.04435.\n[23] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image\nrecognition. In Conference on Computer Vision and Pattern Recognition (CVPR), 2016. URL\nhttps://arxiv.org/abs/1512.03385.\n[24] Warren He, James Wei, Xinyun Chen, Nicholas Carlini, and Dawn Song. Adversarial example\ndefenses: Ensembles of weak defenses are not strong. In USENIX Workshop on Offensive\nTechnologies, 2017. URL https://arxiv.org/abs/1706.04701.\n[25] Alex Huang, Abdullah Al-Dujaili, Erik Hemberg, and Una-May O’Reilly. Adversarial deep\nlearning for robust detection of binary encoded malware. In Security and Privacy Workshops\n(SPW), 2018. URL https://arxiv.org/abs/1801.02950.\n[26] Sandy H. Huang, Nicolas Papernot, Ian J. Goodfellow, Yan Duan, and Pieter Abbeel. Adversarial\nattacks on neural network policies. In International Conference on Learning Representations\n(ICLR), 2017. URL https://arxiv.org/abs/1702.02284.\n[27] Peter J. Huber. Robust Statistics. Wiley, 1981.\n[28] Robin Jia and Percy Liang. Adversarial examples for evaluating reading comprehension systems.\nIn Conference on Empirical Methods in Natural Language Processing (EMNLP), 2017. URL\nhttps://arxiv.org/abs/1707.07328.\n[29] Michael Kearns and Ming Li. Learning in the presence of malicious errors. SIAM Journal on\nComputing, 1993. URL http://dx.doi.org/10.1137/0222052.\n[30] Michael J. Kearns, Robert E. Schapire, and Linda M. Sellie. Toward efﬁcient agnostic learning.\nMachine Learning, 1994. URL https://doi.org/10.1023/A:1022615600103.\n[31] J Zico Kolter and Eric Wong. Provable defenses against adversarial examples via the convex\nouter adversarial polytope. In International Conference on Learning Representations (ICLR),\n2018. URL https://arxiv.org/abs/1711.00851.\n[32] Jernej Kos, Ian Fischer, and Dawn Song. Adversarial examples for generative models. In\nSecurity and Privacy Workshops (SPW), 2018. URL http://arxiv.org/abs/1702.06832.\n[33] Alex Krizhevsky and Geoffrey Hinton. Learning multiple layers of features from\ntiny images. Technical report, 2009. URL https://www.cs.toronto.edu/~kriz/\nlearning-features-2009-TR.pdf .\n11\n[34] Yann LeCun, Corinna Cortes, and Christopher J.C. Burges. The mnist database of handwritten\ndigits. Website, 1998. URL http://yann.lecun.com/exdb/mnist/.\n[35] Daniel Lowd and Christopher Meek. Adversarial learning. In International Conference on\nKnowledge Discovery in Data Mining (KDD), 2005. URL http://doi.acm.org/10.1145/\n1081870.1081950.\n[36] Aleksander Madry, Aleksandar Makelov, Ludwig Schmidt, Dimitris Tsipras, and Adrian Vladu.\nTowards deep learning models resistant to adversarial attacks. In International Conference on\nLearning Representations (ICLR), 2018. URL https://arxiv.org/abs/1706.06083.\n[37] Seyed-Mohsen Moosavi-Dezfooli, Alhussein Fawzi, and Pascal Frossard. Deepfool: A simple\nand accurate method to fool deep neural networks. In Conference on Computer Vision and\nPattern Recognition (CVPR), 2016. URL https://arxiv.org/abs/1511.04599.\n[38] Seyed-Mohsen Moosavi-Dezfooli, Alhussein Fawzi, Omar Fawzi, and Pascal Frossard. Uni-\nversal adversarial perturbations. In Conference on Computer Vision and Pattern Recognition\n(CVPR), 2017. URL https://arxiv.org/abs/1610.08401.\n[39] Nina Narodytska and Shiva Prasad Kasiviswanathan. Simple black-box adversarial perturbations\nfor deep networks. In Conference on Computer Vision and Pattern Recognition (CVPR)\nWorkshops, 2017. URL http://arxiv.org/abs/1612.06299.\n[40] Yuval Netzer, Tao Wang, Adam Coates, Alessandro Bissacco, Bo Wu, and Andrew Y . Ng.\nReading digits in natural images with unsupervised feature learning. In NIPS Workshop on\nDeep Learning and Unsupervised Feature Learning, 2011. URL http://ufldl.stanford.\nedu/housenumbers/.\n[41] Nicolas Papernot, Patrick McDaniel, Xi Wu, Somesh Jha, and Ananthram Swami. Distillation as\na defense to adversarial perturbations against deep neural networks. In Symposium on Security\nand Privacy (SP), 2016. URL https://arxiv.org/abs/1511.04508.\n[42] Nicolas Papernot, Patrick D. McDaniel, Somesh Jha, Matt Fredrikson, Z. Berkay Celik, and\nAnanthram Swami. The limitations of deep learning in adversarial settings. In European\nSymposium on Security and Privacy (EuroS&P) , 2016. URL https://arxiv.org/abs/\n1511.07528.\n[43] Nicolas Papernot, Patrick McDaniel, Arunesh Sinha, and Michael Wellman. Towards the\nscience of security and privacy in machine learning. In European Symposium on Security and\nPrivacy (EuroS&P), 2018. URL https://arxiv.org/abs/1611.03814.\n[44] Aditi Raghunathan, Jacob Steinhardt, and Percy Liang. Certiﬁed defenses against adversarial\nexamples. In International Conference on Learning Representations (ICLR) , 2018. URL\nhttps://arxiv.org/abs/1801.09344.\n[45] Phillippe Rigollet and Jan-Christian Hütter. High-dimensional statistics. Lecture notes, 2017.\nURL http://www-math.mit.edu/~rigollet/PDFs/RigNotes17.pdf.\n[46] Shai Shalev-Shwartz and Shai Ben-David. Understanding Machine Learning: From Theory to\nAlgorithms. Cambridge University Press, 2014. URL http://www.cs.huji.ac.il/~shais/\nUnderstandingMachineLearning/.\n[47] Mahmood Sharif, Sruti Bhagavatula, Lujo Bauer, and Michael K. Reiter. Accessorize to a\ncrime: Real and stealthy attacks on state-of-the-art face recognition. InConference on Computer\nand Communications Security (CCS), 2016. URL http://doi.acm.org/10.1145/2976749.\n2978392.\n[48] Aman Sinha, Hongseok Namkoong, and John Duchi. Certifying some distributional robustness\nwith principled adversarial training. In International Conference on Learning Representations\n(ICLR), 2018. URL https://arxiv.org/abs/1710.10571.\n[49] Liwei Song and Prateek Mittal. Inaudible voice commands. In Conference on Computer and\nCommunications Security (CCS), 2017. URL http://arxiv.org/abs/1708.07238.\n12\n[50] Jiawei Su, Danilo Vasconcellos Vargas, and Kouichi Sakurai. One pixel attack for fooling deep\nneural networks. arXiv, 2017. URL http://arxiv.org/abs/1710.08864.\n[51] Christian Szegedy, Wojciech Zaremba, Ilya Sutskever, Joan Bruna, Dumitru Erhan, Ian J.\nGoodfellow, and Rob Fergus. Intriguing properties of neural networks. In International\nConference on Learning Representations (ICLR), 2014. URL http://arxiv.org/abs/1312.\n6199.\n[52] Florian Tramèr, Nicolas Papernot, Ian J. Goodfellow, Dan Boneh, and Patrick D. McDaniel.\nThe space of transferable adversarial examples. arXiv, 2017. URL http://arxiv.org/abs/\n1704.03453.\n[53] Florian Tramèr, Alexey Kurakin, Nicolas Papernot, Dan Boneh, and Patrick D. McDaniel.\nEnsemble adversarial training: Attacks and defenses. In International Conference on Learning\nRepresentations (ICLR), 2018. URL http://arxiv.org/abs/1705.07204.\n[54] Abraham Wald. Statistical decision functions which minimize the maximum risk. Annals of\nMathematics, 1945.\n[55] Yizhen Wang, Somesh Jha, and Kamalika Chaudhuri. Analyzing the robustness of nearest\nneighbors to adversarial examples. In International Conference on Machine Learning (ICML),\n2018. URL http://proceedings.mlr.press/v80/wang18c/wang18c.pdf.\n[56] Chaowei Xiao, Jun-Yan Zhu, Bo Li, Warren He, Mingyan Liu, and Dawn Song. Spatially\ntransformed adversarial examples. In International Conference on Learning Representations\n(ICLR), 2018. URL https://arxiv.org/abs/1801.02612.\n[57] Huan Xu and Shie Mannor. Robustness and generalization. Machine learning, 2012. URL\nhttps://arxiv.org/abs/1005.2243.\n[58] Huan Xu, Constantine Caramanis, and Shie Mannor. Robustness and regularization of support\nvector machines. Journal of Machine Learning Research (JMLR), 2009. URL http://www.\njmlr.org/papers/v10/xu09b.html.\n[59] Weilin Xu, David Evans, and Yanjun Qi. Feature squeezing: Detecting adversarial examples in\ndeep neural networks. In Network and Distributed System Security Symposium (NDSS), 2017.\nURL https://arxiv.org/abs/1704.01155.\n[60] Xiaojun Xu, Xinyun Chen, Chang Liu, Anna Rohrbach, Trevor Darell, and Dawn Song. Can\nyou fool AI with adversarial examples on a visual Turing test? arXiv, 2017. URL http:\n//arxiv.org/abs/1709.08693.\n[61] Sergey Zagoruyko and Nikos Komodakis. Wide residual networks. In British Machine Vision\nConference (BMVC), 2016. URL http://arxiv.org/abs/1605.07146.\n[62] Guoming Zhang, Chen Yan, Xiaoyu Ji, Taimin Zhang, Tianchen Zhang, and Wenyuan Xu.\nDolphinatack: Inaudible voice commands. In Conference on Computer and Communications\nSecurity (CCS), 2017. URL http://arxiv.org/abs/1708.09537.\n13",
  "values": {
    "Interpretable (to users)": "Yes",
    "Transparent (to users)": "Yes",
    "Not socially biased": "Yes",
    "Respect for Persons": "Yes",
    "Explicability": "Yes",
    "Critiqability": "Yes",
    "Beneficence": "Yes",
    "User influence": "Yes",
    "Respect for Law and public interest": "Yes",
    "Privacy": "Yes",
    "Non-maleficence": "Yes",
    "Justice": "Yes",
    "Fairness": "Yes",
    "Collective influence": "Yes",
    "Autonomy (power to decide)": "Yes",
    "Deferral to humans": "Yes"
  }
}