{
  "pdf": "NeurIPS-2018-hierarchical-graph-representation-learning-with-differentiable-pooling-Paper",
  "title": "Hierarchical Graph Representation Learning with Differentiable Pooling",
  "author": "Zhitao Ying, Jiaxuan You, Christopher Morris, Xiang Ren, Will Hamilton, Jure Leskovec",
  "paper_id": "NeurIPS-2018-hierarchical-graph-representation-learning-with-differentiable-pooling-Paper",
  "text": "Hierarchical Graph Representation Learning with\nDifferentiable Pooling\nRex Ying\nrexying@stanford.edu\nStanford University\nJiaxuan You\njiaxuan@stanford.edu\nStanford University\nChristopher Morris\nchristopher.morris@udo.edu\nTU Dortmund University\nXiang Ren\nxiangren@usc.edu\nUniversity of Southern California\nWilliam L. Hamilton\nwleif@stanford.edu\nStanford University\nJure Leskovec\njure@cs.stanford.edu\nStanford University\nAbstract\nRecently, graph neural networks (GNNs) have revolutionized the ﬁeld of graph\nrepresentation learning through effectively learned node embeddings, and achieved\nstate-of-the-art results in tasks such as node classiﬁcation and link prediction.\nHowever, current GNN methods are inherently ﬂat and do not learn hierarchical\nrepresentations of graphs—a limitation that is especially problematic for the task\nof graph classiﬁcation, where the goal is to predict the label associated with an\nentire graph. Here we propose DIFFPOOL, a differentiable graph pooling module\nthat can generate hierarchical representations of graphs and can be combined with\nvarious graph neural network architectures in an end-to-end fashion. DIFFPOOL\nlearns a differentiable soft cluster assignment for nodes at each layer of a deep\nGNN, mapping nodes to a set of clusters, which then form the coarsened input\nfor the next GNN layer. Our experimental results show that combining existing\nGNN methods with DIFFPOOL yields an average improvement of 5–10% accuracy\non graph classiﬁcation benchmarks, compared to all existing pooling approaches,\nachieving a new state-of-the-art on four out of ﬁve benchmark data sets.\n1 Introduction\nIn recent years there has been a surge of interest in developing graph neural networks (GNNs)—\ngeneral deep learning architectures that can operate over graph structured data, such as social network\ndata [16, 21, 36] or graph-based representations of molecules [7, 11, 15]. The general approach with\nGNNs is to view the underlying graph as a computation graph and learn neural network primitives\nthat generate individual node embeddings by passing, transforming, and aggregating node feature\ninformation across the graph [15, 16]. The generated node embeddings can then be used as input to\nany differentiable prediction layer, e.g., for node classiﬁcation [16] or link prediction [32], and the\nwhole model can be trained in an end-to-end fashion.\nHowever, a major limitation of current GNN architectures is that they are inherently ﬂat as they\nonly propagate information across the edges of the graph and are unable to infer and aggregate the\ninformation in a hierarchical way. For example, in order to successfully encode the graph structure\nof organic molecules, one would ideally want to encode the local molecular structure (e.g., individual\n32nd Conference on Neural Information Processing Systems (NeurIPS 2018), Montréal, Canada.\nOriginal\nnetwork\nPooled network\nat level 1\nPooled network\nat level 2\nGraph \nclassification\nPooled network\nat level 3\nFigure 1: High-level illustration of our proposed method DIFFPOOL. At each hierarchical layer, we\nrun a GNN model to obtain embeddings of nodes. We then use these learned embeddings to cluster\nnodes together and run another GNN layer on this coarsened graph. This whole process is repeated\nforL layers and we use the ﬁnal output representation to classify the graph.\natoms and their direct bonds) as well as the coarse-grained structure of the molecular graph (e.g.,\ngroups of atoms and bonds representing functional units in a molecule). This lack of hierarchical\nstructure is especially problematic for the task of graph classiﬁcation, where the goal is to predict\nthe label associated with an entire graph. When applying GNNs to graph classiﬁcation, the standard\napproach is to generate embeddings for all the nodes in the graph and then to globally pool all these\nnode embeddings together, e.g., using a simple summation or neural network that operates over sets\n[7, 11, 15, 25]. This global pooling approach ignores any hierarchical structure that might be present\nin the graph, and it prevents researchers from building effective GNN models for predictive tasks\nover entire graphs.\nHere we propose DIFFPOOL, a differentiable graph pooling module that can be adapted to various\ngraph neural network architectures in an hierarchical and end-to-end fashion (Figure 1). DIFFPOOL\nallows for developing deeper GNN models that can learn to operate on hierarchical representations\nof a graph. We develop a graph analogue of the spatial pooling operation in CNNs [ 23], which\nallows for deep CNN architectures to iteratively operate on coarser and coarser representations of\nan image. The challenge in the GNN setting—compared to standard CNNs—is that graphs contain\nno natural notion of spatial locality, i.e., one cannot simply pool together all nodes in a “ m×m\npatch” on a graph, because the complex topological structure of graphs precludes any straightforward,\ndeterministic deﬁnition of a “patch”. Moreover, unlike image data, graph data sets often contain\ngraphs with varying numbers of nodes and edges, which makes deﬁning a general graph pooling\noperator even more challenging.\nIn order to solve the above challenges, we require a model that learns how to cluster together nodes\nto build a hierarchical multi-layer scaffold on top of the underlying graph. Our approach DIFFPOOL\nlearns a differentiable soft assignment at each layer of a deep GNN, mapping nodes to a set of clusters\nbased on their learned embeddings. In this framework, we generate deep GNNs by “stacking” GNN\nlayers in a hierarchical fashion (Figure 1): the input nodes at the layer l GNN module correspond\nto the clusters learned at the layer l− 1 GNN module. Thus, each layer of DIFFPOOL coarsens\nthe input graph more and more, and DIFFPOOL is able to generate a hierarchical representation\nof any input graph after training. We show that DIFFPOOL can be combined with various GNN\napproaches, resulting in an average 7% gain in accuracy and a new state of the art on four out of\nﬁve benchmark graph classiﬁcation tasks. Finally, we show thatDIFFPOOL can learn interpretable\nhierarchical clusters that correspond to well-deﬁned communities in the input graphs.\n2 Related Work\nOur work builds upon a rich line of recent research on graph neural networks and graph classiﬁcation.\nGeneral graph neural networks . A wide variety of graph neural network (GNN) models have\nbeen proposed in recent years, including methods inspired by convolutional neural networks [ 5,\n8, 11, 16, 21, 24, 29, 36], recurrent neural networks [ 25], recursive neural networks [ 1, 30] and\nloopy belief propagation [7]. Most of these approaches ﬁt within the framework of “neural message\npassing” proposed by Gilmer et al. [ 15]. In the message passing framework, a GNN is viewed as a\n2\nmessage passing algorithm where node representations are iteratively computed from the features\nof their neighbor nodes using a differentiable aggregation function. Hamilton et al. [ 17] provides a\nconceptual review of recent advancements in this area, and Bronstein et al. [4] outlines connections\nto spectral graph convolutions.\nGraph classiﬁcation with graph neural networks. GNNs have been applied to a wide variety of\ntasks, including node classiﬁcation [16, 21], link prediction [31], graph classiﬁcation [7, 11, 40], and\nchemoinformatics [28, 27, 14, 19, 32]. In the context of graph classiﬁcation—the task that we study\nhere—a major challenge in applying GNNs is going from node embeddings, which are the output of\nGNNs, to a representation of the entire graph. Common approaches to this problem include simply\nsumming up or averaging all the node embeddings in a ﬁnal layer [11], introducing a “virtual node”\nthat is connected to all the nodes in the graph [25], or aggregating the node embeddings using a deep\nlearning architecture that operates over sets [15]. However, all of these methods have the limitation\nthat they do not learn hierarchical representations (i.e., all the node embeddings are globally pooled\ntogether in a single layer), and thus are unable to capture the natural structures of many real-world\ngraphs. Some recent approaches have also proposed applying CNN architectures to the concatenation\nof all the node embeddings [29, 40], but this requires a specifying (or learning) a canonical ordering\nover nodes, which is in general very difﬁcult and equivalent to solving graph isomorphism.\nLastly, there are some recent works that learn hierarchical graph representations by combining GNNs\nwith deterministic graph clustering algorithms [8, 35, 13], following a two-stage approach. However,\nunlike these previous approaches, we seek to learn the hierarchical structure in an end-to-end fashion,\nrather than relying on a deterministic graph clustering subroutine.\n3 Proposed Method\nThe key idea of DIFFPOOL is that it enables the construction of deep, multi-layer GNN models by\nproviding a differentiable module to hierarchically pool graph nodes. In this section, we outline the\nDIFFPOOL module and show how it is applied in a deep GNN architecture.\n3.1 Preliminaries\nWe represent a graphG as (A,F ), whereA∈{ 0, 1}n×n is the adjacency matrix, and F∈ Rn×d\nis the node feature matrix assuming each node has d features.1 Given a set of labeled graphs\nD ={(G1,y 1), (G2,y 2),...} whereyi∈Y is the label corresponding to graph Gi∈G , the goal\nof graph classiﬁcation is to learn a mappingf :G→Y that maps graphs to the set of labels. The\nchallenge—compared to standard supervised machine learning setup—is that we need a way to\nextract useful feature vectors from these input graphs. That is, in order to apply standard machine\nlearning methods for classiﬁcation, e.g., neural networks, we need a procedure to convert each graph\nto an ﬁnite dimensional vector inRD.\nGraph neural networks. In this work, we build upon graph neural networks in order to learn useful\nrepresentations for graph classiﬁcation in an end-to-end fashion. In particular, we consider GNNs\nthat employ the following general “message-passing” architecture:\nH (k) =M (A,H (k−1);θ(k)), (1)\nwhere H (k)∈ Rn×d are the node embeddings (i.e., “messages”) computed after k steps of the\nGNN andM is the message propagation function, which depends on the adjacency matrix, trainable\nparametersθ(k), and the node embeddings H (k−1) generated from the previous message-passing\nstep.2 The input node embeddingsH (0) at the initial message-passing iteration(k = 1), are initialized\nusing the node features on the graph,H (0) =F .\nThere are many possible implementations of the propagation functionM [15, 16]. For example, one\npopular variant of GNNs—Kipf’s et al. [21] Graph Convolutional Networks (GCNs)—implements\nM using a combination of linear transformations and ReLU non-linearities:\nH (k) =M (A,H (k−1);W (k)) = ReLU( ˜D− 1\n2 ˜A ˜D− 1\n2H (k−1)W (k−1)), (2)\n1We do not consider edge features, although one can easily extend the algorithm to support edge features\nusing techniques introduced in [35].\n2For notational convenience, we assume that the embedding dimension is d for all H (k); however, in general\nthis restriction is not necessary.\n3\nwhere ˜A =A +I, ˜D =∑\nj ˜Aij andW (k)∈ Rd×d is a trainable weight matrix. The differentiable\npooling model we propose can be applied to any GNN model implementing Equation (1), and is\nagnostic with regards to the speciﬁcs of howM is implemented.\nA full GNN module will runK iterations of Equation (1) to generate the ﬁnal output node embeddings\nZ =H (K)∈ Rn×d, whereK is usually in the range 2-6. For simplicity, in the following sections we\nwill abstract away the internal structure of the GNNs and useZ = GNN(A,X ) to denote an arbitrary\nGNN module implementingK iterations of message passing according to some adjacency matrixA\nand initial input node featuresX.\nStacking GNNs and pooling layers. GNNs implementing Equation (1) are inherently ﬂat, as they\nonly propagate information across edges of a graph. The goal of this work is to deﬁne a general,\nend-to-end differentiable strategy that allows one to stack multiple GNN modules in a hierarchical\nfashion. Formally, given Z = GNN(A,X ), the output of a GNN module, and a graph adjacency\nmatrixA∈ Rn×n, we seek to deﬁne a strategy to output a new coarsened graph containingm<n\nnodes, with weighted adjacency matrixA\n′\n∈ Rm×m and node embeddingsZ\n′\n∈ Rm×d. This new\ncoarsened graph can then be used as input to another GNN layer, and this whole process can be\nrepeatedL times, generating a model with L GNN layers that operate on a series of coarser and\ncoarser versions of the input graph (Figure 1). Thus, our goal is to learn how to cluster or pool\ntogether nodes using the output of a GNN, so that we can use this coarsened graph as input to another\nGNN layer. What makes designing such a pooling layer for GNNs especially challenging—compared\nto the usual graph coarsening task—is that our goal is not to simply cluster the nodes in one graph,\nbut to provide a general recipe to hierarchically pool nodes across a broad set of input graphs. That is,\nwe need our model to learn a pooling strategy that will generalize across graphs with different nodes,\nedges, and that can adapt to the various graph structures during inference.\n3.2 Differentiable Pooling via Learned Assignments\nOur proposed approach, DIFFPOOL, addresses the above challenges by learning a cluster assignment\nmatrix over the nodes using the output of a GNN model. The key intuition is that we stackL GNN\nmodules and learn to assign nodes to clusters at layerl in an end-to-end fashion, using embeddings\ngenerated from a GNN at layerl− 1. Thus, we are using GNNs to both extract node embeddings that\nare useful for graph classiﬁcation, as well to extract node embeddings that are useful for hierarchical\npooling. Using this construction, the GNNs in DIFFPOOL learn to encode a general pooling strategy\nthat is useful for a large set of training graphs. We ﬁrst describe how theDIFFPOOL module pools\nnodes at each layer given an assignment matrix; following this, we discuss how we generate the\nassignment matrix using a GNN architecture.\nPooling with an assignment matrix. We denote the learned cluster assignment matrix at layerl as\nS(l)∈ Rnl×nl+1. Each row of S(l) corresponds to one of thenl nodes (or clusters) at layerl, and\neach column ofS(l) corresponds to one of thenl+1 clusters at the next layerl + 1. Intuitively,S(l)\nprovides a soft assignment of each node at layerl to a cluster in the next coarsened layerl + 1.\nSuppose thatS(l) has already been computed, i.e., that we have computed the assignment matrix at\nthel-th layer of our model. We denote the input adjacency matrix at this layer as A(l) and denote\nthe input node embedding matrix at this layer as Z (l). Given these inputs, the DIFFPOOL layer\n(A(l+1),X (l+1)) = DIFFPOOL (A(l),Z (l)) coarsens the input graph, generating a new coarsened\nadjacency matrixA(l+1) and a new matrix of embeddingsX (l+1) for each of the nodes/clusters in\nthis coarsened graph. In particular, we apply the two following equations:\nX (l+1) =S(l)T\nZ (l)∈ Rnl+1×d, (3)\nA(l+1) =S(l)T\nA(l)S(l)∈ Rnl+1×nl+1. (4)\nEquation (3) takes the node embeddings Z (l) and aggregates these embeddings according to the\ncluster assignmentsS(l), generating embeddings for each of thenl+1 clusters. Similarly, Equation (4)\ntakes the adjacency matrixA(l) and generates a coarsened adjacency matrix denoting the connectivity\nstrength between each pair of clusters.\nThrough Equations (3) and (4), the DIFFPOOL layer coarsens the graph: the next layer adjacency\nmatrixA(l+1) represents a coarsened graph withnl+1 nodes or cluster nodes, where each individual\n4\ncluster node in the new coarsened graph corresponds to a cluster of nodes in the graph at layer l.\nNote thatA(l+1) is a real matrix and represents a fully connected edge-weighted graph; each entry\nA(l+1)\nij can be viewed as the connectivity strength between clusteri and clusterj. Similarly, thei-th\nrow ofX (l+1) corresponds to the embedding of clusteri. Together, the coarsened adjacency matrix\nA(l+1) and cluster embeddingsX (l+1) can be used as input to another GNN layer, a process which\nwe describe in detail below.\nLearning the assignment matrix. In the following we describe the architecture of DIFFPOOL, i.e.,\nhow DIFFPOOL generates the assignment matrixS(l) and embedding matricesZ (l) that are used in\nEquations (3) and (4). We generate these two matrices using two separate GNNs that are both applied\nto the input cluster node featuresX (l) and coarsened adjacency matrixA(l). The embedding GNN at\nlayerl is a standard GNN module applied to these inputs:\nZ (l) = GNNl,embed(A(l),X (l)), (5)\ni.e., we take the adjacency matrix between the cluster nodes at layer l (from Equation 4) and the\npooled features for the clusters (from Equation 3) and pass these matrices through a standard GNN to\nget new embeddingsZ (l) for the cluster nodes. In contrast, the pooling GNN at layerl, uses the input\ncluster featuresX (l) and cluster adjacency matrixA(l) to generate an assignment matrix:\nS(l) = softmax\n(\nGNNl,pool(A(l),X (l))\n)\n, (6)\nwhere the softmax function is applied in a row-wise fashion. The output dimension of GNNl,pool\ncorresponds to a pre-deﬁned maximum number of clusters in layerl, and is a hyperparameter of the\nmodel.\nNote that these two GNNs consume the same input data but have distinct parameterizations and play\nseparate roles: The embedding GNN generates new embeddings for the input nodes at this layer,\nwhile the pooling GNN generates a probabilistic assignment of the input nodes tonl+1 clusters.\nIn the base case, the inputs to Equations (5) and Equations (6) at layerl = 0 are simply the input\nadjacency matrixA and the node featuresF for the original graph. At the penultimate layer L− 1 of\na deep GNN model using DIFFPOOL, we set the assignment matrixS(L−1) be a vector of 1’s, i.e.,\nall nodes at the ﬁnal layerL are assigned to a single cluster, generating a ﬁnal embedding vector\ncorresponding to the entire graph. This ﬁnal output embedding can then be used as feature input to a\ndifferentiable classiﬁer (e.g., a softmax layer), and the entire system can be trained end-to-end using\nstochastic gradient descent.\nPermutation invariance. Note that in order to be useful for graph classiﬁcation, the pooling layer\nshould be invariant under node permutations. For DIFFPOOL we get the following positive result,\nwhich shows that any deep GNN model based on DIFFPOOL is permutation invariant, as long as the\ncomponent GNNs are permutation invariant.\nProposition 1. Let P ∈ {0, 1}n×n be any permutation matrix, then DIFFPOOL (A,Z ) =\nDIFFPOOL (PAP T,PX ) as long as GNN(A,X ) = GNN(PAP T,X ) (i.e., as long as the GNN\nmethod used is permutation invariant).\nProof. Equations (5) and (6) are permutation invariant by the assumption that the GNN module\nis permutation invariant. And since any permutation matrix is orthogonal, applying P TP = I to\nEquation (3) and (4) ﬁnishes the proof.\n3.3 Auxiliary Link Prediction Objective and Entropy Regularization\nIn practice, it can be difﬁcult to train the pooling GNN (Equation 4) using only gradient signal from\nthe graph classiﬁcation task. Intuitively, we have a non-convex optimization problem and it can be\ndifﬁcult to push the pooling GNN away from spurious local minima early in training. To alleviate\nthis issue, we train the pooling GNN with an auxiliary link prediction objective, which encodes the\nintuition that nearby nodes should be pooled together. In particular, at each layer l, we minimize\nLLP =||A(l),S (l)S(l)T\n||F , where||·|| F denotes the Frobenius norm. Note that the adjacency matrix\nA(l) at deeper layers is a function of lower level assignment matrices, and changes during training.\n5\nAnother important characteristic of the pooling GNN (Equation 4) is that the output cluster assignment\nfor each node should generally be close to a one-hot vector, so that the membership for each cluster\nor subgraph is clearly deﬁned. We therefore regularize the entropy of the cluster assignment by\nminimizingLE = 1\nn\n∑n\ni=1H(Si), whereH denotes the entropy function, andSi is thei-th row ofS.\nDuring training, LLP andLE from each layer are added to the classiﬁcation loss. In practice we\nobserve that training with the side objective takes longer to converge, but nevertheless achieves better\nperformance and more interpretable cluster assignments.\n4 Experiments\nWe evaluate the beneﬁts of DIFFPOOL against a number of state-of-the-art graph classiﬁcation\napproaches, with the goal of answering the following questions:\nQ1 How does DIFFPOOL compare to other pooling methods proposed for GNNs (e.g., using sort\npooling [40] or the SET2S ET method [15])?\nQ2 How does DIFFPOOL combined with GNNs compare to the state-of-the-art for graph classiﬁca-\ntion task, including both GNNs and kernel-based methods?\nQ3 Does DIFFPOOL compute meaningful and interpretable clusters on the input graphs?\nData sets. To probe the ability of DIFFPOOL to learn complex hierarchical structures from graphs in\ndifferent domains, we evaluate on a variety of relatively large graph data sets chosen from benchmarks\ncommonly used in graph classiﬁcation [20]. We use protein data sets including ENZYMES , PRO-\nTEINS [3, 12], D&D [10], the social network data set REDDIT -MULTI -12 K [39], and the scientiﬁc\ncollaboration data set COLLAB [39]. See Appendix A for statistics and properties. For all these data\nsets, we perform 10-fold cross-validation to evaluate model performance, and report the accuracy\naveraged over 10 folds.\nModel conﬁgurations. In our experiments, the GNN model used for DIFFPOOL is built on top of\nthe GRAPH SAGE architecture, as we found this architecture to have superior performance compared\nto the standard GCN approach as introduced in [21]. We use the “mean” variant of GRAPH SAGE [16]\nand apply a DIFFPOOL layer after every two GRAPH SAGE layers in our architecture. A total of 2\nDIFFPOOL layers are used for the datasets. For small datasets such as ENZYMES , PROTEINS and\nCOLLAB , 1 DIFFPOOL layer can achieve similar performance. After each DIFFPOOL layer, 3 layers\nof graph convolutions are performed, before the next DIFFPOOL layer, or the readout layer. The\nembedding matrix and the assignment matrix are computed by two separate GRAPH SAGE models\nrespectively. In the 2 DIFFPOOL layer architecture, the number of clusters is set as25% of the number\nof nodes before applying DIFFPOOL, while in the 1 DIFFPOOL layer architecture, the number of\nclusters is set as 10%. Batch normalization [18] is applied after every layer of GRAPH SAGE. We also\nfound that adding anℓ2 normalization to the node embeddings at each layer made the training more\nstable. In Section 4.2, we also test an analogous variant of DIFFPOOL on the STRUCTURE 2V EC [7]\narchitecture, in order to demonstrate how DIFFPOOL can be applied on top of other GNN models.\nAll models are trained for 3 000 epochs with early stopping applied when the validation loss starts to\ndrop. We also evaluate two simpliﬁed versions of DIFFPOOL:\n• DIFFPOOL -D ET, is a DIFFPOOL model where assignment matrices are generated using a deter-\nministic graph clustering algorithm [9].\n• DIFFPOOL -NOLP is a variant of DIFFPOOL where the link prediction side objective is turned off.\n4.1 Baseline Methods\nIn the performance comparison on graph classiﬁcation, we consider baselines based upon GNNs\n(combined with different pooling methods) as well as state-of-the-art kernel-based approaches.\nGNN-based methods.\n• GRAPH SAGE with global mean-pooling [16]. Other GNN variants such as those proposed in [21]\nare omitted as empirically GraphSAGE obtained higher performance in the task.\n• STRUCTURE 2V EC (S2V ) [7] is a state-of-the-art graph representation learning algorithm that\ncombines a latent variable model with GNNs. It uses global mean pooling.\n• Edge-conditioned ﬁlters in CNN for graphs (ECC ) [35] incorporates edge information into the\nGCN model and performs pooling using a graph coarsening algorithm.\n6\nTable 1: Classiﬁcation accuracies in percent. The far-right column gives the relative increase in\naccuracy compared to the baseline GRAPH SAGE approach.\nMethod Data Set\nENZYMES D&D R EDDIT-MULTI-12K COLLAB PROTEINS Gain\nKernel\nGRAPHLET 41.03 74.85 21.73 64.66 72.91\nSHORTEST-PATH 42.32 78.86 36.93 59.10 76.43\n1-WL 53.43 74.02 39.03 78.61 73.76\nWL-OA 60.13 79.04 44.38 80.74 75.26\nPATCHYSAN – 76.27 41.32 72.60 75.00 4.17\nGNN\nGRAPHSAGE 54.25 75.42 42.24 68.25 70.48 –\nECC 53.50 74.10 41.73 67.79 72.65 0.11\nSET2SET 60.15 78.12 43.49 71.75 74.29 3.32\nSORTPOOL 57.12 79.37 41.82 73.76 75.54 3.39\nDIFFPOOL-DET 58.33 75.47 46.18 82.13 75.62 5.42\nDIFFPOOL-NOLP 61.95 79.98 46.65 75.58 76.22 5.95\nDIFFPOOL 62.53 80.64 47.08 75.48 76.25 6.27\n• PATCHY SAN [29] deﬁnes a receptive ﬁeld (neighborhood) for each node, and using a canonical\nnode ordering, applies convolutions on linear sequences of node embeddings.\n• SET2SET replaces the global mean-pooling in the traditional GNN architectures by the aggrega-\ntion used in SET2SET [38]. Set2Set aggregation has been shown to perform better than mean\npooling in previous work [15]. We use GRAPH SAGE as the base GNN model.\n• SORTPOOL [40] applies a GNN architecture and then performs a single layer of soft pooling\nfollowed by 1D convolution on sorted node embeddings.\nFor all the GNN baselines, we use 10-fold cross validation numbers reported by the original authors\nwhen possible. For the GRAPH SAGE and SET2S ET baselines, we use the base implementation and\nhyperparameter sweeps as in our DIFFPOOL approach. When baseline approaches did not have the\nnecessary published numbers, we contacted the original authors and used their code (if available) to\nrun the model, performing a hyperparameter search based on the original author’s guidelines.\nKernel-based algorithms. We use the GRAPHLET [34], the SHORTEST -PATH [2], WEISFEILER -\nLEHMAN kernel (WL) [33], and WEISFEILER -L EHMAN OPTIMAL ASSIGNMENT KERNEL (WL-\nOA) [22] as kernel baselines. For each kernel, we computed the normalized gram matrix. We\ncomputed the classiﬁcation accuracies using the C-SVM implementation of LIBSVM [6], using\n10-fold cross validation. TheC parameter was selected from{10−3, 10−2,..., 102, 103} by 10-fold\ncross validation on the training folds. Moreover, for WL and WL-OA we additionally selected the\nnumber of iteration from{0,..., 5}.\n4.2 Results for Graph Classiﬁcation\nTable 1 compares the performance ofDIFFPOOL to these state-of-the-art graph classiﬁcation baselines.\nThese results provide positive answers to our motivating questions Q1 and Q2: We observe that\nour DIFFPOOL approach obtains the highest average performance among all pooling approaches for\nGNNs, improves upon the base GRAPH SAGE architecture by an average of 6.27%, and achieves state-\nof-the-art results on 4 out of 5 benchmarks. Interestingly, our simpliﬁed model variant,DIFFPOOL -\nDET, achieves state-of-the-art performance on the COLLAB benchmark. This is because many\ncollaboration graphs in COLLAB show only single-layer community structures, which can be captured\nwell with pre-computed graph clustering algorithm [9]. One observation is that despite signiﬁcant\nperformance improvement, DIFFPOOL could be unstable to train, and there is signiﬁcant variation\nin accuracy across different runs, even with the same hyperparameter setting. It is observed that\nadding the link predictioin objective makes training more stable, and reduces the standard deviation\nof accuracy across different runs.\nDifferentiable Pooling on STRUCTURE 2V EC. DIFFPOOL can be applied to other GNN architec-\ntures besides GRAPH SAGE to capture hierarchical structure in the graph data. To further support\nanswering Q1, we also applied DIFFPOOL on Structure2Vec (S2V). We ran experiments using S2V\nwith three layer architecture, as reported in [7]. In the ﬁrst variant, oneDIFFPOOL layer is applied\nafter the ﬁrst layer ofS2V, and two more S2V layers are stacked on top of the output of DIFFPOOL.\n7\nThe second variant applies one DIFFPOOL layer after the ﬁrst and second layer ofS2V respectively.\nIn both variants, S2V model is used to compute the embedding matrix, while GRAPH SAGE model is\nused to compute the assignment matrix.\nTable 2: Accuracy results of applying DIFFPOOL to S2V.\nData Set Method\nS2V S2V WITH1 DIFFPOOL S2VWITH2 DIFFPOOL\nENZYMES 61.10 62.86 63.33\nD&D 78.92 80.75 82.07\nThe results in terms of classiﬁcation accuracy are summarized in Table 2. We observe thatDIFFPOOL\nsigniﬁcantly improves the performance of S2V on both ENZYMES and D&D data sets. Similar\nperformance trends are also observed on other data sets. The results demonstrate that DIFFPOOL is a\ngeneral strategy to pool over hierarchical structure that can beneﬁt different GNN architectures.\nRunning time. Although applying DIFFPOOL requires additional computation of an assignment\nmatrix, we observed that DIFFPOOL did not incur substantial additional running time in practice. This\nis because each DIFFPOOL layer reduces the size of graphs by extracting a coarser representation of\nthe graph, which speeds up the graph convolution operation in the next layer. Concretely, we found\nthat GRAPH SAGE with DIFFPOOL was 12× faster than the GRAPH SAGE model with SET2S ET\npooling, while still achieving signiﬁcantly higher accuracy on all benchmarks.\n4.3 Analysis of Cluster Assignment in D IFFPOOL\nHierarchical cluster structure. To address Q3, we investigated the extent to which DIFFPOOL\nlearns meaningful node clusters by visualizing the cluster assignments in different layers. Figure\n2 shows such a visualization of node assignments in the ﬁrst and second layers on a graph from\nCOLLAB data set, where node color indicates its cluster membership. Node cluster membership\nis determined by taking the argmax of its cluster assignment probabilities. We observe that even\nwhen learning cluster assignment based solely on the graph classiﬁcation objective, DIFFPOOL\ncan still capture the hierarchical community structure. We also observe signiﬁcant improvement in\nmembership assignment quality with link prediction auxiliary objectives.\nDense vs. sparse subgraph structure . In addition, we observe that DIFFPOOL learns to collapse\nnodes into soft clusters in a non-uniform way, with a tendency to collapse densely-connected\nsubgraphs into clusters. Since GNNs can efﬁciently perform message-passing on dense, clique-like\nsubgraphs (due to their small diameters) [26], pooling together nodes in such a dense subgraph is not\nlikely to lead to any loss of structural information. This intuitively explains why collapsing dense\nsubgraphs is a useful pooling strategy forDIFFPOOL. In contrast, sparse subgraphs may contain many\ninteresting structures, including path-, cycle- and tree-like structures, and given the high-diameter\ninduced by sparsity, GNN message-passing may fail to capture these structures. Thus, by separately\npooling distinct parts of a sparse subgraph, DIFFPOOL can learn to capture the meaningful structures\npresent in sparse graph regions (e.g., as in Figure 2).\nAssignment for nodes with similar representations. Since the assignment network computes the\nsoft cluster assignment based on features of input nodes and their neighbors, nodes with both similar\ninput features and neighborhood structure will have similar cluster assignment. In fact, one can\nconstruct synthetic cases where 2 nodes, although far away, have exactly the same neighborhood\nstructure and features for self and all neighbors. In this case the pooling network is forced to assign\nthem into the same cluster, which is different from the concept of pooling in other architectures such\nas image ConvNets. In some cases we do observe that disconnected nodes are pooled together.\nIn practice we rely on the identiﬁability assumption similar to Theorem 1 in GraphSAGE [ 16],\nwhere nodes are identiﬁable via their features. This holds in many real datasets 3. The auxiliary\nlink prediction objective is observed to also help discouraging nodes that are far away to be pooled\ntogether. Furthermore, it is possible to use more sophisticated GNN aggregation function such as\n3However, some chemistry molecular graph datasets contain many nodes that are structurally similar, and\nassignment network is observed to pool together nodes that are far away.\n8\nPooling at Layer 1 Pooling at Layer 2\n(a)\n (b)\n (c)\nFigure 2: Visualization of hierarchical cluster assignment in DIFFPOOL, using example graphs from\nCOLLAB . The left ﬁgure (a) shows hierarchical clustering over two layers, where nodes in the second\nlayer correspond to clusters in the ﬁrst layer. (Colors are used to connect the nodes/clusters across the\nlayers, and dotted lines are used to indicate clusters.) The right two plots (b and c) show two more\nexamples ﬁrst-layer clusters in different graphs. Note that although we globally set the number of\nclusters to be 25% of the nodes, the assignment GNN automatically learns the appropriate number of\nmeaningful clusters to assign for these different graphs.\nhigh-order moments [37] to distinguish nodes that are similar in structure and feature space. The\noverall framework remains unchanged.\nSensitivity of the Pre-deﬁned Maximum Number of Clusters . We found that the assignment\nvaries according to the depth of the network andC, the maximum number of clusters. With largerC,\nthe pooling GNN can model more complex hierarchical structure. The trade-off is that very large\nC results in more noise and less efﬁciency. Although the value ofC is a pre-deﬁned parameter, the\npooling net learns to use the appropriate number of clusters by end-to-end training. In particular,\nsome clusters might not be used by the assignment matrix. Column corresponding to unused cluster\nhas low values for all nodes. This is observed in Figure 2(c), where nodes are assigned predominantly\ninto 3 clusters.\n5 Conclusion\nWe introduced a differentiable pooling method for GNNs that is able to extract the complex hierarchi-\ncal structure of real-world graphs. By using the proposed pooling layer in conjunction with existing\nGNN models, we achieved new state-of-the-art results on several graph classiﬁcation benchmarks.\nInteresting future directions include learning hard cluster assignments to further reduce computational\ncost in higher layers while also ensuring differentiability, and applying the hierarchical pooling\nmethod to other downstream tasks that require modeling of the entire graph structure.\nAcknowledgement\nThis research has been supported in part by DARPA SIMPLEX, Stanford Data Science Initiative,\nHuawei, JD and Chan Zuckerberg Biohub. Christopher Morris is funded by the German Science\nFoundation (DFG) within the Collaborative Research Center SFB 876 “Providing Information by\nResource-Constrained Data Analysis”, project A6 “Resource-efﬁcient Graph Mining”. The authors\nalso thank Marinka Zitnik for help in visualizing the high-level illustration of the proposed methods.\nReferences\n[1] M. Bianchini, M. Gori, and F. Scarselli. Processing directed acyclic graphs with recursive neural\nnetworks. IEEE Transactions on Neural Networks, 12(6):1464–1470, 2001.\n[2] K. M. Borgwardt and H.-P. Kriegel. Shortest-path kernels on graphs. In IEEE International\nConference on Data Mining, pages 74–81, 2005.\n[3] K. M. Borgwardt, C. S. Ong, S. Schönauer, S. V . N. Vishwanathan, A. J. Smola, and H.-\nP. Kriegel. Protein function prediction via graph kernels. Bioinformatics, 21(Supplement\n1):i47–i56, 2005.\n9\n[4] M. M. Bronstein, J. Bruna, Y . LeCun, A. Szlam, and P. Vandergheynst. Geometric deep learning:\nGoing beyond euclidean data. IEEE Signal Processing Magazine, 34(4):18–42, 2017.\n[5] J. Bruna, W. Zaremba, A. Szlam, and Y . LeCun. Spectral networks and deep locally connected\nnetworks on graphs. In International Conference on Learning Representations, 2014.\n[6] C.-C. Chang and C.-J. Lin. LIBSVM: A library for support vector machines. ACM Transactions\non Intelligent Systems and Technology , 2:27:1–27:27, 2011. Software available at http:\n//www.csie.ntu.edu.tw/~cjlin/libsvm.\n[7] H. Dai, B. Dai, and L. Song. Discriminative embeddings of latent variable models for structured\ndata. In International Conference on Machine Learning, pages 2702–2711, 2016.\n[8] M. Defferrard, X. Bresson, and P. Vandergheynst. Convolutional neural networks on graphs\nwith fast localized spectral ﬁltering. In Advances in Neural Information Processing Systems,\npages 3844–3852, 2016.\n[9] I. S. Dhillon, Y . Guan, and B. Kulis. Weighted graph cuts without eigenvectors a multilevel\napproach. IEEE Transactions on Pattern Analysis and Machine Intelligence, 29(11):1944–1957,\n2007.\n[10] P. D. Dobson and A. J. Doig. Distinguishing enzyme structures from non-enzymes without\nalignments. Journal of Molecular Biology, 330(4):771 – 783, 2003.\n[11] D. K. Duvenaud, D. Maclaurin, J. Iparraguirre, R. Bombarell, T. Hirzel, A. Aspuru-Guzik,\nand R. P. Adams. Convolutional networks on graphs for learning molecular ﬁngerprints. In\nAdvances in Neural Information Processing Systems, pages 2224–2232, 2015.\n[12] A. Feragen, N. Kasenburg, J. Petersen, M. D. Bruijne, and K. M. Borgwardt. Scalable kernels\nfor graphs with continuous attributes. In Advances in Neural Information Processing Systems,\npages 216–224, 2013. Erratum available at http://image.diku.dk/aasa/papers/\ngraphkernels_nips_erratum.pdf.\n[13] M. Fey, J. E. Lenssen, F. Weichert, and H. Müller. SplineCNN: Fast geometric deep learning with\ncontinuous B-spline kernels. In IEEE Conference on Computer Vision and Pattern Recognition,\n2018.\n[14] A. Fout, J. Byrd, B. Shariat, and A. Ben-Hur. Protein interface prediction using graph convo-\nlutional networks. In Advances in Neural Information Processing Systems, pages 6533–6542,\n2017.\n[15] J. Gilmer, S. S. Schoenholz, P. F. Riley, O. Vinyals, and G. E. Dahl. Neural message passing\nfor quantum chemistry. In International Conference on Machine Learning, pages 1263–1272,\n2017.\n[16] W. L. Hamilton, R. Ying, and J. Leskovec. Inductive representation learning on large graphs. In\nAdvances in Neural Information Processing Systems, pages 1025–1035, 2017.\n[17] W. L. Hamilton, R. Ying, and J. Leskovec. Representation learning on graphs: Methods and\napplications. IEEE Data Engineering Bulletin, 40(3):52–74, 2017.\n[18] S. Ioffe and C. Szegedy. Batch normalization: Accelerating deep network training by reducing\ninternal covariate shift. In International Conference on Machine Learning, pages 448–456,\n2015.\n[19] W. Jin, C. W. Coley, R. Barzilay, and T. S. Jaakkola. Predicting organic reaction outcomes with\nWeisfeiler-Lehman network. In Advances in Neural Information Processing Systems, pages\n2604–2613, 2017.\n[20] K. Kersting, N. M. Kriege, C. Morris, P. Mutzel, and M. Neumann. Benchmark data sets for\ngraph kernels, 2016.\n[21] T. N. Kipf and M. Welling. Semi-supervised classiﬁcation with graph convolutional networks.\nIn International Conference on Learning Representations, 2017.\n10\n[22] N. M. Kriege, P.-L. Giscard, and R. Wilson. On valid optimal assignment kernels and appli-\ncations to graph classiﬁcation. In Advances in Neural Information Processing Systems, pages\n1623–1631, 2016.\n[23] A. Krizhevsky, I. Sutskever, and G. E. Hinton. ImageNet classiﬁcation with deep convolutional\nneural networks. In Advances in Neural Information Processing Systems, pages 1097–1105,\n2012.\n[24] T. Lei, W. Jin, R. Barzilay, and T. S. Jaakkola. Deriving neural architectures from sequence and\ngraph kernels. In International Conference on Machine Learning, pages 2024–2033, 2017.\n[25] Y . Li, D. Tarlow, M. Brockschmidt, and R. Zemel. Gated graph sequence neural networks. In\nInternational Conference on Learning Representations, 2016.\n[26] R. Liao, M. Brockschmidt, D. Tarlow, A. L. Gaunt, R. Urtasun, and R. Zemel. Graph partition\nneural networks for semi-supervised classiﬁcation. InInternational Conference on Learning\nRepresentations (Workshop Track), 2018.\n[27] A. Lusci, G. Pollastri, and P. Baldi. Deep architectures and deep learning in chemoinformatics:\nThe prediction of aqueous solubility for drug-like molecules. Journal of Chemical Information\nand Modeling, 53(7):1563–1575, 2013.\n[28] C. Merkwirth and T. Lengauer. Automatic generation of complementary descriptors with\nmolecular graph networks. Journal of Chemical Information and Modeling, 45(5):1159–1168,\n2005.\n[29] M. Niepert, M. Ahmed, and K. Kutzkov. Learning convolutional neural networks for graphs. In\nInternational Conference on Machine Learning, pages 2014–2023, 2016.\n[30] F. Scarselli, M. Gori, A. C. Tsoi, M. Hagenbuchner, and G. Monfardini. The graph neural\nnetwork model. Transactions on Neural Networks, 20(1):61–80, 2009.\n[31] M. Schlichtkrull, T. N. Kipf, P. Bloem, R. van den Berg, I. Titov, and M. Welling. Modeling\nrelational data with graph convolutional networks. In Extended Semantic Web Conference,\n2018.\n[32] K. Schütt, P. J. Kindermans, H. E. Sauceda, S. Chmiela, A. Tkatchenko, and K. R. Müller.\nSchNet: A continuous-ﬁlter convolutional neural network for modeling quantum interactions.\nIn Advances in Neural Information Processing Systems, pages 992–1002, 2017.\n[33] N. Shervashidze, P. Schweitzer, E. J. van Leeuwen, K. Mehlhorn, and K. M. Borgwardt.\nWeisfeiler-Lehman graph kernels. Journal of Machine Learning Research , 12:2539–2561,\n2011.\n[34] N. Shervashidze, S. V . N. Vishwanathan, T. H. Petri, K. Mehlhorn, and K. M. Borgwardt.\nEfﬁcient graphlet kernels for large graph comparison. InInternational Conference on Artiﬁcial\nIntelligence and Statistics, pages 488–495, 2009.\n[35] M. Simonovsky and N. Komodakis. Dynamic edge-conditioned ﬁlters in convolutional neural\nnetworks on graphs. In IEEE Conference on Computer Vision and Pattern Recognition, pages\n29–38, 2017.\n[36] P. Veliˇckovi´c, G. Cucurull, A. Casanova, A. Romero, P. Liò, and Y . Bengio. Graph attention\nnetworks. In International Conference on Learning Representations, 2018.\n[37] S. Verma and Z.-L. Zhang. Graph capsule convolutional neural networks. arXiv preprint\narXiv:1805.08090, 2018.\n[38] O. Vinyals, S. Bengio, and M. Kudlur. Order matters: Sequence to sequence for sets. In\nInternational Conference on Learning Representations, 2015.\n[39] P. Yanardag and S. V . N. Vishwanathan. A structural smoothing framework for robust graph\ncomparison. In Advances in Neural Information Processing Systems, pages 2134–2142, 2015.\n[40] M. Zhang, Z. Cui, M. Neumann, and Y . Chen. An end-to-end deep learning architecture for\ngraph classiﬁcation. In AAAI Conference on Artiﬁcial Intelligence, 2018.\n11",
  "values": {
    "Not socially biased": "No",
    "Explicability": "No",
    "Deferral to humans": "No",
    "Respect for Persons": "No",
    "Collective influence": "No",
    "Critiqability": "No",
    "User influence": "No",
    "Interpretable (to users)": "No",
    "Beneficence": "No",
    "Transparent (to users)": "No",
    "Justice": "No",
    "Respect for Law and public interest": "No",
    "Fairness": "No",
    "Non-maleficence": "No",
    "Autonomy (power to decide)": "No",
    "Privacy": "No"
  }
}