{
  "pdf": "NeurIPS-2018-learning-overparameterized-neural-networks-via-stochastic-gradient-descent-on-structured-data-Paper",
  "title": "Learning Overparameterized Neural Networks via Stochastic Gradient Descent on Structured Data",
  "author": "Yuanzhi Li, Yingyu Liang",
  "paper_id": "NeurIPS-2018-learning-overparameterized-neural-networks-via-stochastic-gradient-descent-on-structured-data-Paper",
  "text": "Learning Overparameterized Neural Networks via\nStochastic Gradient Descent on Structured Data\nYuanzhi Li\nComputer Science Department\nStanford University\nStanford, CA 94305\nyuanzhil@stanford.edu\nYingyu Liang\nDepartment of Computer Sciences\nUniversity of Wisconsin-Madison\nMadison, WI 53706\nyliang@cs.wisc.edu\nAbstract\nNeural networks have many successful applications, while much less theoretical\nunderstanding has been gained. Towards bridging this gap, we study the problem of\nlearning a two-layer overparameterized ReLU neural network for multi-class clas-\nsiﬁcation via stochastic gradient descent (SGD) from random initialization. In the\noverparameterized setting, when the data comes from mixtures of well-separated\ndistributions, we prove that SGD learns a network with a small generalization\nerror, albeit the network has enough capacity to ﬁt arbitrary labels. Furthermore,\nthe analysis provides interesting insights into several aspects of learning neural\nnetworks and can be veriﬁed based on empirical studies on synthetic data and on\nthe MNIST dataset.\n1 Introduction\nNeural networks have achieved great success in many applications, but despite a recent increase\nof theoretical studies, much remains to be explained. For example, it is empirically observed that\nlearning with stochastic gradient descent (SGD) in the overparameterized setting (i.e., learning a large\nnetwork with number of parameters larger than the number of training data points) does not lead to\noverﬁtting [24, 31]. Some recent studies use the low complexity of the learned solution to explain the\ngeneralization, but usually do not explain how the SGD or its variants favors low complexity solutions\n(i.e., the inductive bias or implicit regularization) [3, 23]. It is also observed that overparameterization\nand proper random initialization can help the optimization [ 28, 12, 26, 18], but it is also not well\nunderstood why a particular initialization can improve learning. Moreover, most of the existing\nworks trying to explain these phenomenons in general rely on unrealistic assumptions about the data\ndistribution, such as Gaussian-ness and/or linear separability [32, 25, 10, 17, 7].\nThis paper thus proposes to study the problem of learning a two-layer overparameterized neural\nnetwork using SGD for classiﬁcation, on data with a more realistic structure. In particular, the data\nin each class is a mixture of several components, and components from different classes are well\nseparated in distance (but the components in each class can be close to each other). This is motivated\nby practical data. For example, on the dataset MNIST [15], each class corresponds to a digit and can\nhave several components corresponding to different writing styles of the digit, and an image in it is\na small perturbation of one of the components. On the other hand, images that belong to the same\ncomponent are closer to each other than to an image of another digit. Analysis in this setting can then\nhelp understand how the structure of the practical data affects the optimization and generalization.\nIn this setting, we prove that when the network is sufﬁciently overparameterized, SGD provably\nlearns a network close to the random initialization and with a small generalization error. This result\nshows that in the overparameterized setting and when the data is well structured, though in principle\n32nd Conference on Neural Information Processing Systems (NeurIPS 2018), Montréal, Canada.\nthe network can overﬁt, SGD with random initialization introduces a strong inductive bias and leads\nto good generalization.\nOur result also shows that the overparameterization requirement and the learning time depends on the\nparameters inherent to the structure of the data but not on the ambient dimension of the data. More\nimportantly, the analysis to obtain the result also provides some interesting theoretical insights for\nvarious aspects of learning neural networks. It reveals that the success of learning crucially relies\non overparameterization and random initialization. These two combined together lead to a tight\ncoupling around the initialization between the SGD and another learning process that has a benign\noptimization landscape. This coupling, together with the structure of the data, allows SGD to ﬁnd a\nsolution that has a low generalization error, while still remains in the aforementioned neighborhood of\nthe initialization. Our work makes a step towrads explaining how overparameterization and random\ninitialization help optimization, and how the inductive bias and good generalization arise from the\nSGD dynamics on structured data. Some other more technical implications of our analysis will be\ndiscussed in later sections, such as the existence of a good solution close to the initialization, and the\nlow-rankness of the weights learned. Complementary empirical studies on synthetic data and on the\nbenchmark dataset MNIST provide positive support for the analysis and insights.\n2 Related Work\nGeneralization of neural networks. Empirical studies show interesting phenomena about the\ngeneralization of neural networks: practical neural networks have the capacity to ﬁt random labels of\nthe training data, yet they still have good generalization when trained on practical data [24, 31, 2].\nThese networks are overparameterized in that they have more parameters than statistically necessary,\nand their good generalization cannot be explained by naïvely applying traditional theory. Several\nlines of work have proposed certain low complexity measures of the learned network and derived\ngeneralization bounds to better explain the phenomena. [ 3, 23, 21] proved spectrally-normalized\nmargin-based generalization bounds, [ 9, 23] derived bounds from a PAC-Bayes approach, and\n[1, 33, 4] derived bounds from the compression point of view. They, in general, do not address\nwhy the low complexity arises. This paper takes a step towards this direction, though on two-layer\nnetworks and a simpliﬁed model of the data.\nOverparameterization and implicit regularization. The training objectives of overparameterized\nnetworks in principle have many (approximate) global optima and some generalize better than the\nothers [14, 8, 2], while empirical observations imply that the optimization process in practice prefers\nthose with better generalization. It is then an interesting question how this implicit regularization or\ninductive bias arises from the optimization and the structure of the data. Recent studies are on SGD\nfor different tasks, such as logistic regression [27] and matrix factorization [11, 19, 16]. More related\nto our work is [7], which studies the problem of learning a two-layer overparameterized network on\nlinearly separable data and shows that SGD converges to a global optimum with good generalization.\nOur work studies the problem on data with a well clustered (and potentially not linearly separable)\nstructure that we believe is closer to practical scenarios and thus can advance this line of research.\nTheoretical analysis of learning neural networks. There also exists a large body of work that\nanalyzes the optimization landscape of learning neural networks [13, 26, 30, 10, 25, 29, 6, 32, 17, 5].\nThey in general need to assume unrealistic assumptions about the data such as Gaussian-ness, and/or\nhave strong assumptions about the network such as using only linear activation. They also do not\nstudy the implicit regularization by the optimization algorithms.\n3 Problem Setup\nIn this work, a two-layer neural network with ReLU activation fork-classes classiﬁcation is given by\nf = (f1,f 2,···,fk) such that for eachi∈[k]:\nfi(x) =\nm∑\nr=1\nai,rReLU(⟨wr,x⟩)\nwhere{wr∈Rd}are the weights for them neurons in the hidden layer,{ai,r∈R}are the weights\nof the top layer, and ReLU(z) = max{0,z}.\n2\nAssumptions about the data. The data is generated from a distributionD as follows. There arek×l\nunknown distributions{Di,j}i∈[k],j∈[l] over Rd and probabilitiespi,j≥0 such that∑\ni,jpi,j = 1.\nEach data point (x,y ) is i.i.d. generated by: (1) Samplez∈[k]×[l] such that Pr[z = (i,j )] =pi,j;\n(2) Set labely =z[0], and samplex fromDz. Assume we sampleN points{(xi,yi)}N\ni=1.\nLet us deﬁne the support of a distributionD with densityp overRd as supp(D) ={x :p(x)> 0},\nthe distance between two setsS1,S2⊆Rd as dist(S1,S2) = minx∈S1,y∈S2{∥x−y∥2}, and the\ndiameter of a setS1⊆Rd as diam(S1) = maxx,y∈S1{∥x−y∥2}.Then we are ready to make the\nassumptions about the data.\n(A1) (Separability) There exists δ >0 such that for every i1 ̸= i2 ∈[k] and every\nj1,j 2∈[l], dist (supp(Di1,j1), supp(Di2,j2))≥δ.Moreover, for every i∈[k],j ∈[l],1\ndiam(supp(Di,j))≤λδ,forλ≤1/(8l).\n(A2) (Normalization) Anyx from the distribution has∥x∥2 = 1.\nA few remarks are worthy. Instead of having one distribution for one class, we allow an arbitrary\nl≥1 distributions in each class, which we believe is a better ﬁt to the real data. For example, in\nMNIST, a class can be the number 1, andl can be the different styles of writing 1 (1 or|or/).\nAssumption (A2) is for simplicity, while (A1) is our key assumption. With l≥1 distributions\ninside each class, our assumption allows data that is not linearly separable, e.g., XOR type data in\nR2 where there are two classes, one consisting of two balls of diameter 1/10 with centers (0, 0)\nand (2, 2) and the other consisting of two of the same diameter with centers (0, 2) and (2, 0). See\nFigure 3 in Appendix C for an illustration. Moreover, essentially the only assumption we have\nhere isλ= O(1/l). When l = 1,λ= O(1), which is the minimal requirement on the order of λ\nfor the distribution to be efﬁciently learnable. Our work allows larger l, so that the data can be\nmore complicated inside each class. In this case, we require the separation to also be higher. When\nwe increasel to reﬁne the distributions inside each class, we should expect the diameters of each\ndistribution become smaller as well. As long as the rate of diameter decreasing in each distribution is\ngreater than the total number of distributions, then our assumption will hold.\nAssumptions about the learning process. We will only learn the weightwr to simplify the analysis.\nSince the ReLU activation is positive homogeneous, the effect of overparameterization can still be\nstudied, and a similar approach has been adopted in previous work [7]. So the network is also written\nasy =f(x,w ) = (f1(x,w ),···,fk(x,w )) forw = (w1,···,wr).\nWe assume the learning is from a random initialization:\n(A3) (Random initialization)w(0)\nr ∼N(0,σ2I),ai,r∼N(0, 1), withσ= 1\nm1/2 .\nThe learning process minimizes the cross entropy loss over the softmax, deﬁned as:\nL(w) =−1\nN\nN∑\ns=1\nlogoys(xs,w ), whereoy(x,w ) = efy(x,w)\n∑k\ni=1efi(x,w)\n.\nLetL(w,xs,ys) =−logoys(xs,w ) denote the cross entropy loss for a particular point (xs,ys).\nWe consider a minibatch SGD of batch sizeB, number of iterationsT =N/B and learning rateηas\nthe following process: Randomly divide the total training examples intoT batches, each of sizeB.\nLet the indices of the examples in thet-th batch beBt. At each iteration, the update is2\nw(t+1)\nr =w(t)\nr −η1\nB\n∑\ns∈Bt\n∂L(w(t),xs,ys)\n∂w(t)\nr\n,∀r∈[m], where\n∂L(w,xs,ys)\n∂wr\n=\n\n∑\ni̸=ys\nai,roi(xs,w )−\n∑\ni̸=ys\nays,roi(xs,w )\n\n1⟨wr,xs⟩≥0xs. (1)\n1The assumption 1/(8l) can be made to 1/[(1 +α)l] for anyα>0 by paying a large polynomial in 1/αin\nthe sample complexity. We will not prove it in this paper because we would like to highlight the key factors.\n2Strictly speaking,L(w,x s,y s) does not have gradient everywhere due to the non-smoothness of ReLU.\nOne can view ∂L(w,xs,ys)\n∂wr\nas a convenient notation for the right hand side of (1).\n3\n4 Main Result\nFor notation simplicity, for a target errorε(to be speciﬁed later), with high probability (or w.h.p.)\nmeans with probability 1−1/poly(1/δ,k,l,m,1/ε) for a sufﬁciently large polynomialpoly, and ˜O\nhides factors of poly(log 1/δ,logk, logl, logm, log 1/ε).\nTheorem 4.1. Suppose the assumptions (A1)(A2)(A3) are satisﬁed. Then for every ε >0,\nthere is M = poly(k,l, 1/δ,1/ε) such that for every m ≥M, after doing a minibatch SGD\nwith batch size B = poly(k,l, 1/δ,1/ε,logm) and learning rate η= 1\nm·poly(k,l,1/δ,1/ε,logm) for\nT = poly(k,l, 1/δ,1/ε,logm) iterations, with high probability:\nPr\n(x,y)∼D\n[\n∀j∈[k],j ̸=y,fy(x,w (T ))>f j(x,w (T ))\n]\n≥1−ε.\nOur theorem implies if the data satisﬁes our assumptions, and we parametrize the network properly,\nthen we only need polynomial in k,l, 1/δmany samples to achieve a good prediction error. This\nerror is measured directly on the true distributionD, not merely on the input data used to train this\nnetwork. Our result is also dimension free: There is no dependency on the underlying dimension\nd of the data, the complexity is fully captured by k,l, 1/δ. Moreover, no matter how much the\nnetwork is overparameterized, it will only increase the total iterations by factors of logm. So we can\noverparameterize by an sub-exponential amount without signiﬁcantly increasing the complexity.\nFurthermore, we can always treat each input example as an individual distribution, thusλis always\nzero. In this case, if we use batch size B forT iterations, we would havel =N =BT . Then our\ntheorem indicate that as long asm = poly(N, 1/δ′), whereδ′is the minimal distance between each\nexamples, we can actually ﬁt arbitrary labels of the input data. However, since the total iteration only\ndepends on logm, whenm = poly(N, 1/δ′) but the input data is actually structured (with smallk,l\nand largeδ), then SGD can actually achieve a small generalization error, even when the network has\nenough capacity to ﬁt arbitrary labels of the training examples (and can also be done by SGD). Thus,\nwe prove that SGD has a strong inductive bias on structured data: Instead of ﬁnding a bad global\noptima that can ﬁt arbitrary labels, it actually ﬁnds those with good generalization guarantees. This\ngives more thorough explanation to the empirical observations in [24, 31].\n5 Intuition and Proof Sketch for A Simpliﬁed Case\nTo train a neural network with ReLU activations, there are two questions need to be addressed:\n1 Why can SGD optimize the training loss? Or even ﬁnding a critical point? Since the under-\nlying network is highly non-smooth, existing theorems do not give any ﬁnite convergence\nrate of SGD for training neural network with ReLUs activations.\n2 Why can the trained network generalize? Even when the capacity is large enough to ﬁt\nrandom labels of the input data? This is known as the inductive bias of SGD.\nThis work takes a step towards answering these two questions. We show that when the network is\noverparameterized, it becomes more “pseudo smooth”, which makes it easir for SGD to minimize\nthe training loss, and furthermore, it will not hurt the generalization error. Our proof is based on the\nfollowing important observation:\nThe more we overparameterize the network, the less likely the activation pattern for one\nneuron and one data point will change in a ﬁxed number of iterations.\nThis observation allows us to couple the gradient of the true neural network with a “pseudo gradient”\nwhere the activation pattern for each data point and each neuron is ﬁxed. That is, when computing the\n“pseudo gradient”, for ﬁxedr,i , whether ther-th hidden node is activated on thei-th data pointxi\nwill always be the same for differentt. (But for ﬁxedt, for differentr ori, the sign can be different.)\nWe are able to prove that unless the generalization error is small, the “pseudo gradient” will always\nbe large. Moreover, we show that the network is actually smooth thus SGD can minimize the loss.\nWe then show that when the number m of hidden neurons increases, with a properly decreasing\nlearning rate, the total number of iterations it takes to minimize the loss is roughly not changed.\nHowever, the total number of iterations that we can couple the true gradient with the pseudo one\n4\nincreases. Thus, there is a polynomially largem so that we can couple these two gradients until the\nnetwork reaches a small generalization error.\n5.1 A Simpliﬁed Case: No Variance\nHere we illustrate the proof sketch for a simpliﬁed case and Appendix A provides the proof. The\nproof for the general case is provided in Appendix B. In the simpliﬁed case, we further assume:\n(S) (No variance) EachDa,b is a single data point (xa,b,a ), and also we are doing full batch\ngradient descent as opposite to the minibatch SGD.\nThen we reload the loss notation asL(w) =∑\na∈[k],b∈[l]pa,bL(w,xa,b,a ), and the gradient is\n∂L(w)\n∂wr\n=\n∑\na∈[k],b∈[l]\npa,b\n\n∑\ni̸=a\nai,roi(xa,b,w )−\n∑\ni̸=a\naa,roi(xa,b,w )\n\n1⟨wr,xa,b⟩≥0xa,b.\nFollowing the intuition above, we deﬁne the pseudo gradient as\n˜∂L(w)\n∂wr\n=\n∑\na∈[k],b∈[l]\npa,b\n\n∑\ni̸=a\nai,roi(xa,b,w )−\n∑\ni̸=a\naa,roi(xa,b,w )\n\n1⟨w(0)\nr ,xa,b⟩≥0xa,b,\nwhere it uses 1⟨w(0)\nr ,xa,b⟩≥0 instead of 1⟨wr,xa,b⟩≥0 as in the true gradient. That is, the activation\npattern is set to be that in the initialization. Intuitively, the pseudo gradient is similar to the gradient for\na pseudo networkg (but not exactly the same), deﬁned asgi(x,w ) :=∑m\nr=1ai,r⟨wr,x⟩1⣨\nw(0)\nr ,x\n⟩\n≥0.\nCoupling the gradients is then similar to coupling the networksf andg.\nFor simplicity, let va,a,b := ∑\ni̸=aoi(xa,b,w ) =\n∑\ni̸=aefi(xa,b,w)\n∑k\ni=1efi(xa,b,w) and when s ̸= a, vs,a,b :=\n−os(xa,b,w ) =−efs(xa,b,w)\n∑k\ni=1efi(xa,b,w).Roughly, ifva,a,b is small, then fa(xa,b,w ) is relatively larger\ncompared to the otherfi(xa,b,w ), so the classiﬁcation error is small.\nWe prove the following two main lemmas. The ﬁrst says that at each iteration, the total number of\nhidden units whose gradient can be coupled with the pseudo one is quite large.\nLemma 5.1 (Coupling). W.h.p. over the random initialization, for everyτ >0, for everyt = ˜O\n(\nτ\nη\n)\n,\nwe have that for at least 1−eτkl\nσ fraction ofr∈[m]: ∂L(w(t))\n∂wr\n=\n˜∂L(w(t))\n∂wr\n.\nThe second lemma says that the pseudo gradient is large unless the error is small.\nLemma 5.2. Form = ˜Ω\n(\nk3l2\nδ\n)\n, for every {pa,bvi,a,b}i,a∈[k],b∈[l] ∈[−v,v ] (that depends on\nw(0)\nr ,ai,r, etc.) with max{pa,bvi,a,b}i,a∈[k],b∈[l] =v, there exists at least Ω( δ\nkl ) fraction ofr∈[m]\nsuch that\n\n˜∂L(w)\n∂wr\n\n2\n= ˜Ω\n(vδ\nkl\n)\n.\nWe now illustrate how to use these two lemmas to show the convergence for a small enough learning\nrateη. For simplicity, let us assume thatkl/δ=O(1) andε=o(1). Thus, by Lemma 5.2 we know\nthat unless v≤ε, there are Ω(1) fraction of r such that\n˜∂L(w)/∂wr\n\n2\n= Ω(ε). Moreover, by\nLemma 5.1 we know that we can pickτ= Θ(σε) soeτ/σ= Θ(ε), which implies that there are Ω(1)\nfraction ofr such that∥∂L(w)/∂wr∥2 = Ω(ε) as well. For small enough learning rateη, doing one\nstep of gradient descent will thus decreaseL(w) by Ω(ηmε2), so it converges int =O\n(\n1/ηmε2)\niterations. In the end, we just need to make sure that 1/ηmε2≤O(τ/η) = Θ( σε/η) so we can\nalways apply the coupling Lemma 5.1. By σ= ˜O(1/m−1/2) we know that this is true as long as\nm≥poly(1/ε). A smallv can be shown to lead to a small generalization error.\n6 Discussion of Insights from the Analysis\nOur analysis, though for learning two-layer networks on well structured data, also sheds some light\nupon learning neural networks in more general settings.\n5\nGeneralization. Several lines of recent work explain the generalization phenomenon of overparam-\neterized networks by low complexity of the learned networks, from the point views of spectrally-\nnormalized margins [3, 23, 21], compression [1, 33, 4], and PAC-Bayes [9, 23].\nOur analysis has partially explained how SGD (with proper random initialization) on structured data\nleads to the low complexity from the compression and PCA-Bayes point views. We have shown that\nin a neighborhood of the random initialization, w.h.p. the gradients are similar to those of another\nbenign learning process, and thus SGD can reduce the error and reach a good solution while still\nin the neighborhood. The closeness to the initialization then means the weights (or more precisely\nthe difference between the learned weights and the initialization) can be easily compressed. In fact,\nempirical observations have been made and connected to generalization in [22, 1]. Furthermore, [1]\nexplicitly point out such a compression using a helper string (corresponding to the initialization in\nour setting). [1] also point out that the compression view can be regarded as a more explicit form of\nthe PAC-Bayes view, and thus our intuition also applies to the latter.\nThe existence of a solution of a small generalization error near the initialization is itself not obvious.\nIntuitively, on structured data, the updates are structured signals spread out across the weights of the\nhidden neurons. Then for prediction, the random initialized part in the weights has strong cancellation,\nwhile the structured signal part in the weights collectively affects the output. Therefore, the latter can\nbe much smaller than the former while the network can still give accurate predictions. In other words,\nthere can be a solution not far from the initialization with high probability.\nSome insight is provided on the low rank of the weights. More precisely, when the data are well\nclustered around a few patterns, the accumulated updates (difference between the learned weights\nand the initialization) should be approximately low rank, which can be seen from checking the SGD\nupdates. However, when the difference is small compared to the initialization, the spectrum of the\nﬁnal weight matrix is dominated by that of the initialization and thus will tend to closer to that of a\nrandom matrix. Again, such observations/intuitions have been made in the literature and connected\nto compression and generalization (e.g., [1]).\nImplicit regularization v.s. structure of the data. Existing work has analyzed the implicit regular-\nization of SGD on logistic regression [27], matrix factorization [11, 19, 16], and learning two-layer\nnetworks on linearly separable data [ 7]. Our setting and also the analysis techniques are novel\ncompared to the existing work. One motivation to study on structured data is to understand the role\nof structured data play in the implicit regularization, i.e., the observation that the solution learned\non less structured or even random data is further away from the initialization. Indeed, our analysis\nshows that when the network size is ﬁxed (and sufﬁciently overparameterized), learning over poorly\nstructured data (largerk andℓ) needs more iterations and thus the solution can deviate more from\nthe initialization and has higher complexity. An extreme and especially interesting case is when the\nnetwork is overparameterized so that in principle it can ﬁt the training data by viewing each point as a\ncomponent while actually they come from structured distributions with small number of components.\nIn this case, we can show that it still learns a network with a small generalization error; see the more\ntechnical discussion in Section 4.\nWe also note that our analysis is under the assumption that the network is sufﬁciently overparam-\neterized, i.e., m is a sufﬁciently large polynomial ofk,ℓand other related parameters measuring\nthe structure of the data. There could be the case thatm is smaller than this polynomial but is more\nthan sufﬁcient to ﬁt the data, i.e., the network is still overparameterized. Though in this case the\nanalysis still provides useful insight, it does not fully apply; see our experiments with relatively small\nm. On the other hand, the empirical observations [24, 31] suggest that practical networks are highly\noverparameterized, so our intuition may still be helpful there.\nEffect of random initialization. Our analysis also shows how proper random initializations helps\nthe optimization and consequently generalization. Essentially, this guarantees that w.h.p. for weights\nclose to the initialization, many hidden ReLU units will have the same activation patterns (i.e.,\nactivated or not) as for the initializations, which means the gradients in the neighborhood look like\nthose when the hidden units have ﬁxed activation patterns. This allows SGD makes progress when\nthe loss is large, and eventually learns a good solution. We also note that it is essential to carefully\nset the scale of the initialization, which is a extensively studied topic [20, 28]. Our initialization has\na scale related to the number of hidden units, which is particularly useful when the network size is\nvarying, and thus can be of interest in such practical settings.\n6\n0 50 100 150 200 250 300 350 400\nNumber of steps\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0Test Accuracy\nTest Accuracy v.s. number of steps\nNumber of hidden nodes\n500\n1000\n2000\n4000\n8000\n16000\n32000\n(a) Test accuracy\n0 50 100 150 200 250 300 350 400\nNumber of steps\n0.00\n0.01\n0.02\n0.03\n0.04\n0.05\n0.06\n0.07Activation pattern difference ratio\nActivation difference v.s. number of steps\nNumber of hidden nodes\n500\n1000\n2000\n4000\n8000\n16000\n32000 (b) Coupling\n0 50 100 150 200 250 300 350 400\nNumber of steps\n0.000\n0.005\n0.010\n0.015\n0.020\n0.025\n0.030\n0.035\n0.040Relative distance\nRelative distance v.s. number of steps\nNumber of hidden nodes\n500\n1000\n2000\n4000\n8000\n16000\n32000\n(c) Distance from the initialization\n0 20 40 60 80 100\nSingular value index\n10 6\n10 5\n10 4\n10 3\n10 2\n10 1\n100\nSingular value\nSingular values of weight matrix and accumulated updates\nSpectrum for\nWeight matrix\nAccumulated updates (d) Rank of accumulated updates (y-axis in log-scale)\nFigure 1: Results on the synthetic data.\n7 Experiments\nThis section aims at verifying some key implications: (1) the activation patterns of the hidden units\ncouple with those at initialization; (2) The distance from the learned solution from the initialization is\nrelatively small compared to the size of initialization; (3) The accumulated updates (i.e., the difference\nbetween the learned weight matrix and the initialization) have approximately low rank. These are\nindeed supported by the results on the synthetic and the MNIST data. Additional experiments are\npresented in Appendix D.\nSetup. The synthetic data are of 1000 dimension and consist ofk = 10 classes, each havingℓ= 2\ncomponents. Each component is of equal probability 1/(kl), and is a Gaussian with covariance\nσ2/dI and its mean is i.i.d. sampled from a Gaussian distribution N (0,σ2\n0/d), where σ= 1 and\nσ0 = 5. 1000 training data points and 1000 test data points are sampled.\nThe network structure and the learning process follow those in Section 3; the number of hidden units\nm varies in the experiments, and the weights are initialized withN (0, 1/√m). On the synthetic data,\nthe SGD is run forT = 400 steps with batch sizeB = 16 and learning rateη= 10/m. On MNIST,\nthe SGD is run forT = 2×104 steps with batch sizeB = 64 and learning rateη= 4×103/m.\nBesides the test accuracy, we report three quantities corresponding to the three observa-\ntions/implications to be veriﬁed. First, for coupling, we compute the fraction of hidden units\nwhose activation pattern changed compared to the time at initialization. Here, the activation pattern is\ndeﬁned as1 if the input to the ReLU is positive and 0 otherwise. Second, for distance, we compute\nthe relative ratio∥w(t)−w(0)∥F/∥w(0)∥F , wherew(t) is the weight matrix at timet. Finally, for the\nrank of the accumulated updates, we plot the singular values ofw(T )−w(0) whereT is the ﬁnal step.\nAll experiments are repeated 5 times, and the mean and standard deviation are reported.\n7\n0 2500 5000 7500 10000 12500 15000 17500\nNumber of steps\n0.2\n0.4\n0.6\n0.8Test Accuracy\nTest Accuracy v.s. number of steps\nNumber of hidden nodes\n1000\n2000\n4000\n8000\n16000\n32000\n(a) Test accuracy\n0 2500 5000 7500 10000 12500 15000 17500\nNumber of steps\n0.0\n0.1\n0.2\n0.3\n0.4Activation pattern difference ratio\nActivation difference v.s. number of steps\nNumber of hidden nodes\n1000\n2000\n4000\n8000\n16000\n32000 (b) Coupling\n0 2500 5000 7500 10000 12500 15000 17500\nNumber of steps\n0.00\n0.25\n0.50\n0.75\n1.00\n1.25\n1.50\n1.75\n2.00Relative distance\nRelative distance v.s. number of steps\nNumber of hidden nodes\n1000\n2000\n4000\n8000\n16000\n32000\n(c) Distance from the initialization\n0 10 20 30 40 50 60 70 80\nSingular value index\n10−15\n10−13\n10−11\n10−9\n10−7\n10−5\n10−3\n10−1\n101\nSingular value\nSingular values of the  eight matrix and accumulated updates\nSpectrum for\nWeight matrix\nAccumulated updates (d) Rank of accumulated updates (y-axis in log-scale)\nFigure 2: Results on the MNIST data.\nResults. Figure 1 shows the results on the synthetic data. The test accuracy quickly converges\nto 100%, which is even more signiﬁcant with larger number of hidden units, showing that the\noverparameterization helps the optimization and generalization. Recall that our analysis shows that\nfor a learning rate linearly decreasing with the number of hidden nodesm, the number of iterations\nto get the accuracy to achieve a desired accuracy should be roughly the same, which is also veriﬁed\nhere. The activation pattern difference ratio is less than 0.1, indicating a strong coupling. The relative\ndistance is less than 0.1, so the ﬁnal solution is indeed close to the initialization. Finally, the top 20\nsingular values of the accumulated updates are much larger than the rest while the spectrum of the\nweight matrix do not have such structure, which is also consistent with our analysis.\nFigure 2 shows the results on MNIST. The observation in general is similar to those on the synthetic\ndata (though less signiﬁcant), and also the observed trend become more evident with more overpa-\nrameterization. Some additional results (e.g., varying the variance of the synthetic data) are provided\nin the appendix that also support our theory.\n8 Conclusion\nThis work studied the problem of learning a two-layer overparameterized ReLU neural network\nvia stochastic gradient descent (SGD) from random initialization, on data with structure inspired\nby practical datasets. While our work makes a step towards theoretical understanding of SGD for\ntraining neural networs, it is far from being conclusive. In particular, the real data could be separable\nwith respect to different metric thanℓ2, or even a non-convex distance given by some manifold. We\nview this an important open direction.\n8\nAcknowledgements\nWe would like to thank the anonymous reviewers of NIPS’18 and Jason Lee for helpful comments.\nThis work was supported in part by FA9550-18-1-0166, NSF grants CCF-1527371, DMS-1317308,\nSimons Investigator Award, Simons Collaboration Grant, and ONR-N00014-16-1-2329. Yingyu\nLiang would also like to acknowledge that support for this research was provided by the Ofﬁce of the\nVice Chancellor for Research and Graduate Education at the University of Wisconsin Madison with\nfunding from the Wisconsin Alumni Research Foundation.\nReferences\n[1] Sanjeev Arora, Rong Ge, Behnam Neyshabur, and Yi Zhang. Stronger generalization bounds\nfor deep nets via a compression approach. arXiv preprint arXiv:1802.05296, 2018.\n[2] Devansh Arpit, Stanisław Jastrz˛ ebski, Nicolas Ballas, David Krueger, Emmanuel Bengio,\nMaxinder S Kanwal, Tegan Maharaj, Asja Fischer, Aaron Courville, Yoshua Bengio, et al. A\ncloser look at memorization in deep networks. arXiv preprint arXiv:1706.05394, 2017.\n[3] Peter L Bartlett, Dylan J Foster, and Matus J Telgarsky. Spectrally-normalized margin bounds\nfor neural networks. In Advances in Neural Information Processing Systems, pages 6241–6250,\n2017.\n[4] Cenk Baykal, Lucas Liebenwein, Igor Gilitschenski, Dan Feldman, and Daniela Rus. Data-\ndependent coresets for compressing neural networks with applications to generalization bounds.\narXiv preprint arXiv:1804.05345, 2018.\n[5] Digvijay Boob and Guanghui Lan. Theoretical properties of the global optimizer of two layer\nneural network. arXiv preprint arXiv:1710.11241, 2017.\n[6] Alon Brutzkus and Amir Globerson. Globally optimal gradient descent for a convnet with\ngaussian inputs. arXiv preprint arXiv:1702.07966, 2017.\n[7] Alon Brutzkus, Amir Globerson, Eran Malach, and Shai Shalev-Shwartz. Sgd learns over-\nparameterized networks that provably generalize on linearly separable data. arXiv preprint\narXiv:1710.10174, 2017.\n[8] Laurent Dinh, Razvan Pascanu, Samy Bengio, and Yoshua Bengio. Sharp minima can generalize\nfor deep nets. arXiv preprint arXiv:1703.04933, 2017.\n[9] Gintare Karolina Dziugaite and Daniel M Roy. Computing nonvacuous generalization bounds\nfor deep (stochastic) neural networks with many more parameters than training data. arXiv\npreprint arXiv:1703.11008, 2017.\n[10] Rong Ge, Jason D Lee, and Tengyu Ma. Learning one-hidden-layer neural networks with\nlandscape design. arXiv preprint arXiv:1711.00501, 2017.\n[11] Suriya Gunasekar, Blake E Woodworth, Srinadh Bhojanapalli, Behnam Neyshabur, and Nati\nSrebro. Implicit regularization in matrix factorization. In Advances in Neural Information\nProcessing Systems, pages 6152–6160, 2017.\n[12] Moritz Hardt and Tengyu Ma. Identity matters in deep learning. arXiv preprint\narXiv:1611.04231, 2016.\n[13] Kenji Kawaguchi. Deep learning without poor local minima. In Advances in Neural Information\nProcessing Systems, pages 586–594, 2016.\n[14] Nitish Shirish Keskar, Dheevatsa Mudigere, Jorge Nocedal, Mikhail Smelyanskiy, and Ping\nTak Peter Tang. On large-batch training for deep learning: Generalization gap and sharp minima.\narXiv preprint arXiv:1609.04836, 2016.\n[15] Yann LeCun, Léon Bottou, Yoshua Bengio, and Patrick Haffner. Gradient-based learning\napplied to document recognition. Proceedings of the IEEE, 86(11):2278–2324, 1998.\n9\n[16] Yuanzhi Li, Tengyu Ma, and Hongyang Zhang. Algorithmic regularization in over-parameterized\nmatrix recovery. arXiv preprint arXiv:1712.09203, 2017.\n[17] Yuanzhi Li and Yang Yuan. Convergence analysis of two-layer neural networks with relu\nactivation. In Advances in Neural Information Processing Systems, pages 597–607, 2017.\n[18] Roi Livni, Shai Shalev-Shwartz, and Ohad Shamir. On the computational efﬁciency of training\nneural networks. In Advances in Neural Information Processing Systems, pages 855–863, 2014.\n[19] Cong Ma, Kaizheng Wang, Yuejie Chi, and Yuxin Chen. Implicit regularization in nonconvex\nstatistical estimation: Gradient descent converges linearly for phase retrieval, matrix completion\nand blind deconvolution. arXiv preprint arXiv:1711.10467, 2017.\n[20] James Martens. Deep learning via hessian-free optimization. In ICML, volume 27, pages\n735–742, 2010.\n[21] Cisse Moustapha, Bojanowski Piotr, Grave Edouard, Dauphin Yann, and Usunier Nicolas. Parse-\nval networks: Improving robustness to adversarial examples. arXiv preprint arXiv:1704.08847,\n2017.\n[22] Vaishnavh Nagarajan and Zico Kolter. Generalization in deep networks: The role of distance\nfrom initialization. NIPS workshop on Deep Learning: Bridging Theory and Practice, 2017.\n[23] Behnam Neyshabur, Srinadh Bhojanapalli, David McAllester, and Nathan Srebro. A pac-\nbayesian approach to spectrally-normalized margin bounds for neural networks. arXiv preprint\narXiv:1707.09564, 2017.\n[24] Behnam Neyshabur, Ryota Tomioka, and Nathan Srebro. In search of the real inductive bias:\nOn the role of implicit regularization in deep learning. arXiv preprint arXiv:1412.6614, 2014.\n[25] Mahdi Soltanolkotabi, Adel Javanmard, and Jason D Lee. Theoretical insights into the optimiza-\ntion landscape of over-parameterized shallow neural networks.arXiv preprint arXiv:1707.04926,\n2017.\n[26] Daniel Soudry and Yair Carmon. No bad local minima: Data independent training error\nguarantees for multilayer neural networks. arXiv preprint arXiv:1605.08361, 2016.\n[27] Daniel Soudry, Elad Hoffer, and Nathan Srebro. The implicit bias of gradient descent on\nseparable data. arXiv preprint arXiv:1710.10345, 2017.\n[28] Ilya Sutskever, James Martens, George Dahl, and Geoffrey Hinton. On the importance of\ninitialization and momentum in deep learning. In International conference on machine learning,\npages 1139–1147, 2013.\n[29] Yuandong Tian. An analytical formula of population gradient for two-layered relu network and\nits applications in convergence and critical point analysis. arXiv preprint arXiv:1703.00560,\n2017.\n[30] Bo Xie, Yingyu Liang, and Le Song. Diversity leads to generalization in neural networks. arXiv\npreprint Arxiv:1611.03131, 2016.\n[31] Chiyuan Zhang, Samy Bengio, Moritz Hardt, Benjamin Recht, and Oriol Vinyals. Understanding\ndeep learning requires rethinking generalization. arXiv preprint arXiv:1611.03530, 2016.\n[32] Kai Zhong, Zhao Song, Prateek Jain, Peter L Bartlett, and Inderjit S Dhillon. Recovery\nguarantees for one-hidden-layer neural networks. arXiv preprint arXiv:1706.03175, 2017.\n[33] Wenda Zhou, Victor Veitch, Morgane Austern, Ryan P Adams, and Peter Orbanz. Com-\npressibility and generalization in large-scale deep learning. arXiv preprint arXiv:1804.05862,\n2018.\n10",
  "values": {
    "Privacy": "No",
    "Justice": "No",
    "Respect for Persons": "No",
    "User influence": "No",
    "Interpretable (to users)": "No",
    "Deferral to humans": "No",
    "Critiqability": "No",
    "Collective influence": "No",
    "Respect for Law and public interest": "No",
    "Transparent (to users)": "No",
    "Fairness": "No",
    "Non-maleficence": "No",
    "Not socially biased": "No",
    "Beneficence": "No",
    "Explicability": "No",
    "Autonomy (power to decide)": "No"
  }
}