{
  "pdf": "NeurIPS-2018-neural-tangent-kernel-convergence-and-generalization-in-neural-networks-Paper",
  "title": "Neural Tangent Kernel: Convergence and Generalization in Neural Networks",
  "author": "Arthur Jacot, Franck Gabriel, Clement Hongler",
  "paper_id": "NeurIPS-2018-neural-tangent-kernel-convergence-and-generalization-in-neural-networks-Paper",
  "text": "Neural Tangent Kernel:\nConvergence and Generalization in Neural Networks\nArthur Jacot\n´Ecole Polytechnique F´ed´erale de Lausanne\narthur.jacot@netopera.net\nFranck Gabriel\nImperial College London and ´Ecole Polytechnique F´ed´erale de Lausanne\nfranckrgabriel@gmail.com\nCl´ement Hongler\n´Ecole Polytechnique F´ed´erale de Lausanne\nclement.hongler@gmail.com\nAbstract\nAt initialization, artiﬁcial neural networks (ANNs) are equivalent to Gaussian\nprocesses in the inﬁnite-width limit (12; 9), thus connecting them to kernel methods.\nWe prove that the evolution of an ANN during training can also be described by a\nkernel: during gradient descent on the parameters of an ANN, the network function\nfθ (which maps input vectors to output vectors) follows the kernel gradient of the\nfunctional cost (which is convex, in contrast to the parameter cost) w.r.t. a new\nkernel: the Neural Tangent Kernel (NTK). This kernel is central to describe the\ngeneralization features of ANNs. While the NTK is random at initialization and\nvaries during training, in the inﬁnite-width limit it converges to an explicit limiting\nkernel and it stays constant during training. This makes it possible to study the\ntraining of ANNs in function space instead of parameter space. Convergence of\nthe training can then be related to the positive-deﬁniteness of the limiting NTK.\nWe then focus on the setting of least-squares regression and show that in the inﬁnite-\nwidth limit, the network functionfθ follows a linear differential equation during\ntraining. The convergence is fastest along the largest kernel principal components\nof the input data with respect to the NTK, hence suggesting a theoretical motivation\nfor early stopping.\nFinally we study the NTK numerically, observe its behavior for wide networks,\nand compare it to the inﬁnite-width limit.\n1 Introduction\nArtiﬁcial neural networks (ANNs) have achieved impressive results in numerous areas of machine\nlearning. While it has long been known that ANNs can approximate any function with sufﬁciently\nmany hidden neurons (7; 10), it is not known what the optimization of ANNs converges to. Indeed\nthe loss surface of neural networks optimization problems is highly non-convex: it has a high number\nof saddle points which may slow down the convergence (4). A number of results (3; 13; 14) suggest\nthat for wide enough networks, there are very few “bad” local minima, i.e. local minima with much\nhigher cost than the global minimum. More recently, the investigation of the geometry of the loss\nlandscape at initialization has been the subject of a precise study (8). The analysis of the dynamics\n32nd Conference on Neural Information Processing Systems (NeurIPS 2018), Montr´eal, Canada.\nof training in the large-width limit for shallow networks has seen recent progress as well (11). To\nthe best of the authors knowledge, the dynamics of deep networks has however remained an open\nproblem until the present paper: see the contributions section below.\nA particularly mysterious feature of ANNs is their good generalization properties in spite of their\nusual over-parametrization (16). It seems paradoxical that a reasonably large neural network can ﬁt\nrandom labels, while still obtaining good test accuracy when trained on real data (19). It can be noted\nthat in this case, kernel methods have the same properties (1).\nIn the inﬁnite-width limit, ANNs have a Gaussian distribution described by a kernel (12; 9). These\nkernels are used in Bayesian inference or Support Vector Machines, yielding results comparable to\nANNs trained with gradient descent (9; 2). We will see that in the same limit, the behavior of ANNs\nduring training is described by a related kernel, which we call the neural tangent network (NTK).\n1.1 Contribution\nWe study the network functionfθ of an ANN, which maps an input vector to an output vector, where\nθ is the vector of the parameters of the ANN. In the limit as the widths of the hidden layers tend to\ninﬁnity, the network function at initialization,fθ converges to a Gaussian distribution (12; 9).\nIn this paper, we investigate fully connected networks in this inﬁnite-width limit, and describe the\ndynamics of the network functionfθ during training:\n• During gradient descent, we show that the dynamics offθ follows that of the so-calledkernel\ngradient descent in function space with respect to a limiting kernel, which only depends on\nthe depth of the network, the choice of nonlinearity and the initialization variance.\n• The convergence properties of ANNs during training can then be related to the positive-\ndeﬁniteness of the inﬁnite-width limit NTK. The values of the network functionfθ outside\nthe training set is described by the NTK, which is crucial to understand how ANN generalize.\n• For a least-squares regression loss, the network function fθ follows a linear differential\nequation in the inﬁnite-width limit, and the eigenfunctions of the Jacobian are the kernel\nprincipal components of the input data. This shows a direct connection to kernel methods\nand motivates the use of early stopping to reduce overﬁtting in the training of ANNs.\n• Finally we investigate these theoretical results numerically for an artiﬁcial dataset (of points\non the unit circle) and for the MNIST dataset. In particular we observe that the behavior of\nwide ANNs is close to the theoretical limit.\n2 Neural networks\nIn this article, we consider fully-connected ANNs with layers numbered from 0 (input) toL (output),\neach containingn0,...,n L neurons, and with a Lipschitz, twice differentiable nonlinearity function\nσ : R→ R, with bounded second derivative 1.\nThis paper focuses on the ANN realization function F (L) : RP →F , mapping parameters θ to\nfunctionsfθ in a spaceF. The dimension of the parameter space isP =∑L−1\nℓ=0 (nℓ + 1)nℓ+1: the\nparameters consist of the connection matricesW (ℓ)∈ Rnℓ×nℓ+1 and bias vectorsb(ℓ)∈ Rnℓ+1 for\nℓ = 0,...,L − 1. In our setup, the parameters are initialized as iid GaussiansN (0, 1).\nFor a ﬁxed distribution pin on the input space Rn0, the function space F is deﬁned as\n{f : Rn0→ RnL}. On this space, we consider the seminorm ||·|| pin, deﬁned in terms of the\nbilinear form\n⟨f,g⟩pin = Ex∼pin\n[\nf(x)Tg(x)\n]\n.\nIn this paper, we assume that the input distributionpin is the empirical distribution on a ﬁnite dataset\nx1,...,x N , i.e the sum of Dirac measures 1\nN\n∑N\ni=0δxi.\n1While these smoothness assumptions greatly simplify the proofs of our results, they do not seem to be\nstrictly needed for the results to hold true.\n2\nWe deﬁne the network function byfθ(x) := ˜α(L)(x;θ), where the functions ˜α(ℓ)(·;θ) : Rn0→ Rnℓ\n(called preactivations) andα(ℓ)(·;θ) : Rn0→ Rnℓ (called activations) are deﬁned from the0-th to\ntheL-th layer by:\nα(0)(x;θ) =x\n˜α(ℓ+1)(x;θ) = 1√nℓ\nW (ℓ)α(ℓ)(x;θ) +βb(ℓ)\nα(ℓ)(x;θ) =σ(˜α(ℓ)(x;θ)),\nwhere the nonlinearityσ is applied entrywise. The scalarβ >0 is a parameter which allows us to\ntune the inﬂuence of the bias on the training.\nRemark 1. Our deﬁnition of the realization function F (L) slightly differs from the classical one.\nUsually, the factors 1√nℓ\nand the parameterβ are absent and the parameters are initialized using\nwhat is sometimes called LeCun initialization, taking W (ℓ)\nij ∼N (0, 1\nnℓ\n) andb(ℓ)\nj ∼N (0, 1) (or\nsometimesb(ℓ)\nj = 0) to compensate. While the set of representable functionsF (L)(RP ) is the same\nfor both parametrizations (with or without the factors 1√nℓ\nandβ), the derivatives of the realization\nfunction with respect to the connections ∂W (ℓ)\nij\nF (L) and bias ∂b(ℓ)\nj\nF (L) are scaled by 1√nℓ\nandβ\nrespectively in comparison to the classical parametrization.\nThe factors 1√nℓ\nare key to obtaining a consistent asymptotic behavior of neural networks as the\nwidths of the hidden layersn1,...,n L−1 grow to inﬁnity. However a side-effect of these factors is\nthat they reduce greatly the inﬂuence of the connection weights during training whennℓ is large: the\nfactorβ is introduced to balance the inﬂuence of the bias and connection weights. In our numerical\nexperiments, we takeβ = 0.1 and use a learning rate of 1.0, which is larger than usual, see Section 6.\nThis gives a behaviour similar to that of a classical network of width 100 with a learning rate of 0.01.\n3 Kernel gradient\nThe training of an ANN consists in optimizingfθ in the function spaceF with respect to a functional\ncostC :F→ R, such as a regression or cross-entropy cost. Even for a convex functional cost C,\nthe composite cost C◦F (L) : RP → R is in general highly non-convex ( 3). We will show that\nduring training, the network functionfθ follows a descent along the kernel gradient with respect to\nthe Neural Tangent Kernel (NTK) which we introduce in Section 4. This makes it possible to study\nthe training of ANNs in the function spaceF, on which the costC is convex.\nA multi-dimensional kernelK is a function Rn0× Rn0→ RnL×nL, which maps any pair (x,x′) to\nannL×nL-matrix such thatK(x,x′) =K(x′,x )T (equivalentlyK is a symmetric tensor inF⊗F ).\nSuch a kernel deﬁnes a bilinear map onF, taking the expectation over independentx,x′∼pin:\n⟨f,g⟩K := Ex,x′∼pin\n[\nf(x)TK(x,x′)g(x′)\n]\n.\nThe kernelK is positive deﬁnite with respect to||·|| pin if||f||pin > 0 =⇒ ||f||K > 0.\nWe denote byF∗ the dual ofF with respect topin, i.e. the set of linear formsµ :F→ R of the form\nµ =⟨d,·⟩pin for somed∈F . Two elements ofF deﬁne the same linear form if and only if they\nare equal on the data. The constructions in the paper do not depend on the element d∈F chosen in\norder to representµ as⟨d,·⟩pin. Using the fact that the partial application of the kernelKi,·(x,·) is\na function inF, we can deﬁne a mapΦK :F∗→F mapping a dual elementµ =⟨d,·⟩pin to the\nfunctionfµ = ΦK(µ) with values:\nfµ,i(x) =µKi,·(x,·) =⟨d,Ki,·(x,·)⟩pin.\nFor our setup, which is that of a ﬁnite datasetx1,...,x n∈ Rn0, the cost functionalC only depends\non the values off∈F at the data points. As a result, the (functional) derivative of the costC at a\npointf0∈F can be viewed as an element ofF∗, which we write∂in\nf C|f0. We denote byd|f0∈F ,\na corresponding dual element, such that∂in\nf C|f0 =⟨d|f0,·⟩pin.\n3\nThe kernel gradient∇KC|f0∈F is deﬁned asΦK\n(\n∂in\nf C|f0\n)\n. In contrast to ∂in\nf C which is only\ndeﬁned on the dataset, the kernel gradient generalizes to valuesx outside the dataset thanks to the\nkernelK:\n∇KC|f0(x) = 1\nN\nN∑\nj=1\nK(x,xj)d|f0(xj).\nA time-dependent functionf(t) follows the kernel gradient descent with respect toK if it satisﬁes\nthe differential equation\n∂tf(t) =−∇KC|f(t).\nDuring kernel gradient descent, the costC(f(t)) evolves as\n∂tC|f(t) =−\n⟨\nd|f(t),∇KC|f(t)\n⟩\npin =−\nd|f(t)\n2\nK.\nConvergence to a critical point of C is hence guaranteed if the kernel K is positive deﬁnite with\nrespect to||·|| pin: the cost is then strictly decreasing except at points such that ||d|f(t)||pin = 0.\nIf the cost is convex and bounded from below, the function f(t) therefore converges to a global\nminimum ast→∞ .\n3.1 Random functions approximation\nAs a starting point to understand the convergence of ANN gradient descent to kernel gradient descent\nin the inﬁnite-width limit, we introduce a simple model, inspired by the approach of (15).\nA kernelK can be approximated by a choice of P random functionsf(p) sampled independently\nfrom any distribution onF whose (non-centered) covariance is given by the kernelK:\nE[f(p)\nk (x)f(p)\nk′ (x′)] =Kkk′(x,x′).\nThese functions deﬁne a random linear parametrizationFlin : RP→F\nθ↦→flin\nθ = 1√\nP\nP∑\np=1\nθpf(p).\nThe partial derivatives of the parametrization are given by\n∂θpFlin(θ) = 1√\nP\nf(p).\nOptimizing the costC◦Flin through gradient descent, the parameters follow the ODE:\n∂tθp(t) =−∂θp(C◦Flin)(θ(t)) =− 1√\nP\n∂in\nf C|flin\nθ(t)\nf(p) =− 1√\nP\n⣨\nd|flin\nθ(t)\n,f (p)\n⟩\npin\n.\nAs a result the functionflin\nθ(t) evolves according to\n∂tflin\nθ(t) = 1√\nP\nP∑\np=1\n∂tθp(t)f(p) =− 1\nP\nP∑\np=1\n⣨\nd|flin\nθ(t)\n,f (p)\n⟩\npin\nf(p),\nwhere the right-hand side is equal to the kernel gradient−∇ ˜KC with respect to the tangent kernel\n˜K =\nP∑\np=1\n∂θpFlin(θ)⊗∂θpFlin(θ) = 1\nP\nP∑\np=1\nf(p)⊗f(p).\nThis is a randomnL-dimensional kernel with values ˜Kii′(x,x′) = 1\nP\n∑P\np=1f(p)\ni (x)f(p)\ni′ (x′).\nPerforming gradient descent on the costC◦Flin is therefore equivalent to performing kernel gradient\ndescent with the tangent kernel ˜K in the function space. In the limit asP→∞ , by the law of large\nnumbers, the (random) tangent kernel ˜K tends to the ﬁxed kernelK, which makes this method an\napproximation of kernel gradient descent with respect to the limiting kernelK.\n4\n4 Neural tangent kernel\nFor ANNs trained using gradient descent on the compositionC◦F (L), the situation is very similar to\nthat studied in the Section 3.1. During training, the network function fθ evolves along the (negative)\nkernel gradient\n∂tfθ(t) =−∇Θ(L)C|fθ(t)\nwith respect to the neural tangent kernel (NTK)\nΘ(L)(θ) =\nP∑\np=1\n∂θpF (L)(θ)⊗∂θpF (L)(θ).\nHowever, in contrast toFlin, the realization functionF (L) of ANNs is not linear. As a consequence,\nthe derivatives∂θpF (L)(θ) and the neural tangent kernel depend on the parameters θ. The NTK\nis therefore random at initialization and varies during training, which makes the analysis of the\nconvergence offθ more delicate.\nIn the next subsections, we show that, in the inﬁnite-width limit, the NTK becomes deterministic at\ninitialization and stays constant during training. Since fθ at initialization is Gaussian in the limit, the\nasymptotic behavior offθ during training can be explicited in the function spaceF.\n4.1 Initialization\nAs observed in (12; 9), the output functionsfθ,i fori = 1,...,n L tend to iid Gaussian processes in\nthe inﬁnite-width limit (a proof in our setup is given in the appendix):\nProposition 1. For a network of depthL at initialization, with a Lipschitz nonlinearityσ, and in the\nlimit asn1,...,n L−1→∞ , the output functionsfθ,k, fork = 1,...,n L, tend (in law) to iid centered\nGaussian processes of covariance Σ(L), where Σ(L) is deﬁned recursively by:\nΣ(1)(x,x′) = 1\nn0\nxTx′ +β2\nΣ(L+1)(x,x′) = Ef∼N(0,Σ(L))[σ(f(x))σ(f(x′))] +β2,\ntaking the expectation with respect to a centered Gaussian processf of covariance Σ(L).\nRemark 2. Strictly speaking, the existence of a suitable Gaussian measure with covariance Σ(L) is\nnot needed: we only deal with the values off atx,x′ (the joint measure onf(x),f (x′) is simply a\nGaussian vector in 2D). For the same reasons, in the proof of Proposition 1 and Theorem 1, we will\nfreely speak of Gaussian processes without discussing their existence.\nThe ﬁrst key result of our paper (proven in the appendix) is the following: in the same limit, the\nNeural Tangent Kernel (NTK) converges in probability to an explicit deterministic limit.\nTheorem 1. For a network of depthL at initialization, with a Lipschitz nonlinearityσ, and in the\nlimit as the layers widthn1,...,n L−1→∞ , the NTK Θ(L) converges in probability to a deterministic\nlimiting kernel:\nΘ(L)→ Θ(L)\n∞ ⊗IdnL.\nThe scalar kernel Θ(L)\n∞ : Rn0× Rn0→ R is deﬁned recursively by\nΘ(1)\n∞ (x,x′) = Σ(1)(x,x′)\nΘ(L+1)\n∞ (x,x′) = Θ(L)\n∞ (x,x′) ˙Σ(L+1)(x,x′) + Σ(L+1)(x,x′),\nwhere\n˙Σ(L+1) (x,x′) = Ef∼N(0,Σ(L)) [ ˙σ (f (x)) ˙σ (f (x′))],\ntaking the expectation with respect to a centered Gaussian processf of covariance Σ(L), and where\n˙σ denotes the derivative ofσ.\nRemark 3. By Rademacher’s theorem, ˙σ is deﬁned everywhere, except perhaps on a set of zero\nLebesgue measure.\nNote that the limiting Θ(L)\n∞ only depends on the choice ofσ, the depth of the network and the variance\nof the parameters at initialization (which is equal to 1 in our setting).\n5\n4.2 Training\nOur second key result is that the NTK stays asymptotically constant during training. This applies\nfor a slightly more general deﬁnition of training: the parameters are updated according to a training\ndirectiondt∈F :\n∂tθp(t) =\n⣨\n∂θpF (L)(θ(t)),dt\n⟩\npin\n.\nIn the case of gradient descent, dt =−d|fθ(t) (see Section 3), but the direction may depend on\nanother network, as is the case for e.g. Generative Adversarial Networks (6). We only assume that\nthe integral\n∫T\n0 ∥dt∥pindt stays stochastically bounded as the width tends to inﬁnity, which is veriﬁed\nfor e.g. least-squares regression, see Section 5.\nTheorem 2. Assume thatσ is a Lipschitz, twice differentiable nonlinearity function, with bounded\nsecond derivative. For anyT such that the integral\n∫T\n0 ∥dt∥pindt stays stochastically bounded, as\nn1,...,n L−1→∞ , we have, uniformly fort∈ [0,T ],\nΘ(L)(t)→ Θ(L)\n∞ ⊗IdnL.\nAs a consequence, in this limit, the dynamics offθ is described by the differential equation\n∂tfθ(t) = ΦΘ(L)\n∞⊗IdnL\n(\n⟨dt,·⟩pin\n)\n.\nRemark 4. As the proof of the theorem (in the appendix) shows, the variation during training of the\nindividual activations in the hidden layers shrinks as their width grows. However their collective\nvariation is signiﬁcant, which allows the parameters of the lower layers to learn: in the formula of\nthe limiting NTK Θ(L+1)\n∞ (x,x′) in Theorem 1, the second summand Σ(L+1) represents the learning\ndue to the last layer, while the ﬁrst summand represents the learning performed by the lower layers.\nAs discussed in Section 3, the convergence of kernel gradient descent to a critical point of the cost\nC is guaranteed for positive deﬁnite kernels. The limiting NTK is positive deﬁnite if the span of\nthe derivatives∂θpF (L),p = 1,...,P becomes dense inF w.r.t. thepin-norm as the width grows\nto inﬁnity. It seems natural to postulate that the span of the preactivations of the last layer (which\nthemselves appear in∂θpF (L), corresponding to the connection weights of the last layer) becomes\ndense inF, for a large family of measures pin and nonlinearities (see e.g. ( 7; 10) for classical\ntheorems about ANNs and approximation).\n5 Least-squares regression\nGiven a goal functionf∗ and input distributionpin, the least-squares regression cost is\nC(f) = 1\n2||f−f∗||2\npin = 1\n2 Ex∼pin\n[\n∥f(x)−f∗(x)∥2]\n.\nTheorems 1 and 2 apply to an ANN trained on such a cost. Indeed the norm of the training direction\n∥d(f)∥pin =∥f∗−f∥pin is strictly decreasing during training, bounding the integral. We are\ntherefore interested in the behavior of a functionft during kernel gradient descent with a kernelK\n(we are of course especially interested in the caseK = Θ(L)\n∞ ⊗IdnL):\n∂tft = ΦK\n(\n⟨f∗−f,·⟩pin\n)\n.\nThe solution of this differential equation can be expressed in terms of the map Π : f ↦→\nΦK\n(\n⟨f,·⟩pin\n)\n:\nft =f∗ +e−tΠ(f0−f∗)\nwheree−tΠ =∑∞\nk=0\n(−t)k\nk! Πk is the exponential of−tΠ. If Π can be diagonalized by eigenfunctions\nf(i) with eigenvaluesλi, the exponentiale−tΠ has the same eigenfunctions with eigenvaluese−tλi.\nFor a ﬁnite datasetx1,...,x N of sizeN, the map Π takes the form\nΠ(f)k(x) = 1\nN\nN∑\ni=1\nnL∑\nk′=1\nfk′(xi)Kkk′(xi,x ).\n6\nThe map Π has at mostNnL positive eigenfunctions, and they are the kernel principal components\nf(1),...,f (NnL) of the data with respect to to the kernelK (17; 18). The corresponding eigenvalues\nλi is the variance captured by the component.\nDecomposing the difference (f∗−f0) = ∆0\nf + ∆1\nf +... + ∆NnL\nf along the eigenspaces of Π, the\ntrajectory of the functionft reads\nft =f∗ + ∆0\nf +\nNnL∑\ni=1\ne−tλi∆i\nf,\nwhere ∆0\nf is in the kernel (null-space) of Π and ∆i\nf∝f(i).\nThe above decomposition can be seen as a motivation for the use of early stopping. The convergence\nis indeed faster along the eigenspaces corresponding to larger eigenvaluesλi. Early stopping hence\nfocuses the convergence on the most relevant kernel principal components, while avoiding to ﬁt\nthe ones in eigenspaces with lower eigenvalues (such directions are typically the ‘noisier’ ones: for\ninstance, in the case of the RBF kernel, lower eigenvalues correspond to high frequency functions).\nNote that by the linearity of the mape−tΠ, iff0 is initialized with a Gaussian distribution (as is the\ncase for ANNs in the inﬁnite-width limit), thenft is Gaussian for all timest. Assuming that the kernel\nis positive deﬁnite on the data (implying that theNnL×NnL Gram marix ˜K = (Kkk′(xi,xj))ik,jk ′\nis invertible), ast→∞ limit, we get thatf∞ =f∗ + ∆0\nf =f0−∑\ni ∆i\nf takes the form\nf∞,k(x) =κT\nx,k ˜K−1y∗ +\n(\nf0(x)−κT\nx,k ˜K−1y0\n)\n,\nwith theNnl-vectorsκx,k,y∗ andy0 given by\nκx,k = (Kkk′(x,xi))i,k′\ny∗ = (f∗\nk (xi))i,k\ny0 = (f0,k(xi))i,k.\nThe ﬁrst term, the mean, has an important statistical interpretation: it is the maximum-a-posteriori\n(MAP) estimate given a Gaussian prior on functionsfk∼N (0, Θ(L)\n∞ ) and the conditionsfk(xi) =\nf∗\nk (xi) . Equivalently, it is equal to the kernel ridge regression ( 18) as the regularization goes to\nzero (λ→ 0). The second term is a centered Gaussian whose variance vanishes on the points of the\ndataset.\n6 Numerical experiments\nIn the following numerical experiments, fully connected ANNs of various widths are compared to the\ntheoretical inﬁnite-width limit. We choose the size of the hidden layers to all be equal to the same\nvaluen :=n1 =... =nL−1 and we take the ReLU nonlinearityσ(x) = max(0,x ).\nIn the ﬁrst two experiments, we consider the casen0 = 2. Moreover, the input elements are taken on\nthe unit circle. This can be motivated by the structure of high-dimensional data, where the centered\ndata points often have roughly the same norm 2.\nIn all experiments, we took nL = 1 (note that by our results, a network with nL outputs behaves\nasymptotically likenL networks with scalar outputs trained independently). Finally, the value of the\nparameterβ is chosen as 0.1, see Remark 1.\n6.1 Convergence of the NTK\nThe ﬁrst experiment illustrates the convergence of the NTKΘ(L) of a network of depthL = 4 for\ntwo different widths n = 500, 10000. The function Θ(4)(x0,x ) is plotted for a ﬁxedx0 = (1, 0)\nandx = (cos(γ),sin (γ)) on the unit circle in Figure 1. To observe the distribution of the NTK, 10\nindependent initializations are performed for both widths. The kernels are plotted at initialization\n2The classical example is for data following a Gaussian distribution N (0,Idn0 ): as the dimensionn0 grows,\nall data points have approximately the same norm √n0.\n7\n3\n 2\n 1\n 0 1 2 3\n0.05\n0.10\n0.15\n0.20\n0.25\n0.30\n0.35\n0.40 n = 500, t = 0\nn = 500, t = 20\nn = 10000, t = 0\nn = 10000, 0\nn = 500, t = 0\nn = 500, t = 200\nn = 10000, t = 0\nn = 10000, t = 200\nFigure 1: Convergence of the NTK to a ﬁxed limit\nfor two widthsn and two timest.\n3\n 2\n 1\n 0 1 2 3\n0.4\n0.2\n0.0\n0.2\n0.4\nf (sin( ), cos( ))\nn = 50\nn = 1000\nn = , P50\nn = , {P10, P90}\nFigure 2: Networks functionfθ near convergence\nfor two widths n and 10th, 50th and 90th per-\ncentiles of the asymptotic Gaussian distribution.\nt = 0 and then after 200 steps of gradient descent with learning rate 1.0 (i.e. at t = 200 ). We\napproximate the functionf∗(x) =x1x2 with a least-squares cost on randomN (0, 1) inputs.\nFor the wider network, the NTK shows less variance and is smoother. It is interesting to note that\nthe expectation of the NTK is very close for both networks widths. After 200 steps of training, we\nobserve that the NTK tends to “inﬂate”. As expected, this effect is much less apparent for the wider\nnetwork (n = 10000) where the NTK stays almost ﬁxed, than for the smaller network (n = 500).\n6.2 Kernel regression\nFor a regression cost, the inﬁnite-width limit network functionfθ(t) has a Gaussian distribution for\nall timest and in particular at convergencet→∞ (see Section 5). We compared the theoretical\nGaussian distribution att→∞ to the distribution of the network function fθ(T) of a ﬁnite-width\nnetwork for a large time T = 1000 . For two different widths n = 50, 1000 and for 10 random\ninitializations each, a network is trained on a least-squares cost on 4 points of the unit circle for 1000\nsteps with learning rate 1.0 and then plotted in Figure 2.\nWe also approximated the kernels Θ(4)\n∞ and Σ(4) using a large-width network (n = 10000) and used\nthem to calculate and plot the 10th, 50th and 90-th percentiles of the t→∞ limiting Gaussian\ndistribution.\nThe distributions of the network functions are very similar for both widths: their mean and variance\nappear to be close to those of the limiting distribution t→∞ . Even for relatively small widths\n(n = 50), the NTK gives a good indication of the distribution offθ(t) ast→∞ .\n6.3 Convergence along a principal component\nWe now illustrate our result on the MNIST dataset of handwritten digits made up of grayscale images\nof dimension 28× 28, yielding a dimension ofn0 = 784.\nWe computed the ﬁrst 3 principal components of a batch ofN = 512 digits with respect to the NTK\nof a high-width networkn = 10000 (giving an approximation of the limiting kernel) using a power\niteration method. The respective eigenvalues are λ1 = 0.0457,λ2 = 0.00108 andλ3 = 0.00078.\nThe kernel PCA is non-centered, the ﬁrst component is therefore almost equal to the constant function,\nwhich explains the large gap between the ﬁrst and second eigenvalues3. The next two components are\nmuch more interesting as can be seen in Figure 3a, where the batch is plotted withx andy coordinates\ncorresponding to the 2nd and 3rd components.\nWe have seen in Section 5 how the convergence of kernel gradient descent follows the kernel principal\ncomponents. If the difference at initializationf0−f∗ is equal (or proportional) to one of the principal\n3It can be observed numerically, that if we chooseβ = 1.0 instead of our recommended 0.1, the gap between\nthe ﬁrst and the second principal component is about ten times bigger, which makes training more difﬁcult.\n8\n3\n 2\n 1\n 0 1 2\nf(2)(x)\n2\n1\n0\n1\n2\nf(3)(x)\n(a) The 2nd and 3rd principal\ncomponents of MNIST.\n0 500 1000 1500 2000 2500 3000 3500 4000\nt\n0.00\n0.02\n0.04\n0.06\n0.08\n0.10\n0.12\n0.14 n = 100\nn = 1000\nn = 10000\n||ht||pin\n(b) Deviation of the network function\nfθ from the straight line.\n0 500 1000 1500 2000 2500 3000 3500 4000\nt\n0.0\n0.1\n0.2\n0.3\n0.4\n0.5 n = 100\nn = 1000\nn = 10000\nn =\n||gt||pin\n(c) Convergence offθ along the 2nd\nprincipal component.\nFigure 3\ncomponentsf(i), then the function will converge along a straight line (in the function space) tof∗ at\nan exponential ratee−λit.\nWe tested whether ANNs of various widthsn = 100, 1000, 10000 behave in a similar manner. We\nset the goal of the regression cost tof∗ =fθ(0) + 0.5f(2) and let the network converge. At each time\nstept, we decomposed the differencefθ(t)−f∗ into a componentgt proportional tof(2) and another\noneht orthogonal tof(2). In the inﬁnite-width limit, the ﬁrst component decays exponentially fast\n||gt||pin = 0.5e−λ2t while the second is null (ht = 0), as the function converges along a straight line.\nAs expected, we see in Figure 3b that the wider the network, the less it deviates from the straight line\n(for each widthn we performed two independent trials). As the width grows, the trajectory along the\n2nd principal component (shown in Figure 3c) converges to the theoretical limit shown in blue.\nA surprising observation is that smaller networks appear to converge faster than wider ones. This may\nbe explained by the inﬂation of the NTK observed in our ﬁrst experiment. Indeed, multiplying the\nNTK by a factora is equivalent to multiplying the learning rate by the same factor. However, note\nthat since the NTK of large-width network is more stable during training, larger learning rates can in\nprinciple be taken. One must hence be careful when comparing the convergence speed in terms of the\nnumber of steps (rather than in terms of the time t): both the inﬂation effect and the learning rate\nmust be taken into account.\n7 Conclusion\nThis paper introduces a new tool to study ANNs, the Neural Tangent Kernel (NTK), which describes\nthe local dynamics of an ANN during gradient descent. This leads to a new connection between ANN\ntraining and kernel methods: in the inﬁnite-width limit, an ANN can be described in the function\nspace directly by the limit of the NTK, an explicit constant kernel Θ(L)\n∞ , which only depends on\nits depth, nonlinearity and parameter initialization variance. More precisely, in this limit, ANN\ngradient descent is shown to be equivalent to a kernel gradient descent with respect to Θ(L)\n∞ . The\nlimit of the NTK is hence a powerful tool to understand the generalization properties of ANNs, and\nit allows one to study the inﬂuence of the depth and nonlinearity on the learning abilities of the\nnetwork. The analysis of training using NTK allows one to relate convergence of ANN training with\nthe positive-deﬁniteness of the limiting NTK and leads to a characterization of the directions favored\nby early stopping methods.\nAcknowledgements\nThe authors thank K. Kyt¨ol¨a for many interesting discussions. The second author was supported by\nthe ERC CG CRITICAL. The last author acknowledges support from the ERC SG Constamis, the\nNCCR SwissMAP, the Blavatnik Family Foundation and the Latsis Foundation.\n9\nReferences\n[1] M. Belkin, S. Ma, and S. Mandal. To understand deep learning we need to understand kernel\nlearning. arXiv preprint, Feb 2018.\n[2] Y . Cho and L. K. Saul. Kernel methods for deep learning. InAdvances in Neural Information\nProcessing Systems 22, pages 342–350. Curran Associates, Inc., 2009.\n[3] A. Choromanska, M. Henaff, M. Mathieu, G. B. Arous, and Y . LeCun. The Loss Surfaces of\nMultilayer Networks. Journal of Machine Learning Research, 38:192–204, nov 2015.\n[4] Y . N. Dauphin, R. Pascanu, C. Gulcehre, K. Cho, S. Ganguli, and Y . Bengio. Identifying\nand attacking the saddle point problem in high-dimensional non-convex optimization. In\nProceedings of the 27th International Conference on Neural Information Processing Systems -\nVolume 2, NIPS’14, pages 2933–2941, Cambridge, MA, USA, 2014. MIT Press.\n[5] S. S. Dragomir. Some Gronwall Type Inequalities and Applications. Nova Science Publishers,\n2003.\n[6] I. J. Goodfellow, J. Pouget-Abadie, M. Mirza, B. Xu, D. Warde-Farley, S. Ozair, A. Courville,\nand Y . Bengio. Generative Adversarial Networks.NIPS’14 Proceedings of the 27th International\nConference on Neural Information Processing Systems - Volume 2, pages 2672–2680, jun 2014.\n[7] K. Hornik, M. Stinchcombe, and H. White. Multilayer feedforward networks are universal\napproximators. Neural Networks, 2(5):359 – 366, 1989.\n[8] R. Karakida, S. Akaho, and S.-i. Amari. Universal Statistics of Fisher Information in Deep\nNeural Networks: Mean Field Approach. jun 2018.\n[9] J. H. Lee, Y . Bahri, R. Novak, S. S. Schoenholz, J. Pennington, and J. Sohl-Dickstein. Deep\nneural networks as gaussian processes. ICLR, 2018.\n[10] M. Leshno, V . Lin, A. Pinkus, and S. Schocken. Multilayer feedforward networks with a non-\npolynomial activation function can approximate any function. Neural Networks, 6(6):861–867,\n1993.\n[11] S. Mei, A. Montanari, and P.-M. Nguyen. A mean ﬁeld view of the landscape of two-layer\nneural networks. Proceedings of the National Academy of Sciences, 115(33):E7665–E7671,\n2018.\n[12] R. M. Neal. Bayesian Learning for Neural Networks. Springer-Verlag New York, Inc., Secaucus,\nNJ, USA, 1996.\n[13] R. Pascanu, Y . N. Dauphin, S. Ganguli, and Y . Bengio. On the saddle point problem for\nnon-convex optimization. arXiv preprint, 2014.\n[14] J. Pennington and Y . Bahri. Geometry of neural network loss surfaces via random matrix\ntheory. In Proceedings of the 34th International Conference on Machine Learning, volume 70\nof Proceedings of Machine Learning Research, pages 2798–2806, International Convention\nCentre, Sydney, Australia, 06–11 Aug 2017. PMLR.\n[15] A. Rahimi and B. Recht. Random features for large-scale kernel machines. In Advances in\nNeural Information Processing Systems 20, pages 1177–1184. Curran Associates, Inc., 2008.\n[16] L. Sagun, U. Evci, V . U. G¨uney, Y . Dauphin, and L. Bottou. Empirical analysis of the hessian\nof over-parametrized neural networks. CoRR, abs/1706.04454, 2017.\n[17] B. Sch¨olkopf, A. Smola, and K.-R. M¨uller. Nonlinear component analysis as a kernel eigenvalue\nproblem. Neural Computation, 10(5):1299–1319, 1998.\n[18] J. Shawe-Taylor and N. Cristianini. Kernel Methods for Pattern Analysis. Cambridge University\nPress, New York, NY , USA, 2004.\n[19] C. Zhang, S. Bengio, M. Hardt, B. Recht, and O. Vinyals. Understanding deep learning requires\nrethinking generalization. ICLR 2017 proceedings, Feb 2017.\n10",
  "values": {
    "Non-maleficence": "No",
    "Not socially biased": "No",
    "Autonomy (power to decide)": "No",
    "Explicability": "No",
    "Transparent (to users)": "No",
    "Critiqability": "No",
    "Interpretable (to users)": "No",
    "Deferral to humans": "No",
    "Respect for Law and public interest": "No",
    "Respect for Persons": "No",
    "User influence": "No",
    "Beneficence": "No",
    "Collective influence": "No",
    "Fairness": "No",
    "Privacy": "No",
    "Justice": "No"
  }
}