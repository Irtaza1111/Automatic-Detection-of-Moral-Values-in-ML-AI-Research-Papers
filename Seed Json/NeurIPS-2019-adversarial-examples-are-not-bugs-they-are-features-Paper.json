{
  "pdf": "NeurIPS-2019-adversarial-examples-are-not-bugs-they-are-features-Paper",
  "title": "NeurIPS-2019-adversarial-examples-are-not-bugs-they-are-features-Paper",
  "author": "Unknown",
  "paper_id": "NeurIPS-2019-adversarial-examples-are-not-bugs-they-are-features-Paper",
  "text": "Adversarial Examples are not Bugs, they are Features\nAndrew Ilyas∗\nMIT\nailyas@mit.edu\nShibani Santurkar∗\nMIT\nshibani@mit.edu\nDimitris Tsipras∗\nMIT\ntsipras@mit.edu\nLogan Engstrom∗\nMIT\nengstrom@mit.edu\nBrandon Tran\nMIT\nbtran115@mit.edu\nAleksander M ˛ adry\nMIT\nmadry@mit.edu\nAbstract\nAdversarial examples have attracted signiﬁcant attention in machine learning, but\nthe reasons for their existence and pervasiveness remain unclear. We demonstrate\nthat adversarial examples can be directly attributed to the presence of non-robust\nfeatures: features (derived from patterns in the data distribution) that are highly\npredictive, yet brittle and (thus) incomprehensible to humans. After capturing\nthese features within a theoretical framework, we establish their widespread ex-\nistence in standard datasets. Finally, we present a simple setting where we can\nrigorously tie the phenomena we observe in practice to a misalignment between\nthe (human-speciﬁed) notion of robustness and the inherent geometry of the data.\n1 Introduction\nThe pervasive brittleness of deep neural networks [Sze+14; Eng+19b; HD19; Ath+18] has attracted\nsigniﬁcant attention in recent years. Particularly worrisome is the phenomenon of adversarial ex-\namples [Big+13; Sze+14], imperceptibly perturbed natural inputs that induce erroneous predictions\nin state-of-the-art classiﬁers. Previous work has proposed a variety of explanations for this phe-\nnomenon, ranging from theoretical models [Sch+18; BPR18] to arguments based on concentration\nof measure in high-dimensions [Gil+18; MDM18; Sha+19a]. These theories, however, are often\nunable to fully capture behaviors we observe in practice (we discuss this further in Section 5).\nMore broadly, previous work in the ﬁeld tends to view adversarial examples as aberrations arising\neither from the high dimensional nature of the input space or statistical ﬂuctuations in the training\ndata [GSS15; Gil+18]. From this point of view, it is natural to treat adversarial robustness as a goal\nthat can be disentangled and pursued independently from maximizing accuracy [Mad+18; SHS19;\nSug+19], either through improved standard regularization methods [TG16] or pre/post-processing\nof network inputs/outputs [Ues+18; CW17a; He+17].\nIn this work, we propose a new perspective on the phenomenon of adversarial examples. In con-\ntrast to the previous models, we cast adversarial vulnerability as a fundamental consequence of the\ndominant supervised learning paradigm. Speciﬁcally, we claim that:\nAdversarial vulnerability is a direct result of sensitivity to well-generalizing features in the data.\nRecall that we usually train classiﬁers to solely maximize (distributional) accuracy. Consequently,\nclassiﬁers tend to use any available signal to do so, even those that look incomprehensible to hu-\nmans. After all, the presence of “a tail” or “ears” is no more natural to a classiﬁer than any other\nequally predictive feature. In fact, we ﬁnd that standard ML datasets do admit highly predictive\n33rd Conference on Neural Information Processing Systems (NeurIPS 2019), Vancouver, Canada.\nyet imperceptible features. We posit that our models learn to rely on these “non-robust” features,\nleading to adversarial perturbations that exploit this dependence.2\nOur hypothesis also suggests an explanation for adversarial transferability: the phenomenon that\nperturbations computed for one model often transfer to other, independently trained models. Since\nany two models are likely to learn similar non-robust features, perturbations that manipulate such\nfeatures will apply to both. Finally, this perspective establishes adversarial vulnerability as a human-\ncentric phenomenon, since, from the standard supervised learning point of view, non-robust features\ncan be as important as robust ones. It also suggests that approaches aiming to enhance the inter-\npretability of a given model by enforcing “priors” for its explanation [MV15; OMS17; Smi+17]\nactually hide features that are “meaningful” and predictive to standard models. As such, produc-\ning human-meaningful explanations that remain faithful to underlying models cannot be pursued\nindependently from the training of the models themselves.\nTo corroborate our theory, we show that it is possible to disentangle robust from non-robust features\nin standard image classiﬁcation datasets. Speciﬁcally, given a training dataset, we construct:\n1. A “robustiﬁed” version for robust classiﬁcation (Figure 1a)3 . We are able to effectively\nremove non-robust features from a dataset. Concretely, we create a training set (semanti-\ncally similar to the original) on whichstandard training yields good robust accuracyon the\noriginal, unmodiﬁed test set. This ﬁnding establishes that adversarial vulnerability is not\nnecessarily tied to the standard training framework, but is also a property of the dataset.\n2. A “non-robust” version for standard classiﬁcation (Figure 1b) 2. We are also able to\nconstruct a training dataset for which the inputs are nearly identical to the originals, but\nall appear incorrectly labeled. In fact, the inputs in the new training set are associated\nto their labels only through small adversarial perturbations (and hence utilize only non-\nrobust features). Despite the lack of any predictive human-visible information, training on\nthis dataset yields good accuracy on the original, unmodiﬁed test set. This demonstrates\nthat adversarial perturbations can arise from ﬂipping features in the data that are useful for\nclassiﬁcation of correct inputs (hence not being purely aberrations).\nFinally, we present a concrete classiﬁcation task where the connection between adversarial examples\nand non-robust features can be studied rigorously. This task consists of separating Gaussian distri-\nbutions, and is loosely based on the model presented in Tsipras et al. [Tsi+19], while expanding\nupon it in a few ways. First, adversarial vulnerability in our setting can be precisely quantiﬁed as a\ndifference between the intrinsic data geometry and that of the adversary’s perturbation set. Second,\nrobust training yields a classiﬁer which utilizes a geometry corresponding to a combination of these\ntwo. Lastly, the gradients of standard models can be signiﬁcantly misaligned with the inter-class\ndirection, capturing a phenomenon that has been observed in practice [Tsi+19].\n2 The Robust Features Model\nWe begin by developing a framework, loosely based on the setting of Tsipras et al. [Tsi+19], that\nenables us to rigorously refer to “robust” and “non-robust” features. In particular, we present a set of\ndeﬁnitions which allow us to formally describe our setup, theoretical results, and empirical evidence.\nSetup. We study binary classiﬁcation, where input-label pairs (x,y ) ∈ X × {± 1} are sampled\nfrom a distribution D; the goal is to learn a classiﬁerC : X → {± 1} predictingy givenx.\nWe deﬁne a feature to be a function mapping from the input space X to real numbers, with the\nset of all features thus being F = {f : X → R}. For convenience, we assume that the features\nin F are shifted/scaled to be mean-zero and unit-variance (i.e., so that E(x,y)∼D[f(x)] = 0 and\nE(x,y)∼D[f(x)2] = 1 ), making the following deﬁnitions scale-invariant. Note that this deﬁnition\ncaptures what we abstractly think of as features (e.g., a function capturing how “furry” an image is).\n2It is worth emphasizing that while our ﬁndings demonstrate that adversarial vulnerability does arise from\nnon-robust features, they do not preclude the possibility of adversarial vulnerability also arising from other\nphenomena [Nak19a]. Nevertheless, the mere existence of useful non-robust features sufﬁces to establish that\nwithout explicitly preventing models from utilizing these features, adversarial vulnerability will persist.\n3The corresponding datasets for CIFAR-10 are publicly available at http://git.io/adv-datasets.\n2\nRobust dataset\nTrain\ngood standard accuracy \ngood robust accuracy\ngood standard accuracy \nbad robust accuracy\nUnmodiﬁed \ntest set\nTraining image\nfrog\nfrog\nfrog\nNon-robust dataset\nTrain\n(a)\nEvaluate on \noriginal test set\nTraining image\nRobust Features: dog \nNon-Robust Features: dog\ndog\nRelabel as cat\nRobust Features: dog \nNon-Robust Features: cat\ncat\ncat\nmax P(cat) \nAdversarial example \ntowards “cat” \nTraingood accuracy (b)\nFigure 1: A conceptual diagram of the experiments of Section 3: (a) we disentangle features into\nrobust and non-robust (Section 3.1), (b) we construct a dataset which appears mislabeled to humans\n(via adversarial examples) but results in good accuracy on the original test set (Section 3.2).\nUseful, robust, and non-robust features. We now deﬁne the key concepts required for formulat-\ning our framework. To this end, we categorize features in the following manner:\n• ρ-useful features: For a given distribution D, we call a feature f ρ-useful (ρ >0) if it is\ncorrelated with the true label in expectation, that is if\nE(x,y)∼D[y ·f(x)] ≥ρ. (1)\nWe then deﬁneρD(f) as the largestρ for which featuref isρ-useful under distribution D.\n(Note that if a feature f is negatively correlated with the label, then −f is useful instead.)\nCrucially, a linear classiﬁer trained onρ-useful features can attain non-trivial performance.\n• γ-robustly useful features : Suppose we have a ρ-useful feature f (ρD(f) > 0). We\nrefer to f as a robust feature (formally a γ-robustly useful feature for γ > 0) if, under\nadversarial perturbation (for some speciﬁed set of valid perturbations ∆), f remains γ-\nuseful. Formally, if we have that\nE(x,y)∼D\n[\ninf\nδ∈∆(x)\ny ·f(x +δ)\n]\n≥γ. (2)\n• Useful, non-robust features: A useful, non-robust featureis a feature which isρ-useful for\nsomeρ bounded away from zero, but is not aγ-robust feature for anyγ ≥ 0. These features\nhelp with classiﬁcation in the standard setting, but may hinder accuracy in the adversarial\nsetting, as the correlation with the label can be ﬂipped.\nClassiﬁcation. In our framework, a classiﬁer C = (F,w,b ) is comprised of a set of features\nF ⊆ F , a weight vectorw, and a scalar biasb. For an inputx, the classiﬁer predicts the labely as\nC(x) = sgn\n\nb +\n∑\nf∈F\nwf ·f(x)\n\n.\nFor convenience, we denote the set of features learned by a classiﬁerC asFC.\nStandard Training. Training a classiﬁer is performed by minimizing a loss function Lθ(x,y )\nover input-label pairs (x,y ) from the training set (via empirical risk minimization (ERM)) that de-\ncreases with the correlation between the weighted combination of the features and the label. When\nminimizing classiﬁcation loss, no distinction exists between robust and non-robust features: the\nonly distinguishing factor of a feature is itsρ-usefulness. Furthermore, the classiﬁer will utilizeany\nρ-useful feature inF to decrease the loss of the classiﬁer.\nRobust training. In the presence of an adversary, any useful but non-robust features can be made\nanti-correlated with the true label, leading to adversarial vulnerability. Therefore, ERM is no longer\nsufﬁcient to train classiﬁers that are robust, and we need to explicitly account for the effect of the\n3\nadversary on the classiﬁer. To do so, we use an adversarial loss function that can discern between\nrobust and non-robust features [Mad+18]:\nE(x,y)∼D\n[\nmax\nδ∈∆(x)\nLθ(x +δ,y )\n]\n, (3)\nfor an appropriately deﬁned set of perturbations ∆. Since the adversary can exploit non-robust\nfeatures to degrade classiﬁcation accuracy, minimizing this adversarial loss [GSS15; Mad+18] can\nbe viewed as explicitly preventing the classiﬁer from relying on non-robust features.\nRemark. We want to note that even though this framework enables us to describe and predict\nthe outcome of our experiments, it does not capture the notion of non-robust features exactly as\nwe intuitively might think of them. For instance, in principle, our theoretical framework would\nallow for useful non-robust features to arise as combinations of useful robust features and useless\nnon-robust features [Goh19b]. These types of constructions, however, are actually precluded by our\nexperimental results (for instance, the classiﬁers trained in Section 3 would not generalize). This\nshows that our experimental ﬁndings capture a stronger, more ﬁne-grained statement than our formal\ndeﬁnitions are able to express. We view bridging this gap as an interesting direction for future work.\n3 Finding Robust (and Non-Robust) Features\nThe central premise of our proposed framework is that there exist both robust and non-robust features\nthat constitute useful signals for standard classiﬁcation. We now provide evidence in support of this\nhypothesis by disentangling these two sets of features (see conceptual description in Figure 1).\nOn one hand, we will construct a “robustiﬁed” dataset, consisting of samples that primarily contain\nrobust features. Using such a dataset, we are able to train robust classiﬁers (with respect to the\nstandard test set) using standard (i.e., non-robust) training. This demonstrates that robustness can\narise by removing certain features from the dataset (as, overall, the new dataset contains less infor-\nmation about the original training set). Moreover, it provides evidence that adversarial vulnerability\nis caused by non-robust features and is not inherently tied to the standard training framework.\nOn the other hand, we will construct datasets where the input-label association is based purely on\nnon-robust features (and thus the resulting dataset appears completely mislabeled to humans). We\nshow that this dataset sufﬁces to train a classiﬁer with good performance on the standard test set. This\nindicates that natural models use non-robust features to make predictions, even in the presence of\nrobust features. These features alone are sufﬁcient for non-trivial generalization to natural images,\nindicating that they are indeed predictive, rather than artifacts of ﬁnite-sample overﬁtting.\n3.1 Disentangling robust and non-robust features\nRecall that the features a classiﬁer learns to rely on are based purely on how useful these features\nare for (standard) generalization. Thus, under our conceptual framework, if we can ensure that only\nrobust features are useful, standard training should result in a robust classiﬁer. Unfortunately, we\ncannot directly manipulate the features of very complex, high-dimensional datasets. Instead, we will\nleverage a robust model and modify our dataset to contain only the features relevant to that model.\nConceptually, given a robust (i.e., adversarially trained [Mad+18]) model C, we aim to construct a\ndistribution ˆDR for which features used byC are as useful as they were on the original distribution\nD while ensuring that the rest of the features are not useful. In terms of our formal framework:\nE(x,y)∼ ˆDR\n[f(x) ·y] =\n{E(x,y)∼D [f(x) ·y] iff ∈FC\n0 otherwise, (4)\nwhereFC again represents the set of features utilized byC.\nWe will construct a training set for ˆDR via a one-to-one mappingx ↦→xr from the original training\nset for D. In the case of a deep neural network,FC corresponds to exactly the set of activations in the\npenultimate layer (since these correspond to inputs to a linear classiﬁer). To ensure that features used\nby the model are equally useful under both training sets, we (approximately) enforce all features in\nFC to have similar values for bothx andxr through the following optimization:\nmin\nxr\n∥g(xr) −g(x)∥2, (5)\n4\n“airplane’’ “ship’’ “dog’’ “frog’’“truck’’\nDˆ\nD NR\nˆ\nD R\n(a)\nStd Training \n using \nAdv Training \n using \nStd Training \n using R\nStd Training \n using NR\n0\n20\n40\n60\n80\n100\nTest Accuracy on  (%)\nStd accuracy Adv accuracy ( = 0.25)\n (b)\nFigure 2: (a): Random samples from our variants of the CIFAR-10 [Kri09] training set: the original\ntraining set; the robust training set ˆDR, restricted to features used by a robust model; and the non-\nrobust training set ˆDNR , restricted to features relevant to a standard model (labels appear incorrect\nto humans). (b): Standard and robust accuracy on the CIFAR-10 test set ( D) for models trained\nwith: (i) standard training (on D) ; (ii) standard training on ˆDNR ; (iii) adversarial training (on\nD); and (iv) standard training on ˆDR. Models trained on ˆDR and ˆDNR reﬂect the original models\nused to create them: notably, standard training on ˆDR yields nontrivial robust accuracy. Results for\nRestricted-ImageNet [Tsi+19] are in D.8 Figure 12.\nwherex is the original input and g is the mapping from x to the representation layer. We optimize\nthis objective using (normalized) gradient descent (see details in Appendix C).\nSince we don’t have access to features outside FC, there is no way to ensure that the expectation\nin (4) is zero for allf ̸∈FC. To approximate this condition, we choose the starting point of gradient\ndescent for the optimization in (5) to be an input x0 which is drawn from D independently of the\nlabel of x (we also explore sampling x0 from noise in Appendix D.1). This choice ensures that\nany feature present in that input will not be useful since they are not correlated with the label in\nexpectation over x0. The underlying assumption here is that, when performing the optimization\nin (5), features that are not being directly optimized (i.e., features outside FC) are not affected. We\nprovide pseudocode for the construction in Figure 5 (Appendix C).\nGiven the new training set for ˆDR (a few random samples are visualized in Figure 2a), we train a\nclassiﬁer using standard (non-robust) training. We then test this classiﬁer on the original test set (i.e.\nD). The results (Figure 2b) indicate that the classiﬁer learned using the new dataset attains good\naccuracy in both standard and adversarial settings (see additional evaluation in Appendix D.2.) 4.\nAs a control, we repeat this methodology using a standard (non-robust) model for C in our con-\nstruction of the dataset. Sample images from the resulting “non-robust dataset” ˆDNR are shown\nin Figure 2a—they tend to resemble more the source image of the optimization x0 than the target\nimage x. We ﬁnd that training on this dataset leads to good standard accuracy, yet yields almost\nno robustness (Figure 2b). We also verify that this procedure is not simply a matter of encoding\nthe weights of the original model—we get the same results for both ˆDR and ˆDNR if we train with\ndifferent architectures than that of the original models.\nOverall, our ﬁndings corroborate the hypothesis that adversarial examples can arise from (non-\nrobust) features of the data itself. By ﬁltering out non-robust features from the dataset (e.g. by\nrestricting the set of available features to those used by a robust model), one can train a signiﬁcantly\nmore robust model using standard training.\n3.2 Non-robust features sufﬁce for standard classiﬁcation\nThe results of the previous section show that by restricting the dataset to only contain features that\nare used by a robust model, standard training results in classiﬁers that are signiﬁcantly more robust.\n4In an attempt to explain the gap in accuracy between the model trained on ˆDR and the original robust\nclassiﬁerC, we test distributional shift, by reporting results on the “robustiﬁed” test set in Appendix D.3.\n5\nThis suggests that when training on the standard dataset, non-robust features take on a large role in\nthe resulting learned classiﬁer. Here we will show that this is not merely incidental. In particular, we\ndemonstrate that non-robust featuresalone sufﬁce for standard generalization— i.e., a model trained\nsolely on non-robust features can generalize to the standard test set.\nTo show this, we construct a dataset where the only features that are useful for classiﬁcation are\nnon-robust features (or in terms of our formal model from Section 2, all featuresf that areρ-useful\nare non-robust). To accomplish this, we modify each input-label pair (x,y ) as follows. We select a\ntarget classt either (a) uniformly at random (hence features become uncorrelated with the labels) or\n(b) deterministically according to the source class (e.g. permuting the labels). Then, we add a small\nadversarial perturbation tox to cause it to be classiﬁed ast by a standard model:\nxadv = arg min\n∥x′−x∥≤ε\nLC(x′,t ), (6)\nwhereLC is the loss under a standard (non-robust) classiﬁerC andε is a small constant. The result-\ning inputs are indistinguishable from the originals (Appendix D Figure 9)—to a human observer,\nit thus appears that the label t assigned to the modiﬁed input is simply incorrect. The resulting\ninput-label pairs (xadv,t ) make up the new training set (pseudocode in Appendix C Figure 6).\nNow, since ∥xadv −x∥ is small, by deﬁnition the robust features of xadv are still correlated with\nclassy (and nott) in expectation over the dataset. After all, humans still recognize the original class.\nOn the other hand, since everyxadv is strongly classiﬁed ast by a standard classiﬁer, it must be that\nsome of the non-robust features are now strongly correlated witht (in expectation).\nIn the case wheret is chosen at random, the robust features are originally uncorrelated with the label\nt (in expectation), and after the perturbation can be only slightly correlated (hence being signiﬁcantly\nless useful for classiﬁcation than before)5. Formally, we aim to construct a dataset ˆDrand where\nE(x,y)∼ ˆDrand\n[y ·f(x)]\n{> 0 iff non-robustly useful under D,\n≃ 0 otherwise. (7)\nIn contrast, when t is chosen deterministically based on y, the robust features actually point away\nfrom the assigned label t. In particular, all of the inputs labeled with class t exhibit non-robust\nfeatures correlated with t, but robust features correlated with the original class y. Thus, robust\nfeatures on the original training set provide signiﬁcant predictive power on the training set, but will\nactually hurt generalization on the standard test set. Formally, our goal is to construct ˆDdet such that\nE(x,y)∼ ˆDdet\n[y ·f(x)]\n\n\n\n> 0 iff non-robustly useful under D,\n< 0 iff robustly useful under D\n∈ R otherwise (f not useful under D).\n(8)\nWe ﬁnd that standard training on these datasets actually generalizes to theoriginal test set, as shown\nin Table 1). This indicates that non-robust features are indeed useful for classiﬁcation in the standard\nsetting. Remarkably, even training on ˆDdet (where all the robust features are correlated with the\nwrong class), results in a well-generalizing classiﬁer. This indicates that non-robust features can be\npicked up by models during standard training, even in the presence of predictive robust features 6\n3.3 Transferability can arise from non-robust features\nOne of the most intriguing properties of adversarial examples is that theytransfer across models with\ndifferent architectures and independently sampled training sets [Sze+14; PMG16; CRP19]. Here, we\nshow that this phenomenon can in fact be viewed as a natural consequence of the existence of non-\nrobust features. Recall that, according to our main thesis, adversarial examples can arise as a result\nof perturbing well-generalizing, yet brittle features. Given that such features are inherent to the data\ndistribution, different classiﬁers trained on independent samples from that distribution are likely\nto utilize similar non-robust features. Consequently, perturbations constructed by exploiting non-\nrobust features learned by one classiﬁer will transfer to other classiﬁers utilizing similar features.\n5Goh [Goh19a] provides an approach to quantifying this “robust feature leakage” and ﬁnds that one can\nobtain a (small) amount of test accuracy by leveraging robust feature leakage on ˆDrand.\n6Additional results and analysis are in App. D.5, D.6, and D.7.\n6\nIn order to illustrate and corroborate this hypothesis, we train ﬁve different architectures on the\ndataset generated in Section 3.2 (adversarial examples with deterministic labels) for a standard\nResNet-50 [He+16]. Our hypothesis would suggest that architectures which learn better from this\ntraining set (in terms of performance on the standard test set) are more likely to learn similar non-\nrobust features to the original classiﬁer. Indeed, we ﬁnd that the test accuracy of each architecture is\npredictive of how often adversarial examples transfer from the original model to standard classiﬁers\nwith that architecture (Figure 3). In a similar vein, Nakkiran [Nak19a] constructs a set of adversarial\nperturbations that is explicitly non-transferable and ﬁnds that these perturbations cannot be used to\nlearn a good classiﬁer. These ﬁndings thus corroborate our hypothesis that adversarial transferability\narises when models learn similar brittle features of the underlying dataset.\n25 30 35 40 45 50\nTest accuracy (%; trained on Dy + 1)\n60\n70\n80\n90\n100Transfer success rate (%)\nVGG-16\nInception-v3\nResNet-18 DenseNet\nResNet-50\nFigure 3: Transfer rate of adversarial exam-\nples from a ResNet-50 to different architec-\ntures alongside test set performance of these\narchitecture when trained on the dataset gen-\nerated in Section 3.2. Architectures more\nsusceptible to transfer attacks also performed\nbetter on the standard test set supporting\nour hypothesis that adversarial transferability\narises from using similarnon-robust features.\nSource Dataset Dataset\nCIFAR-10 ImageNet R\nD 95.3% 96.6%\nˆDrand 63.3% 87.9%\nˆDdet 43.7% 64.4%\nTable 1: Test accuracy (on D) of classiﬁers\ntrained on the D, ˆDrand, and ˆDdet train-\ning sets created using a standard (non-robust)\nmodel. For both ˆDrand and ˆDdet, only non-\nrobust features correspond to useful features\non both the train set and D. These datasets\nare constructed using adversarial perturba-\ntions of x towards a class t (random for\nˆDrand and deterministic for ˆDdet); the result-\ning images are relabeled ast.\n4 A Theoretical Framework for Studying (Non)-Robust Features\nThe experiments from the previous section demonstrate that the conceptual framework of robust and\nnon-robust features is strongly predictive of the empirical behavior of state-of-the-art models on real-\nworld datasets. In order to further strengthen our understanding of the phenomenon, we instantiate\nthe framework in a concrete setting that allows us to theoretically study various properties of the\ncorresponding model. Our model is similar to that of Tsipras et al. [Tsi+19] in the sense that it\ncontains a dichotomy between robust and non-robust features, extending upon it in a few ways: a)\nthe adversarial vulnerability can be explicitly expressed as a difference between the inherent data\nmetric and the ℓ2 metric, b) robust learning corresponds exactly to learning a combination of these\ntwo metrics, c) the gradients of robust models align better with the adversary’s metric.\nSetup. We study a simple problem of maximum likelihood classiﬁcation between two Gaussian\ndistributions. In particular, given samples (x,y ) sampled from D according to\ny\nu.a.r.\n∼ {− 1, +1}, x ∼ N (y ·µ∗, Σ∗), (9)\nour goal is to learn parameters Θ = (µ, Σ) such that\nΘ = arg min\nµ,Σ\nE(x,y)∼D [ℓ(x;y ·µ, Σ)], (10)\nwhere ℓ(x;µ, Σ) represents the Gaussian negative log-likelihood (NLL) function. Intuitively, we\nﬁnd the parametersµ, Σ which maximize the likelihood of the sampled data under the given model.\nClassiﬁcation can be accomplished via likelihood test: given an unlabeled samplex, we predicty as\ny = arg max\ny\nℓ(x;y ·µ, Σ) = sign\n(\nx⊤Σ−1µ\n)\n.\n7\nIn turn, the robust analogue of this problem arises from replacingℓ(x;y ·µ, Σ) with the NLL under\nadversarial perturbation. The resulting robust parameters Θr can be written as\nΘr = arg min\nµ,Σ\nE(x,y)∼D\n[\nmax\n∥δ∥2≤ε\nℓ(x +δ;y ·µ, Σ)\n]\n, (11)\nA detailed analysis appears in Appendix E—here we present a high-level overview of the results.\n(1) Vulnerability from metric misalignment (non-robust features). Note that in this model, one\ncan rigorously refer to an inner product (and thus a metric) induced by the features. In particular,\none can view the learned parameters of a GaussianΘ = (µ, Σ) as deﬁning an inner product over the\ninput space given by⟨x,y ⟩Θ = (x−µ)⊤Σ−1(y−µ). This in turn induces the Mahalanobis distance,\nwhich represents how a change in the input affects the features of the classiﬁer. This metric is not\nnecessarily aligned with the metric in which the adversary is constrained, theℓ2-norm. Actually, we\nshow that adversarial vulnerability arises exactly as a misalignment of these two metrics.\nTheorem 1 (Adversarial vulnerability from misalignment) . Consider an adversary whose pertur-\nbation is determined by the “Lagrangian penalty” form of (11), i.e.\nmax\nδ\nℓ(x +δ;y ·µ, Σ) −C · ∥δ∥2,\nwhereC ≥ 1\nσmin(Σ∗) is a constant trading off NLL minimization and the adversarial constraint (the\nbound onC ensures the problem is concave). Then, the adversarial loss Ladv incurred by (µ, Σ) is\nLadv(Θ) − L(Θ) = tr\n[(\nI + (C · Σ∗ −I)−1\n)2]\n−d,\nand, for a ﬁxed tr(Σ∗) =k the above is minimized by Σ∗ = k\ndI.\nIn fact, note that such a misalignment corresponds precisely to the existence ofnon-robust features—\n“small” changes in the adversary’s metric along certain directions can cause large changes under the\nnotion of distance established by the parameters (illustrated in Figure 4).\n(2) Robust Learning. The (non-robust) maximum likelihood estimate is Θ = Θ∗, and thus the\nvulnerability for the standard MLE depends entirely on the data distribution. The following theorem\ncharacterizes the behaviour of the learned parameters in the robust problem (we study a slight relax-\nation of (11) that becomes exact exponentially fast asd → ∞, see Appendix E.3.3). In fact, we can\nprove (Section E.3.4) that performing (sub)gradient descent on the inner maximization (known as\nadversarial training [GSS15; Mad+18]) yields exactly Θr. We ﬁnd that as the perturbation budgetε\nincreases, the metric induced by the classiﬁermixesℓ2 and the metric induced by the data features.\nTheorem 2 (Robustly Learned Parameters). Just as in the non-robust case,µr =µ∗, i.e. the true\nmean is learned. For the robust covarianceΣr, there exists anε0 > 0, such that for anyε ∈ [0,ε 0),\nΣr = 1\n2Σ∗+ 1\nλ ·I+\n√\n1\nλ · Σ∗ + 1\n4Σ2∗, where Ω\n( 1 +ε1/2\nε1/2 +ε3/2\n)\n≤λ ≤O\n(1 +ε1/2\nε1/2\n)\n.\nThe effect of robust optimization under an ℓ2-constrained adversary is visualized in Figure 4. As ϵ\ngrows, the learned covariance becomes more aligned with identity. For instance, we can see that the\nclassiﬁer learns to be less sensitive in certain directions, despite their usefulness for classiﬁcation.\n(3) Gradient Interpretability. Tsipras et al. [Tsi+19] observe that gradients of robust models\ntend to look more semantically meaningful. It turns out that under our model, this behaviour arises\nas a natural consequence of Theorem 2. In particular, we show that the resulting robustly learned\nparameters cause the gradient of the linear classiﬁer and the vector connecting the means of the two\ndistributions to better align (in a worst-case sense) under theℓ2 inner product.\nTheorem 3 (Gradient alignment). Letf(x) andfr(x) be monotonic classiﬁers based on the linear\nseparator induced by standard and ℓ2-robust maximum likelihood classiﬁcation, respectively. The\nmaximum angle formed between the gradient of the classiﬁer (wrt input) and the vector connecting\nthe classes can be smaller for the robust model:\nmin\nµ\n⟨µ, ∇xfr(x)⟩\n∥µ∥ · ∥∇xfr(x)∥ > min\nµ\n⟨µ, ∇xf(x)⟩\n∥µ∥ · ∥∇xf(x)∥.\nFigure 4 illustrates this phenomenon in the two-dimensional case, where ℓ2-robustness causes the\ngradient direction to become increasingly aligned with the vector between the means (µ).\n8\n20\n 15\n 10\n 5\n 0 5 10 15 20\nFeature x1\n10.0\n7.5\n5.0\n2.5\n0.0\n2.5\n5.0\n7.5\n10.0\nFeature x2\nMaximum likelihood estimate\n2 unit ball\n1-induced metric unit ball\nSamples from (0, )\n20\n 15\n 10\n 5\n 0 5 10 15 20\nFeature x1\n10.0\n7.5\n5.0\n2.5\n0.0\n2.5\n5.0\n7.5\n10.0\nFeature x2\nTrue Parameters ( = 0)\nSamples from ( , )\nSamples from ( , )\n20\n 15\n 10\n 5\n 0 5 10 15 20\nFeature x1\n10.0\n7.5\n5.0\n2.5\n0.0\n2.5\n5.0\n7.5\n10.0\nFeature x2\nRobust parameters, = 1.0\n20\n 15\n 10\n 5\n 0 5 10 15 20\nFeature x1\n10.0\n7.5\n5.0\n2.5\n0.0\n2.5\n5.0\n7.5\n10.0\nFeature x2\nRobust parameters, = 10.0\nFigure 4: An empirical demonstration of the effect illustrated by Theorem 2—as the adversarial\nperturbation budgetε is increased, the learned meanµ remains constant, but the learned covariance\n“blends” with the identity matrix, effectively adding uncertainty onto the non-robust feature.\nDiscussion. Our analysis suggests that rather than offering quantitative classiﬁcation beneﬁts, a\nnatural way to view the role of robust optimization is as enforcing a prior over the features learned\nby the classiﬁer. In particular, training with an ℓ2-bounded adversary prevents the classiﬁer from\nrelying heavily on features which induce a metric dissimilar to the ℓ2 metric. The strength of the\nadversary then allows for a trade-off between the enforced prior, and the data-dependent features.\nRobustness and accuracy. Note that in the setting described so far, robustness can be at odds\nwith accuracy since robust training prevents us from learning the most accurate classiﬁer (a similar\nconclusion is drawn in [Tsi+19]). However, we note that there are very similar settings where non-\nrobust features manifest themselves in the same way, yet a classiﬁer with perfect robustness and\naccuracy is still attainable. Concretely, consider the distributions pictured in Figure 14 in Appendix\nD.10. It is straightforward to show that while there are many perfectly accurate classiﬁers, any\nstandard loss function will learn an accurate yet non-robust classiﬁer. Only when robust training is\nemployed does the classiﬁer learn a perfectly accurate and perfectly robust decision boundary.\n5 Related Work\nSeveral models for explaining adversarial examples have been proposed in prior work, utilizing\nideas ranging from ﬁnite-sample overﬁtting to high-dimensional statistical phenomena [Gil+18;\nFFF18; For+19; TG16; Sha+19a; MDM18; Sha+19b; GSS15; BPR18]. The key differentiating\naspect of our model is that adversarial perturbations arise as well-generalizing, yet brittle, features,\nrather than statistical anomalies. In particular, adversarial vulnerability does not stem from using\na speciﬁc model class or a speciﬁc training method, since standard training on the “robustiﬁed”\ndata distribution of Section 3.1 leads to robust models. At the same time, as shown in Section 3.2,\nthese non-robust features are sufﬁcient to learn a good standard classiﬁer. We discuss the connection\nbetween our model and others in detail in Appendix A and additional related work in Appendix B.\n6 Conclusion\nIn this work, we cast the phenomenon of adversarial examples as a natural consequence of the\npresence of highly predictive but non-robust features in standard ML datasets. We provide support\nfor this hypothesis by explicitly disentangling robust and non-robust features in standard datasets,\nas well as showing that non-robust features alone are sufﬁcient for good generalization. Finally,\nwe study these phenomena in more detail in a theoretical setting where we can rigorously study\nadversarial vulnerability, robust training, and gradient alignment.\nOur ﬁndings prompt us to view adversarial examples as a fundamentally human phenomenon. In\nparticular, we should not be surprised that classiﬁers exploit highly predictive features that happen\nto be non-robust under a human-selected notion of similarity, given such features exist in real-world\ndatasets. In the same manner, from the perspective of interpretability, as long as models rely on these\nnon-robust features, we cannot expect to have model explanations that are both human-meaningful\nand faithful to the models themselves. Overall, attaining models that are robust and interpretable\nwill require explicitly encoding human priors into the training process.\n9\nAcknowledgements\nWe thank Preetum Nakkiran for suggesting the experiment of Appendix D.9 (i.e. replicating Figure 3\nwith targeted attacks). We also are grateful to the authors of Engstrom et al. [Eng+19a] (Chris Olah,\nDan Hendrycks, Justin Gilmer, Reiichiro Nakano, Preetum Nakkiran, Gabriel Goh, Eric Wallace)—\nfor their insights and efforts replicating, extending, and discussing our experimental results.\nWork supported in part by the NSF grants CCF-1553428, CCF-1563880, CNS-1413920, CNS-\n1815221, IIS-1447786, IIS-1607189, the Microsoft Corporation, the Intel Corporation, the MIT-\nIBM Watson AI Lab research grant, and an Analog Devices Fellowship.\nReferences\n[ACW18] Anish Athalye, Nicholas Carlini, and David A. Wagner. “Obfuscated Gradients Give a\nFalse Sense of Security: Circumventing Defenses to Adversarial Examples”. In: Inter-\nnational Conference on Machine Learning (ICML). 2018.\n[Ath+18] Anish Athalye et al. “Synthesizing Robust Adversarial Examples”. In: International\nConference on Machine Learning (ICML). 2018.\n[BCN06] Cristian Bucilu ˇa, Rich Caruana, and Alexandru Niculescu-Mizil. “Model compres-\nsion”. In: International Conference on Knowledge Discovery and Data Mining (KDD).\n2006.\n[Big+13] Battista Biggio et al. “Evasion attacks against machine learning at test time”. In:\nJoint European conference on machine learning and knowledge discovery in databases\n(ECML-KDD). 2013.\n[BPR18] Sébastien Bubeck, Eric Price, and Ilya Razenshteyn. “Adversarial examples from com-\nputational constraints”. In: arXiv preprint arXiv:1805.10204. 2018.\n[Car+19] Nicholas Carlini et al. “On Evaluating Adversarial Robustness”. In: ArXiv preprint\narXiv:1902.06705. 2019.\n[CRK19] Jeremy M Cohen, Elan Rosenfeld, and J Zico Kolter. “Certiﬁed adversarial robustness\nvia randomized smoothing”. In: arXiv preprint arXiv:1902.02918. 2019.\n[CRP19] Zachary Charles, Harrison Rosenberg, and Dimitris Papailiopoulos. “A Geometric Per-\nspective on the Transferability of Adversarial Directions”. In:International Conference\non Artiﬁcial Intelligence and Statistics (AISTATS). 2019.\n[CW17a] Nicholas Carlini and David Wagner. “Adversarial Examples Are Not Easily Detected:\nBypassing Ten Detection Methods”. In: Workshop on Artiﬁcial Intelligence and Secu-\nrity (AISec). 2017.\n[CW17b] Nicholas Carlini and David Wagner. “Towards evaluating the robustness of neural net-\nworks”. In: Symposium on Security and Privacy (SP). 2017.\n[Dan67] John M. Danskin. The Theory of Max-Min and its Application to Weapons Allocation\nProblems. 1967.\n[Das+19] Constantinos Daskalakis et al. “Efﬁcient Statistics, in High Dimensions, from Trun-\ncated Samples”. In: Foundations of Computer Science (FOCS). 2019.\n[Din+19] Gavin Weiguang Ding et al. “On the Sensitivity of Adversarial Robustness to Input\nData Distributions”. In: International Conference on Learning Representations. 2019.\n[Eng+19a] Logan Engstrom et al. “A Discussion of ’Adversarial Examples Are Not Bugs, They\nAre Features’”. In: Distill (2019). https://distill.pub/2019/advex-bugs-discussion. DOI:\n10.23915/distill.00019.\n[Eng+19b] Logan Engstrom et al. “A Rotation and a Translation Sufﬁce: Fooling CNNs with\nSimple Transformations”. In: International Conference on Machine Learning (ICML).\n2019.\n[FFF18] Alhussein Fawzi, Hamza Fawzi, and Omar Fawzi. “Adversarial vulnerability for any\nclassiﬁer”. In: Advances in Neural Information Processing Systems (NeuRIPS). 2018.\n[FMF16] Alhussein Fawzi, Seyed-Mohsen Moosavi-Dezfooli, and Pascal Frossard. “Robustness\nof classiﬁers: from adversarial to random noise”. In: Advances in Neural Information\nProcessing Systems. 2016.\n10\n[For+19] Nic Ford et al. “Adversarial Examples Are a Natural Consequence of Test Error in\nNoise”. In: arXiv preprint arXiv:1901.10513. 2019.\n[Fur+18] Tommaso Furlanello et al. “Born-Again Neural Networks”. In: International Confer-\nence on Machine Learning (ICML). 2018.\n[Gei+19] Robert Geirhos et al. “ImageNet-trained CNNs are biased towards texture; increasing\nshape bias improves accuracy and robustness.” In: International Conference on Learn-\ning Representations. 2019.\n[Gil+18] Justin Gilmer et al. “Adversarial spheres”. In: Workshop of International Conference\non Learning Representations (ICLR). 2018.\n[Goh19a] Gabriel Goh. “A Discussion of ’Adversarial Examples Are Not Bugs, They Are\nFeatures’: Robust Feature Leakage”. In: Distill (2019). https://distill.pub/2019/advex-\nbugs-discussion/response-2. DOI: 10.23915/distill.00019.2.\n[Goh19b] Gabriel Goh. “A Discussion of ’Adversarial Examples Are Not Bugs, They\nAre Features’: Two Examples of Useful, Non-Robust Features”. In: Distill\n(2019). https://distill.pub/2019/advex-bugs-discussion/response-3. DOI: 10 . 23915 /\ndistill.00019.3.\n[GSS15] Ian J Goodfellow, Jonathon Shlens, and Christian Szegedy. “Explaining and Harness-\ning Adversarial Examples”. In: International Conference on Learning Representations\n(ICLR). 2015.\n[HD19] Dan Hendrycks and Thomas G. Dietterich. “Benchmarking Neural Network Robust-\nness to Common Corruptions and Surface Variations”. In:International Conference on\nLearning Representations (ICLR). 2019.\n[He+16] Kaiming He et al. “Deep Residual Learning for Image Recognition”. In: Conference\non Computer Vision and Pattern Recognition (CVPR). 2016.\n[He+17] Warren He et al. “Adversarial example defense: Ensembles of weak defenses are not\nstrong”. In: USENIX Workshop on Offensive Technologies (WOOT). 2017.\n[HVD14] Geoffrey Hinton, Oriol Vinyals, and Jeff Dean. “Distilling the Knowledge in a Neu-\nral Network”. In: Neural Information Processing Systems (NeurIPS) Deep Learning\nWorkshop. 2014.\n[JLT18] Saumya Jetley, Nicholas Lord, and Philip Torr. “With friends like these, who needs ad-\nversaries?” In: Advances in Neural Information Processing Systems (NeurIPS). 2018.\n[Kri09] Alex Krizhevsky. “Learning Multiple Layers of Features from Tiny Images”. In: Tech-\nnical report. 2009.\n[KSJ19] Beomsu Kim, Junghoon Seo, and Taegyun Jeon. “Bridging Adversarial Robustness and\nGradient Interpretability”. In: International Conference on Learning Representations\nWorkshop on Safe Machine Learning (ICLR SafeML). 2019.\n[Lec+19] Mathias Lecuyer et al. “Certiﬁed robustness to adversarial examples with differential\nprivacy”. In: Symposium on Security and Privacy (SP). 2019.\n[Liu+17] Yanpei Liu et al. “Delving into Transferable Adversarial Examples and Black-box At-\ntacks”. In: International Conference on Learning Representations (ICLR). 2017.\n[LM00] Beatrice Laurent and Pascal Massart. “Adaptive estimation of a quadratic functional\nby model selection”. In: Annals of Statistics. 2000.\n[Mad+18] Aleksander Madry et al. “Towards deep learning models resistant to adversarial at-\ntacks”. In: International Conference on Learning Representations (ICLR). 2018.\n[MDM18] Saeed Mahloujifar, Dimitrios I Diochnos, and Mohammad Mahmoody. “The curse of\nconcentration in robust learning: Evasion and poisoning attacks from concentration of\nmeasure”. In: AAAI Conference on Artiﬁcial Intelligence (AAAI). 2018.\n[Moo+17] Seyed-Mohsen Moosavi-Dezfooli et al. “Universal adversarial perturbations”. In: con-\nference on computer vision and pattern recognition (CVPR). 2017.\n[MV15] Aravindh Mahendran and Andrea Vedaldi. “Understanding deep image representations\nby inverting them”. In: computer vision and pattern recognition (CVPR). 2015.\n[Nak19a] Preetum Nakkiran. “A Discussion of ’Adversarial Examples Are Not Bugs,\nThey Are Features’: Adversarial Examples are Just Bugs, Too”. In: Distill\n(2019). https://distill.pub/2019/advex-bugs-discussion/response-5. DOI: 10 . 23915 /\ndistill.00019.5.\n11\n[Nak19b] Preetum Nakkiran. “Adversarial robustness may be at odds with simplicity”. In: arXiv\npreprint arXiv:1901.00532. 2019.\n[OMS17] Chris Olah, Alexander Mordvintsev, and Ludwig Schubert. “Feature Visualization”.\nIn: Distill. 2017.\n[Pap+17] Nicolas Papernot et al. “Practical black-box attacks against machine learning”. In: Asia\nConference on Computer and Communications Security. 2017.\n[PMG16] Nicolas Papernot, Patrick McDaniel, and Ian Goodfellow. “Transferability in Machine\nLearning: from Phenomena to Black-box Attacks using Adversarial Samples”. In:\nArXiv preprint arXiv:1605.07277. 2016.\n[Rec+19] Benjamin Recht et al. “Do CIFAR-10 Classiﬁers Generalize to CIFAR-10?” In: Inter-\nnational Conference on Machine Learning (ICML). 2019.\n[RSL18] Aditi Raghunathan, Jacob Steinhardt, and Percy Liang. “Certiﬁed defenses against\nadversarial examples”. In: International Conference on Learning Representations\n(ICLR). 2018.\n[Rus+15] Olga Russakovsky et al. “ImageNet Large Scale Visual Recognition Challenge”. In:\nInternational Journal of Computer Vision (IJCV). 2015.\n[Sch+18] Ludwig Schmidt et al. “Adversarially Robust Generalization Requires More Data”. In:\nAdvances in Neural Information Processing Systems (NeurIPS). 2018.\n[Sha+19a] Ali Shafahi et al. “Are adversarial examples inevitable?” In: International Conference\non Learning Representations (ICLR). 2019.\n[Sha+19b] Adi Shamir et al. “A Simple Explanation for the Existence of Adversarial Examples\nwith Small Hamming Distance”. In: arXiv preprint arXiv:1901.10861. 2019.\n[SHS19] David Stutz, Matthias Hein, and Bernt Schiele. “Disentangling Adversarial Robustness\nand Generalization”. In: Computer Vision and Pattern Recognition (CVPR). 2019.\n[Smi+17] D. Smilkov et al. “SmoothGrad: removing noise by adding noise”. In: ICML workshop\non visualization for deep learning. 2017.\n[Sug+19] Arun Sai Suggala et al. “Revisiting Adversarial Risk”. In: Conference on Artiﬁcial\nIntelligence and Statistics (AISTATS). 2019.\n[Sze+14] Christian Szegedy et al. “Intriguing properties of neural networks”. In: International\nConference on Learning Representations (ICLR). 2014.\n[TG16] Thomas Tanay and Lewis Grifﬁn. “A Boundary Tilting Perspective on the Phenomenon\nof Adversarial Examples”. In: ArXiv preprint arXiv:1608.07690. 2016.\n[Tra+17] Florian Tramer et al. “The Space of Transferable Adversarial Examples”. In: ArXiv\npreprint arXiv:1704.03453. 2017.\n[Tsi+19] Dimitris Tsipras et al. “Robustness May Be at Odds with Accuracy”. In: International\nConference on Learning Representations (ICLR). 2019.\n[Ues+18] Jonathan Uesato et al. “Adversarial Risk and the Dangers of Evaluating Against Weak\nAttacks”. In: International Conference on Machine Learning (ICML). 2018.\n[Wan+18] Tongzhou Wang et al. “Dataset Distillation”. In: ArXiv preprint arXiv:1811.10959 .\n2018.\n[WK18] Eric Wong and J Zico Kolter. “Provable defenses against adversarial examples via the\nconvex outer adversarial polytope”. In:International Conference on Machine Learning\n(ICML). 2018.\n[Xia+19] Kai Y . Xiao et al. “Training for Faster Adversarial Robustness Veriﬁcation via Inducing\nReLU Stability”. In: International Conference on Learning Representations (ICLR) .\n2019.\n[Zou+18] Haosheng Zou et al. “Geometric Universality of Adversarial Examples in Deep Learn-\ning”. In: Geometry in Machine Learning ICML Workshop (GIML). 2018.\n12",
  "values": {
    "Deferral to humans": "No",
    "Justice": "No",
    "Privacy": "No",
    "Explicability": "No",
    "Respect for Law and public interest": "No",
    "Fairness": "No",
    "Collective influence": "No",
    "User influence": "No",
    "Interpretable (to users)": "No",
    "Transparent (to users)": "No",
    "Critiqability": "No",
    "Non-maleficence": "No",
    "Respect for Persons": "No",
    "Autonomy (power to decide)": "No",
    "Not socially biased": "No",
    "Beneficence": "No"
  }
}