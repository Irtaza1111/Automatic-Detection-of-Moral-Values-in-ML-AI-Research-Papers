{
  "pdf": "NeurIPS-2019-adversarial-training-and-robustness-for-multiple-perturbations-Paper",
  "title": "Adversarial Training and Robustness for Multiple Perturbations",
  "author": "Florian Tramer, Dan Boneh",
  "paper_id": "NeurIPS-2019-adversarial-training-and-robustness-for-multiple-perturbations-Paper",
  "text": "Adversarial Training and Robustness for\nMultiple Perturbations\nFlorian Tramèr\nStanford University\nDan Boneh\nStanford University\nAbstract\nDefenses against adversarial examples, such as adversarial training, are typically\ntailored to a single perturbation type (e.g., smallℓ∞-noise). For other perturbations,\nthese defenses offer no guarantees and, at times, even increase the model’s vulnera-\nbility. Our aim is to understand the reasons underlying this robustness trade-off,\nand to train models that are simultaneously robust to multiple perturbation types.\nWe prove that a trade-off in robustness to different types ofℓp-bounded and spatial\nperturbations must exist in a natural and simple statistical setting. We corroborate\nour formal analysis by demonstrating similar robustness trade-offs on MNIST\nand CIFAR10. We propose new multi-perturbation adversarial training schemes,\nas well as an efﬁcient attack for theℓ1-norm, and use these to show that models\ntrained against multiple attacks fail to achieve robustness competitive with that of\nmodels trained on each attack individually. In particular, we ﬁnd that adversarial\ntraining with ﬁrst-orderℓ∞,ℓ 1 andℓ2 attacks on MNIST achieves merely 50%\nrobust accuracy, partly because of gradient-masking. Finally, we propose afﬁne\nattacks that linearly interpolate between perturbation types and further degrade the\naccuracy of adversarially trained models.\n1 Introduction\nAdversarial examples [37, 15] are proving to be an inherent blind-spot in machine learning (ML)\nmodels. Adversarial examples highlight the tendency of ML models to learn superﬁcial and brittle\ndata statistics [19, 13, 18], and present a security risk for models deployed in cyber-physical systems\n(e.g., virtual assistants [5], malware detectors [16] or ad-blockers [39]).\nKnown successful defenses are tailored to a speciﬁc perturbation type (e.g., a smallℓp-ball [25, 28, 42]\nor small spatial transforms [ 11]). These defenses provide empirical (or certiﬁable) robustness\nguarantees for one perturbation type, but typically offer no guarantees against other attacks [ 35,\n31]. Worse, increasing robustness to one perturbation type has sometimes been found to increase\nvulnerability to others [11, 31]. This leads us to the central problem considered in this paper:\nCan we achieve adversarial robustness to different types of perturbations simultaneously?\nNote that even though prior work has attained robustness to different perturbation types [25, 31, 11],\nthese results may not compose. For instance, an ensemble of two classiﬁers—each of which is robust\nto a single type of perturbation—may be robust to neither perturbation. Our aim is to study the extent\nto which it is possible to learn models that are simultaneously robust to multiple types of perturbation.\nTo gain intuition about this problem, we ﬁrst study a simple and natural classiﬁcation task, that has\nbeen used to analyze trade-offs between standard and adversarial accuracy [ 41], and the sample-\ncomplexity of adversarial generalization [30]. We deﬁneMutually Exclusive Perturbations (MEPs)as\npairs of perturbation types for which robustness to one type implies vulnerability to the other. For this\ntask, we prove thatℓ∞ andℓ1-perturbations are MEPs and thatℓ∞-perturbations and input rotations\n33rd Conference on Neural Information Processing Systems (NeurIPS 2019), Vancouver, Canada.\n0 2 4 6 8 10\nEpochs\n0.00\n0.25\n0.50\n0.75\n1.00\nAccuracy\nAdv∞\nAdv1\nAdv2\nAdvmax tested on ℓ∞\nAdvmax tested on ℓ1\nAdvmax tested on ℓ2\nAdvmax tested on all\n(a) MNIST models trained onℓ1,ℓ 2 &ℓ∞ attacks.\n0 2 4 6 8 10\nEpochs\n0.00\n0.25\n0.50\n0.75\n1.00\nAccuracy\nAdv∞\nAdvR T\nAdvmax tested on ℓ∞\nAdvmax tested on RT\nAdvmax tested on both (b) MNIST models trained onℓ∞ and RT attacks.\n0 20000 40000 60000 80000\nSteps\n0.4\n0.5\n0.6\n0.7\n0.8\nAccuracy\nAdv∞\nAdv1\nAdvmax tested on ℓ∞\nAdvmax tested on ℓ1\nAdvmax tested on both\n(c) CIFAR10 models trained onℓ1 andℓ∞ attacks.\n0 20000 40000 60000 80000\nSteps\n0.4\n0.5\n0.6\n0.7\n0.8\nAccuracy\nAdv∞\nAdvR T\nAdvmax tested on ℓ∞\nAdvmax tested on RT\nAdvmax tested on both (d) CIFAR10 models trained onℓ∞ and RT attacks.\nFigure 1: Robustness trade-off on MNIST (top) and CIFAR10 (bottom). For a union ofℓp-balls\n(left), or ofℓ∞-noise and rotation-translations (RT) (right), we train models Advmax on the strongest\nperturbation-type for each input. We report the test accuracy of Adv max against each individual\nperturbation type (solid line) and against their union (dotted brown line). The vertical lines show the\nadversarial accuracy of models trained and evaluated on a single perturbation type.\nand translations [11] are also MEPs. Moreover, for these MEP pairs, we ﬁnd that robustness to either\nperturbation type requires fundamentally different features. The existence of such a trade-off for this\nsimple classiﬁcation task suggests that it may be prevalent in more complex statistical settings.\nTo complement our formal analysis, we introduce new adversarial training schemes for multiple\nperturbations. For each training point, these schemes build adversarial examples for all perturbation\ntypes and then train either on all examples (the “avg” strategy) or only the worst example (the “max”\nstrategy). These two strategies respectively minimize the average error rate across perturbation types,\nor the error rate against an adversary that picks the worst perturbation type for each input.\nFor adversarial training to be practical, we also need efﬁcient and strong attacks [25]. We show that\nProjected Gradient Descent [22, 25] is inefﬁcient in theℓ1-case, and design a new attack, Sparseℓ1\nDescent (SLIDE), that is both efﬁcient and competitive with strong optimization attacks [8],\nWe experiment with MNIST and CIFAR10. MNIST is an interesting case-study, asdistinct models\nfrom prior work attain strong robustness to all perturbations we consider [25, 31, 11], yet no single\nclassiﬁer is robust to all attacks [31, 32, 11]. For models trained on multiple ℓp-attacks (ℓ1,ℓ 2,ℓ∞\nfor MNIST, and ℓ1,ℓ∞ for CIFAR10), or on both ℓ∞ and spatial transforms [ 11], we conﬁrm a\nnoticeable robustness trade-off. Figure 1 plots the test accuracy of models Advmax trained using our\n“max” strategy. In all cases, robustness to multiple perturbations comes at a cost—usually of 5-10%\nadditional error—compared to models trained against each attack individually (the horizontal lines).\nRobustness toℓ1,ℓ 2 andℓ∞-noise on MNIST is a striking failure case, where the robustness trade-off\nis compounded by gradient-masking [27, 40, 1]. Extending prior observations [25, 31, 23], we show\nthat models trained against anℓ∞-adversary learn representations that mask gradients for attacks in\notherℓp-norms. When trained against ﬁrst-order ℓ1,ℓ 2 andℓ∞-attacks, the model learns to resist\nℓ∞-attacks while giving the illusion of robustness to ℓ1 andℓ2 attacks. This model only achieves\n52% accuracy when evaluated on gradient-free attacks [3, 31]. This shows that, unlike previously\nthought [41], adversarial training with strong ﬁrst-order attacks can suffer from gradient-masking.\nWe thus argue that attaining robustness toℓp-noise on MNIST requires new techniques (e.g., training\non expensive gradient-free attacks, or scaling certiﬁed defenses to multiple perturbations).\nMNIST has sometimes been said to be a poor dataset for evaluating adversarial examples defenses,\nas some attacks are easy to defend against (e.g., input-thresholding or binarization works well for\nℓ∞-attacks [41, 31]). Our results paint a more nuanced view: the simplicity of these ℓ∞-defenses\n2\nbecomes a disadvantage when training against multiple ℓp-norms. We thus believe that MNIST\nshould not be abandoned as a benchmark just yet. Our inability to achieve multi-ℓp robustness for this\nsimple dataset raises questions about the viability of scaling current defenses to more complex tasks.\nLooking beyond adversaries that choose from a union of perturbation types, we introduce a newafﬁne\nadversary that may linearly interpolate between perturbations (e.g., by compoundingℓ∞-noise with\na small rotation). We prove that for locally-linear models, robustness to a union ofℓp-perturbations\nimplies robustness to afﬁne attacks. In contrast, afﬁne combinations ofℓ∞ and spatial perturbations\nare provably stronger than either perturbation individually. We show that this discrepancy translates\nto neural networks trained on real data. Thus, in some cases, attaining robustness to a union of\nperturbation types remains insufﬁcient against a more creative adversary that composes perturbations.\nOur results show that despite recent successes in achieving robustness to single perturbation types,\nmany obstacles remain towards attaining truly robust models. Beyond the robustness trade-off,\nefﬁcient computational scaling of current defenses to multiple perturbations remains an open problem.\nThe code used for all of our experiments can be found here: https://github.com/ftramer/\nMultiRobustness\nProofs of all theorems, experimental setups, and additional experiments are in the full version of this\nextended abstract [38].\n2 Theoretical Limits to Multi-perturbation Robustness\nWe study statistical properties of adversarial robustness in a natural statistical model introduced in [41],\nand which exhibits many phenomena observed on real data, such as trade-offs between robustness and\naccuracy [41] or a higher sample complexity for robust generalization [31]. This model also proves\nuseful in analyzing and understanding adversarial robustness for multiple perturbations. Indeed,\nwe prove a number of results that correspond to phenomena we observe on real data, in particular\ntrade-offs in robustness to differentℓp or rotation-translation attacks [11].\nWe follow a line of works that study distributions for which adversarial examples existuncondition-\nally [41, 21, 33, 12, 14, 26]. These distributions, including ours, are much simpler than real-world\ndata, and thus need not be evidence that adversarial examples are inevitable in practice. Rather, we\nhypothesize that current ML models are highly vulnerable to adversarial examples because they learn\nsuperﬁcial data statistics [19, 13, 18] that share some properties of these simple distributions.\nIn prior work, a robustness trade-off forℓ∞ andℓ2-noise is shown in [21] for data distributed over\ntwo concentric spheres. Our conceptually simpler model has the advantage of yielding results beyond\nℓp-norms (e.g., for spatial attacks) and which apply symmetrically to both classes. Building on\nwork by Xu et al. [43], Demontis et al. [9] show a robustness trade-off for dual norms (e.g.,ℓ∞ and\nℓ1-noise) in linear classiﬁers.\n2.1 Adversarial Risk for Multiple Perturbation Models\nConsider a classiﬁcation task for a distributionD over examples x∈ Rd and labelsy∈ [C]. Let\nf : Rd→ [C] denote a classiﬁer and letl(f (x),y ) be the zero-one loss (i.e., 1 f (x)̸=y).\nWe assumen perturbation types, each characterized by a setS of allowed perturbations for an input\nx. The setS can be anℓp-ball [37, 15] or capture other perceptually small transforms such as image\nrotations and translations [11]. For a perturbation r∈S, an adversarial example is ˆx = x + r (this\nis pixel-wise addition forℓp perturbations, but can be a more complex operation, e.g., for rotations).\nFor a perturbation setS and modelf, we deﬁneRadv(f ;S) := E(x,y)∼D [maxr∈S l(f (x + r),y )]\nas the adversarial error rate. To extendRadv to multiple perturbation setsS1,...,S n, we can consider\nthe average error rate for eachSi, denotedRavg\nadv. This metric most clearly captures the trade-off in\nrobustness across independent perturbation types, but is not the most appropriate from a security\nperspective on adversarial examples. A more natural metric, denotedRmax\nadv , is the error rate against an\nadversary that picks, for each input, the worst perturbation from the union of theSi. More formally,\nRmax\nadv (f ;S1,...,S n) :=Radv(f ;∪iSi), Ravg\nadv(f ;S1,...,S n) := 1\nn\n∑\niRadv(f ;Si). (1)\nMost results in this section are lower bounds onRavg\nadv, which also hold forRavg\nmax sinceRmax\nadv ≥R avg\nadv.\n3\nTwo perturbation typesS1,S 2 are Mutually Exclusive Perturbations (MEPs), ifRavg\nadv(f ;S1,S 2)≥\n1/|C| for all modelsf (i.e., no model has non-trivial average risk against both perturbations).\n2.2 A binary classiﬁcation task\nWe analyze the adversarial robustness trade-off for different perturbation types in a natural statistical\nmodel introduced by Tsipras et al. [41]. Their binary classiﬁcation task consists of input-label pairs\n(x,y ) sampled from a distributionD as follows (note thatD is (d + 1)-dimensional):\ny\nu.a.r\n∼ {−1, +1}, x 0 =\n{+y, w.p.p0,\n−y, w.p. 1−p0\n, x 1,...,x d\ni.i.d\n∼N (yη, 1), (2)\nwherep0≥ 0.5,N (µ,σ 2) is the normal distribution andη =α/\n√\nd for some positive constantα.\nFor this distribution, Tsipras et al. [41] show a trade-off between standard and adversarial accuracy\n(forℓ∞ attacks), by drawing a distinction between the “robust” feature x0 that smallℓ∞-noise cannot\nmanipulate, and the “non-robust” features x1,...,x d that can be fully overridden by smallℓ∞-noise.\n2.3 Small ℓ∞ andℓ1 Perturbations are Mutually Exclusive\nThe starting point of our analysis is the observation that the robustness of a feature depends on the\nconsidered perturbation type. To illustrate, we recall two classiﬁers from [41] that operate on disjoint\nfeature sets. The ﬁrst,f (x) = sign(x0), achieves accuracyp0 for allℓ∞-perturbations withϵ< 1 but\nis highly vulnerable toℓ1-perturbations of sizeϵ≥ 1. The second classiﬁer,h(x) = sign(∑d\ni=1xi)\nis robust toℓ1-perturbations of average norm below E[∑d\ni=1xi] = Θ(\n√\nd), yet it is fully subverted\nby a ℓ∞-perturbation that shifts the features x1,...,x d by±2η = Θ(1/\n√\nd). We prove that this\ntension betweenℓ∞ andℓ1 robustness, and of the choice of “robust” features, is inherent for this task:\nTheorem 1. Letf be a classiﬁer forD. LetS∞ be the set ofℓ∞-bounded perturbations withϵ = 2η,\nandS1 the set ofℓ1-bounded perturbations withϵ = 2. Then,Ravg\nadv(f ;S∞,S 1)≥ 1/2.\nThe proof is in Appendix F. The bound shows that no classiﬁer can attain betterRavg\nadv (and thusRmax\nadv )\nthan a trivial constant classiﬁerf (x) = 1 , which satisﬁesRadv(f ;S∞) =Radv(f ;S1) = 1/2.\nSimilar to [9], our analysis extends to arbitrary dual normsℓp andℓq with 1/p + 1/q = 1 andp< 2.\nThe perturbation required to ﬂip the features x1,...,x n has anℓp norm ofΘ(d\n1\np− 1\n2 ) = ω(1) and an\nℓq norm ofΘ(d\n1\nq− 1\n2 ) = Θ(d\n1\n2− 1\np ) = o(1). Thus, featurex0 is more robust than featuresx1,...,x n\nwith respect to theℓq-norm, whereas for the dualℓp-norm the situation is reversed.\n2.4 Small ℓ∞ and Spatial Perturbations are (nearly) Mutually Exclusive\nWe now analyze two other orthogonal perturbation types,ℓ∞-noise and rotation-translations [11]. In\nsome cases, increasing robustness toℓ∞-noise has been shown to decrease robustness to rotation-\ntranslations [11]. We prove that such a trade-off is inherent for our binary classiﬁcation task.\nTo reason about rotation-translations, we assume that the featuresxi form a 2D grid. We also letx0\nbe distributed asN (y,α−2), a technicality that does not qualitatively change our prior results. Note\nthat the distribution of the featuresx1,...,x d is permutation-invariant. Thus, the only power of a\nrotation-translation adversary is to “move” feature x0. Without loss of generality, we identify a small\nrotation-translation of an input x with a permutation of its features that sendsx0 to one ofN ﬁxed\npositions (e.g., with translations of±3px as in [11],x0 can be moved toN = 49 different positions).\nA model can be robust to these permutations by ignoring theN positions that featurex0 can be moved\nto, and focusing on the remaining permutation-invariant features. Yet, this model is vulnerable to\nℓ∞-noise, as it ignoresx0. In turn, a model that relies on featurex0 can be robust toℓ∞-perturbations,\nbut is vulnerable to a spatial perturbation that “hides” x0 among other features. Formally, we show:\nTheorem 2. Letf be a classiﬁer forD (withx0∼N (y,α−2)). LetS∞ be the set ofℓ∞-bounded\nperturbations withϵ = 2η, andSRT be the set of perturbations for an RT adversary with budgetN.\nThen,Ravg\nadv(f ;S∞,S RT)≥ 1/2−O(1/\n√\nN ).\nThe proof, given in Appendix G, is non-trivial and yields an asymptotic lower-bound onRavg\nadv. We\ncan also provide tight numerical estimates for concrete parameter settings (see Appendix G.1).\n4\n2.5 Afﬁne Combinations of Perturbations\nWe deﬁnedRmax\nadv as the error rate against an adversary that may choose a different perturbation type\nfor each input. If a model were robust to this adversary, what can we say about the robustness to\na more creative adversary that combines different perturbation types? To answer this question, we\nintroduce a new adversary that mixes different attacks by linearly interpolating between perturbations.\nFor a perturbation setS andβ∈ [0, 1], we denoteβ·S the set of perturbations scaled down byβ.\nFor anℓp-ball with radius ϵ, this is the ball with radius β·ϵ. For rotation-translations, the attack\nbudgetN is scaled toβ·N. For two setsS1,S 2, we deﬁneSafﬁne(S1,S 2) as the set of perturbations\nthat compound a perturbation r1∈β·S1 with a perturbation r2∈ (1−β)·S2, for anyβ∈ [0, 1].\nConsider one adversary that chooses, for each input,ℓp orℓq-noise from ballsSp andSq, forp,q > 0.\nThe afﬁne adversary picks perturbations from the setSafﬁne deﬁned as above. We show:\nClaim 3. For a linear classiﬁerf (x) = sign(wT x +b), we haveRmax\nadv (f ;Sp,S q) =Radv(f ;Safﬁne).\nThus, for linear classiﬁers, robustness to a union of ℓp-perturbations implies robustness to afﬁne\nadversaries (this holds for any distribution). The proof, in Appendix H extends to models that are\nlocally linear within ballsSp andSq around the data points. For the distributionD of Section 2.2, we\ncan further show that there are settings (distinct from the one in Theorem 1) where: (1) robustness\nagainst a union ofℓ∞ andℓ1-perturbations is possible; (2) this requires the model to be non-linear;\n(3) yet, robustness to afﬁne adversaries is impossible (see Appendix I for details). Our experiments\nin Section 4 show that neural networks trained on CIFAR10 have a behavior that is consistent with\nlocally-linear models, in that they are as robust to afﬁne adversaries as against a union ofℓp-attacks.\nIn contrast, compoundingℓ∞ and spatial perturbations yields a stronger attack, even for linear models:\nTheorem 4. Letf (x) = sign(wT x +b) be a linear classiﬁer forD (withx0∼N (y,α−2)). Let\nS∞ be someℓ∞-ball andSRT be rotation-translations with budgetN >2. Deﬁne Safﬁne as above.\nAssumew0 >w i > 0,∀i∈ [1,d ]. ThenRadv(f ;Safﬁne)>Rmax\nadv (f ;S∞,S RT).\nThis result (the proof is in Appendix J) draws a distinction between the strength of afﬁne combinations\nofℓp-noise, and combinations of ℓ∞ and spatial perturbations. It also shows that robustness to a\nunion of perturbations can be insufﬁcient against a more creative afﬁne adversary. These results are\nconsistent with behavior we observe in models trained on real data (see Section 4).\n3 New Attacks and Adversarial Training Schemes\nWe complement our theoretical results with empirical evaluations of the robustness trade-off on\nMNIST and CIFAR10. To this end, we ﬁrst introduce new adversarial training schemes tailored to\nthe multi-perturbation risks deﬁned in Equation (1), as well as a novel attack for theℓ1-norm.\nMulti-perturbation adversarial training. Let\nˆRadv(f ;S) =\nm∑\ni=1\nmax\nr∈S\nL(f (x(i) + r),y (i)),\nbet the empirical adversarial risk, where L is the training loss and D is the training set. For a\nsingle perturbation type, ˆRadv can be minimized with adversarial training [25]: the maximal loss is\napproximated by an attack procedureA(x), such that maxr∈SL(f (x + r),y )≈L(f (A(x)),y ).\nFori∈ [1,d ], letAi be an attack for the perturbation setSi. The two multi-attack robustness metrics\nintroduced in Equation (1) immediately yield the following natural adversarial training strategies:\n1. “Max” strategy: For each input x, we train on the strongest adversarial example from all attacks,\ni.e., the max in ˆRadv is replaced byL(f (Ak∗ (x)),y ), fork∗ = arg maxkL(f (Ak(x)),y ).\n2. “Avg” strategy: This strategy simultaneously trains on adversarial examples from all attacks.\nThat is, the max in ˆRadv is replaced by 1\nn\n∑n\ni=1L(f (Ai(x),y )).\nThe sparseℓ1-descent attack (SLIDE). Adversarial training is contingent on astrong and efﬁcient\nattack. Training on weak attacks gives no robustness [40], while strong optimization attacks (e.g., [6,\n5\nInput: Input x ∈ [0, 1]d, stepsk, step-sizeγ, percentileq,ℓ1-boundϵ\nOutput: ˆx = x + r s.t. ∥r∥1 ≤ϵ\nr ← 0d\nfor 1 ≤i ≤k do\ng ← ∇ rL(θ, x + r,y )\nei = sign(gi) if |gi| ≥ Pq(|g|), else 0\nr ← r +γ · e/∥e∥1\nr ←ΠSϵ\n1 (r)\nend\nAlgorithm 1: The Sparseℓ1 Descent Attack (SLIDE).Pq(|g|) denotes theqth percentile of|g| and\nΠSϵ\n1 is the projection onto theℓ1-ball (see [10]).\n8]) are prohibitively expensive. Projected Gradient Descent (PGD) [22, 25] is a popular choice of\nattack that is both efﬁcient and produces strong perturbations. To complement our formal results,\nwe want to train models on ℓ1-perturbations. Yet, we show that the ℓ1-version of PGD is highly\ninefﬁcient, and propose a better approach suitable for adversarial training.\nPGD is a steepest descent algorithm [24]. In each iteration, the perturbation is updated in the steepest\ndescent direction arg max∥v∥≤1 vT g, where g is the gradient of the loss. For the ℓ∞-norm, the\nsteepest descent direction is sign(g) [15], and for ℓ2, it is g/∥g∥2. For the ℓ1-norm, the steepest\ndescent direction is the unit vector e withei∗ = sign(gi∗ ), fori∗ = arg maxi|gi|.\nThis yields an inefﬁcient attack, as each iteration updates a single index of the perturbationr. We\nthus design a new attack with ﬁner control over the sparsity of an update step. For q∈ [0, 1], let\nPq(|g|) be the qth percentile of|g|. We set ei = sign(gi) if|gi|≥ Pq(|g|) and 0 otherwise, and\nnormalize e to unitℓ1-norm. Forq≫ 1/d, we thus update many indices of r at once. We introduce\nanother optimization to handle clipping, by ignoring gradient components where the update step\ncannot make progress (i.e., where xi +ri∈{ 0, 1} andgi points outside the domain). To project\nr onto an ℓ1-ball, we use an algorithm of Duchi et al. [ 10]. Algorithm 1 describes our attack. It\noutperforms the steepest descent attack as well as a recently proposed Frank-Wolfe algorithm for\nℓ1-attacks [20] (see Appendix B). Our attack is competitive with the more expensive EAD attack [8]\n(see Appendix C).\n4 Experiments\nWe use our new adversarial training schemes to measure the robustness trade-off on MNIST and\nCIFAR10.1 MNIST is an interesting case-study as distinct models achieve strong robustness to\ndifferentℓp and spatial attacks[ 31, 11]. Despite the dataset’s simplicity, we show that no single\nmodel achieves strongℓ∞,ℓ 1 andℓ2 robustness, and that new techniques are required to close this\ngap. The code used for all of our experiments can be found here: https://github.com/ftramer/\nMultiRobustness\nTraining and evaluation setup. We ﬁrst use adversarial training to train models on a single\nperturbation type. For MNIST, we useℓ1(ϵ = 10),ℓ2(ϵ = 2) andℓ∞(ϵ = 0.3). For CIFAR10 we use\nℓ∞(ϵ = 4\n255 ) andℓ1(ϵ = 2000\n255 ). We also train on rotation-translation attacks with±3px translations\nand±30◦ rotations as in [11]. We denote these models Adv1, Adv2, Adv∞, and AdvRT. We then use\nthe “max” and “avg” strategies from Section 3 to train models Adv max and Advavg against multiple\nperturbations. We train once on all ℓp-perturbations, and once on both ℓ∞ and RT perturbations.\nWe use the same CNN (for MNIST) and wide ResNet model (for CIFAR10) as Madry et al. [ 25].\nAppendix A has more details on the training setup, and attack and training hyper-parameters.\nWe evaluate robustness of all models using multiple attacks: (1) we use gradient-based attacks\nfor all ℓp-norms, i.e., PGD [ 25] and our SLIDE attack with 100 steps and 40 restarts (20 restarts\non CIFAR10), as well as Carlini and Wagner’sℓ2-attack [6] (C&W), and anℓ1-variant—EAD [ 8];\n1Kang et al. [20] recently studied the transfer betweenℓ∞,ℓ 1 andℓ2-attacks for adversarially trained models\non ImageNet. They show that models trained on one type of perturbation are not robust to others, but they do not\nattempt to train models against multiple attacks simultaneously.\n6\nTable 1: Evaluation of MNIST models trained onℓ∞,ℓ 1 andℓ2 attacks (left) orℓ∞ and rotation-\ntranslation (RT) attacks (right). Models Adv∞, Adv1, Adv2 and AdvRT are trained on a single\nattack, while Advavg and Advmax are trained on multiple attacks using the “avg” and “max” strategies.\nThe columns show a model’s accuracy on individual perturbation types, on the union of them\n(1−R max\nadv ), and the average accuracy across them (1−R avg\nadv). The best results are in bold (at 95%\nconﬁdence). Results in red indicate gradient-masking, see Appendix C for a breakdown of all attacks.\nModel Acc. ℓ∞ ℓ1 ℓ2 1 − Rmax\nadv 1 − Ravg\nadv\nNat 99.4 0.0 12.4 8.5 0.0 7.0\nAdv∞ 99.1 91.1 12.1 11.3 6.8 38.2\nAdv1 98.9 0.0 78.5 50.6 0.0 43.0\nAdv2 98.5 0.4 68.0 71.8 0.4 46.7\nAdvavg 97.3 76.7 53.9 58.3 49.9 63.0\nAdvmax 97.2 71.7 62.6 56.0 52.4 63.4\nModel Acc. ℓ∞ RT 1 − Rmax\nadv 1 − Ravg\nadv\nNat 99.4 0.0 0.0 0.0 0.0\nAdv∞ 99.1 91.4 0.2 0.2 45.8\nAdvRT 99.3 0.0 94.6 0.0 47.3\nAdvavg 99.2 88.2 86.4 82.9 87.3\nAdvmax 98.9 89.6 85.6 83.8 87.6\n(2) to detect gradient-masking, we use decision-based attacks: the Boundary Attack [3] forℓ2, the\nPointwise Attack [31] forℓ1, and the Boundary Attack++ [7] forℓ∞; (3) for spatial attacks, we use\nthe optimal attack of [11] that enumerates all small rotations and translations. For unbounded attacks\n(C&W, EAD and decision-based attacks), we discard perturbations outside theℓp-ball.\nFor each model, we report accuracy on 1000 test points for: (1) individual perturbation types; (2) the\nunion of these types, i.e., 1−R max\nadv ; and (3) the average of all perturbation types,1−R avg\nadv. We brieﬂy\ndiscuss the optimal error that can be achieved if there is no robustness trade-off. For perturbation sets\nS1,...S n, letR1,..., Rn be the optimal risks achieved by distinct models. Then, a single model can\nat best achieve riskRi for eachSi, i.e., OPT(Ravg\nadv) = 1\nn\n∑n\ni=1Ri. If the errors are fully correlated,\nso that a maximal number of inputs admit no attack, we have OPT(Rmax\nadv ) = max{R1,..., Rn}.\nOur experiments show that these optimal error rates are not achieved.\nResults on MNIST. Results are in Table 1. The left table is for the union of ℓp-attacks, and the\nright table is for the union of ℓ∞ and RT attacks. In both cases, the multi-perturbation training\nstrategies “succeed”, in that models Adv avg and Advmax achieve higher multi-perturbation accuracy\nthan any of the models trained against a single perturbation type.\nThe results forℓ∞ and RT attacks are promising, although the best model Advmax only achieves 1−\nRmax\nadv = 83.8% and 1−R avg\nadv = 87.6%, which is far less than the optimal values, 1− OPT(Rmax\nadv ) =\nmin{91.4%, 94.6%} = 91.4% and 1− OPT(Ravg\nadv) = (91 .4% + 94.6%)/2 = 93% . Thus, these\nmodels do exhibit some form of the robustness trade-off analyzed in Section 2.\nTheℓp results are surprisingly mediocre and re-raise questions about whether MNIST can be consid-\nered “solved” from a robustness perspective. Indeed, while training separate models to resistℓ1,ℓ 2\norℓ∞ attacks works well, resisting all attacks simultaneously fails. This agrees with the results of\nSchott et al. [31], whose models achieve either highℓ∞ orℓ2 robustness, but not both simultaneously.\nWe show that in our case, this lack of robustness is partly due to gradient masking.\nFirst-order adversarial training and gradient masking on MNIST. The model Adv∞ is not\nrobust toℓ1 andℓ2-attacks. This is unsurprising as the model was only trained onℓ∞-attacks. Yet,\ncomparing the model’s accuracy against multiple types ofℓ1 andℓ2 attacks (see Appendix C) reveals\na more curious phenomenon: Adv∞ has high accuracy against ﬁrst-orderℓ1 andℓ2-attacks such as\nPGD, but is broken by decision-free attacks. This is an indication of gradient-masking [27, 40, 1].\nThis issue had been observed before [31, 23], but an explanation remained illusive, especially since\nℓ∞-PGD does not appear to suffer from gradient masking (see [25]). We explain this phenomenon by\ninspecting the learned features of model Adv∞, as in [25]. We ﬁnd that the model’s ﬁrst layer learns\nthreshold ﬁltersz = ReLU(α· (x−ϵ)) forα> 0. As most pixels in MNIST are zero, most of the\nzi cannot be activated by anϵ-boundedℓ∞-attack. Theℓ∞-PGD thus optimizes a smooth (albeit ﬂat)\nloss function. In contrast,ℓ1- andℓ2-attacks can move a pixelxi = 0 to ˆxi >ϵ thus activatingzi, but\nhave no gradients to rely on (i.e, dzi/dxi = 0 for anyxi≤ϵ). Figure 3 in Appendix D shows that\nthe model’s loss resembles a step-function, for which ﬁrst-order attacks such as PGD are inadequate.\nNote that training against ﬁrst-orderℓ1 orℓ2-attacks directly (i.e., models Adv1 and Adv2 in Table 1),\nseems to yield genuine robustness to these perturbations. This is surprising in that, because of gradient\n7\nTable 2: Evaluation of CIFAR10 models trained against ℓ∞ and ℓ1 attacks (left) or ℓ∞ and\nrotation-translation (RT) attacks (right). Models Adv∞, Adv1 and AdvRT are trained against a\nsingle attack, while Advavg and Advmax are trained against two attacks using the “avg” and “max”\nstrategies. The columns show a model’s accuracy on individual perturbation types, on the union of\nthem (1−R max\nadv ), and the average accuracy across them (1−R avg\nadv). The best results are in bold (at\n95% conﬁdence). A breakdown of allℓ1 attacks is in Appendix C.\nModel Acc. ℓ∞ ℓ1 1 − Rmax\nadv 1 − Ravg\nadv\nNat 95.7 0.0 0.0 0.0 0.0\nAdv∞ 92.0 71.0 16.4 16.4 44.9\nAdv1 90.8 53.4 66.2 53.1 60.0\nAdvavg 91.1 64.1 60.8 59.4 62.5\nAdvmax 91.2 65.7 62.5 61.1 64.1\nModel Acc. ℓ∞ RT 1 − Rmax\nadv 1 − Ravg\nadv\nNat 95.7 0.0 5.9 0.0 3.0\nAdv∞ 92.0 71.0 8.9 8.7 40.0\nAdvRT 94.9 0.0 82.5 0.0 41.3\nAdvavg 93.6 67.8 78.2 65.2 73.0\nAdvmax 93.1 69.6 75.2 65.7 72.4\nTable 3: Evaluation of afﬁne attacks. For models trained with the “max” strategy, we evaluate\nagainst attacks from a unionSU of perturbation sets, and against an afﬁne adversary that interpolates\nbetween perturbations. Examples of afﬁne attacks are in Figure 4.\nDataset Attacks acc. on SU acc. onSafﬁne\nMNIST ℓ∞ & RT 83.8 62.6\nCIFAR10 ℓ∞ & RT 65.7 56.0\nCIFAR10 ℓ∞ &ℓ1 61.1 58.0\nmasking, model Adv∞ actually achieves lower training loss against ﬁrst-orderℓ1 andℓ2-attacks than\nmodels Adv1 and Adv2. That is, Adv 1 and Adv2 converged to sub-optimal local minima of their\nrespective training objectives, yet these minima generalize much better to stronger attacks.\nThe models Adv avg and Adv max that are trained against ℓ∞,ℓ 1 and ℓ2-attacks also learn to use\nthresholding to resistℓ∞-attacks while spuriously masking gradient for ℓ1 andℓ2-attacks. This is\nevidence that, unlike previously thought [41], training against a strong ﬁrst-order attack (such as PGD)\ncan cause the model to minimize its training loss via gradient masking. To circumvent this issue,\nalternatives to ﬁrst-order adversarial training seem necessary. Potential (costly) approaches include\ntraining on gradient-free attacks, or extending certiﬁed defenses [28, 42] to multiple perturbations.\nCertiﬁed defenses provide provable bounds that are much weaker than the robustness attained by\nadversarial training, and certifying multiple perturbation types is likely to exacerbate this gap.\nResults on CIFAR10. The left table in Table 2 considers the union of ℓ∞ andℓ1 perturbations,\nwhile the right table considers the union of ℓ∞ and RT perturbations. As on MNIST, the models\nAdvavg and Advmax achieve better multi-perturbation robustness than any of the models trained on a\nsingle perturbation, but fail to match the optimal error rates we could hope for. Forℓ1 andℓ∞-attacks,\nwe achieve 1−R max\nadv = 61.1% and 1−R avg\nadv = 64.1%, again signiﬁcantly below the optimal values,\n1− OPT(Rmax\nadv ) = min{71.0%, 66.2%} = 66.2% and 1− OPT(Ravg\nadv) = (71.0% + 66.2%)/2 =\n68.6%. The results forℓ∞ and RT attacks are qualitatively and quantitatively similar. 2\nInterestingly, models Advavg and Advmax achieve 100% training accuracy. Thus, multi-perturbation\nrobustness increases the adversarial generalization gap [30]. These models might be resorting to\nmore memorization because they fail to ﬁnd features robust to both attacks.\nAfﬁne Adversaries. Finally, we evaluate the afﬁne attacks introduced in Section 2.5. These attacks\ntake afﬁne combinations of two perturbation types, and we apply them on the models Advmax (we\nomit theℓp-case on MNIST due to gradient masking). To compound ℓ∞ andℓ1-noise, we devise\nan attack that updates both perturbations in alternation. To compoundℓ∞ and RT attacks, we pick\nrandom rotation-translations (with±3βpx translations and±30β◦ rotations), apply an ℓ∞-attack\nwith budget (1−β)ϵ to each, and retain the worst example.\n2An interesting open question is why the model Advavg trained onℓ∞ and RT attacks does not attain optimal\naverage robustness Ravg\nadv. Indeed, on CIFAR10, detecting the RT attack of [11] is easy, due to the black in-painted\npixels in a transformed image. The following “ensemble” model thus achieves optimal Ravg\nadv (but not necessarily\noptimal Rmax\nadv ): on input ˆx, return AdvRT( ˆx) if there are black in-painted pixels, otherwise return Adv ∞( ˆx).\nThe fact that model Advavg did not learn such a function might hint at some limitation of adversarial training.\n8\nThe results in Table 3 match the predictions of our formal analysis: (1) afﬁne combinations of ℓp\nperturbations are no stronger than their union. This is expected given Claim 3 and prior observations\nthat neural networks are close to linear near the data [15, 29]; (2) combining ofℓ∞ and RT attacks\ndoes yield a stronger attack, as shown in Theorem 4. This demonstrates that robustness to a union of\nperturbations can still be insufﬁcient to protect against more complex combinations of perturbations.\n5 Discussion and Open Problems\nDespite recent success in defending ML models against some perturbation types [25, 11, 31], extend-\ning these defenses to multiple perturbations unveils a clear robustness trade-off. This tension may be\nrooted in its unconditional occurrence in natural and simple distributions, as we proved in Section 2.\nOur new adversarial training strategies fail to achieve competitive robustness to more than one attack\ntype, but narrow the gap towards multi-perturbation robustness. We note that the optimal risksRmax\nadv\nandRavg\nadv that we achieve are very close. Thus, for most data points, the models are either robust to all\nperturbation types or none of them. This hints that some points (sometimes referred to as prototypical\nexamples [4, 36]) are inherently easier to classify robustly, regardless of the perturbation type.\nWe showed that ﬁrst-order adversarial training for multipleℓp-attacks suffers from gradient masking\non MNIST. Achieving better robustness on this simple dataset is an open problem. Another challenge\nis reducing the cost of our adversarial training strategies, which scale linearly in the number of pertur-\nbation types. Breaking this linear dependency requires efﬁcient techniques for ﬁnding perturbations\nin a union of sets, which might be hard for sets with near-empty intersection (e.g.,ℓ∞ andℓ1-balls).\nThe cost of adversarial training has also be reduced by merging the inner loop of a PGD attack and\ngradient updates of the model parameters [34, 44], but it is unclear how to extend this approach to a\nunion of perturbations (some of which are not optimized using PGD, e.g., rotation-translations).\nHendrycks and Dietterich [17], and Geirhos et al. [13] recently measured robustness of classiﬁers\nto multiple common (i.e., non-adversarial) image corruptions (e.g., random image blurring). In that\nsetting, they also ﬁnd that different classiﬁers achieve better robustness to some corruptions, and\nthat no single classiﬁer achieves the highest accuracy under all forms. The interplay between multi-\nperturbation robustness in the adversarial and common corruption case is worth further exploration.\nReferences\n[1] A. Athalye, N. Carlini, and D. Wagner. Obfuscated gradients give a false sense of security: Circumventing\ndefenses to adversarial examples. In International Conference on Machine Learning (ICML), 2018.\n[2] A. C. Berry. The accuracy of the gaussian approximation to the sum of independent variates. Transactions\nof the american mathematical society, 49(1):122–136, 1941.\n[3] W. Brendel, J. Rauber, and M. Bethge. Decision-based adversarial attacks: Reliable attacks against\nblack-box machine learning models. In International Conference on Learning Representations, 2018.\n[4] N. Carlini, U. Erlingsson, and N. Papernot. Prototypical examples in deep learning: Metrics, characteristics,\nand utility. 2018.\n[5] N. Carlini, P. Mishra, T. Vaidya, Y . Zhang, M. Sherr, C. Shields, D. Wagner, and W. Zhou. Hidden voice\ncommands. In USENIX Security Symposium, pages 513–530, 2016.\n[6] N. Carlini and D. Wagner. Towards evaluating the robustness of neural networks. In IEEE Symposium on\nSecurity and Privacy, 2017.\n[7] J. Chen and M. I. Jordan. Boundary attack++: Query-efﬁcient decision-based adversarial attack. arXiv\npreprint arXiv:1904.02144, 2019.\n[8] P.-Y . Chen, Y . Sharma, H. Zhang, J. Yi, and C.-J. Hsieh. Ead: elastic-net attacks to deep neural networks\nvia adversarial examples. In AAAI Conference on Artiﬁcial Intelligence, 2018.\n[9] A. Demontis, P. Russu, B. Biggio, G. Fumera, and F. Roli. On security and sparsity of linear classiﬁers\nfor adversarial settings. In Joint IAPR International Workshops on Statistical Techniques in Pattern\nRecognition (SPR) and Structural and Syntactic Pattern Recognition (SSPR), pages 322–332. Springer,\n2016.\n9\n[10] J. Duchi, S. Shalev-Shwartz, Y . Singer, and T. Chandra. Efﬁcient projections onto the l1-ball for learning\nin high dimensions. In International Conference on Machine Learning (ICML), 2008.\n[11] L. Engstrom, B. Tran, D. Tsipras, L. Schmidt, and A. Madry. A rotation and a translation sufﬁce: Fooling\nCNNs with simple transformations. arXiv preprint arXiv:1712.02779, 2017.\n[12] A. Fawzi, H. Fawzi, and O. Fawzi. Adversarial vulnerability for any classiﬁer. In Advances in Neural\nInformation Processing Systems, pages 1186–1195, 2018.\n[13] R. Geirhos, P. Rubisch, C. Michaelis, M. Bethge, F. A. Wichmann, and W. Brendel. ImageNet-trained\nCNNs are biased towards texture; increasing shape bias improves accuracy and robustness. InInternational\nConference on Learning Representations (ICLR), 2019.\n[14] J. Gilmer, L. Metz, F. Faghri, S. S. Schoenholz, M. Raghu, M. Wattenberg, and I. Goodfellow. Adversarial\nspheres. arXiv preprint arXiv:1801.02774, 2018.\n[15] I. J. Goodfellow, J. Shlens, and C. Szegedy. Explaining and harnessing adversarial examples. In Interna-\ntional Conference on Learning Representations (ICLR), 2015.\n[16] K. Grosse, N. Papernot, P. Manoharan, M. Backes, and P. McDaniel. Adversarial examples for malware\ndetection. In European Symposium on Research in Computer Security, 2017.\n[17] D. Hendrycks and T. Dietterich. Benchmarking neural network robustness to common corruptions and\nperturbations. In International Conference on Learning Representations (ICLR), 2019.\n[18] A. Ilyas, S. Santurkar, D. Tsipras, L. Engstrom, B. Tran, and A. Madry. Adversarial examples are not bugs,\nthey are features. arXiv preprint arXiv:1905.02175, 2019.\n[19] J. Jo and Y . Bengio. Measuring the tendency of CNNs to learn surface statistical regularities.arXiv preprint\narXiv:1711.11561, 2017.\n[20] D. Kang, Y . Sun, T. Brown, D. Hendrycks, and J. Steinhardt. Transfer of adversarial robustness between\nperturbation types. arXiv preprint arXiv:1905.01034, 2019.\n[21] M. Khoury and D. Hadﬁeld-Menell. On the geometry of adversarial examples, 2019.\n[22] A. Kurakin, I. Goodfellow, and S. Bengio. Adversarial machine learning at scale. In International\nConference on Learning Representations (ICLR), 2017.\n[23] B. Li, C. Chen, W. Wang, and L. Carin. Second-order adversarial attack and certiﬁable robustness.arXiv\npreprint arXiv:1809.03113, 2018.\n[24] A. Madry and Z. Kolter. Adversarial robustness: Theory and practice. In Tutorial at NeurIPS 2018, 2018.\n[25] A. Madry, A. Makelov, L. Schmidt, D. Tsipras, and A. Vladu. Towards deep learning models resistant to\nadversarial attacks. In International Conference on Learning Representations (ICLR), 2018.\n[26] S. Mahloujifar, D. I. Diochnos, and M. Mahmoody. The curse of concentration in robust learning: Evasion\nand poisoning attacks from concentration of measure. arXiv preprint arXiv:1809.03063, 2018.\n[27] N. Papernot, P. McDaniel, I. Goodfellow, S. Jha, Z. B. Celik, and A. Swami. Practical black-box attacks\nagainst machine learning. In ASIACCS, pages 506–519. ACM, 2017.\n[28] A. Raghunathan, J. Steinhardt, and P. Liang. Certiﬁed defenses against adversarial examples. InInterna-\ntional Conference on Learning Representations (ICLR), 2018.\n[29] M. T. Ribeiro, S. Singh, and C. Guestrin. Why should i trust you?: Explaining the predictions of any\nclassiﬁer. InKDD. ACM, 2016.\n[30] L. Schmidt, S. Santurkar, D. Tsipras, K. Talwar, and A. Madry. Adversarially robust generalization requires\nmore data. In Advances in Neural Information Processing Systems, pages 5019–5031, 2018.\n[31] L. Schott, J. Rauber, M. Bethge, and W. Brendel. Towards the ﬁrst adversarially robust neural network\nmodel on mnist. In International Conference on Learning Representations (ICLR), 2019.\n[32] L. Schott, J. Rauber, M. Bethge, and W. Brendel. Towards the ﬁrst adversarially robust neural network\nmodel on mnist (OpenReview comment on spatial transformations), 2019.\n[33] A. Shafahi, W. R. Huang, C. Studer, S. Feizi, and T. Goldstein. Are adversarial examples inevitable? In\nInternational Conference on Learning Representations (ICLR), 2019.\n10\n[34] A. Shafahi, M. Najibi, A. Ghiasi, Z. Xu, J. Dickerson, C. Studer, L. S. Davis, G. Taylor, and T. Goldstein.\nAdversarial training for free! arXiv preprint arXiv:1904.12843, 2019.\n[35] Y . Sharma and P.-Y . Chen. Attacking the madry defense model with l1-based adversarial examples.arXiv\npreprint arXiv:1710.10733, 2017.\n[36] P. Stock and M. Cisse. Convnets and imagenet beyond accuracy: Understanding mistakes and uncovering\nbiases. In Proceedings of the European Conference on Computer Vision (ECCV), pages 498–512, 2018.\n[37] C. Szegedy, W. Zaremba, I. Sutskever, J. Bruna, D. Erhan, I. Goodfellow, and R. Fergus. Intriguing\nproperties of neural networks. In International Conference on Learning Representations (ICLR), 2014.\n[38] F. Tramèr and D. Boneh. Adversarial training and robustness for multiple perturbations. In Neural\nInformation Processing Systems (NeurIPS) 2019, 2019. arXiv preprint arXiv:1904.13000.\n[39] F. Tramèr, P. Dupré, G. Rusak, G. Pellegrino, and D. Boneh. Ad-versarial: Perceptual ad-blocking meets\nadversarial machine learning. arXiv preprint arXiv:1811:03194, Nov 2018.\n[40] F. Tramèr, A. Kurakin, N. Papernot, I. Goodfellow, D. Boneh, and P. McDaniel. Ensemble adversarial\ntraining: Attacks and defenses. In International Conference on Learning Representations (ICLR), 2018.\n[41] D. Tsipras, S. Santurkar, L. Engstrom, A. Turner, and A. Madry. Robustness may be at odds with accuracy.\nIn International Conference on Learning Representations (ICLR), 2019.\n[42] E. Wong and Z. Kolter. Provable defenses against adversarial examples via the convex outer adversarial\npolytope. In International Conference on Machine Learning, pages 5283–5292, 2018.\n[43] H. Xu, C. Caramanis, and S. Mannor. Robustness and regularization of support vector machines. Journal\nof Machine Learning Research, 10(Jul):1485–1510, 2009.\n[44] D. Zhang, T. Zhang, Y . Lu, Z. Zhu, and B. Dong. You only propagate once: Painless adversarial training\nusing maximal principle. arXiv preprint arXiv:1905.00877, 2019.\n11",
  "values": {
    "Transparent (to users)": "No",
    "Non-maleficence": "No",
    "Critiqability": "No",
    "Interpretable (to users)": "No",
    "Respect for Law and public interest": "No",
    "Deferral to humans": "No",
    "Autonomy (power to decide)": "No",
    "Respect for Persons": "No",
    "Explicability": "No",
    "Not socially biased": "No",
    "User influence": "No",
    "Beneficence": "No",
    "Collective influence": "No",
    "Privacy": "No",
    "Justice": "No",
    "Fairness": "No"
  }
}