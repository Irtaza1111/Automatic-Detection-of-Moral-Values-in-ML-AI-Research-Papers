{
  "pdf": "NeurIPS-2019-adversarial-training-for-free-Paper",
  "title": "Adversarial training for free!",
  "author": "Ali Shafahi, Mahyar Najibi, Mohammad Amin Ghiasi, Zheng Xu, John Dickerson, Christoph Studer, Larry S. Davis, Gavin Taylor, Tom Goldstein",
  "paper_id": "NeurIPS-2019-adversarial-training-for-free-Paper",
  "text": "Adversarial Training for Free!\nAli Shafahi\nUniversity of Maryland\nashafahi@cs.umd.edu\nMahyar Najibi\nUniversity of Maryland\nnajibi@cs.umd.edu\nAmin Ghiasi\nUniversity of Maryland\namin@cs.umd.edu\nZheng Xu\nUniversity of Maryland\nxuzh@cs.umd.edu\nJohn Dickerson\nUniversity of Maryland\njohn@cs.umd.edu\nChristoph Studer\nCornell University\nstuder@cornell.edu\nLarry S. Davis\nUniversity of Maryland\nlsd@umiacs.umd.edu\nGavin Taylor\nUnited States Naval Academy\ntaylor@usna.edu\nTom Goldstein\nUniversity of Maryland\ntomg@cs.umd.edu\nAbstract\nAdversarial training, in which a network is trained on adversarial examples, is one\nof the few defenses against adversarial attacks that withstands strong attacks. Un-\nfortunately, the high cost of generating strong adversarial examples makes standard\nadversarial training impractical on large-scale problems like ImageNet. We present\nan algorithm that eliminates the overhead cost of generating adversarial examples\nby recycling the gradient information computed when updating model parameters.\nOur “free” adversarial training algorithm achieves comparable robustness to PGD\nadversarial training on the CIFAR-10 and CIFAR-100 datasets at negligible addi-\ntional cost compared to natural training, and can be 7 to 30 times faster than other\nstrong adversarial training methods. Using a single workstation with 4 P100 GPUs\nand 2 days of runtime, we can train a robust model for the large-scale ImageNet\nclassiﬁcation task that maintains 40% accuracy against PGD attacks.\n1 Introduction\nDeep learning has been widely applied to various computer vision tasks with excellent performance.\nPrior to the realization of the adversarial example phenomenon by Biggio et al. [2013], Szegedy et al.\n[2013], model performance on clean examples was the the main evaluation criteria. However, in\nsecurity-critical applications, robustness to adversarial attacks has emerged as a critical factor.\nA robust classiﬁer is one that correctly labels adversarially perturbed images. Alternatively, robustness\nmay be achieved by detecting and rejecting adversarial examples [Ma et al., 2018, Meng and Chen,\n2017, Xu et al., 2017]. Recently, Athalye et al. [2018] broke a complete suite of allegedly robust\ndefenses, leaving adversarial training, in which the defender augments each minibatch of training\ndata with adversarial examples [Madry et al., 2017], among the few that remain resistant to attacks.\nAdversarial training is time-consuming—in addition to the gradient computation needed to update\nthe network parameters, each stochastic gradient descent (SGD) iteration requires multiple gradient\ncomputations to produce adversarial images. In fact, it takes 3-30 times longer to form a robust\nnetwork with adversarial training than forming a non-robust equivalent. Put simply, the actual\nslowdown factor depends on the number of gradient steps used for adversarial example generation.\nThe high cost of adversarial training has motivated a number of alternatives. Some recent works\nreplace the perturbation generation in adversarial training with a parameterized generator network\n33rd Conference on Neural Information Processing Systems (NeurIPS 2019), Vancouver, Canada.\n[Baluja and Fischer, 2018, Poursaeed et al., 2018, Xiao et al., 2018]. This approach is slower than\nstandard training, and problematic on complex datasets, such as ImageNet, for which it is hard to\nproduce highly expressive GANs that cover the entire image space. Another popular defense strategy\nis to regularize the training loss using label smoothing, logit squeezing, or a Jacobian regularization\n[Shafahi et al., 2019a, Mosbach et al., 2018, Ross and Doshi-Velez, 2018, Hein and Andriushchenko,\n2017, Jakubovitz and Giryes, 2018, Yu et al., 2018]. These methods have not been applied to\nlarge-scale problems, such as ImageNet, and can be applied in parallel to adversarial training.\nRecently, there has been a surge of certiﬁed defenses [Wong and Kolter, 2017, Wong et al., 2018,\nRaghunathan et al., 2018a,b, Wang et al., 2018]. These methods were mostly demonstrated for\nsmall networks, low-res datasets, and relatively small perturbation budgets (ϵ). Lecuyer et al. [2018]\npropose randomized smoothing as a certiﬁed defense and which was later improved by Li et al.\n[2018a]. Cohen et al. [2019] prove a tight robustness guarantee under the ℓ2 norm for smoothing\nwith Gaussian noise. Their study was the ﬁrst certiﬁable defense for the ImageNet dataset [Deng\net al., 2009]. They claim to achieve 12% robustness against non-targeted attacks that are within an\nℓ2 radius of 3 (for images with pixels in [0, 1]). This is roughly equivalent to anℓ∞ radius ofϵ = 2\nwhen pixels lie in [0, 255] .\nAdversarial training remains among the most trusted defenses, but it is nearly intractable on large-\nscale problems. Adversarial training on high-resolution datasets, including ImageNet, has only been\nwithin reach for research labs having hundreds of GPUs1. Even on reasonably-sized datasets, such as\nCIFAR-10 and CIFAR-100, adversarial training is time consuming and can take multiple days.\nContributions\nWe propose a fast adversarial training algorithm that produces robust models with almost no extra\ncost relative to natural training. The key idea is to update both the model parameters and image\nperturbations using one simultaneous backward pass, rather than using separate gradient computations\nfor each update step. Our proposed method has the same computational cost as conventional natural\ntraining, and can be 3-30 times faster than previous adversarial training methods [Madry et al.,\n2017, Xie et al., 2019]. Our robust models trained on CIFAR-10 and CIFAR-100 achieve accuracies\ncomparable and even slightly exceeding models trained with conventional adversarial training when\ndefending against strong PGD attacks.\nWe can apply our algorithm to the large-scale ImageNet classiﬁcation task on a single workstation\nwith four P100 GPUs in about two days, achieving 40% accuracy against non-targeted PGD attacks.\nTo the best of our knowledge, our method is the ﬁrst to successfully train a robust model for ImageNet\nbased on the non-targeted formulation and achieves results competitive with previous (signiﬁcantly\nmore complex) methods [Kannan et al., 2018, Xie et al., 2019].\n2 Non-targeted adversarial examples\nAdversarial examples come in two ﬂavors: non-targeted and targeted. Given a ﬁxed classiﬁer with\nparametersθ, an imagex with true labely, and classiﬁcation proxy lossl, a bounded non-targeted\nattack sneaks an example out of its natural class and into another. This is done by solving\nmax\nδ\nl(x +δ,y,θ ), subject to ||δ||p ≤ϵ, (1)\nwhereδ is the adversarial perturbation, ||.||p is someℓp-norm distance metric, andϵ is the adversarial\nmanipulation budget. In contrast to non-targeted attacks, a targeted attack scooches an image into a\nspeciﬁc class of the attacker’s choice.\nIn what follows, we will use non-targeted adversarial examples both for evaluating the robustness of\nour models and also for adversarial training. We brieﬂy review some of the closely related methods\nfor generating adversarial examples. In the context ofℓ∞-bounded attacks, the Fast Gradient Sign\nMethod (FGSM) by Goodfellow et al. [2015] is one of the most popular non-targeted methods that\nuses the sign of the gradients to construct an adversarial example in one iteration:\nxadv =x +ϵ ·sign(∇xl(x,y,θ )). (2)\n1Xie et al. [2019] use 128 V100s and Kannan et al. [2018] use 53 P100s for targeted adv training ImageNet.\n2\nThe Basic Iterative Method (BIM) by Kurakin et al. [2016a] is an iterative version of FGSM. The\nPGD attack is a variant of BIM with uniform random noise as initialization, which is recognized by\nAthalye et al. [2018] to be one of the most powerful ﬁrst-order attacks. The initial random noise was\nﬁrst studied by Tramèr et al. [2017] to enable FGSM to attack models that rely on “gradient masking.”\nIn the PGD attack algorithm, the number of iterations K plays an important role in the strength\nof attacks, and also the computation time for generating adversarial examples. In each iteration, a\ncomplete forward and backward pass is needed to compute the gradient of the loss with respect to the\nimage. Throughout this paper we will refer to aK-step PGD attack as PGD-K.\n3 Adversarial training\nAdversarial training can be traced back to [Goodfellow et al., 2015], in which models were hardened\nby producing adversarial examples and injecting them into training data. The robustness achieved\nby adversarial training depends on the strength of the adversarial examples used. Training on fast\nnon-iterative attacks such as FGSM and Rand+FGSM only results in robustness against non-iterative\nattacks, and not against PGD attacks [Kurakin et al., 2016b, Madry et al., 2017]. Consequently, Madry\net al. [2017] propose training on multi-step PGD adversaries, achieving state-of-the-art robustness\nlevels againstℓ∞ attacks on MNIST and CIFAR-10 datasets.\nWhile many defenses were broken by Athalye et al. [2018], PGD-based adversarial training was\namong the few that withstood strong attacks. Many other defenses build on PGD adversarial training\nor leverage PGD adversarial generation during training. Examples include Adversarial Logit Pairing\n(ALP) [Kannan et al., 2018], Feature Denoising [Xie et al., 2019], Defensive Quantization [Lin et al.,\n2019], Thermometer Encoding [Buckman et al., 2018], PixelDefend [Song et al., 2017], Robust\nManifold Defense [Ilyas et al., 2017], L2-nonexpansive nets [Qian and Wegman, 2018], Jacobian\nRegularization [Jakubovitz and Giryes, 2018], Universal Perturbation [Shafahi et al., 2018], and\nStochastic Activation Pruning [Dhillon et al., 2018].\nWe focus on the min-max formulation of adversarial training [Madry et al., 2017], which has been\ntheoretically and empirically justiﬁed. This widely usedK-PGD adversarial training algorithm has\nan inner loop that constructs adversarial examples by PGD-K, while the outer loop updates the model\nusing minibatch SGD on the generated examples. In the inner loop, the gradient ∇xl(xadv,y,θ ) for\nupdating adversarial examples requires a forward-backward pass of the entire network, which has\nsimilar computation cost as calculating the gradient ∇θl(xadv,y,θ ) for updating network parameters.\nCompared to natural training, which only requires ∇θl(x,y,θ ) and does not have an inner loop,\nK-PGD adversarial training needs roughlyK + 1 times more computation.\n4 “Free” adversarial training\nK-PGD adversarial training [Madry et al., 2017] is generally slow. For example, the 7-PGD training\nof a WideResNet [Zagoruyko and Komodakis, 2016] on CIFAR-10 in Madry et al. [2017] takes about\nfour days on a Titan X GPU. To scale the algorithm to ImageNet, Xie et al. [2019] and Kannan et al.\n[2018] had to deploy large GPU clusters at a scale far beyond the reach of most organizations.\nHere, we propose free adversarial training, which has a negligible complexity overhead compared to\nnatural training. Our free adversarial training algorithm (alg. 1) computes the ascent step by re-using\nthe backward pass needed for the descent step. To update the network parameters, the current training\nminibatch is passed forward through the network. Then, the gradient with respect to the network\nparameters is computed on the backward pass. When the “free” method is used, the gradient of the\nloss with respect to the input image is also computed on this same backward pass.\nUnfortunately, this approach does not allow for multiple adversarial updates to be made to the same\nimage without performing multiple backward passes. To overcome this restriction, we propose a\nminor yet nontrivial modiﬁcation to training: train on the same minibatchm times in a row. Note\nthat we divide the number of epochs bym such that the overall number of training iterations remains\nconstant. This strategy provides multiple adversarial updates to each training image, thus providing\nstrong/iterative adversarial examples. Finally, when a new minibatch is formed, the perturbation\ngenerated on the previous minibatch is used to warm-start the perturbation for the new minibatch.\n3\nAlgorithm 1 “Free” Adversarial Training (Free-m)\nRequire: Training samplesX, perturbation boundϵ, learning rateτ, hop stepsm\n1: Initializeθ\n2: δ ← 0\n3: for epoch = 1...N ep/m do\n4: for minibatchB ⊂X do\n5: for i = 1...m do\n6: Updateθ with stochastic gradient descent\n7: gθ ← E(x,y)∈B[∇θl(x +δ,y,θ )]\n8: gadv ← ∇xl(x +δ,y,θ )]\n9: θ ←θ −τgθ\n10: Use gradients calculated for the minimization step to updateδ\n11: δ ←δ +ϵ · sign(gadv)\n12: δ ← clip(δ, −ϵ,ϵ )\n13: end for\n14: end for\n15: end for\n(a) CIFAR-10 sensitivity to m\n (b) CIFAR-100 sensitivity to m\nFigure 1: Natural validation accuracy of Wide Resnet 32-10 models using varied mini-batch replay\nparametersm. Herem = 1 corresponds to natural training. For largem’s, validation accuracy drops\ndrastically. However, smallm’s have little effect. For reference, CIFAR-10 and CIFAR-100 models\nthat are 7-PGD adversarially trained have natural accuracies of 87.25% and 59.87%, respectively.\nThe effect of mini-batch replay on natural training\nWhile the hope for alg. 1 is to build robust models, we still want models to perform well on natural\nexamples. As we increasem in alg. 1, there is risk of increasing generalization error. Furthermore,\nit may be possible that catastrophic forgetting happens. Consider the worst case where all the\n“informative” images of one class are in the ﬁrst few mini-batches. In this extreme case, we do not see\nuseful examples for most of the epoch, and forgetting may occur. Consequently, a natural question is:\nhow much does mini-batch replay hurt generalization?\nTo answer this question, we naturally train wide-resnet 32-10 models on CIFAR-10 and CIFAR-100\nusing different levels of replay. Fig. 1 plots clean validation accuracy as a function of the replay\nparameterm. We see some dropoff in accuracy for small values ofm. Note that a small compromise\nin accuracy is acceptable given a large increase in robustness due to the fundamental tradeoffs\nbetween robustness and generalization [Tsipras et al., 2018, Zhang et al., 2019a, Shafahi et al.,\n2019b]. As a reference, CIFAR-10 and CIFAR-100 models that are 7-PGD adversarially trained\nhave natural accuracies of 87.25% and 59.87%, respectively. These same accuracies are exceeded by\nnatural training withm = 16. We see in section 5 that good robustness can be achieved using “free”\nadversarial training with justm ≤ 10.\n4\nTable 1: Validation accuracy and robustness of CIFAR-10 models trained with various methods.\nTraining Evaluated Against Train\nTime\n(min)Nat. Images PGD-20 PGD-100 CW-100 10 restart\nPGD-20\nNatural 95.01% 0.00% 0.00% 0.00% 0.00% 780\nFreem = 2 91.45% 33.92% 33.20% 34.57% 33.41% 816\nFreem = 4 87.83% 41.15% 40.35% 41.96% 40.73% 800\nFreem = 8 85.96% 46.82% 46.19% 46.60% 46.33% 785\nFreem = 10 83.94% 46.31% 45.79% 45.86% 45.94% 785\n7-PGD trained 87.25% 45.84% 45.29% 46.52% 45.53% 5418\nTable 2: Validation accuracy and robustness of CIFAR-100 models trained with various methods.\nTraining Evaluated Against Training Time\n(minutes)Natural Images PGD-20 PGD-100\nNatural 78.84% 0.00% 0.00% 811\nFreem = 2 69.20% 15.37% 14.86% 816\nFreem = 4 65.28% 20.64% 20.15% 767\nFreem = 6 64.87% 23.68% 23.18% 791\nFreem = 8 62.13% 25.88% 25.58% 780\nFreem = 10 59.27% 25.15% 24.88% 776\nMadry et al. (2-PGD trained) 67.94% 17.08% 16.50% 2053\nMadry et al. (7-PGD trained) 59.87% 22.76% 22.52% 5157\n5 Robust models on CIFAR-10 and 100\nIn this section, we train robust models on CIFAR-10 and CIFAR-100 using our “free” adversarial\ntraining ( alg. 1) and compare them to K-PGD adversarial training 23. We ﬁnd that free training is\nable to achieve state-of-the-art robustness on the CIFARs without the overhead of standard PGD\ntraining.\nCIFAR-10\nWe train various CIFAR-10 models using the Wide-Resnet 32-10 model and standard hyper-\nparameters used by Madry et al. [2017]. In the proposed method (alg. 1), we repeat ( i.e. replay) each\nminibatchm times before switching to the next minibatch. We present the experimental results for\nvarious choices ofm in table 1. Training each of these models costs roughly the same as natural\ntraining since we preserve the same number of iterations. We compare with the 7-PGD adversarially\ntrained model from Madry et al. [2017] 4, whose training requires roughly 7× more time than all of\nour free training variations. We attack all models using PGD attacks withK iterations on both the\ncross-entropy loss (PGD-K) and the Carlini-Wagner loss (CW-K) [Carlini and Wagner, 2017]. We\ntest using the PGD-20 attack following Madry et al. [2017], and also increase the number of attack\niterations and employ random restarts to verify robustness under stronger attacks. To measure the\nsensitivity of our method to initialization, we perform ﬁve trials for the Free-m = 8 case and ﬁnd that\nour results are insensitive. The natural accuracy is 85.95±0.14 and robustness against a 20-random\nrestart PGD-20 attack is 46.49 ±0.19. Note that gradient free-attacks such as SPSA will result in\ninferior results for adversarially trained models in comparison to optimization based attacks such\nas PGD as noted by Uesato et al. [2018]. Gradient-free attacks are superior in settings where the\ndefense works by masking or obfuscating the gradients.\n2Adversarial Training for Free code for CIFAR-10 in Tensorﬂow can be found here: https://github.\ncom/ashafahi/free_adv_train/\n3ImageNet Adversarial Training for Free code in Pytorch can be found here: https://github.com/\nmahyarnajibi/FreeAdversarialTraining\n4Results based on the “adv_trained” model in Madry’s CIFAR-10 challenge repo.\n5\nplane\n cat\n dog\n cat\n ship\n cat\n dog\n car\n horse\ncar\n dog\n cat\n deer\n cat\n frog\n bird\n frog\n dog\ncar\n dog\n cat\n deer\n cat\n horse\n cat\n plane\n car\nFigure 2: Attack images built for adversarially trained models look like the class into which they get\nmisclassiﬁed. We display the last 9 CIFAR-10 clean validation images (top row) and their adversarial\nexamples built for a 7-PGD adversarially trained (middle) and our “free” trained (bottom) models.\nOur “free training” algorithm successfully reaches robustness levels comparable to a 7-PGD ad-\nversarially trained model. As we increase m, the robustness is increased at the cost of validation\naccuracy on natural images. Additionally note that we achieve reasonable robustness over a wide\nrange of choices of the main hyper-parameter of our model, 10 ≥m> 2, and the proposed method\nis signiﬁcantly faster than7-PGD adversarial training. Recently, a new method called YOPO [Zhang\net al., 2019b] has been proposed for speeding up adversarial training, in their CIFAR-10 results they\nuse a wider networks (WRN-34-10) with larger batch-sizes (256). As shown in our supplementary,\nboth of these factors increase robustness. To do a direct comparison, we a train WRN-34-10 using\nm = 10 and batch-size=256. We match their best reported result (48.03% against PGD-20 attacks\nfor “Free” training v.s. 47.98% for YOPO 5-3).\nCIFAR-100\nWe also study the robustness results of “free training” on CIFAR-100 which is a more difﬁcult dataset\nwith more classes. As we will see in sec. 4, training with large m values on this dataset hurts the\nnatural validation accuracy more in comparison to CIFAR-10. This dataset is less studied in the\nadversarial machine learning community and therefore for comparison purposes, we adversarially\ntrain our own Wide ResNet 32-10 models for CIFAR-100. We train two robust models by varyingK\nin theK-PGD adversarial training algorithm. One is trained on PGD-2 with a computational cost\nalmost 3× that of free training, and the other is trained on PGD-7 with a computation time roughly\n7× that of free training. We adopt the code for adversarial training from Madry et al. [2017], which\nproduces state-of-the-art robust models on CIFAR-10. We summarize the results in table. 2.\nWe see that “free training” exceeds the accuracy on both natural images and adversarial images when\ncompared to traditional adversarial training. Similar to the effect of increasingm, increasingK in\nK-PGD adversarial training results in increased robustness at the cost of clean validation accuracy.\nHowever, unlike the proposed “free training” where increasing m has no extra cost, increasingK for\nstandardK-PGD substantially increases training time.\n6 Does “free” training behave like standard adversarial training?\nHere, we analyze two properties that are associated with PGD adversarially trained models: The\ninterpretability of their gradients and the ﬂattness of their loss surface. We ﬁnd that “free” training\nenjoys these beneﬁts as well.\nGenerative behavior for largely perturbed examples\nTsipras et al. [2018] observed that hardened classiﬁers have interpretable gradients; adversarial\nexamples built for PGD trained models often look like the class into which they get misclassiﬁed.\nFig. 2 plots “weakly bounded” adversarial examples for the CIFAR-10 7-PGD adversarially trained\nmodel [Madry et al., 2017] and our freem = 8 trained model. Both models were trained to resist ℓ∞\nattacks withϵ = 8. The examples are made using a 50 iteration BIM attack withϵ = 30 andϵs = 2.\n6\n(a) Free m = 8\n (b) 7-PGD adv trained\n(c) Free m = 8 both rad\n (d) 7-PGD adv trained both rad\nFigure 3: The loss surface of a 7-PGD adversarially trained model and our “free” trained model for\nCIFAR-10 on the ﬁrst 2 validation images. In (a) and (b) we display the cross-entropy loss projected\non one random (Rademacher) and one adversarial direction. In (c) and (d) we display the the cross\nentropy loss projected along two random directions. Both training methods behave similarly and do\nnot operate by masking the gradients as the adversarial direction is indeed the direction where the\ncross-entropy loss changes the most.\n“Free training” maintains generative properties, as our model’s adversarial examples resemble the\ntarget class.\nSmooth and ﬂattened loss surface\nAnother property of PGD adversarial training is that it ﬂattens and smoothens the loss landscape.\nIn contrast, some defenses work by “masking” the gradients, i.e., making it difﬁcult to identify\nadversarial examples using gradient methods, even though adversarial examples remain present.\nReference Engstrom et al. [2018] argues that gradient masking adds little security. We show in ﬁg. 3a\nthat free training does not operate by masking gradients using a rough loss surface. In ﬁg. 3 we\nplot the cross-entropy loss projected along two directions in image space for the ﬁrst few validation\nexamples of CIFAR-10 [Li et al., 2018b]. In addition to the loss of the freem = 8 model, we plot the\nloss of the 7-PGD adversarially trained model for comparison.\n7 Robust ImageNet classiﬁers\nImageNet is a large image classiﬁcation dataset of over 1 million high-res images and 1000 classes\n(Russakovsky et al. [2015]). Due to the high computational cost of ImageNet training, only a\nfew research teams have been able to afford building robust models for this problem. Kurakin\net al. [2016b] ﬁrst hardened ImageNet classiﬁers by adversarial training with non-iterative attacks.5\nAdversarial training was done using a targeted FGSM attack. They found that while their model\nbecame robust against targeted non-iterative attacks, the targeted BIM attack completely broke it.\nLater, Kannan et al. [2018] attempted to train a robust model that withstands targeted PGD attacks.\nThey trained against 10 step PGD targeted attacks (a process that costs 11 times more than natural\ntraining) to build a benchmark model. They also generated PGD targeted attacks to train their\nadversarial logit paired (ALP) ImageNet model. Their baseline achieves a top-1 accuracy of 3.1%\nagainst PGD-20 targeted attacks with ϵ = 16 . Very recently, Xie et al. [2019] trained a robust\nImageNet model against targeted PGD-30 attacks, with a cost 31× that of natural training. Training\nthis model required a distributed implementation on 128 GPUs with batch size 4096. Their robust\nResNet-101 model achieves a top-1 accuracy of 35.8% on targeted PGD attacks with many iterations.\n5Training using a non-iterative attack such as FGSM only doubles the training cost.\n7\nTable 3: ImageNet validation accuracy and robustness of ResNet-50 models trained with various\nreplay parameters andϵ = 2.\nTraining Evaluated Against\nNatural Images PGD-10 PGD-50 PGD-100\nNatural 76.038% 0.166% 0.052% 0.036%\nFreem = 2 71.210% 37.012% 36.340% 36.250%\nFreem = 4 64.446% 43.522% 43.392% 43.404%\nFreem = 6 60.642% 41.996% 41.900% 41.892%\nFreem = 8 58.116% 40.044% 40.008% 39.996%\n(a) Clean\n (b) PGD-100\nFigure 4: The effect of the perturbation boundϵ and the mini-batch replay hyper-parameterm on the\nrobustness achieved by free training.\nFree training results\nOur alg. 1 is designed for non-targeted adversarial training. As Athalye et al. [2018] state, defending\non this task is important and more challenging than defending against targeted attacks, and for this\nreason smallerϵ values are typically used. Even for ϵ = 2 (the smallestϵ we consider defensing\nagainst), a PGD-50 non-targeted attack on a natural model achieves roughly 0.05% top-1 accuracy.\nTo put things further in perspective, Uesato et al. [2018] broke three defenses forϵ = 2 non-targeted\nattacks on ImageNet [Guo et al., 2017, Liao et al., 2018, Xie et al., 2017], degrading their performance\nbelow 1%. Our free training algorithm is able to achieve 43% robustness against PGD attacks bounded\nbyϵ = 2. Furthermore, we ran each experiment on a single workstation with four P100 GPUs. Even\nwith this modest setup, training time for each ResNet-50 experiment is below 50 hours.\nWe summarize our results for variousϵ’s andm’s in table 3 and ﬁg. 4. To craft attacks, we used a\nstep-size of 1 and the corresponding ϵ used during training. In all experiments, the training batch\nsize was 256. Table 3 shows the robustness of Resnet-50 on ImageNet withϵ = 2. The validation\naccuracy for natural images decreases when we increase the minibatch replaym, just like it did for\nCIFAR in section 5.\nThe naturally trained model is vulnerable to PGD attacks (ﬁrst row of table 3), while free training\nproduces robust models that achieve over 40% accuracy vs PGD attacks ( m = 4, 6, 8 in table 3).\nAttacking the models using PGD-100 does not result in a meaningful drop in accuracy compared to\nPGD-50. Therefore, we did not experiment with increasing the number of PGD iterations further.\nFig. 4 summarizes experimental results for robust models trained and tested under different per-\nturbation bounds ϵ. Each curve represents one training method (natural training or free training)\nwith hyperparameter choicem. Each point on the curve represents the validation accuracy for an\nϵ-bounded robust model. These results are also provided as tables in the appendix. The proposed\nmethod consistently improves the robust accuracy under PGD attacks for ϵ = 2 − 7, and m = 4\nperforms the best. It is difﬁcult to train robust models when ϵ is large, which is consistent with\nprevious studies showing that PGD-based adversarial training has limited robustness for ImageNet\n[Kannan et al., 2018].\n8\nTable 4: Validation accuracy and robustness of “free” and 2-PGD trained ResNet-50 models – both\ntrained to resistℓ∞ϵ = 4 attacks. Note that 2-PGD training time is 3.46× that of “free” training.\nModel & Training Evaluated Against Train time\n(minutes)Natural Images PGD-10 PGD-50 PGD-100\nRN50 – Freem = 4 60.206% 32.768% 31.878% 31.816% 3016\nRN50 – 2-PGD trained 64.134% 37.172% 36.352% 36.316% 10,435\nTable 5: Validation accuracy and robustness of free-m = 4 trained ResNets with various capacities.\nArchitecture Evaluated Against\nNatural Images PGD-10 PGD-50 PGD-100\nResNet-50 60.206% 32.768% 31.878% 31.816%\nResNet-101 63.340% 35.388% 34.402% 34.328%\nResNet-152 64.446% 36.992% 36.044% 35.994%\nComparison with PGD-trained models\nWe compare “free” training to a more costly method using 2-PGD adversarial examples ϵ = 4 .\nWe run the conventional adversarial training algorithm and set ϵs = 2 , ϵ = 4 , and K = 2 . All\nother hyper-parameters were identical to those used for training our “free” models. Note that in our\nexperiments, we do not use any label-smoothing or other common tricks for improving robustness\nsince we want to do a fair comparison between PGD training and our “free” training. These extra\nregularizations can likely improve results for both approaches.\nWe compare our “free trained” m = 4 ResNet-50 model and the 2-PGD trained ResNet-50 model in\ntable 4. 2-PGD adversarial training takes roughly 3.4× longer than “free training” and only achieves\nslightly better results (≈4.5%). This gap is less than 0.5% if we free train a higher capacity model\n(i.e. ResNet-152, see below).\nFree training on models with more capacity\nIt is believed that increased network capacity leads to greater robustness from adversarial training\n[Madry et al., 2017, Kurakin et al., 2016b]. We verify that this is the case by “free training” ResNet-\n101 and ResNet-152 withϵ = 4. The comparison between ResNet-152, ResNet-101, and ResNet-50\nis summarized in table 5. Free training on ResNet-101 and ResNet-152 each take roughly 1.7×\nand 2.4× more time than ResNet-50 on the same machine, respectively. The higher capacity model\nenjoys a roughly 4% boost to accuracy and robustness.\n8 Conclusions\nAdversarial training is a well-studied method that boosts the robustness and interpretability of neural\nnetworks. While it remains one of the few effective ways to harden a network to attacks, few can\nafford to adopt it because of its high computation cost. We present a “free” version of adversarial\ntraining with cost nearly equal to natural training. Free training can be further combined with\nother defenses to produce robust models without a slowdown. We hope that this approach can put\nadversarial training within reach for organizations with modest compute resources.\nAcknowledgements: Goldstein and his students were supported by DARPA GARD, DARPA QED\nfor RML, DARPA L2M, and the YFA program. Additional support was provided by the AFOSR\nMURI program. Davis and his students were supported by the Ofﬁce of the Director of National\nIntelligence (ODNI), and IARPA (2014-14071600012). Studer was supported by Xilinx, Inc. and\nthe US NSF under grants ECCS-1408006, CCF-1535897, CCF-1652065, CNS-1717559, and ECCS-\n1824379. Taylor was supported by the Ofﬁce of Naval Research (N0001418WX01582) and the\nDepartment of Defense High Performance Computing Modernization Program. The views and\nconclusions contained herein are those of the authors and should not be interpreted as necessarily\nrepresenting the ofﬁcial policies or endorsements, either expressed or implied, of the ODNI, IARPA,\nor the U.S. Government. The U.S. Government is authorized to reproduce and distribute reprints for\nGovernmental purposes notwithstanding any copyright annotation thereon.\n9\nReferences\nBattista Biggio, Igino Corona, Davide Maiorca, Blaine Nelson, Nedim Šrndi´c, Pavel Laskov, Giorgio\nGiacinto, and Fabio Roli. Evasion attacks against machine learning at test time. In ECML-PKDD,\npages 387–402. Springer, 2013.\nChristian Szegedy, Wojciech Zaremba, Ilya Sutskever, Joan Bruna, Dumitru Erhan, Ian Goodfellow,\nand Rob Fergus. Intriguing properties of neural networks. ICLR, 2013.\nXingjun Ma, Bo Li, Yisen Wang, Sarah M Erfani, Sudanthi Wijewickrema, Grant Schoenebeck,\nDawn Song, Michael E Houle, and James Bailey. Characterizing adversarial subspaces using local\nintrinsic dimensionality. arXiv preprint arXiv:1801.02613, 2018.\nDongyu Meng and Hao Chen. Magnet: a two-pronged defense against adversarial examples. In\nProceedings of the 2017 ACM SIGSAC Conference on Computer and Communications Security ,\npages 135–147. ACM, 2017.\nWeilin Xu, David Evans, and Yanjun Qi. Feature squeezing: Detecting adversarial examples in deep\nneural networks. arXiv preprint arXiv:1704.01155, 2017.\nAnish Athalye, Nicholas Carlini, and David Wagner. Obfuscated gradients give a false sense of\nsecurity: Circumventing defenses to adversarial examples. ICML, 2018.\nAleksander Madry, Aleksandar Makelov, Ludwig Schmidt, Dimitris Tsipras, and Adrian Vladu.\nTowards deep learning models resistant to adversarial attacks. ICLR, 2017.\nShumeet Baluja and Ian Fischer. Adversarial transformation networks: Learning to generate adver-\nsarial examples. AAAI, 2018.\nOmid Poursaeed, Isay Katsman, Bicheng Gao, and Serge Belongie. Generative adversarial perturba-\ntions. CVPR, 2018.\nChaowei Xiao, Bo Li, Jun-Yan Zhu, Warren He, Mingyan Liu, and Dawn Song. Generating adversarial\nexamples with adversarial networks. IJCAI, 2018.\nAli Shafahi, Amin Ghiasi, Furong Huang, and Tom Goldstein. Label smoothing and logit squeezing:\nA replacement for adversarial training?, 2019a.\nMarius Mosbach, Maksym Andriushchenko, Thomas Trost, Matthias Hein, and Dietrich Klakow.\nLogit pairing methods can fool gradient-based attacks. arXiv preprint arXiv:1810.12042, 2018.\nAndrew Slavin Ross and Finale Doshi-Velez. Improving the adversarial robustness and interpretability\nof deep neural networks by regularizing their input gradients. In AAAI, 2018.\nMatthias Hein and Maksym Andriushchenko. Formal guarantees on the robustness of a classiﬁer\nagainst adversarial manipulation. In NeurIPS, pages 2266–2276, 2017.\nDaniel Jakubovitz and Raja Giryes. Improving dnn robustness to adversarial attacks using jacobian\nregularization. In ECCV, pages 514–529, 2018.\nFuxun Yu, Chenchen Liu, Yanzhi Wang, and Xiang Chen. Interpreting adversarial robustness: A\nview from decision surface in input space. arXiv preprint arXiv:1810.00144, 2018.\nEric Wong and J Zico Kolter. Provable defenses against adversarial examples via the convex outer\nadversarial polytope. ICML, 2017.\nEric Wong, Frank Schmidt, Jan Hendrik Metzen, and J Zico Kolter. Scaling provable adversarial\ndefenses. In NeurIPS, pages 8400–8409, 2018.\nAditi Raghunathan, Jacob Steinhardt, and Percy S Liang. Semideﬁnite relaxations for certifying\nrobustness to adversarial examples. In NeurIPS, pages 10877–10887, 2018a.\nAditi Raghunathan, Jacob Steinhardt, and Percy Liang. Certiﬁed defenses against adversarial\nexamples. arXiv preprint arXiv:1801.09344, 2018b.\n10\nShiqi Wang, Yizheng Chen, Ahmed Abdou, and Suman Jana. Mixtrain: Scalable training of formally\nrobust neural networks. arXiv preprint arXiv:1811.02625, 2018.\nMathias Lecuyer, Vaggelis Atlidakis, Roxana Geambasu, Daniel Hsu, and Suman Jana. Certiﬁed\nrobustness to adversarial examples with differential privacy. arXiv preprint arXiv:1802.03471,\n2018.\nBai Li, Changyou Chen, Wenlin Wang, and Lawrence Carin. Second-order adversarial attack and\ncertiﬁable robustness. CoRR, abs/1809.03113, 2018a. URL http://arxiv.org/abs/1809.\n03113.\nJeremy M Cohen, Elan Rosenfeld, and J Zico Kolter. Certiﬁed adversarial robustness via randomized\nsmoothing. arXiv preprint arXiv:1902.02918, 2019.\nJ. Deng, W. Dong, R. Socher, L.-J. Li, K. Li, and L. Fei-Fei. ImageNet: A Large-Scale Hierarchical\nImage Database. In CVPR09, 2009.\nCihang Xie, Yuxin Wu, Laurens van der Maaten, Alan Yuille, and Kaiming He. Feature denoising for\nimproving adversarial robustness. CVPR, 2019.\nHarini Kannan, Alexey Kurakin, and Ian Goodfellow. Adversarial logit pairing. arXiv preprint\narXiv:1803.06373, 2018.\nIan J Goodfellow, Jonathon Shlens, and Christian Szegedy. Explaining and harnessing adversarial\nexamples. ICLR, 2015.\nAlexey Kurakin, Ian Goodfellow, and Samy Bengio. Adversarial examples in the physical world.\narXiv preprint arXiv:1607.02533, 2016a.\nFlorian Tramèr, Alexey Kurakin, Nicolas Papernot, Ian Goodfellow, Dan Boneh, and Patrick Mc-\nDaniel. Ensemble adversarial training: Attacks and defenses. arXiv preprint arXiv:1705.07204,\n2017.\nAlexey Kurakin, Ian Goodfellow, and Samy Bengio. Adversarial machine learning at scale. ICLR,\n2016b.\nJi Lin, Chuang Gan, and Song Han. Defensive quantization: When efﬁciency meets robustness.\nICLR, 2019.\nJacob Buckman, Aurko Roy, Colin Raffel, and Ian Goodfellow. Thermometer encoding: One hot\nway to resist adversarial examples. ICLR, 2018.\nYang Song, Taesup Kim, Sebastian Nowozin, Stefano Ermon, and Nate Kushman. Pixeldefend:\nLeveraging generative models to understand and defend against adversarial examples. arXiv\npreprint arXiv:1710.10766, 2017.\nAndrew Ilyas, Ajil Jalal, Eirini Asteri, Constantinos Daskalakis, and Alexandros G Dimakis.\nThe robust manifold defense: Adversarial training using generative models. arXiv preprint\narXiv:1712.09196, 2017.\nHaifeng Qian and Mark N Wegman. L2-nonexpansive neural networks. arXiv preprint\narXiv:1802.07896, 2018.\nAli Shafahi, Mahyar Najibi, Zheng Xu, John Dickerson, Larry S Davis, and Tom Goldstein. Universal\nadversarial training. arXiv preprint arXiv:1811.11304, 2018.\nGuneet S Dhillon, Kamyar Azizzadenesheli, Zachary C Lipton, Jeremy Bernstein, Jean Kossaiﬁ, Aran\nKhanna, and Anima Anandkumar. Stochastic activation pruning for robust adversarial defense.\narXiv preprint arXiv:1803.01442, 2018.\nSergey Zagoruyko and Nikos Komodakis. Wide residual networks. arXiv preprint arXiv:1605.07146,\n2016.\nDimitris Tsipras, Shibani Santurkar, Logan Engstrom, Alexander Turner, and Aleksander Madry.\nRobustness may be at odds with accuracy. ICLR, 1050:11, 2018.\n11\nHongyang Zhang, Yaodong Yu, Jiantao Jiao, Eric P Xing, Laurent El Ghaoui, and Michael I Jordan.\nTheoretically principled trade-off between robustness and accuracy. ICML, 2019a.\nAli Shafahi, W Ronny Huang, Christoph Studer, Soheil Feizi, and Tom Goldstein. Are adversarial\nexamples inevitable? ICLR, 2019b.\nNicholas Carlini and David Wagner. Towards evaluating the robustness of neural networks. In 2017\nIEEE Symposium on Security and Privacy (SP) , pages 39–57. IEEE, 2017.\nJonathan Uesato, Brendan O’Donoghue, Aaron van den Oord, and Pushmeet Kohli. Adversarial risk\nand the dangers of evaluating against weak attacks. arXiv preprint arXiv:1802.05666, 2018.\nDinghuai Zhang, Tianyuan Zhang, Yiping Lu, Zhanxing Zhu, and Bin Dong. You only propagate\nonce: Painless adversarial training using maximal principle. arXiv preprint arXiv:1905.00877,\n2019b.\nLogan Engstrom, Andrew Ilyas, and Anish Athalye. Evaluating and understanding the robustness of\nadversarial logit pairing. arXiv preprint arXiv:1807.10272, 2018.\nHao Li, Zheng Xu, Gavin Taylor, Christoph Studer, and Tom Goldstein. Visualizing the loss landscape\nof neural nets. In NeurIPS, pages 6389–6399, 2018b.\nOlga Russakovsky, Jia Deng, Hao Su, Jonathan Krause, Sanjeev Satheesh, Sean Ma, Zhiheng Huang,\nAndrej Karpathy, Aditya Khosla, Michael Bernstein, et al. Imagenet large scale visual recognition\nchallenge. IJCV, 115(3):211–252, 2015.\nChuan Guo, Mayank Rana, Moustapha Cisse, and Laurens van der Maaten. Countering adversarial\nimages using input transformations. arXiv preprint arXiv:1711.00117, 2017.\nFangzhou Liao, Ming Liang, Yinpeng Dong, Tianyu Pang, Xiaolin Hu, and Jun Zhu. Defense against\nadversarial attacks using high-level representation guided denoiser. In Proceedings of the IEEE\nConference on Computer Vision and Pattern Recognition, pages 1778–1787, 2018.\nCihang Xie, Jianyu Wang, Zhishuai Zhang, Zhou Ren, and Alan Yuille. Mitigating adversarial effects\nthrough randomization. arXiv preprint arXiv:1711.01991, 2017.\n12",
  "values": {
    "Critiqability": "No",
    "Privacy": "No",
    "Collective influence": "No",
    "Fairness": "No",
    "Justice": "No",
    "Explicability": "No",
    "Deferral to humans": "No",
    "User influence": "No",
    "Non-maleficence": "No",
    "Respect for Law and public interest": "No",
    "Beneficence": "No",
    "Respect for Persons": "No",
    "Interpretable (to users)": "No",
    "Transparent (to users)": "No",
    "Autonomy (power to decide)": "No",
    "Not socially biased": "No"
  }
}