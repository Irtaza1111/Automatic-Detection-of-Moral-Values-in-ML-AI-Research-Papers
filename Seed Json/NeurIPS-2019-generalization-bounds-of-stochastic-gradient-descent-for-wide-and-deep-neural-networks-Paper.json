{
  "pdf": "NeurIPS-2019-generalization-bounds-of-stochastic-gradient-descent-for-wide-and-deep-neural-networks-Paper",
  "title": "NeurIPS-2019-generalization-bounds-of-stochastic-gradient-descent-for-wide-and-deep-neural-networks-Paper",
  "author": "Unknown",
  "paper_id": "NeurIPS-2019-generalization-bounds-of-stochastic-gradient-descent-for-wide-and-deep-neural-networks-Paper",
  "text": "Generalization Bounds of Stochastic Gradient\nDescent for Wide and Deep Neural Networks\nYuan Cao\nDepartment of Computer Science\nUniversity of California, Los Angeles\nCA 90095, USA\nyuancao@cs.ucla.edu\nQuanquan Gu\nDepartment of Computer Science\nUniversity of California, Los Angeles\nCA 90095, USA\nqgu@cs.ucla.edu\nAbstract\nWe study the training and generalization of deep neural networks (DNNs) in the\nover-parameterized regime, where the network width (i.e., number of hidden nodes\nper layer) is much larger than the number of training data points. We show that, the\nexpected 0-1 loss of a wide enough ReLU network trained with stochastic gradient\ndescent (SGD) and random initialization can be bounded by the training loss of\na random feature model induced by the network gradient at initialization, which\nwe call a neural tangent random feature (NTRF) model. For data distributions\nthat can be classiﬁed by NTRF model with sufﬁciently small error, our result\nyields a generalization error bound in the order of rOpn´1{2q that is independent\nof the network width. Our result is more general and sharper than many existing\ngeneralization error bounds for over-parameterized neural networks. In addition,\nwe establish a strong connection between our generalization error bound and the\nneural tangent kernel (NTK) proposed in recent work.\n1 Introduction\nDeep learning has achieved great success in a wide range of applications including image processing\n[20], natural language processing [ 17] and reinforcement learning [ 34]. Most of the deep neural\nnetworks used in practice are highly over-parameterized, such that the number of parameters is much\nlarger than the number of training data. One of the mysteries in deep learning is that, even in an\nover-parameterized regime, neural networks trained with stochastic gradient descent can still give\nsmall test error and do not overﬁt. In fact, a famous empirical study by Zhang et al. [38] shows the\nfollowing phenomena:\n• Even if one replaces the real labels of a training data set with purely random labels, an over-\nparameterized neural network can still ﬁt the training data perfectly. However since the labels are\nindependent of the input, the resulting neural network does not generalize to the test dataset.\n• If the same over-parameterized network is trained with real labels, it not only achieves small\ntraining loss, but also generalizes well to the test dataset.\nWhile a series of recent work has theoretically shown that a sufﬁciently over-parameterized (i.e.,\nsufﬁciently wide) neural network can ﬁt random labels [12, 2, 11, 39], the reason why it can generalize\nwell when trained with real labels is less understood. Existing generalization bounds for deep neural\nnetworks [29, 6, 27, 15, 13, 5, 24, 35, 28] based on uniform convergence usually cannot provide\nnon-vacuous bounds [21, 13] in the over-parameterized regime. In fact, the empirical observation by\nZhang et al. [38] indicates that in order to understand deep learning, it is important to distinguish the\ntrue data labels from random labels when studying generalization. In other words, it is essential to\nquantify the “classiﬁability” of the underlying data distribution, i.e., how difﬁcult it can be classiﬁed.\n33rd Conference on Neural Information Processing Systems (NeurIPS 2019), Vancouver, Canada.\nCertain effort has been made to take the “classiﬁability” of the data distribution into account for\ngeneralization analysis of neural networks. Brutzkus et al. [7] showed that stochastic gradient descent\n(SGD) can learn an over-parameterized two-layer neural network with good generalization for linearly\nseparable data. Li and Liang [25] proved that, if the data satisfy certain structural assumption, SGD\ncan learn an over-parameterized two-layer network with ﬁxed second layer weights and achieve a\nsmall generalization error. Allen-Zhu et al. [1] studied the generalization performance of SGD and\nits variants for learning two-layer and three-layer networks, and used the risk of smaller two-layer or\nthree-layer networks with smooth activation functions to characterize the classiﬁability of the data\ndistribution. There is another line of studies on the algorithm-dependent generalization bounds of\nneural networks in the over-parameterized regime [10, 4, 8, 37, 14], which quantiﬁes the classiﬁability\nof the data with a reference function class deﬁned by random features [31, 32] or kernels1. Speciﬁcally,\nDaniely [10] showed that a neural network of large enough size is competitive with the best function\nin the conjugate kernel class of the network. Arora et al.[4] gave a generalization error bound for two-\nlayer ReLU networks with ﬁxed second layer weights based on a ReLU kernel function. Cao and Gu\n[8] showed that deep ReLU networks trained with gradient descent can achieve small generalization\nerror if the data can be separated by certain random feature model [32] with a margin. Yehudai and\nShamir [37] used the expected loss of a similar random feature model to quantify the generalization\nerror of two-layer neural networks with smooth activation functions. A similar generalization error\nbound was also given by E et al. [14], where the authors studied the optimization and generalization\nof two-layer networks trained with gradient descent. However, all the aforementioned results are still\nfar from satisfactory: they are either limited to two-layer networks, or restricted to very simple and\nspecial reference function classes.\nIn this paper, we aim at providing a sharper and generic analysis on the generalization of deep ReLU\nnetworks trained by SGD. In detail, we base our analysis upon the key observations that near random\ninitialization, the neural network function is almost a linear function of its parameters and the loss\nfunction is locally almost convex. This enables us to prove a cumulative loss bound of SGD, which\nfurther leads to a generalization bound by online-to-batch conversion [9]. The main contributions of\nour work are summarized as follows:\n• We give a bound on the expected0-1 error of deep ReLU networks trained by SGD with random\ninitialization. Our result relates the generalization bound of an over-parameterized ReLU network\nwith a random feature model deﬁned by the network gradients, which we call neural tangent\nrandom feature (NTRF) model. It also suggests an algorithm-dependent generalization error bound\nof order rOpn´1{2q, which is independent of network width, if the data can be classiﬁed by the\nNTRF model with small enough error.\n• Our analysis is general enough to cover recent generalization error bounds for neural networks\nwith random feature based reference function classes, and provides better bounds. Our expected\n0-1 error bound directly covers the result by Cao and Gu [8], and gives a tighter sample complexity\nwhen reduced to their setting, i.e., rOp1{ϵ2q versus rOp1{ϵ4q whereϵ is the target generalization\nerror. Compared with recent results by Yehudai and Shamir [37], E et al. [14] who only studied\ntwo-layer networks, our bound not only works for deep networks, but also uses a larger reference\nfunction class when reduced to the two-layer setting, and therefore is sharper.\n• Our result has a direct connection to the neural tangent kernel studied in Jacot et al. [18]. When\ninterpreted in the language of kernel method, our result gives a generalization bound in the form of\nrOpL¨\na\nyJpΘpLqq´1y{nq, where y is the training label vector, and ΘpLq is the neural tangent\nkernel matrix deﬁned on the training input data. This form of generalization bound is similar to,\nbut more general and tighter than the bound given by Arora et al. [4].\nNotation We use lower case, lower case bold face, and upper case bold face letters to denote scalars,\nvectors and matrices respectively. For a vector v“pv1,...,v dqT P Rd and a number 1ďpă8 ,\nlet}v}p “p řd\ni“1|vi|pq1{p. We also deﬁne }v}8 “ maxi|vi|. For a matrix A“p Ai,jqmˆn, we\nuse}A}0 to denote the number of non-zero entries of A, and denote }A}F “ přd\ni,j“1A2\ni,jq1{2\nand}A}p “ max}v}p“1}Av}p forpě 1. For two matrices A, BP Rmˆn, we deﬁnexA, By“\nTrpAJBq. We denote by A ľ B if A´ B is positive semideﬁnite. In addition, we deﬁne the\n1Since random feature models and kernel methods are highly related [31, 32], we group them into the same\ncategory. More details are discussed in Section 3.2.\n2\nasymptotic notationsOp¨q, rOp¨q, Ωp¨qand rΩp¨qas follows. Suppose thatan andbn be two sequences.\nWe writean “Opbnq if lim supnÑ8|an{bn|ă8 , and an “ Ωpbnq if lim infnÑ8|an{bn|ą 0.\nWe use rOp¨qand rΩp¨qto hide the logarithmic factors inOp¨qand Ωp¨q.\n2 Problem Setup\nIn this section we introduce the basic problem setup. Following the same standard setup implemented\nin the line of recent work [2, 11, 39, 8], we consider fully connected neural networks with widthm,\ndepthL and input dimensiond. Such a network is deﬁned by its weight matrices at each layer: for\nLě 2, let W1P Rmˆd, WlP Rmˆm,l“ 2,...,L ´ 1 and WLP R1ˆm be the weight matrices\nof the network. Then the neural network with input xP Rd is deﬁned as\nfWpxq“ ?m¨WLσpWL´1σpWL´2¨¨¨σpW1xq¨¨¨qq, (2.1)\nwhereσp¨qis the entry-wise activation function. In this paper, we only consider the ReLU activation\nfunctionσpzq“ maxt0,zu, which is the most commonly used activation function in applications. It\nis also arguably one of the most difﬁcult activation functions to analyze, due to its non-smoothess. We\nremark that our result can be generalized to many other Lipschitz continuous and smooth activation\nfunctions. For simplicity, we follow Allen-Zhu et al. [2], Du et al. [11] and assume that the widths of\neach hidden layer are the same. Our result can be easily extended to the setting that the widths of\neach layer are not equal but in the same order, as discussed in Zou et al. [39], Cao and Gu [8].\nWhenL“ 1, the neural network reduces to a linear function, which has been well-studied. Therefore,\nfor notational simplicity we focus on the caseLě 2, where the parameter space is deﬁned as\nW :“ Rmˆdˆp RmˆmqL´2ˆ R1ˆm.\nWe also use W“p W1,..., WLqP W to denote the collection of weight matrices for all layers.\nFor W, W1PW, we deﬁne their inner product asxW, W1y :“ řL\nl“1 TrpWJ\nl W1\nlq.\nThe goal of neural network learning is to minimize the expected risk, i.e.,\nmin\nW\nLDpWq :“ Epx,yq„DLpx,yqpWq, (2.2)\nwhereLpx,yqpWq “ℓry¨fWpxqs is the loss deﬁned on any example px,yq, and ℓpzq is the loss\nfunction. Without loss of generality, we consider the cross-entropy loss in this paper, which is deﬁned\nasℓpzq“ logr1` expp´zqs. We would like to emphasize that our results also hold for most convex\nand Lipschitz continuous loss functions such as hinge loss. We now introduce stochastic gradient\ndescent based training algorithm for minimizing the expected risk in (2.2). The detailed algorithm is\ngiven in Algorithm 1.\nAlgorithm 1 SGD for DNNs starting at Gaussian initialization\nInput: Number of iterationsn, step sizeη.\nGenerate each entry of Wp0q\nl independently fromNp0, 2{mq,lPrL´ 1s.\nGenerate each entry of Wp0q\nL independently fromNp0, 1{mq.\nfori“ 1, 2,...,n do\nDrawpxi,y iq fromD.\nUpdate Wpiq“ Wpi´1q´η¨∇WLpxi,yiqpWpiqq.\nend for\nOutput: Randomly choose xW uniformly fromtWp0q,..., Wpn´1qu.\nThe initialization scheme for Wp0q given in Algorithm 1 generates each entry of the weight matrices\nfrom a zero-mean independent Gaussian distribution, whose variance is determined by the rule that\nthe expected length of the output vector in each hidden layer is equal to the length of the input.\nThis initialization method is also known as He initialization [16]. Here the last layer parameter is\ninitialized with variance 1{m instead of 2{m since the last layer is not associated with the ReLU\nactivation function.\n3\n3 Main Results\nIn this section we present the main results of this paper. In Section 3.1 we give an expected 0-1 error\nbound against a neural tangent random feature reference function class. In Section 3.2, we discuss\nthe connection between our result and the neural tangent kernel proposed in Jacot et al. [18].\n3.1 An Expected 0-1 Error Bound\nIn this section we give a bound on the expected0-1 errorL0´1\nD pWq :“ Epx,yq„Dr1ty¨fWpxqă 0us\nobtained by Algorithm 1. Our result is based on the following assumption.\nAssumption 3.1. The data inputs are normalized:}x}2“ 1 for allpx,yqP supppDq.\nAssumption 3.1 is a standard assumption made in almost all previous work on optimization and\ngeneralization of over-parameterized neural networks [ 12, 2, 11, 39, 30, 14]. As is mentioned in\nCao and Gu [8], this assumption can be relaxed toc1ď} x}2ďc2 for allpx,yqP supppDq, where\nc2ąc1ą 0 are absolute constants.\nFor any WPW, we deﬁne itsω-neighborhood as\nBpW,ωq :“t W1PW :}W1\nl´ Wl}F ďω,l PrLsu.\nBelow we introduce the neural tangent random feature function class, which serves as a reference\nfunction class to measure the “classiﬁability” of the data, i.e., how easy it can be classiﬁed.\nDeﬁnition 3.2 (Neural Tangent Random Feature) . Let Wp0q be generated via the initialization\nscheme in Algorithm 1. The neural tangent random feature (NTRF) function class is deﬁned as\nFpWp0q,Rq“\n␣\nfp¨q“fWp0qp¨q`x∇WfWp0qp¨q, Wy : WPBp0,R ¨m´1{2q\n(\n,\nwhereRą 0 measures the size of the function class, andm is the width of the neural network.\nThe name “neural tangent random feature” is inspired by the neural tangent kernel proposed by\nJacot et al. [18], because the random features are the gradients of the neural network with random\nweights. Connections between the neural tangent random features and the neural tangent kernel will\nbe discussed in Section 3.2.\nWe are ready to present our main result on the expected 0-1 error bound of Algorithm 1.\nTheorem 3.3. For anyδPp 0,e´1s andRą 0, there exists\nm˚pδ,R,L,n q“ rO\n`\npolypR,Lq\n˘\n¨n7¨logp1{δq\nsuch that ifměm˚pδ,R,L,n q, then with probability at least 1´δ over the randomness of Wp0q,\nthe output of Algorithm 1 with step sizeη“κ¨R{pm?nq for some small enough absolute constant\nκ satisﬁes\nE\n“\nL0´1\nD p xWq\n‰\nď inf\nfPFpWp0q,Rq\n#\n4\nn\nnÿ\ni“1\nℓryi¨fpxiqs\n+\n`O\n«\nLR?n`\nc\nlogp1{δq\nn\nﬀ\n, (3.1)\nwhere the expectation is taken over the uniform draw of xW fromtWp0q,..., Wpn´1qu.\nThe expected 0-1 error bound given by Theorem 3.3 consists of two terms: The ﬁrst term in (3.1)\nrelates the expected 0-1 error achieved by Algorithm 1 with a reference function class–the NTRF\nfunction class in Deﬁnition 3.2. The second term in (3.1) is a standard large-deviation error term. As\nlong asR“ rOp1q, this term matches the standard rOpn´1{2q rate in PAC learning bounds [33].\nRemark 3.4. The parameterR in Theorem 3.3 is from the NTRF class and introduces a trade-off\nin the bound: when R is small, the corresponding NTRF classFpWp0q,Rq is small, making the\nﬁrst term in (3.1) large, and the second term in (3.1) is small. When R is large, the corresponding\nfunction classFpWp0q,Rq is large, so the ﬁrst term in (3.1) is small, and the second term will be\nlarge. In particular, if we setR“ rOp1q, the second term in (3.1) will be rOpn´1{2q. In this case, the\n“classiﬁability” of the underlying data distributionD is determined by how well its i.i.d. samples\ncan be classiﬁed byFpWp0q, rOp1qq. In other words, Theorem 3.3 suggests that if the data can be\nclassiﬁed by a function in the NTRF function classFpWp0q, rOp1qq with a small training error, the\nover-parameterized ReLU network learnt by Algorithm 1 will have a small generalization error.\n4\nRemark 3.5. The expected 0-1 error bound given by Theorem 3.3 is in a very general form. It\ndirectly covers the result given by Cao and Gu [8]. In Appendix A.1, we show that under the same\nassumptions made in Cao and Gu [8], to achieveϵ expected 0-1 error, our result requires a sample\ncomplexity of order rOpϵ´2q, which outperforms the result in Cao and Gu [8] by a factor ofϵ´2.\nRemark 3.6. Our generalization bound can also be compared with two recent results [37, 14] for\ntwo-layer neural networks. WhenL“ 2, the NTRF function classFpWp0q, rOp1qq can be written as\n␣\nfWp0qp¨q`x∇W1fWp0qp¨q, W1y`x∇W2fWp0qp¨q, W2y :}W1}F,}W2}F ď rOpm´1{2q\n(\n.\nIn contrast, the reference function classes studied by Yehudai and Shamir[37] and E et al. [14] are\ncontained in the following random feature class:\nF“\n␣\nfWp0qp¨q`x∇W2fWp0qp¨q, W2y :}W2}F ď rOpm´1{2q\n(\n,\nwhere Wp0q “p Wp0q\n1 , Wp0q\n2 qP Rmˆdˆ R1ˆm are the random weights generated by the initial-\nization schemes in Yehudai and Shamir [37], E et al. [14]2. Evidently, our NTRF function class\nFpWp0q, rOp1qq is richer thanF–it also contains the features corresponding to the ﬁrst layer gradient\nof the network at random initialization, i.e.,∇W1fWp0qp¨q. As a result, our generalization bound is\nsharper than those in Yehudai and Shamir [37], E et al. [14] in the sense that we can show that neural\nnetworks trained with SGD can compete with the best function in a larger reference function class.\nAs previously mentioned, the result of Theorem 3.3 can be easily extended to the setting where the\nwidths of different layers are different. We should expect that the result remains almost the same,\nexcept that we assume the widths of hidden layers are all larger than or equal to m˚pδ,R,L,n q.\nWe would also like to point out that although this paper considers the cross-entropy loss, the proof\nof Theorem 3.3 offers a general framework based on the fact that near initialization, the neural\nnetwork function is almost linear in terms of its weights. We believe that this proof framework can\npotentially be applied to most practically useful loss functions: wheneverℓp¨qis convex/Lipschitz\ncontinuous/smooth, near initialization,LipWq is also almost convex/Lipschitz continuous/smooth\nin W for all i P rns, and therefore standard online optimization analysis can be invoked with\nonline-to-batch conversion to provide a generalization bound. We refer to Section 4 for more details.\n3.2 Connection to Neural Tangent Kernel\nBesides quantifying the classiﬁability of the data with the NTRF function classFpWp0q, rOp1qq, an\nalternative way to apply Theorem 3.3 is to check how large the parameterR needs to be in order to\nmake the ﬁrst term in (3.1) small enough (e.g., smaller thann´1{2). In this subsection, we show that\nthis type of analysis connects Theorem 3.3 to the neural tangent kernel proposed in Jacot et al. [18]\nand later studied by Yang [36], Lee et al. [23], Arora et al. [3]. Speciﬁcally, we provide an expected\n0-1 error bound in terms of the neural tangent kernel matrix deﬁned over the training data. We ﬁrst\ndeﬁne the neural tangent kernel matrix for the neural network function in (2.1).\nDeﬁnition 3.7 (Neural Tangent Kernel Matrix). For anyi,j Prns, deﬁne\nrΘp1q\ni,j “ Σp1q\ni,j “x xi, xjy, Aplq\nij “\n˜\nΣplq\ni,i Σplq\ni,j\nΣplq\ni,j Σplq\nj,j\n¸\n,\nΣpl`1q\ni,j “ 2¨Epu,vq„N\n`\n0,Aplq\nij\n˘rσpuqσpvqs,\nrΘpl`1q\ni,j “ rΘplq\ni,j¨2¨Epu,vq„N\n`\n0,Aplq\nij\n˘rσ1puqσ1pvqs` Σpl`1q\ni,j .\nThen we call ΘpLq“rp rΘpLq\ni,j ` ΣpLq\ni,jq{2snˆn the neural tangent kernel matrix of anL-layer ReLU\nnetwork on training inputs x1,..., xn.\nDeﬁnition 3.7 is the same as the original deﬁnition in Jacot et al. [18] when restricting the kernel\nfunction ontx1,..., xnu, except that there is an extra coefﬁcient2 in the second and third lines. This\nextra factor is due to the difference in initialization schemes–in our paper the entries of hidden layer\n2Normalizing weights to the same scale is necessary for a proper comparison. See Appendix A.2 for details.\n5\nmatrices are randomly generated with variance 2{m, while in Jacot et al. [18] the variance of the\nrandom initialization is 1{m. We remark that this extra factor 2 in Deﬁnition 3.7 will remove the\nexponential dependence on the network depthL in the kernel matrix, which is appealing. In fact, it is\neasy to check that under our scaling, the diagonal entries of ΣpLq are all 1’s, and the diagonal entries\nof rΘpLq are allL’s.\nThe following lemma is a summary of Theorem 1 and Proposition 2 in Jacot et al.[18], which ensures\nthat ΘpLq is the inﬁnite-width limit of the Gram matrixpm´1x∇WfWp0qpxiq,∇WfWp0qpxjqyqnˆn,\nand is positive-deﬁnite as long as no two training inputs are parallel.\nLemma 3.8 (Jacot et al. [18]). For anL layer ReLU network with parameter set Wp0q initialized in\nAlgorithm 1, as the network widthmÑ8 3, it holds that\nm´1x∇WfWp0qpxiq,∇WfWp0qpxjqy PÝ ÑΘpLq\ni,j ,\nwhere the expectation is taken over the randomness ofWp0q. Moreover, as long as each pair of inputs\namong x1,..., xnPSd´1 are not parallel, ΘpLq is positive-deﬁnite.\nRemark 3.9. Lemmas 3.8 clearly shows the difference between our neural tangent kernel matrix\nΘpLq in Deﬁnition 3.7 and the Gram matrix KpLq deﬁned in Deﬁnition 5.1 in Du et al. [11]. For any\ni,j Prns, by Lemma 3.8 we have\nΘpLq\ni,j “ lim\nmÑ8\nm´1řL\nl“1x∇WlfWp0qpxiq,∇WlfWp0qpxjqy.\nIn contrast, the corresponding entry in KpLq is\nKpLq\ni,j “ lim\nmÑ8\nm´1x∇WL´1fWp0qpxiq,∇WL´1fWp0qpxjqy.\nIt can be seen that our deﬁnition of kernel matrix takes all layers into consideration, while Du\net al. [11] only considered the last hidden layer (i.e., second last layer). Moreover, it is clear that\nΘpLq ľ KpLq. Since the smallest eigenvalue of the kernel matrix plays a key role in the analysis of\noptimization and generalization of over-parameterized neural networks [12, 11, 4], our neural tangent\nkernel matrix can potentially lead to better bounds than the Gram matrix studied in Du et al. [11].\nCorollary 3.10. Let y“p y1,...,y nqJ andλ0“ λminpΘpLqq. For any δPp 0,e´1s, there exists\nrm˚pδ,L,n,λ 0q that only depends on δ,L,n andλ0 such that if m ě rm˚pδ,L,n,λ 0q, then with\nprobability at least 1´δ over the randomness of Wp0q, the output of Algorithm 1 with step size\nη“κ¨inf ryiyiě1\na\nryJpΘpLqq´1ry{pm?nq for some small enough absolute constantκ satisﬁes\nE\n“\nL0´1\nD p xWq\n‰\nď rO\n«\nL¨ inf\nryiyiě1\nc\nryJpΘpLqq´1ry\nn\nﬀ\n`O\n«c\nlogp1{δq\nn\nﬀ\n,\nwhere the expectation is taken over the uniform draw of xW fromtWp0q,..., Wpn´1qu.\nRemark 3.11. Corollary 3.10 gives an algorithm-dependent generalization error bound of over-\nparameterizedL-layer neural networks trained with SGD. It is worth noting that recently Arora et al.\n[4] gives a generalization bound rO\n`a\nyJpH8q´1y{n\n˘\nfor two-layer networks with ﬁxed second\nlayer weights, where H8 is deﬁned as\nH8\ni,j“x xi, xjy¨Ew„Np0,Iqrσ1pwJxiqσ1pwJxjqs.\nOur result in Corollary 3.10 can be specialized to two-layer neural networks by choosingL“ 2, and\nyields a bound rO\n`a\nyJpΘp2qq´1y{n\n˘\n, where\nΘp2q\ni,j “ H8\ni,j` 2¨Ew„Np0,IqrσpwJxiqσpwJxjqs.\nHere the extra term 2¨Ew„Np0,IqrσpwJxiqσpwJxjqs corresponds to the training of the second\nlayer–it is the limit of 1\nmx∇W2fWp0qpxiq,∇W2fWp0qpxjqy. Since we have Θp2q ľ H8, our bound\nis sharper than theirs. This comparison also shows that, our result generalizes the result in Arora et al.\n[4] from two-layer, ﬁxed second layer networks to deep networks with all parameters being trained.\n3The original result by Jacot et al. [18] requires that the widths of different layers go to inﬁnity sequen-\ntially. Their result was later improved by Yang [36] such that the widths of different layers can go to inﬁnity\nsimultaneously.\n6\nRemark 3.12. Corollary 3.10 is based on the asymptotic convergence result in Lemma 3.8, which\ndoes not show how wide the network need to be in order to make the Gram matrix close enough\nto the NTK matrix. Very recently, Arora et al. [3] provided a non-asymptotic convergence result\nfor the Gram matrix, and showed the equivalence between an inﬁnitely wide network trained by\ngradient ﬂow and a kernel regression predictor using neural tangent kernel, which suggests that the\ngeneralization of deep neural networks trained by gradient ﬂow can potentially be measured by the\ncorresponding NTK. Utilizing this non-asymptotic convergence result, one can potentially specify\nthe detailed dependency of rm˚pδ,L,n,λ 0q onδ,L,n andλ0 in Corollary 3.10.\nRemark 3.13. Corollary 3.10 demonstrates that the generalization bound given by Theorem 3.3\ndoes not increase with network widthm, as long asm is large enough. Moreover, it provides a clear\ncharacterization of the classiﬁability of data. In fact, the\na\nryJpΘpLqq´1ry factor in the generalization\nbound given in Corollary 3.10 is exactly the NTK-induced RKHS norm of the kernel regression\nclassiﬁer on datatpxi, ryiqun\ni“1. Therefore, ify“f˚pxq for somef˚p¨qwith bounded norm in the\nNTK-induced reproducing kernel Hilbert space (RKHS), then over-parameterized neural networks\ntrained with SGD generalize well. In Appendix E, we provide some numerical evaluation of the\nleading terms in the generalization bounds in Theorem 3.3 and Corollary 3.10 to demonstrate that\nthey are very informative on real-world datasets.\n4 Proof of Main Theory\nIn this section we provide the proof of Theorem 3.3 and Corollary 3.10, and explain the intuition\nbehind the proof. For notational simplicity, foriPrns we denoteLipWq“ Lpxi,yiqpWq.\n4.1 Proof of Theorem 3.3\nBefore giving the proof of Theorem 3.3, we ﬁrst introduce several lemmas. The following lemma\nstates that near initialization, the neural network function is almost linear in terms of its weights.\nLemma 4.1. There exists an absolute constantκ such that, with probability at least 1´OpnL2q¨\nexpr´Ωpmω2{3Lqs over the randomness of Wp0q, for all iPr ns and W, W1PBpWp0q,ωq with\nωďκL´6rlogpmqs´3{2, it holds uniformly that\n|fW1pxiq´ fWpxiq´x∇fWpxiq, W1´ Wy|ď O\n´\nω1{3L2a\nm logpmq\n¯\n¨řL´1\nl“1}W1\nl´ Wl}2.\nSince the cross-entropy lossℓp¨qis convex, given Lemma 4.1, we can show in the following lemma\nthat near initialization,LipWq is also almost a convex function of W for anyiPrns.\nLemma 4.2. There exists an absolute constantκ such that, with probability at least 1´OpnL2q¨\nexpr´Ωpmω2{3Lqs over the randomness of Wp0q, for anyϵą 0,iPrns and W, W1PBpWp0q,ωq\nwithωďκL´6m´3{8rlogpmqs´3{2ϵ3{4, it holds uniformly that\nLipW1qě LipWq`x∇WLipWq, W1´ Wy´ ϵ.\nThe locally almost convex property of the loss function given by Lemma 4.2 implies that the dynamics\nof Algorithm 1 is similar to the dynamics of convex optimization. We can therefore derive a bound of\nthe cumulative loss. The result is given in the following lemma.\nLemma 4.3. For anyϵ,δ,R ą 0, there exists\nm˚pϵ,δ,R,L q“ rO\n`\npolypR,Lq\n˘\n¨ϵ´14¨logp1{δq\nsuch that ifměm˚pϵ,δ,R,L q, then with probability at least 1´δ over the randomness of Wp0q,\nfor any W˚ PBpWp0q,Rm´1{2q, Algorithm 1 with η “ νϵ{pLmq,n“ L2R2{p2νϵ2q for some\nsmall enough absolute constantν has the following cumulative loss bound:\nřn\ni“1LipWpi´1qqď řn\ni“1LipW˚q` 3nϵ.\nWe now ﬁnalize the proof by applying an online-to-batch conversion argument [9], and use Lemma 4.1\nto relate the neural network function with a function in the NTRF function class.\n7\nProof of Theorem 3.3. Fori P rns, let L0´1\ni pWpi´1qq “1\n␣\nyi¨fWpi´1qpxiq ă0\n(\n. Since cross-\nentropy loss satisﬁes 1tz ď 0u ď 4ℓpzq, we have L0´1\ni pWpi´1qq ď 4LipWpi´1qq. Therefore,\nsettingϵ“LR{\n?\n2νn in Lemma 4.3 gives that, ifη is set as\na\nν{2R{pm?nq, then with probability\nat least 1´δ,\n1\nn\nnÿ\ni“1\nL0´1\ni pWpi´1qqď 4\nn\nnÿ\ni“1\nLipW˚q` 12?\n2ν ¨LR?n. (4.1)\nNote that for anyiPrns, Wpi´1q only depends onpx1,y 1q,..., pxi´1,y i´1q and is independent of\npxi,y iq. Therefore by Proposition 1 in Cesa-Bianchi et al. [9], with probability at least 1´δ we have\n1\nn\nnÿ\ni“1\nL0´1\nD pWpi´1qqď 1\nn\nnÿ\ni“1\nL0´1\ni pWpi´1qq`\nc\n2 logp1{δq\nn . (4.2)\nBy deﬁnition, we have 1\nn\nřn\ni“1L0´1\nD pWpi´1qq “E\n“\nL0´1\nD p xWq\n‰\n. Therefore combining (4.1) and\n(4.2) and applying union bound, we obtain that with probability at least 1´ 2δ,\nE\n“\nL0´1\nD p xWq\n‰\nď 4\nn\nnÿ\ni“1\nLipW˚q` 12?\n2ν ¨LR?n`\nc\n2 logp1{δq\nn (4.3)\nfor all W˚PBpWp0q,Rm´1{2q. We now compare the neural network functionfW˚pxiq with the\nfunctionFWp0q,W˚pxiq :“fWp0qpxiq`x∇fWp0qpxiq, W˚´ Wp0qyP FpWp0q,Rq. We have\nLipW˚qď ℓryi¨FWp0q,W˚pxiqs`O\n´\npRm´1{2q1{3L2a\nm logpmq\n¯\n¨řL´1\nl“1\n››W˚\nl ´ Wp0q\nl\n››\n2\nďℓryi¨FWp0q,W˚pxiqs`O\n´\nL3a\nm logpmq\n¯\n¨R4{3¨m´2{3\nďℓryi¨FWp0q,W˚pxiqs` LRn´1{2,\nwhere the ﬁrst inequality is by the1-Lipschitz continuity ofℓp¨qand Lemma 4.1, the second inequality\nis by W˚PBpWp0q,Rm´1{2q, and last inequality holds as long asměC1R2L12rlogpmqs3n3 for\nsome large enough absolute constantC1. Plugging the inequality above into (4.3) gives\nE\n“\nL0´1\nD p xWq\n‰\nď 4\nn\nnÿ\ni“1\nℓryi¨FWp0q,W˚pxiqs`\nˆ\n1` 12?\n2ν\n˙\n¨LR?n`\nc\n2 logp1{δq\nn .\nTaking inﬁmum overW˚PBpWp0q,Rm´1{2q and rescalingδ ﬁnishes the proof.\n4.2 Proof of Corollary 3.10\nIn this subsection we prove Corollary 3.10. The following lemma shows that at initialization, with\nhigh probability, the neural network function value at all the training inputs are of order rOp1q.\nLemma 4.4. For anyδą 0, ifměKL logpnL{δq for a large enough absolute constant K, then\nwith probability at least 1´δ,|fWp0qpxiq|ď Op\na\nlogpn{δqq for alliPrns.\nWe now present the proof of Corollary 3.10. The idea is to construct suitable target values py1,..., pyn,\nand then bound the norm of the solution of the linear equations pyi“x∇fWp0qpxiq, Wy,iPrns. In\nspeciﬁc, for any ry with ryiyiě 1, we examine the minimum distance solution to Wp0q that ﬁt the\ndatatpxi, ryiqun\ni“1 well and use it to construct a speciﬁc function inF\n`\nWp0q, rO\n`a\nryJpΘpLqq´1ry\n˘˘\n.\nProof of Corollary 3.10. SetB “ logt1{rexppn´1{2q´ 1su“ Oplogpnqq, then for cross-entropy\nloss we have ℓpzq ďn´1{2 for z ě B. Moreover, let B1 “ maxiPrns|fWp0qpxiq|. Then by\nLemma 4.4, with probability at least 1´δ,B1 ďOp\na\nlogpn{δqq for all iPr ns. For any ry with\nryiyiě 1, letB“B`B1 and py“B¨ry, then it holds that for anyiPrns,\nyi¨rpyi`fWp0qpxiqs“ yi¨pyi`yi¨fWp0qpxiqě B`B1´B1ěB,\n8\nand therefore\nℓtyi¨rpyi`fWp0qpxiqsuď n´1{2, iPrns. (4.4)\nDenote F“m´1{2¨pvecr∇fWp0qpx1qs,..., vecr∇fWp0qpxnqsqP Rrmd`m`m2pL´2qsˆn. Note that\nentries of ΘpLq are all bounded byL. Therefore, the largest eigenvalue of ΘpLq is at mostnL, and\nwe have ryJpΘpLqq´1ryěn´1L´1}ry}2\n2“L´1. By Lemma 3.8 and standard matrix perturbation\nbound, there existsm˚pδ,L,n,λ 0q such that, ifměm˚pδ,L,n,λ 0q, then with probability at least\n1´δ, FJF is strictly positive-deﬁnite and\n}pFJFq´1´p ΘpLqq´1}2ď inf\nryiyiě1\nryJpΘpLqq´1ry{n. (4.5)\nLet F “ PΛQJ be the singular value decomposition of F, where P P Rmˆn, Q P Rnˆn have\northogonal columns, and ΛP Rnˆn is a diagonal matrix. Let wvec“ PΛ´1QJpy, then we have\nFJwvec“p QΛPJqpPΛ´1QJpyq“ py. (4.6)\nMoreover, by direct calculation we have\n}wvec}2\n2“} PΛ´1QJpy}2\n2“} Λ´1QJpy}2\n2“ pyJQΛ´2QJpy“ pyJpFJFq´1py.\nTherefore by (4.5) and the fact that}py}2\n2“B\n2\nn, we have\n}wvec}2\n2“ pyJrpFJFq´1´p ΘpLqq´1spy` pyJpΘpLqq´1py\nďB\n2\n¨n¨}pFJFq´1´p ΘpLqq´1}2`B\n2\n¨ryJpΘpLqq´1ry\nď 2B\n2\n¨ryJpΘpLqq´1ry.\nLet WPW be the parameter collection reshaped fromm´1{2wvec. Then clearly\n}Wl}F ďm´1{2}wvec}2ď rO\n´b\nryJpΘpLqq´1ry¨m´1{2\n¯\n,\nand therefore W P B\n`\n0,O\n`a\nryJpΘpLqq´1ry¨m´1{2˘˘\n. Moreover, by (4.6), we have pyi “\nx∇WfWp0qpxiq, Wy. Plugging this into (4.4) then gives\nℓ\n␣\nyi¨\n“\nfWp0qpxiq`x∇WfWp0qpxiq, Wy\n‰(\nďn´1{2.\nSince pfp¨q “fWp0qp¨q`x∇WfWp0qp¨q, Wy PF\n`\nWp0q, rO\n`a\nryJpΘpLqq´1ry\n˘˘\n, applying Theo-\nrem 3.3 and taking inﬁmum over ry completes the proof.\n5 Conclusions and Future Work\nIn this paper we provide an expected 0-1 error bound for wide and deep ReLU networks trained with\nSGD. This generalization error bound is measured by the NTRF function class. The connection to\nthe neural tangent kernel function studied in Jacot et al. [18] is also discussed. Our result covers a\nseries of recent generalization bounds for wide enough neural networks, and provides better bounds.\nAn important future work is to improve the over-parameterization condition in Theorem 3.3 and\nCorollary 3.10. Other future directions include proving sample complexity lower bounds in the\nover-parameterized regime, implementing the results in Jain et al. [19] to obtain last iterate bound\nof SGD, and establishing uniform convergence based generalization bounds for over-parameterized\nneural networks with methods developped in Bartlett et al. [6], Neyshabur et al. [27], Long and\nSedghi [26].\nAcknowledgement\nWe would like to thank Peter Bartlett for a valuable discussion, and Simon S. Du for pointing out a\nrelated work [3]. We also thank the anonymous reviewers and area chair for their helpful comments.\nThis research was sponsored in part by the National Science Foundation CAREER Award IIS-\n1906169, IIS-1903202, and Salesforce Deep Learning Research Award. The views and conclusions\ncontained in this paper are those of the authors and should not be interpreted as representing any\nfunding agencies.\n9\nReferences\n[1] ALLEN -ZHU, Z. , LI, Y. and LIANG , Y. (2018). Learning and generalization in overparameter-\nized neural networks, going beyond two layers. arXiv preprint arXiv:1811.04918 .\n[2] ALLEN -Z HU, Z. , LI, Y. and SONG , Z. (2018). A convergence theory for deep learning via\nover-parameterization. arXiv preprint arXiv:1811.03962 .\n[3] ARORA , S. , DU, S. S. , HU, W., LI, Z. , SALAKHUTDINOV , R. and WANG , R. (2019). On\nexact computation with an inﬁnitely wide neural net. arXiv preprint arXiv:1904.11955 .\n[4] ARORA , S. , DU, S. S. , HU, W. , LI, Z. and WANG , R. (2019). Fine-grained analysis of\noptimization and generalization for overparameterized two-layer neural networks.arXiv preprint\narXiv:1901.08584 .\n[5] ARORA , S. , GE, R. , NEYSHABUR , B. and ZHANG , Y. (2018). Stronger generalization bounds\nfor deep nets via a compression approach. arXiv preprint arXiv:1802.05296 .\n[6] BARTLETT , P. L. , FOSTER , D. J. and TELGARSKY , M. J. (2017). Spectrally-normalized\nmargin bounds for neural networks. In Advances in Neural Information Processing Systems .\n[7] BRUTZKUS , A. , GLOBERSON , A. , MALACH , E. and SHALEV -SHWARTZ , S. (2017). Sgd\nlearns over-parameterized networks that provably generalize on linearly separable data. arXiv\npreprint arXiv:1710.10174 .\n[8] CAO, Y. and GU, Q. (2019). A generalization theory of gradient descent for learning over-\nparameterized deep relu networks. arXiv preprint arXiv:1902.01384 .\n[9] CESA -BIANCHI , N. , CONCONI , A. and GENTILE , C. (2004). On the generalization ability of\non-line learning algorithms. IEEE Transactions on Information Theory 50 2050–2057.\n[10] DANIELY, A. (2017). Sgd learns the conjugate kernel class of the network. In Advances in\nNeural Information Processing Systems.\n[11] DU, S. S. , LEE, J. D. , LI, H. , WANG , L. and ZHAI , X. (2018). Gradient descent ﬁnds global\nminima of deep neural networks. arXiv preprint arXiv:1811.03804 .\n[12] DU, S. S. , ZHAI , X., POCZOS , B. and SINGH , A. (2018). Gradient descent provably optimizes\nover-parameterized neural networks. arXiv preprint arXiv:1810.02054 .\n[13] DZIUGAITE , G. K. and ROY, D. M. (2017). Computing nonvacuous generalization bounds for\ndeep (stochastic) neural networks with many more parameters than training data. arXiv preprint\narXiv:1703.11008 .\n[14] E, W. , MA, C. , WU, L. ET AL . (2019). A comparative analysis of the optimization and\ngeneralization property of two-layer neural network and random feature models under gradient\ndescent dynamics. arXiv preprint arXiv:1904.04326 .\n[15] GOLOWICH , N. , RAKHLIN , A. and SHAMIR , O. (2017). Size-independent sample complexity\nof neural networks. arXiv preprint arXiv:1712.06541 .\n[16] HE, K. , ZHANG , X. , REN, S. and SUN, J. (2015). Delving deep into rectiﬁers: Surpassing\nhuman-level performance on imagenet classiﬁcation. In Proceedings of the IEEE international\nconference on computer vision.\n[17] HINTON , G. , DENG , L. , YU, D. , DAHL , G. E. , MOHAMED , A.- R., JAITLY, N. , SENIOR ,\nA., VANHOUCKE , V., NGUYEN , P., SAINATH , T. N. ET AL . (2012). Deep neural networks\nfor acoustic modeling in speech recognition: The shared views of four research groups. IEEE\nSignal Processing Magazine 29 82–97.\n[18] J ACOT, A., G ABRIEL , F. and H ONGLER , C. (2018). Neural tangent kernel: Convergence and\ngeneralization in neural networks. arXiv preprint arXiv:1806.07572 .\n[19] JAIN , P., NAGARAJ , D. and NETRAPALLI , P. (2019). Making the last iterate of sgd information\ntheoretically optimal. arXiv preprint arXiv:1904.12443 .\n10\n[20] KRIZHEVSKY , A., SUTSKEVER , I. and HINTON , G. E. (2012). Imagenet classiﬁcation with\ndeep convolutional neural networks. In Advances in neural information processing systems .\n[21] LANGFORD , J. and CARUANA , R. (2002). (not) bounding the true error. In Advances in Neural\nInformation Processing Systems.\n[22] LECUN, Y., BOTTOU , L. , BENGIO , Y., HAFFNER , P. ET AL . (1998). Gradient-based learning\napplied to document recognition. Proceedings of the IEEE 86 2278–2324.\n[23] LEE, J., XIAO, L., SCHOENHOLZ , S. S. , BAHRI , Y., SOHL -D ICKSTEIN , J. and PENNINGTON ,\nJ. (2019). Wide neural networks of any depth evolve as linear models under gradient descent.\narXiv preprint arXiv:1902.06720 .\n[24] LI, X., LU, J., WANG , Z., HAUPT, J. and ZHAO, T. (2018). On tighter generalization bound\nfor deep neural networks: Cnns, resnets, and beyond. arXiv preprint arXiv:1806.05159 .\n[25] LI, Y. and LIANG , Y. (2018). Learning overparameterized neural networks via stochastic\ngradient descent on structured data. arXiv preprint arXiv:1808.01204 .\n[26] LONG , P. M. and SEDGHI , H. (2019). Size-free generalization bounds for convolutional neural\nnetworks. arXiv preprint arXiv:1905.12600 .\n[27] NEYSHABUR , B., BHOJANAPALLI , S., MCALLESTER , D. and SREBRO , N. (2017). A pac-\nbayesian approach to spectrally-normalized margin bounds for neural networks. arXiv preprint\narXiv:1707.09564 .\n[28] NEYSHABUR , B., LI, Z., BHOJANAPALLI , S., LECUN, Y. and SREBRO , N. (2018). The role\nof over-parametrization in generalization of neural networks .\n[29] NEYSHABUR , B. , TOMIOKA , R. and SREBRO , N. (2015). Norm-based capacity control in\nneural networks. In Conference on Learning Theory.\n[30] OYMAK , S. and SOLTANOLKOTABI , M. (2019). Towards moderate overparameteriza-\ntion: global convergence guarantees for training shallow neural networks. arXiv preprint\narXiv:1902.04674 .\n[31] RAHIMI , A. and RECHT , B. (2008). Random features for large-scale kernel machines. In\nAdvances in neural information processing systems .\n[32] RAHIMI , A. and RECHT , B. (2009). Weighted sums of random kitchen sinks: Replacing\nminimization with randomization in learning. In Advances in neural information processing\nsystems.\n[33] SHALEV -SHWARTZ , S. and BEN-DAVID, S. (2014). Understanding machine learning: From\ntheory to algorithms. Cambridge university press.\n[34] SILVER , D., HUANG , A., MADDISON , C. J. , GUEZ , A., SIFRE , L., VAN DEN DRIESSCHE ,\nG., SCHRITTWIESER , J., ANTONOGLOU , I., PANNEERSHELVAM , V., LANCTOT , M. ET AL .\n(2016). Mastering the game of go with deep neural networks and tree search. Nature 529\n484–489.\n[35] WEI, C. , LEE, J. D. , LIU, Q. and MA, T. (2018). On the margin theory of feedforward neural\nnetworks. arXiv preprint arXiv:1810.05369 .\n[36] YANG , G. (2019). Scaling limits of wide neural networks with weight sharing: Gaussian\nprocess behavior, gradient independence, and neural tangent kernel derivation. arXiv preprint\narXiv:1902.04760 .\n[37] YEHUDAI , G. and SHAMIR , O. (2019). On the power and limitations of random features for\nunderstanding neural networks. arXiv preprint arXiv:1904.00687 .\n[38] ZHANG , C., BENGIO , S., HARDT , M., RECHT , B. and VINYALS , O. (2016). Understanding\ndeep learning requires rethinking generalization. arXiv preprint arXiv:1611.03530 .\n[39] ZOU, D. , CAO, Y., ZHOU , D. and GU, Q. (2018). Stochastic gradient descent optimizes\nover-parameterized deep relu networks. arXiv preprint arXiv:1811.08888 .\n11",
  "values": {
    "Non-maleficence": "No",
    "Deferral to humans": "No",
    "Collective influence": "No",
    "Critiqability": "No",
    "Not socially biased": "No",
    "Explicability": "No",
    "User influence": "No",
    "Fairness": "No",
    "Beneficence": "No",
    "Respect for Law and public interest": "No",
    "Autonomy (power to decide)": "No",
    "Transparent (to users)": "No",
    "Interpretable (to users)": "No",
    "Respect for Persons": "No",
    "Privacy": "No",
    "Justice": "No"
  }
}