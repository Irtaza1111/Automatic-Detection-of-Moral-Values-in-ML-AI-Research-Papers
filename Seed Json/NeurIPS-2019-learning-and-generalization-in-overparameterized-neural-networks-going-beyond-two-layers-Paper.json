{
  "pdf": "NeurIPS-2019-learning-and-generalization-in-overparameterized-neural-networks-going-beyond-two-layers-Paper",
  "title": "Learning and Generalization in Overparameterized Neural Networks, Going Beyond Two Layers",
  "author": "Zeyuan Allen-Zhu, Yuanzhi Li, Yingyu Liang",
  "paper_id": "NeurIPS-2019-learning-and-generalization-in-overparameterized-neural-networks-going-beyond-two-layers-Paper",
  "text": "Learning and Generalization in Overparameterized\nNeural Networks, Going Beyond Two Layers∗\nZeyuan Allen-Zhu\nMicrosoft Research AI\nzeyuan@csail.mit.edu\nYuanzhi Li\nCarnegie Mellon University\nyuanzhil@andrew.cmu.edu\nYingyu Liang\nUniversity of Wisconsin-Madison\nyliang@cs.wisc.edu\nAbstract\nThe fundamental learning theory behind neural networks remains largely open.\nWhat classes of functions can neural networks actually learn? Why doesn’t the\ntrained network overﬁt when it is overparameterized?\nIn this work, we prove that overparameterized neural networks can learn some\nnotable concept classes, including two and three-layer networks with fewer pa-\nrameters and smooth activations. Moreover, the learning can be simply done by\nSGD (stochastic gradient descent) or its variants in polynomial time using poly-\nnomially many samples. The sample complexity can also be almost independent\nof the number of parameters in the network.\nOn the technique side, our analysis goes beyond the so-called NTK (neural tan-\ngent kernel) linearization of neural networks in prior works. We establish a new\nnotion of quadratic approximation of the neural network, and connect it to the\nSGD theory of escaping saddle points.\n1 Introduction\nNeural network learning has become a key machine learning approach and has achieved remarkable\nsuccess in a wide range of real-world domains, such as computer vision, speech recognition, and\ngame playing [25, 26, 30, 41]. In contrast to the widely accepted empirical success, much less\ntheory is known. Despite a recent boost of theoretical studies, many questions remain largely open,\nincluding fundamental ones about the optimization and generalization in learning neural networks.\nOne key challenge in analyzing neural networks is that the corresponding optimization is non-convex\nand is theoretically hard in the general case [40, 55]. This is in sharp contrast to the fact that simple\noptimization algorithms like stochastic gradient descent (SGD) and its variants usually produce good\nsolutions in practice even on both training and test data. Therefore,\nwhat functions can neural networks provably learn?\nAnother key challenge is that, in practice, neural networks are heavily overparameterized (e.g., [53]):\nthe number of learnable parameters is much larger than the number of the training samples. It\nis observed that overparameterization empirically improves both optimization and generalization,\nappearing to contradict traditional learning theory.2 Therefore,\nwhy do overparameterized networks (found by those training algorithms) generalize?\n∗Full version and future updates can be found on https://arxiv.org/abs/1811.04918.\n2For example, Livni et al. [36] observed that on synthetic data generated from a target network, SGD con-\nverges faster when the learned network has more parameters than the target. Perhaps more interestingly, Arora\net al. [6] found that overparameterized networks learned in practice can often be compressed to simpler ones\nwith much fewer parameters, without hurting their ability to generalize; however, directly learning such simpler\nnetworks runs into worse results due to the optimization difﬁculty. We also have experiments in Figure 1(a).\n33rd Conference on Neural Information Processing Systems (NeurIPS 2019), Vancouver, Canada.\n1.1 What Can Neural Networks Provably Learn?\nMost existing works analyzing the learnability of neural networks [9, 12, 13, 20, 21, 28, 33, 34, 42,\n43, 47, 49, 50, 56] make unrealistic assumptions about the data distribution (such as being random\nGaussian), and/or make strong assumptions about the network (such as using linear activations).\nLi and Liang [32] show that two-layer ReLU networks can learn classiﬁcation tasks when the data\ncome from mixtures of arbitrary but well-separated distributions.\nA theorem without distributional assumptions on data is often more desirable. Indeed, how to obtain\na result that does not depend on the data distribution, but only on the concept class itself, lies in the\ncenter of PAC-learning which is one of the foundations of machine learning theory [48]. Also,\nstudying non-linear activations is critical because otherwise one can only learn linear functions,\nwhich can also be easily learned via linear models without neural networks.\nBrutzkus et al. [14] prove that two-layer networks with ReLU activations can learn linearly-separable\ndata (and thus the class of linear functions) using just SGD. This is an (improper) PAC-learning\ntype of result because it makes no assumption on the data distribution. Andoni et al. [5] proves that\ntwo-layer networks can learn polynomial functions of degreer overd-dimensional inputs in sample\ncomplexityO(dr). Their learner networks use exponential activation functions, where in practice the\nrectiﬁed linear unit (ReLU) activation has been the dominant choice across vastly different domains.\nOn a separate note, if one treats all but the last layer of neural networks as generating a random\nfeature map, then training only the last layer is a convex task, so one can learn the class of linear\nfunctions in this implicit feature space [15, 16]. This result implies low-degree polynomials and\ncompositional kernels can be learned by neural networks in polynomial time. Empirically, training\nlast layer greatly weakens the power of neural networks (see Figure 1).\nOur Result. We prove that an important concept class that contains three-layer (resp. two-layer)\nneural networks equipped with smooth activations can be efﬁciently learned by three-layer (resp.\ntwo-layer) ReLU neural networks via SGD or its variants.\nSpeciﬁcally, suppose in aforementioned class the best network (called the target function or target\nnetwork) achieves a population risk OPT with respect to some convex loss function. We show that\none can learn up to population risk OPT +ε, using three-layer (resp. two-layer) ReLU networks of\nsize greater than a ﬁxedpolynomial in the size of the target network, in1/ε, and in the “complexity”\nof the activation function used in the target network. Furthermore, the sample complexity is also\npolynomial in these parameters, and only poly-logarithmic in the size of the learner ReLU network.\nWe stress here that this is agnostic PAC-learning because we allow the target function to have er-\nror (e.g., OPT can be positive for regression), and is improper learning because the concept class\nconsists of smaller neural networks comparing to the networks being trained.\nOur Contributions. We believe our result gives further insights to the fundamental questions about\nthe learning theory of neural networks.\n• To the best of our knowledge, this is the ﬁrst result showing that usinghidden layers of neural\nnetworks one can provably learn the concept class containing two (or even three) layer neural\nnetworks with non-trivial activation functions.3\n• Our three-layer result gives the ﬁrst theoretical proof that learning neural networks, even with\nnon-convex interactions across layers, can still be plausible. In contrast, in the two-layer case\nthe optimization landscape with overparameterization is almost convex [17, 32]; and in previ-\nous studies on the multi-layer case, researchers have weakened the network by applying the so-\ncalled NTK (neural tangent kernel) linearization to remove all non-convex interactions [4, 27].\n• To some extent we explain the reason why overparameterization improves testing accuracy:\nwith larger overparameterization, one can hope to learn better target functions with possibly\nlarger size, more complex activations, smaller risk OPT, and to a smaller errorε.\n• We establish new tools to tackle the learning process of neural networks in general, which can\nbe useful for studying other network architectures and learning tasks. (E.g., the new tools here\n3In contrast, Daniely [15] focuses on training essentially only the last layer (and the hidden-layer movement\nis negligible). After this paper has appeared online, Arora et al. [8] showed that neural networks can provably\nlearn two-layer networks with a slightly weaker class of smooth activation functions. Namely, the activation\nfunctions that are either linear functions or even functions.\n2\nhave allowed researchers to study also the learning of recurrent neural networks [2].)\nOther Related Works. We acknowledge a different line of research using kernels as improper\nlearners to learn the concept class of neural networks [22, 23, 36, 54]. This is very different from us\nbecause we use “neural networks” as learners. In other words, we study the question of “what can\nneural networks learn” but they study “what alternative methods can replace neural networks.”\nThere is also a line of work studying the relationship between neural networks and NTKs (neural\ntangent kernels) [3, 4, 7, 27, 31, 51]. These works study neural networks by considering their\n“linearized approximations.” There is a known performance gap between the power of real neural\nnetworks and the power of their linearized approximations. For instance, ResNet achieves 96%\ntest error on the CIFAR-10 data set but NTK (even with inﬁnite width) achieves 77% [7]. We also\nillustrate this in Figure 1.\n1.2 Why Do Overparameterized Networks Generalize?\nOur result above assumes that the learner network is sufﬁciently overparameterized. So, why does it\ngeneralize to the population risk and give small test error? More importantly, why does it generalize\nwith a number of samples that is (almost) independent of the number of parameters?\nThis question cannot be studied under the traditional VC-dimension learning theory since the VC\ndimension grows with the number of parameters. Several works [6, 11, 24, 39] explain generaliza-\ntion by studying some other “complexity” of the learned networks. Most related to the discussion\nhere is [11] where the authors prove a generalization bound in the norms (of weight matrices) of\neach layer, as opposed to the number of parameters. There are two main concerns with those results.\n• Learnability = Trainability + Generalization. It is not clear from those results how a network\nwith both low “complexity” and small training loss can be found by the training method.\nTherefore, they do not directly imply PAC-learnability for non-trivial concept classes (at least\nfor those concept classes studied by this paper).\n• Their norms are “sparsity induced norms”: for the norm not to scale with the number of hidden\nneuronsm, essentially, it requires the number of neurons with non-zero weights not to scale\nwithm. This more or less reduces the problem to the non-overparameterized case.\nAt a high level, our generalization is made possible with the following sequence of conceptual steps.\n• Good networks with small risks are plentiful: thanks to overparameterization, with high prob-\nability over random initialization, there exists a good network in the close neighborhood of\nany point on the SGD training trajectory. (This corresponds to Section 6.2 and 6.3.)\n• The optimization in overparameterized neural networks has benign properties: essentially\nalong the training trajectory, there is no second-order critical points for learning three-layer\nnetworks, and no ﬁrst-order critical points for two-layer. (This corresponds to Section 6.4.)\n• In the learned networks, information is also evenly distributed among neurons, by utilizing\neither implicit or explicit regularization. This structure allows a new generalization bound that\nis (almost) independent of the number of neurons. (This corresponds to Section 6.5 and 6.6,\nand we also empirically verify it in Section 7.1.)\nSince practical neural networks are typically overparameterized, we genuinely hope that our results\ncan provide theoretical insights to networks used in various applications.\n1.3 Roadmap\nIn the main body of this paper, we introduce notations in Section 2, present our main results and\ncontributions for two and three-layer networks in Section 3 and 4, and conclude in Section 5.\nFor readers interested in our novel techniques, we present in Section 6 an 8-paged proof sketch of our\nthree-layer result. For readers more interested in the practical relevance, we give more experiments\nin Section 7. In the appendix, we begin with mathematical preliminaries in Appendix A. Our full\nthree-layer proof is in Appendix C. Our two-layer proof is much easier and in Appendix B.\n3\n0.063\n0.25\n1.\n4.\n16.\n64.\nTest error\nm = number of hidden neurons\n3layer\n2layer\n3layer(last)\n2layer(last)\n3layer(NTK)\n(a) N = 1000 and varym\n0.001\n0.004\n0.016\n0.063\n0.25\n1.\n4.\n16.\n64.\nTest error\nN = number of samples\n3layer\n2layer\n3layer(last)\n2layer(last)\n3layer(NTK) (b) m = 2000 and varyN\nFigure 1: Performance comparison. 3layer/2layer stands for training (hidden weights) in three and\ntwo-layer neural networks. (last) stands for conjugate kernel [15], meaning training only the\noutput layer. (NTK) stands for neural tangent kernel [27] with ﬁnite width. We also implemented\nother direct kernels such as [54] but they perform much worse.\nSetup. We consider ℓ2 regression task on synthetic data where feature vectors x ∈ R4\nare generated as normalized random Gaussian, and label is generated by target function\nF∗(x) = (sin(3 x1) + sin(3x2) + sin(3x3)− 2)2· cos(7x4). We use N training samples,\nand SGD with mini-batch size 50 and best tune learning rates and weight decay parameters. See\nAppendix 7 for our experiment setup, how we choose such target function, and more experiments.\n2 Notations\nσ(·) denotes the ReLU function σ(x) = max{x, 0}. Given f : R→ R and a vectorx∈ Rm,f(x)\ndenotesf(x) = (f(x1),...,f (xm)). For a vector w,∥w∥p denote its p-th norm, and when clear\nfrom the context, abbreviate∥w∥ =∥w∥2. For a matrix W ∈ Rm×d, use Wi or sometimes wi to\ndenote thei-th row ofW . The rowℓp norm is∥W∥2,p :=\n(∑\ni∈[m]∥Wi∥p\n2\n)1/p\n, the spectral norm is\n∥W∥2, and the Frobenius norm is∥W∥F =∥W∥2,2. We sayf : Rd→ R isL-Lipschitz continuous\nif|f(x)−f(y)|≤ L∥x−y∥2; isL-Lipschitz smooth if∥∇f(x)−∇f(y)∥2≤L∥x−y∥2.\nFunction complexity. The following notion measures the complexity of any smooth activation\nfunctionφ(z). Supposeφ(z) = ∑∞\ni=0cizi. Given a non-negativeR, the complexity\nCε(φ,R ) := ∑∞\ni=0\n(\n(C∗R)i +\n(√\nlog(1/ε)√\ni C∗R\n)i)\n|ci|, Cs(φ,R ) :=C∗ ∑∞\ni=0(i + 1)1.75Ri|ci|\nwhereC∗ is a sufﬁciently large constant (e.g.,104). Intuitively, Cs measures the sample complexity:\nhow many samples are required to learn φ correctly; while Cε bounds the network size: how much\nover-parameterization is needed for the algorithm to efﬁciently learn φ up to ε error. It is always\ntrue that Cs(φ,R )≤ Cε(φ,R )≤ Cs(φ,O (R))× poly(1/ε).4 While for sinz, exp(z) or low degree\npolynomials, Cs(φ,O (R)) and Cε(φ,R ) only differ byo(1/ε).\nExample 2.1. Ifφ(z) = ec·z− 1, φ(z) = sin(c·z), φ(z) = cos(c·z) for constant c orφ(z) is\nlow degree polynomial, then Cε(φ, 1) = o(1/ε) and Cs(φ, 1) = O(1). If φ(z) = sigmoid(z) or\ntanh(z), we can truncate their Taylor series at degree Θ(log 1\nε) to get ε approximation. One can\nverify this gives Cε(φ, 1)≤ poly(1/ε) and Cs(φ, 1)≤O(1).\n3 Result for Two-Layer Networks\nWe consider learning some unknown distributionD of data points z = (x,y )∈ Rd×Y , where x\nis the input point and y is the label. Without loss of generality, assume ∥x∥2 = 1 andxd = 1\n2.5\nConsider a loss function L: Rk× R→Y such that for every y∈Y , the function L(·,y ) is non-\nnegative, convex, 1-Lipschitz continuous and 1-Lipschitz smooth andL(0,y )∈ [0, 1]. This includes\nboth the cross-entropy loss and theℓ2-regression loss (for boundedY).\n4Recall\n(√\nlog(1/ε)√\ni C∗)i\n≤eO(log(1/ε)) = 1\npoly(ε) for everyi≥ 1.\n5 1\n2 can always be padded to the last coordinate, and ∥x∥2 = 1 can always be ensured from∥x∥2≤ 1 by\npadding\n√\n1−∥x∥2\n2. This assumption is for simplifying the presentation.\n4\nConcept class and target functionF∗(x). Consider target functionsF∗ : Rd→ Rk of\nF∗ = (f∗\n1,...,f ∗\nk ) and f∗\nr (x) =\np∑\ni=1\na∗\nr,iφi(⟨w∗\n1,i,x⟩)⟨w∗\n2,i,x⟩ (3.1)\nwhere eachφi : R→ R is inﬁnite-order smooth and the weightsw∗\n1,i∈ Rd,w∗\n2,i∈ Rd anda∗\nr,i∈ R.\nWe assume for simplicity∥w∗\n1,i∥2 =∥w∗\n2,i∥2 = 1 and|a∗\nr,i|≤ 1.6 We denote by\nCε(φ,R ) := maxj∈[p]{Cε(φj,R )} and Cs(φ,R ) := maxj∈[p]{Cs(φj,R )}\nthe complexity ofF∗ and assume they are bounded.\nIn the agnostic PAC-learning language, our concept class consists of all functions F∗ in the form\nof (3.1) with complexity bounded by threshold C and parameter p bounded by threshold p0. Let\nOPT = E[L(F∗(x),y )] be the population risk achieved by the best target function in this concept\nclass. Then, our goal is to learn this concept class with population risk OPT +ε using sample and\ntime complexity polynomial inC,p0 and 1/ε. In the remainder of this paper, to simplify notations,\nwe do not explicitly deﬁne this concept class parameterized by C andp. Instead, we equivalently\nstate our theorem with respect to any (unknown) target functionF∗ with speciﬁc parametersC and\np satisfying OPT = E[L(F∗(x),y )]. We assume OPT∈ [0, 1] for simplicity.\nRemark. Standard two-layer networksf∗\nr (x) = ∑p\ni=1a∗\nr,iφ(⟨w∗\n1,i,x⟩) are special cases of (3.1) (by\nsettingw∗\n2,i = (0,..., 0, 1) andφi =φ). Our formulation (3.1) additionally captures combinations\nof correlations between non-linear and linear measurements of different directions ofx.\nLearner networkF (x;W ). Using a data setZ ={z1,...,z N}ofN i.i.d. samples fromD, we\ntrain a networkF = (f1,··· ,fk): Rd→ Rk with\nfr(x) :=\nm∑\ni=1\nar,iσ(⟨wi,x⟩ +bi) =a⊤\nrσ(Wx +b) (3.2)\nwhereσ is the ReLU activation,W = (w1,...,w m)∈ Rm×d is the hidden weight matrix,b∈ Rm\nis the bias vector, andar∈ Rm is the output weight vector. To simplify analysis, we only updateW\nand keepb andar at initialization values. For such reason, we write the learner network asfr(x;W )\nandF (x;W ). We sometimes useb(0) =b anda(0)\nr =ar to emphasize they are randomly initialized.\nOur goal is to learn a weight matrixW with population risk E\n[\nL(F (x;W ),y )\n]\n≤ OPT +ε.\nLearning Process. LetW (0) denote the initial value of the hidden weight matrix, and letW (0)+Wt\ndenote the value at time t. (Note that Wt is the matrix of increments.) The weights are initialized\nwith Gaussians and thenW is updated by the vanilla SGD. More precisely,\n• entries ofW (0) andb(0) are i.i.d. random Gaussians fromN (0, 1/m),\n• entries of eacha(0)\nr are i.i.d. random Gaussians fromN (0,ε 2\na) for some ﬁxedεa∈ (0, 1].7\nAt timet, SGD samplesz = (x,y )∼Z and updatesWt+1 =Wt−η∇L(F (x;W (0) +Wt),y ).\n3.1 Main Theorem\nFor notation simplicity, with high probability (or w.h.p.) means with probability1−e−c log2m for a\nsufﬁciently large constantc, and ˜O hides factors of polylog(m).\nTheorem 1 (two-layer). For everyε∈\n(\n0, 1\npkCs(φ,1)\n)\n, there exists\nM0 = poly(Cε(φ, 1), 1/ε) and N0 = poly(Cs(φ, 1), 1/ε)\nsuch that for every m≥ M0 and everyN≥ ˜Ω(N0), choosingεa = ε/ ˜Θ(1) for the initialization,\nchoosing learning rateη = ˜Θ\n( 1\nεkm\n)\nand\nT = ˜Θ\n((Cs(φ, 1))2·k3p2\nε2\n)\n,\n6For general∥w∗\n1,i∥2 ≤ B,∥w∗\n2,i∥2 ≤ B,|a∗\nr,i| ≤B, the scaling factor B can be absorbed into the\nactivation functionφ′(x) =φ(Bx). Our results then hold by replacing the complexity ofφ withφ′.\n7We shall chooseεa =˜Θ(ε) in the proof due to technical reason. As we shall see in the three-layer case, if\nweight decay is used, one can relax this toεa = 1.\n5\nwith high probability over the random initialization, SGD afterT iteration satisﬁes\nEsgd\n[\n1\nT\n∑T−1\nt=0 E(x,y)∼DL(F (x;W (0) +Wt),y )\n]\n≤ OPT +ε.\nExample 3.1. For functions such asφ(z) =ez, sinz, sigmoid(z), tanh(z) or low degree polynomi-\nals, using Example 2.1, our theorem indicates that for target networks with such activation functions,\nwe can learn them using two-layer ReLU networks with\nsizem = poly(k,p )\npoly(ε) and sample complexity min{N,T}= poly(k,p, logm)\nε2\nWe note sample complexityT is (almost) independent ofm, the amount of overparametrization.\n3.2 Our Interpretations\nOverparameterization improves generalization. By increasing m, Theorem 1 supports more\ntarget functions with possibly larger size, more complex activations, and smaller population risk\nOPT. In other words, when m is ﬁxed, among the class of target functions whose complexities\nare captured by m, SGD can learn the best function approximator of the data, with the smallest\npopulation risk. This gives intuition how overparameterization improves test error, see Figure 1(a).\nLarge margin non-linear classiﬁer. Theorem 1 is a nonlinear analogue of the margin theory for\nlinear classiﬁers. The target function with a small population risk (and of bounded norm) can be\nviewed as a “large margin non-linear classiﬁer.” In this view, Theorem 1 shows that assuming the\nexistence of such large-margin classiﬁer, SGD ﬁnds a good solution with sample complexity mostly\ndetermined by the margin, instead of the dimension of the data.\nInductive bias. Recent works (e.g., [4, 32]) show that when the network is heavily overparame-\nterized (that is,m is polynomial in the number of training samples) and no two training samples are\nidentical, then SGD can ﬁnd a global optimum with 0 classiﬁcation error (or ﬁnd a solution with\nε training loss) in polynomial time. This does not come with generalization, since it can even ﬁt\nrandom labels. Our theorem, combined with [4], conﬁrms the inductive bias of SGD for two-layer\nnetworks: when the labels are random, SGD ﬁnds a network that memorizes the training data; when\nthe labels are (even only approximately) realizable by some target network, then SGD learns and\ngeneralizes. This gives an explanation towards the well-known empirical observations of such in-\nductive bias (e.g., [53]) in the two-layer setting, and is more general than Brutzkus et al. [14] in\nwhich the target network is only linear.\n4 Result for Three-Layer Networks\nConcept class and target functionF∗(x). This time we consider more powerful target functions\nF∗ = (f∗\n1,··· ,f∗\nk ) of the form\nf∗\nr (x) :=\n∑\ni∈[p1]\na∗\nr,iΦi\n\n ∑\nj∈[p2]\nv∗\n1,i,jφ1,j(⟨w∗\n1,j,x⟩)\n\n\n\n ∑\nj∈[p2]\nv∗\n2,i,jφ2,j(⟨w∗\n2,j,x⟩)\n\n (4.1)\nwhere each φ1,j,φ 2,j, Φi : R → R is inﬁnite-order smooth, and the weights w∗\n1,i,w∗\n2,i ∈ Rd,\nv∗\n1,i,v∗\n2,i∈ Rp2 anda∗\nr,i∈ R satisfy∥w∗\n1,j∥2 =∥w∗\n2,j∥2 =∥v∗\n1,i∥2 =∥v∗\n2,i∥2 = 1 and|a∗\nr,i|≤ 1.\nLet\nCε(φ,R ) = maxj∈[p2],s∈[1,2]{Cε(φs,j,R )}, Cε(Φ,R ) = maxj∈[p1]{Cε(Φj,R )}\nCs(φ,R ) = maxj∈[p2],s∈[1,2]{Cs(φs,j,R )}, Cs(Φ,R ) = maxj∈[p1]{Cs(Φj,R )}\nto denote the complexity of the two layers, and assume they are bounded.\nOur concept class contains measures of correlations between composite non-linear functions and\nnon-linear functions of the input, there are plenty of functions in this new concept class that may not\nnecessarily have small-complexity representation in the previous formulation (3.1), and as we shall\nsee in Figure 1(a), this is the critical advantage of using three-layer networks compared to two-\nlayer ones or their NTKs. The learnability of this correlation is due to the non-convex interactions\nbetween hidden layers. As a comparison, [15] studies the regime where the changes in hidden layers\nare negligible thus can not show how to learn this concept class with a three-layer network.\n6\nRemark 4.1. Standard three-layer networks\nf∗\nr (x) = ∑\ni∈[p1]a∗\nr,iΦi\n(∑\nj∈[p2]v∗\ni,jφj(⟨w∗\nj,x⟩)\n)\nare only special cases of (4.1). Also, even in the special case of Φi(z) =z, the target\nf∗\nr (x) = ∑\ni∈[p1]a∗\nr,i\n(∑\nj∈[p2]v∗\n1,i,jφ1(⟨w∗\n1,j,x⟩)\n) (∑\nj∈[p2]v∗\n2,i,jφ2(⟨w∗\n2,j,x⟩)\n)\ncaptures combinations of correlations of non-linear measurements in different directions ofx.\nLearner networkF (x;W,V ). Our learners are three-layer networksF = (f1,...,f k) with\nfr(x) =\n∑\ni∈[m2]\nar,iσ(ni(x) +b2,i) where eachni(x) =\n∑\nj∈[m1]\nvi,jσ (⟨wj,x⟩ +b1,j)\nThe ﬁrst and second layers havem1 andm2 hidden neurons. Let W∈ Rm1×d andV ∈ Rm2×m1\nrepresent the weights of the ﬁrst and second hidden layers respectively, andb1∈ Rm1 andb2∈ Rm2\nrepresent the corresponding bias vectors,ar∈ Rm2 represent the output weight vector.\n4.1 Learning Process\nAgain for simplicity, we only updateW andV . The weights are randomly initialized as:\n• entries ofW (0) andb1 =b(0)\n1 are i.i.d. fromN (0, 1/m1),\n• entries ofV (0) andb2 =b(0)\n2 are i.i.d. fromN (0, 1/m2),\n• entries of eachar =a(0)\nr are i.i.d. fromN (0,ε 2\na) forεa = 1.\nAs for the optimization algorithm, we use SGD with weight decay and an explicit regularizer.\nFor some λ∈ (0, 1], we will use λF (x;W,V ) as the learner network, i.e., linearly scale F down\nbyλ. This is equivalent to replacing W , V with\n√\nλW ,\n√\nλV , since a ReLU network is positive\nhomogenous. The SGD will start withλ = 1 and slowly decrease it, similar to weight decay.8\nWe also use an explicit regularizer for someλw,λv > 0 with9\nR(\n√\nλW,\n√\nλV ) :=λv∥\n√\nλV∥2\nF +λw∥\n√\nλW∥4\n2,4 .\nNow, in each round t = 1, 2,...,T , we use (noisy) SGD to minimize the following stochastic\nobjective for some ﬁxedλt−1:\nL2(λt−1;W′,V′) :=L\n(\nλt−1F\n(\nx;W (0) +Wρ + ΣW′,V (0) +Vρ +V′Σ\n))\n+R(\n√\nλt−1W′,\n√\nλt−1V′) (4.2)\nAbove, the objective is stochastic because (1) z∼Z is a random sample from the training set, (2)\nWρ andVρ are two small perturbation random matrices with entries i.i.d. drawn from N (0,σ 2\nw)\nandN (0,σ 2\nv) respectively, and (3) Σ∈ Rm1×m1 is a random diagonal matrix with diagonals i.i.d.\nuniformly drawn from {+1,−1}. We note that the use of Wρ and Vρ is standard for Gaussian\nsmoothing on the objective (and not needed in practice). 10 The use of Σ may be reminiscent of the\nDropout technique [46] in practice which randomly masks out neurons, and can also be removed.11\n8We illustrate the technical necessity of adding weight decay. During training, it is easy to add new infor-\nmation to the current network, but hard to forget “false” information that is already in the network. Such false\ninformation can be accumulated from randomness of SGD, non-convex landscapes, and so on. Thus, by scaling\ndown the network we can effectively forget false information.\n9This∥·∥ 2,4 norm on W encourages weights to be more evenly distributed across neurons. It can be\nreplaced with∥√λt−1Wt−1∥2+α\n2,2+α for any constantα >0 for our theoretical purpose. We choose α = 2 for\nsimplicity, and observe that in practice, weights are automatically spread out due to data randomness, so this\nexplicit regularization may not be needed. See Section 7.1 for an experiment.\n10Similar to known non-convex literature [19] or smooth analysis, we introduce Gaussian perturbation Wρ\nandVρ for theoretical purpose and it is not needed in practice. Also, we apply noisy SGD which is the vanilla\nSGD plus Gaussian perturbation, which again is needed in theory but believed unnecessary for practice [19].\n11In the full paper we study two variants of SGD. This present version is the “second variant,” and the ﬁrst\nvariantL1(λt−1;W′,V′) is the same as (4.2) by removing Σ. Due to technical difﬁculty, the best sample\ncomplexity we can prove forL1 is a bit higher.\n7\nAlgorithm 1 SGD for three-layer networks (second variant (4.2))\nInput: Data setZ, initializationW (0),V (0), step sizeη, number of inner stepsTw,σw,σv,λw,λv.\n1: W0 = 0,V 0 = 0,λ 1 = 1,T = Θ\n(\nη−1 log log(m1m2)\nε0\n)\n.\n2: fort = 1, 2,...,T do\n3: Apply noisy SGD with step size η on the stochastic objectiveL2(λt−1;W,V ) forTw steps; the starting\npoint isW =Wt−1,V =Vt−1 and suppose it reachesWt,Vt. ⋄ see Lemma A.9\n4: λt+1 = (1−η)λt. ⋄ weight decay\n5: end for\n6: Randomly sample ˆΣ with diagonal entries i.i.d. uniform on{1,−1}\n7: Randomly sample ˜Θ(1/ε2\n0) many noise matrices\n{\nWρ,j,V ρ,j}\n. Let\nj∗ = arg minj\n{\nEz∈ZL\n(\nλTF\n(\nx;W (0) +Wρ,j +ˆΣWT,V (0) +Vρ,j +VTˆΣ\n))}\n8: Output W (out)\nT =W (0) +Wρ,j∗\n+ˆΣWT ,V (out)\nT =V (0) +Vρ,j∗\n+VTˆΣ.\nAlgorithm 1 presents the details. Speciﬁcally, in each round t, Algorithm 1 starts with weight ma-\ntricesWt−1,Vt−1 and performs Tw iterations. In each iteration it goes in the negative direction of\nthe stochastic gradient∇W′,V′L2(λt;W′,V′). Let the ﬁnal matrices be Wt,Vt. At the end of this\nroundt, Algorithm 1 performs weight decay by settingλt = (1−η)λt−1 for someη >0.\n4.2 Main Theorems\nFor notation simplicity, with high probability (or w.h.p.) means with probability1−e−c log2(m1m2)\nand ˜O hides factors of polylog(m1,m 2).\nTheorem 2 (three-layer, second variant). Consider Algorithm 1. For every constantγ∈ (0, 1/4],\neveryε0∈ (0, 1/100], everyε = ε0\nkp1p2\n2Cs(Φ,p2Cs(φ,1))Cs(φ,1)2 , there exists\nM = poly\n(\nCε(Φ,√p2Cε(φ, 1)), 1\nε\n)\nsuch that for everym2 =m1 =m≥M, and properly setλw,λv,σw,σv in Table 1, as long as\nN≥ ˜Ω\n((Cε(Φ,√p2Cε(φ, 1))· Cε(φ, 1)·√p2p1k2\nε0\n)2)\nthere is a choiceη = 1/poly(m1,m 2) andT = poly(m1,m 2) such that with probability≥ 99/100,\nE(x,y)∼DL(λTF (x;W (out)\nT ,V (out)\nT ),y )≤ (1 +γ)OPT +ε0.\n4.3 Our Contributions\nOur sample complexity N scales polynomially with the complexity of the target network, and is\n(almost) independent ofm, the amount of overparameterization. This itself can be quite surprising,\nbecause recent results on neural network generalization [6, 11, 24, 39] require N to be polynomial\nin m. Furthermore, Theorem 2 shows three-layer networks can efﬁciently learn a bigger concept\nclass (4.1) comparing to what we know about two-layer networks (3.1).\nFrom a practical standpoint, one can construct target functions of the form (4.1) that cannot be\n(efﬁciently) approximated by any two-layer target function in (3.1). If data is generated according\nto such functions, then it may be necessary to use three-layer networks as learners (see Figure 1).\nFrom a theoretical standpoint, even in the special case of Φ(z) = z, our target function can cap-\nture correlations between non-linear measurements of the data (recall Remark 4.1). This means\nCε(Φ, Cε(φ, 1)√p2) ≈ O(√p2Cε(φ, 1)), so learning it is essentially in the same complexity as\nlearning eachφs,j. For example, a three-layer network can learn cos(100⟨w∗\n1,x⟩)·e100⟨w∗\n2,x⟩ up to\naccuracyε in complexity poly(1/ε), while it is unclear how to do so using two-layer networks.\nTechnical Contributions. We highlight some technical contributions in the proof of Theorem 2.\nIn recent results on the training convergence of neural networks for more than two layers [3, 4], the\noptimization process stays in a close neighborhood of the initialization so that, with heavy overpa-\nrameterization, the network becomes “linearized” and the interactions across layers are negligible.\nIn our three-layer case, this means that the matrix W never interacts withV . They then argue that\n8\nSGD simulates a neural tangent kernel so the learning process is almost convex [27]. In our analysis,\nwe directly tackle non-convex interactions betweenW andV , by studying a “quadratic approxima-\ntion” of the network. (See Remark 6.1 for a mathematical comparison.) Our new proofs techniques\nthat could be useful for future theoretical applications.\nAlso, for the results [3, 4] and our two-layer Theorem 1 to hold, it sufﬁces to analyze a regime where\nthe “sign pattern” of ReLUs can be replaced with that of the random initialization. (Recall σ(x) =\nIx≥0·x and we call Ix≥0 the “sign pattern.”) In our three-layer analysis, the optimization process\nhas moved sufﬁciently away from initialization , so that the sign pattern change can signiﬁcantly\naffect output. This brings in additional technical challenge because we have to tackle non-convex\ninteractions betweenW andV together with changing sign patterns.12\nComparison to Daniely [15]. Daniely [15] studies the learnability of multi-layer networks when\n(essentially) only the output layer is trained, which reduces to a convex task. He shows that multi-\nlayer networks can learn a compositional kernel space, which implies two/three-layer networks can\nefﬁciently learn low-degree polynomials. He did not derive the general sample/time complexity\nbounds for more complex functions such as those in our concept classes (3.1) and (4.1), but showed\nthat they are ﬁnite.\nIn contrast, our learnability result of concept class (4.1) is due to thenon-convex interaction between\nhidden layers. Since Daniely [15] studies the regime when the changes in hidden layers are negli-\ngible, if three layer networks are used, to the best of our knowledge, their theorem cannot lead to\nsimilar sample complexity bounds comparing to Theorem 2 by only training the last layer of a three-\nlayer network. Empirically, one can also observe that training hidden layers is better than training\nthe last layer (see Figure 1).\n5 Conclusion and Discussion\nWe show by training the hidden layers of two-layer (resp. three-layer) overparameterized neu-\nral networks, one can efﬁciently learn some important concept classes including two-layer (resp.\nthree-layer) networks equipped with smooth activation functions. Our result is in the agnostic PAC-\nlearning language thus is distribution-free. We believe our work opens up a new direction in both\nalgorithmic and generalization perspectives of overparameterized neural networks, and pushing for-\nward can possibly lead to more understanding about deep learning.\nOur results apply to other more structured neural networks. As a concrete example, consider con-\nvolutional neural networks (CNN). Suppose the input is a two dimensional matrixx∈ Rd×s which\ncan be viewed as d-dimensional vectors in s channels, then a convolutional layer on top of x is\ndeﬁned as follows. There are d′ ﬁxed subsets{S1,S 2,...,S d′}of [d] each of size k′. The output\nof the convolution layer is a matrix of size d′×m, whose (i,j )-th entry is φ(⟨wj,xSi⟩), where\nxSi∈ Rk′×s is the submatrix ofx with rows indexed bySi;wj∈ Rk′×s is the weight matrix of the\nj-th channel; andφ is the activation function. Overparameterization then means a larger number of\nchannelsm in our learned network comparing to the target. Our analysis can be adapted to show a\nsimilar result for this type of networks.\nOne can also combine this paper with properties of recurrent neural networks (RNNs) [3] to derive\nPAC-learning results for RNNs [2], or use the existential tools of this paper to derive PAC-learning\nresults for three-layer residual networks (ResNet) [1]. The latter gives a provable separation between\nneural networks and kernels in the efﬁcient PAC-learning regime.\nAcknowledgements\nThis work was supported in part by FA9550-18-1-0166. Y . Liang would also like to acknowl-\nedge that support for this research was provided by the Ofﬁce of the Vice Chancellor for Research\nand Graduate Education at the University of Wisconsin-Madison with funding from the Wisconsin\nAlumni Research Foundation.\n12For instance, the number of sign changes can be m0.999 for the second hidden layer (see Lemma 6.5).\nIn this region, the network output can be affected by m0.499 since each neuron is of value roughly m−1/2.\nTherefore, if after training we replace the sign pattern with random initialization, the output will be meaningless.\n9\nReferences\n[1] Zeyuan Allen-Zhu and Yuanzhi Li. What Can ResNet Learn Efﬁciently, Going Beyond Ker-\nnels? In NeurIPS, 2019. Full version available at http://arxiv.org/abs/1905.10337.\n[2] Zeyuan Allen-Zhu and Yuanzhi Li. Can SGD Learn Recurrent Neural Networks with Provable\nGeneralization? In NeurIPS, 2019. Full version available athttp://arxiv.org/abs/1902.\n01028.\n[3] Zeyuan Allen-Zhu, Yuanzhi Li, and Zhao Song. On the convergence rate of training recurrent\nneural networks. In NeurIPS, 2019. Full version available athttp://arxiv.org/abs/1810.\n12065.\n[4] Zeyuan Allen-Zhu, Yuanzhi Li, and Zhao Song. A convergence theory for deep learning via\nover-parameterization. In ICML, 2019. Full version available at http://arxiv.org/abs/\n1811.03962.\n[5] Alexandr Andoni, Rina Panigrahy, Gregory Valiant, and Li Zhang. Learning polynomials with\nneural networks. In International Conference on Machine Learning, pages 1908–1916, 2014.\n[6] Sanjeev Arora, Rong Ge, Behnam Neyshabur, and Yi Zhang. Stronger generalization bounds\nfor deep nets via a compression approach. arXiv preprint arXiv:1802.05296, 2018.\n[7] Sanjeev Arora, Simon S Du, Wei Hu, Zhiyuan Li, Ruslan Salakhutdinov, and Ruosong Wang.\nOn exact computation with an inﬁnitely wide neural net. arXiv preprint arXiv:1904.11955 ,\n2019.\n[8] Sanjeev Arora, Simon S Du, Wei Hu, Zhiyuan Li, and Ruosong Wang. Fine-grained analysis\nof optimization and generalization for overparameterized two-layer neural networks. arXiv\npreprint arXiv:1901.08584, 2019.\n[9] Ainesh Bakshi, Rajesh Jayaram, and David P Woodruff. Learning two layer rectiﬁed neural\nnetworks in polynomial time. arXiv preprint arXiv:1811.01885, 2018.\n[10] Peter L. Bartlett and Shahar Mendelson. Rademacher and gaussian complexities: Risk bounds\nand structural results. Journal of Machine Learning Research, 3(Nov):463–482, 2002.\n[11] Peter L Bartlett, Dylan J Foster, and Matus J Telgarsky. Spectrally-normalized margin bounds\nfor neural networks. InAdvances in Neural Information Processing Systems, pages 6241–6250,\n2017.\n[12] Digvijay Boob and Guanghui Lan. Theoretical properties of the global optimizer of two layer\nneural network. arXiv preprint arXiv:1710.11241, 2017.\n[13] Alon Brutzkus and Amir Globerson. Globally optimal gradient descent for a convnet with\ngaussian inputs. arXiv preprint arXiv:1702.07966, 2017.\n[14] Alon Brutzkus, Amir Globerson, Eran Malach, and Shai Shalev-Shwartz. Sgd learns over-\nparameterized networks that provably generalize on linearly separable data. In International\nConference on Learning Representations, 2018.\n[15] Amit Daniely. Sgd learns the conjugate kernel class of the network. In Advances in Neural\nInformation Processing Systems, pages 2422–2430, 2017.\n[16] Amit Daniely, Roy Frostig, and Yoram Singer. Toward deeper understanding of neural net-\nworks: The power of initialization and a dual view on expressivity. In Advances in Neural\nInformation Processing Systems (NIPS), pages 2253–2261, 2016.\n[17] Simon S Du, Xiyu Zhai, Barnabas Poczos, and Aarti Singh. Gradient descent provably opti-\nmizes over-parameterized neural networks. arXiv preprint arXiv:1810.02054, 2018.\n[18] Ronen Eldan, Dan Mikulincer, and Alex Zhai. The clt in high dimensions: quantitative bounds\nvia martingale embedding. arXiv preprint arXiv:1806.09087, 2018.\n10\n[19] Rong Ge, Furong Huang, Chi Jin, and Yang Yuan. Escaping from saddle pointsonline stochas-\ntic gradient for tensor decomposition. In Conference on Learning Theory , pages 797–842,\n2015.\n[20] Rong Ge, Jason D Lee, and Tengyu Ma. Learning one-hidden-layer neural networks with\nlandscape design. arXiv preprint arXiv:1711.00501, 2017.\n[21] Rong Ge, Rohith Kuditipudi, Zhize Li, and Xiang Wang. Learning two-layer neural networks\nwith symmetric inputs. In International Conference on Learning Representations, 2019.\n[22] Surbhi Goel and Adam Klivans. Learning neural networks with two nonlinear layers in poly-\nnomial time. arXiv preprint arXiv:1709.06010v4, 2018.\n[23] Surbhi Goel, Varun Kanade, Adam Klivans, and Justin Thaler. Reliably learning the relu in\npolynomial time. In Conference on Learning Theory, pages 1004–1042, 2017.\n[24] Noah Golowich, Alexander Rakhlin, and Ohad Shamir. Size-independent sample complexity\nof neural networks. In Proceedings of the Conference on Learning Theory, 2018.\n[25] Alex Graves, Abdel-rahman Mohamed, and Geoffrey Hinton. Speech recognition with deep\nrecurrent neural networks. In Acoustics, speech and signal processing (icassp), 2013 ieee\ninternational conference on, pages 6645–6649. IEEE, 2013.\n[26] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image\nrecognition. In Proceedings of the IEEE conference on computer vision and pattern recogni-\ntion, pages 770–778, 2016.\n[27] Arthur Jacot, Franck Gabriel, and Cl ´ement Hongler. Neural tangent kernel: Convergence\nand generalization in neural networks. In Advances in neural information processing systems,\npages 8571–8580, 2018.\n[28] Kenji Kawaguchi. Deep learning without poor local minima. In Advances in Neural Informa-\ntion Processing Systems, pages 586–594, 2016.\n[29] Robert Kleinberg, Yuanzhi Li, and Yang Yuan. An alternative view: When does sgd escape\nlocal minima? arXiv preprint arXiv:1802.06175, 2018.\n[30] Alex Krizhevsky, Ilya Sutskever, and Geoffrey E Hinton. Imagenet classiﬁcation with deep\nconvolutional neural networks. In Advances in neural information processing systems , pages\n1097–1105, 2012.\n[31] Jaehoon Lee, Lechao Xiao, Samuel S Schoenholz, Yasaman Bahri, Jascha Sohl-Dickstein, and\nJeffrey Pennington. Wide neural networks of any depth evolve as linear models under gradient\ndescent. arXiv preprint arXiv:1902.06720, 2019.\n[32] Yuanzhi Li and Yingyu Liang. Learning overparameterized neural networks via stochastic\ngradient descent on structured data. In Advances in Neural Information Processing Systems ,\n2018.\n[33] Yuanzhi Li and Yang Yuan. Convergence analysis of two-layer neural networks with relu\nactivation. In Advances in Neural Information Processing Systems, pages 597–607, 2017.\n[34] Yuanzhi Li, Tengyu Ma, and Hongyang Zhang. Algorithmic regularization in over-\nparameterized matrix recovery. arXiv preprint arXiv:1712.09203, 2017.\n[35] Percy Liang. CS229T/STAT231: Statistical Learning Theory (Winter 2016). https://web.\nstanford.edu/class/cs229t/notes.pdf, April 2016. accessed January 2019.\n[36] Roi Livni, Shai Shalev-Shwartz, and Ohad Shamir. On the computational efﬁciency of training\nneural networks. In Advances in Neural Information Processing Systems , pages 855–863,\n2014.\n[37] Martin J. Wainwright. Basic tail and concentration bounds. https://www.stat.berkeley.\nedu/~mjwain/stat210b/Chap2_TailBounds_Jan22_2015.pdf, 2015. Online; accessed\nOct 2018.\n11\n[38] Andreas Maurer. A vector-contraction inequality for rademacher complexities. InInternational\nConference on Algorithmic Learning Theory, pages 3–17. Springer, 2016.\n[39] Behnam Neyshabur, Srinadh Bhojanapalli, David McAllester, and Nathan Srebro. A pac-\nbayesian approach to spectrally-normalized margin bounds for neural networks.arXiv preprint\narXiv:1707.09564, 2017.\n[40] Ohad Shamir. Distribution-speciﬁc hardness of learning neural networks. Journal of Machine\nLearning Research, 19(32), 2018.\n[41] David Silver, Aja Huang, Chris J Maddison, Arthur Guez, Laurent Sifre, George Van\nDen Driessche, Julian Schrittwieser, Ioannis Antonoglou, Veda Panneershelvam, Marc Lanc-\ntot, et al. Mastering the game of go with deep neural networks and tree search. nature, 529\n(7587):484, 2016.\n[42] Mahdi Soltanolkotabi, Adel Javanmard, and Jason D Lee. Theoretical insights into the\noptimization landscape of over-parameterized shallow neural networks. arXiv preprint\narXiv:1707.04926, 2017.\n[43] Daniel Soudry and Yair Carmon. No bad local minima: Data independent training error guar-\nantees for multilayer neural networks. arXiv preprint arXiv:1605.08361, 2016.\n[44] Daniel Spielman and Shang-Hua Teng. Smoothed analysis of algorithms: Why the simplex\nalgorithm usually takes polynomial time. In Proceedings of the thirty-third annual ACM sym-\nposium on Theory of computing, pages 296–305. ACM, 2001.\n[45] Karthik Sridharan. Machine Learning Theory (CS 6783). http://www.cs.cornell.edu/\ncourses/cs6783/2014fa/lec7.pdf, 2014. accessed January 2019.\n[46] Nitish Srivastava, Geoffrey Hinton, Alex Krizhevsky, Ilya Sutskever, and Ruslan Salakhut-\ndinov. Dropout: a simple way to prevent neural networks from overﬁtting. The Journal of\nMachine Learning Research, 15(1):1929–1958, 2014.\n[47] Yuandong Tian. An analytical formula of population gradient for two-layered relu network and\nits applications in convergence and critical point analysis. arXiv preprint arXiv:1703.00560,\n2017.\n[48] Leslie Valiant. A theory of the learnable. Communications of the ACM, 1984.\n[49] Santosh Vempala and John Wilmes. Polynomial convergence of gradient descent for training\none-hidden-layer neural networks. arXiv preprint arXiv:1805.02677, 2018.\n[50] Bo Xie, Yingyu Liang, and Le Song. Diversity leads to generalization in neural networks.\narXiv preprint Arxiv:1611.03131, 2016.\n[51] Greg Yang. Scaling limits of wide neural networks with weight sharing: Gaussian pro-\ncess behavior, gradient independence, and neural tangent kernel derivation. arXiv preprint\narXiv:1902.04760, 2019.\n[52] Alex Zhai. A high-dimensional CLT inW2 distance with near optimal convergence rate.Prob-\nability Theory and Related Fields, 170(3-4):821–845, 2018.\n[53] Chiyuan Zhang, Samy Bengio, Moritz Hardt, Benjamin Recht, and Oriol Vinyals. Understand-\ning deep learning requires rethinking generalization. In ICLR, 2017. arXiv 1611.03530.\n[54] Yuchen Zhang, Jason D Lee, and Michael I Jordan. l1-regularized neural networks are improp-\nerly learnable in polynomial time. In International Conference on Machine Learning , pages\n993–1001, 2016.\n[55] Yuchen Zhang, Jason Lee, Martin Wainwright, and Michael Jordan. On the learnability of\nfully-connected neural networks. In Artiﬁcial Intelligence and Statistics, pages 83–91, 2017.\n[56] Kai Zhong, Zhao Song, Prateek Jain, Peter L Bartlett, and Inderjit S Dhillon. Recovery guar-\nantees for one-hidden-layer neural networks. arXiv preprint arXiv:1706.03175, 2017.\n12",
  "values": {
    "Interpretable (to users)": "No",
    "Not socially biased": "No",
    "Critiqability": "No",
    "Explicability": "No",
    "Transparent (to users)": "No",
    "Non-maleficence": "No",
    "Collective influence": "No",
    "User influence": "No",
    "Deferral to humans": "No",
    "Respect for Law and public interest": "No",
    "Respect for Persons": "No",
    "Autonomy (power to decide)": "No",
    "Beneficence": "No",
    "Fairness": "No",
    "Privacy": "No",
    "Justice": "No"
  }
}