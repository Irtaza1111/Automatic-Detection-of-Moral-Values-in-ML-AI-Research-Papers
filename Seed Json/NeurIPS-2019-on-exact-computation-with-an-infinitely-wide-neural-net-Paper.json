{
  "pdf": "NeurIPS-2019-on-exact-computation-with-an-infinitely-wide-neural-net-Paper",
  "title": "On Exact Computation with an Infinitely Wide Neural Net",
  "author": "Sanjeev Arora, Simon S. Du, Wei Hu, Zhiyuan Li, Russ R. Salakhutdinov, Ruosong Wang",
  "paper_id": "NeurIPS-2019-on-exact-computation-with-an-infinitely-wide-neural-net-Paper",
  "text": "On Exact Computation with an Inﬁnitely Wide\nNeural Net⇤\nSanjeev Arora† Simon S. Du ‡ Wei Hu§ Zhiyuan Li¶\nRuslan Salakhutdinovk Ruosong Wang⇤⇤\nAbstract\nHow well does a classic deep net architecture like AlexNet or VGG19 classify on a\nstandard dataset such as CIFAR-10 when its “width”— namely, number of channels\nin convolutional layers, and number of nodes in fully-connected internal layers —\nis allowed to increase to inﬁnity? Such questions have come to the forefront in the\nquest to theoretically understand deep learning and its mysteries about optimization\nand generalization. They also connect deep learning to notions such as Gaussian\nprocesses and kernels. A recent paper [ Jacot et al. , 2018] introduced the Neural\nTangent Kernel (NTK) which captures the behavior of fully-connected deep nets in\nthe inﬁnite width limit trained by gradient descent; this object was implicit in some\nother recent papers. An attraction of such ideas is that a pure kernel-based method\nis used to capture the power of a fully-trained deep net of inﬁnite width.\nThe current paper gives the ﬁrst efﬁcient exact algorithm for computing the ex-\ntension of NTK to convolutional neural nets, which we call Convolutional NTK\n(CNTK), as well as an efﬁcient GPU implementation of this algorithm. This results\nin a signiﬁcant new benchmark for performance of a pure kernel-based method on\nCIFAR-10, being 10% higher than the methods reported in [ Novak et al., 2019],\nand only 6% lower than the performance of the corresponding ﬁnite deep net\narchitecture (once batch normalization etc. are turned off). Theoretically, we also\ngive the ﬁrst non-asymptotic proof showing that a fully-trained sufﬁciently wide\nnet is indeed equivalent to the kernel regression predictor using NTK.\n1 Introduction\nHow well does a classic deep net architecture like AlexNet or VGG19 perform on a standard dataset\nsuch as CIFAR-10 when its “width”— namely, number of channels in convolutional layers, and\nnumber of nodes in fully-connected internal layers — is allowed to increase to inﬁnity? Questions\nabout these “inﬁnite limits” of deep nets have naturally emerged in the ongoing effort to understand\nthe power of deep learning. In mathematics it is often easier to study objects in the inﬁnite limit. Fur-\nthermore, the inﬁnite limit could conceivably make sense in deep learning, since over-parametrization\nseems to help optimization a lot and doesn’t hurt generalization much [ Zhang et al. , 2017]: deep\nneural nets with millions of parameters work well even for datasets with 50k training examples. So\nwhy not imagine nets whose width goes to inﬁnity?\n⇤The latest full version of this paper can be found at https://arxiv.org/abs/1904.11955.\n†Princeton University and Institute for Advanced Study. Email: arora@cs.princeton.edu\n‡Institute for Advanced Study. Email: ssdu@ias.edu\n§Princeton University. Email: huwei@cs.princeton.edu\n¶Princeton University. Email: zhiyuanli@cs.princeton.edu\nkCarnegie Mellon University. Email:rsalakhu@cs.cmu.edu\n⇤⇤Carnegie Mellon University. Email: ruosongw@andrew.cmu.edu\n33rd Conference on Neural Information Processing Systems (NeurIPS 2019), V ancouver, Canada.\nAllowing width to go to inﬁnity also connects deep learning in an interesting way with other areas of\nmachine learning. A single hidden-layer neural network with i.i.d. random parameters, in the limit\nof inﬁnite width, is a function drawn from a Gaussian process (GP) [Neal, 1996]. This model as\nwell as analogous ones with multiple layers [ Lee et al., 2018, Matthews et al., 2018], convolutional\nﬁlters [Novak et al., 2019, Garriga-Alonso et al. , 2019] and other architectures [ Y ang, 2019] make up\nthe GP view of deep learning. These correspond to inﬁnitely wide deep nets whose all parameters are\nchosen randomly (with careful scaling), and only the top (classiﬁcation) layer is optimized.\nFrom now on we will use weakly-trained nets to refer to nets whose layers receive random initial-\nization and only the top layer is trained by gradient descent. We use fully-trained to refer to nets\nwhose all parameters are trained by gradient descent. It has long been known that weakly-trained\nconvolutional nets have reasonable performance on MNIST and CIFAR-10. Weakly-trained nets that\nare fully-connected instead of convolutional, can also be thought of as “multi-layer random kitchen\nsinks,” which also have a long history.\nWeakly-trained nets — whether of ﬁnite or inﬁnite width — also deﬁne interesting kernels. Speciﬁ-\ncally, if f (✓, x) 2 R denotes the output of the network on input x where ✓ denotes the parameters in\nthe network, and W is an initialization distribution over ✓ (usually Gaussian), then training just the\ntop layer with an `2 loss is equivalent to kernel regression for the following kernel:\nker (x, x0)= E\n✓⇠W\n[f (✓, x) · f (✓, x0)], (1)\nwhere x, x0are two inputs. This kernel method makes sense when the width goes to inﬁnity.\nThe objects of interest in this paper are not weakly-trained nets, but fully-trained nets. In the ﬁnite\ncase, analysis of optimization and generalization of fully-trained nets is of course an open problem.\nOne may also ask:\nCan we understand the power of fully-trained nets whose width goes to inﬁnity?\nA priori this question doesn’t seem any easier than the ﬁnite case, and empirical evaluation seems\ncomputationally infeasible due to the inﬁnite limit. They also do not correspond to a kernel method\nin any obvious way.\nRecent papers suggest that neural nets whose width greatly exceeds the number of training data points\ncan rapidly reduce training error to 0 via gradient descent, and under some conditions, the trained\nnet also exhibits good generalization [ Du et al., 2019, 2018b, Li and Liang, 2018, Allen-Zhu et al.,\n2018a,b, Zou et al. , 2018, Arora et al. , 2019, Cao and Gu , 2019]. Extra-wideness plays a crucial\nrole in the proof: it is shown that as width increases, training causes increasingly smaller changes\n(in a proportionate sense) in the parameters. This raises the possibility that as one increases the\nwidth to inﬁnity, a certain limiting behavior can emerge even in the fully-trained net. A recent paper\nby Jacot et al. [2018] isolated a notion implicit in the above papers, which they called the Neural\nTangent Kernel (NTK). They suggested — via a proof that is slightly heuristic — that this ﬁxed kernel\ncharacterizes the behavior of fully-connected inﬁnite width neural networks whose layers have been\ntrained by gradient descent. The NTK is different from the Gaussian process kernels discussed earlier,\nand is deﬁned using the gradient of the output of the randomly initialized net with respect to its\nparameters, i.e.,\nker (x, x0)= E\n✓⇠W\n⌧ @f (✓, x)\n@✓ , @f (✓, x0)\n@✓\n\u0000\n. (2)\nHere, the gradient @f (✓,x)\n@✓ appears from considering gradient descent, as will be explained in Section 3.\nOne may also generalize the NTK to convolutional neural nets, and we call the corresponding kernel\nConvolutional Neural Tangent Kernel (CNTK).\nThough NTK and CNTK are deﬁned by an inﬁnite limit, a recent paper [ Lee et al., 2019] attempted\nto understand their properties via a ﬁnite approximation of the inﬁnite limit kernel by Monte Carlo\nmethods. However, as will be shown in Section B, using random features generated from practically\nsized nets can degrade the performance a lot. It was still open what is the full power of exact CNTK\non modern datasets. This is a challenging question especially for CNTK with pooling operations,\nsince when convolution with pooling is involved, it was believed that exact computation of kernels\n(for either convolutional Gaussian process kernel or CNTK) is infeasible for large datasets like\nCIFAR-10 [Novak et al., 2019].\n2\nOur contributions. We give an exact and efﬁcient dynamic programming algorithm to compute\nCNTKs for ReLU activation (namely, to compute ker (x, x0) given x and x0). Using this algorithm\n— as well as implementation tricks for GPUs — we can settle the question of the performance of\nfully-trained inﬁnitely wide nets with a variety of architectures. For instance, we ﬁnd that their\nperformance on CIFAR-10 is within 5% of the performance of the same architectures in the ﬁnite case\n(note that the proper comparison in the ﬁnite case involves turning off batch norm, data augmentation,\netc., in the optimization). In particular, the CNTK corresponding to a 11-layer convolutional net\nwith global average pooling achieves 77% classiﬁcation accuracy. This is 10% higher than the best\nreported performance of a Gaussian process with ﬁxed kernel on CIFAR-10 [ Novak et al., 2019].8\nFurthermore, we give a more rigorous, non-asymptotic proof that the NTK captures the behavior of a\nfully-trained wide neural net under weaker condition than previous proofs. We also experimentally\nshow that the random feature methods for approximating CNTK in earlier work do not compute good\napproximations, which is clear from their much worse performance on CIFAR.\n1.1 Notation\nWe use bold-faced letters for vectors, matrices and tensors. For a vector a, let [a]i be its i-th entry; for\na matrix A, let [A]i,j be its (i, j)-th entry; for a 4th-order tensor T , let [A]ij,i0j0 be its (i, j, i0,j 0)-th\nentry. Let I be the identity matrix, and [n]= {1, 2,...,n }. Let ei be an indicator vector with i-th\nentry being 1 and other entries being 0, and let 1 denote the all-one vector. We use \u0000 to denote the\nentry-wise product and ⌦ to denote the tensor product. We use h·, ·ito denote the standard inner\nproduct. We use diag(·) to transform a vector to a diagonal matrix. We use \u0000 (·) to denote the\nactivation function, such as the rectiﬁed linear unit (ReLU) function: \u0000 (z) = max {z, 0}, and ˙\u0000 (·)\nto denote the derivative of \u0000 (·). Denote by N (µ, ⌃) the Gaussian distribution with mean µ and\ncovariance ⌃.\n2 Related Work\nFrom a Gaussian process (GP) viewpoint, the correspondence between inﬁnite neural networks and\nkernel machines was ﬁrst noted by Neal [1996]. Follow-up work extended this correspondence\nto more general shallow neural networks [ Williams, 1997, Roux and Bengio , 2007, Hazan and\nJaakkola, 2015]. More recently, this was extended to deep and convolutional neural networks [ Lee\net al., 2018, Matthews et al., 2018, Novak et al., 2019, Garriga-Alonso et al., 2019] and a variety of\nother architectures [ Y ang, 2019]. However, these kernels, as we discussed in Section 1, represent\nweakly-trained nets, instead of fully-trained nets.\nBeyond GPs, the connection between neural networks and kernels is also studied in the compositional\nkernel literature. Cho and Saul [2009] derived a closed-form kernel formula for rectiﬁed polynomial\nactivations, which include ReLU as a special case. Daniely et al. [2016] proposed a general framework\nto transform a neural network to a compositional kernel and later Daniely [2017] showed for\nsufﬁciently wide neural networks, stochastic gradient descent can learn functions that lie in the\ncorresponding reproducing kernel Hilbert space. However, the kernels studied in these works still\ncorrespond to weakly-trained neural networks.\nThis paper is inspired by a line of recent work on over-parameterized neural networks [ Du et al., 2019,\n2018b, Du and Hu, 2019, Li and Liang, 2018, Allen-Zhu et al., 2018b,a, Zou et al., 2018, Cao and Gu,\n2019]. These papers established that for (convolutional) neural networks with large but ﬁnite width,\n(stochastic) gradient descent can achieve zero training error. A key component in these papers is\nshowing that the weight matrix at each layer is close to its initialization. This observation implies that\nthe kernel deﬁned in Equation (2) is still close to its initialization. Arora et al. [2019] explicitly used\nthis observation to derive generalization bounds for two-layer over-parameterized neural networks.\nChizat and Bach [2018] argued that these results in the kernel regime may be too simple to be able to\nexplain the success of deep learning, while on the other hand, out results show that CNTK is at least\nable to perform well on tasks like CIFAR-10 classiﬁcation. Also see the survey Fan et al. [2019] for\nrecent advance in deep learning theory.\n8We only consider ﬁxed kernels deﬁned without using the training data. We do not compare to methods that\ntune the kernels using training data [ V an der Wilk et al., 2017] or use a neural network to extract features and\nthen applying a kernel method on top of them [ Mairal et al., 2014].\n3\nJacot et al. [2018] derived the exact same kernel from kernel gradient descent. They showed that\nif the number of neurons per layer goes to inﬁnity in a sequential order, then the kernel remains\nunchanged for a ﬁnite training time. They termed the derived kernel Neural Tangent Kernel (NTK).\nWe follow the same naming convention and name its convolutional extension Convolutional Neural\nTangent Kernel (CNTK). Later, Y ang[2019] derived a formula of CNTK as well as a mechanistic\nway to derive NTK for different architectures. Comparing with [ Y ang, 2019], our CNTK formula has\na more explicit convolutional structure and results in an efﬁcient GPU-friendly computation method.\nRecently, Lee et al. [2019] tried to empirically verify the theory in [ Jacot et al. , 2018] by studying the\nlinearization of neural nets. They observed that in the ﬁrst few iterations, the linearization is close\nto the actual neural net. However, as will be shown in Section B, such linearization can decrease\nthe classiﬁcation accuracy by 5% even on a “CIFAR-2\" (airplane V .S. car) dataset. Therefore, exact\nkernel evaluation is important to study the power of NTK and CNTK.\n3 Neural Tangent Kernel\nIn this section we describe fully-connected deep neural net architecture and its inﬁnite width limit,\nand how training it with respect to the `2 loss gives rise to a kernel regression problem involving\nthe neural tangent kernel (NTK). We denote by f (✓, x) 2 R the output of a neural network where\n✓ 2 RN is all the parameters in the network and x 2 Rd is the input. 9 Given a training dataset\n{(xi,y i)}n\ni=1 ⇢ Rd ⇥ R, consider training the neural network by minimizing the squared loss over\ntraining data: `(✓)= 1\n2\nP n\ni=1 (f (✓, xi) \u0000 yi)2 . The proof of the following lemma uses simple\ndifferentiation and appears in Section C.\nLemma 3.1. Consider minimizing the squared loss `(✓) by gradient descent with inﬁnitesimally\nsmall learning rate: d✓(t)\ndt = \u0000r `(✓(t)). Let u(t)=( f (✓(t), xi))i2[n] 2 Rn be the network\noutputs on all xi’s at time t, and y =( yi)i2[n] be the desired outputs. Then u(t) follows the\nfollowing evolution, where H(t) is an n ⇥ n positive semideﬁnite matrix whose (i, j)-th entry isD\n@f (✓(t),xi )\n@✓ , @f (✓(t),xj )\n@✓\nE\n:\ndu(t)\ndt = \u0000H(t) · (u(t) \u0000 y). (3)\nThe statement of Lemma 3.1 involves a matrix H(t). Below we deﬁne a deep net architecture whose\nwidth is allowed to go to inﬁnity, while ﬁxing the training data as above. In the limit, it can be\nshown that the matrix H(t) remains constant during training i.e., equal to H(0). Moreover, under a\nrandom initialization of parameters, the random matrix H(0) converges in probability to a certain\ndeterministic kernel matrix H⇤ as the width goes to inﬁnity, which is the Neural Tangent Kernel\nker(·, ·) (Equation (2)) evaluated on the training data. If H(t)= H⇤ for all t, then Equation (3)\nbecomes\ndu(t)\ndt = \u0000H⇤ · (u(t) \u0000 y). (4)\nNote that the above dynamics is identical to the dynamics of kernel regression under gradient ﬂow,\nfor which at time t !1 the ﬁnal prediction function is (assuming u(0) = 0)\nf ⇤(x)=( k e r (x, x1),..., ker(x, xn)) · (H⇤)\u00001y. (5)\nIn Theorem 3.2, we rigorously prove that a fully-trained sufﬁciently wide ReLU neural network is\nequivalent to the kernel regression predictor ( 5) on any given data point.\nFully-connected deep neural net and its inﬁnite width limit. Now we deﬁne a fully-connected\nneural net formally. Let x 2 Rd be the input, and denote g(0)(x)= x and d0 = d for notational\nconvenience. We deﬁne an L-hidden-layer fully-connected neural network recursively:\nf (h)(x)= W (h)g(h\u00001)(x) 2 Rdh , g(h)(x)=\nr c\u0000\ndh\n\u0000\n⇣\nf (h)(x)\n⌘\n2 Rdh ,h =1 , 2,...,L ,\n(6)\n9For simplicity, we only consider a single output here. The generalization to multiple outputs is straightfor-\nward.\n4\nwhere W (h) 2 Rdh ⇥dh\u0000 1 is the weight matrix in the h-th layer (h 2 [L]), \u0000 : R ! R is a coordinate-\nwise activation function, and c\u0000 =\n⇣\nEz⇠N (0,1)\nh\n\u0000 (z)2\ni⌘\u00001\n. The last layer of the neural network\nis\nf (✓, x)= f (L+1)(x)= W (L+1) · g(L)(x)\n= W (L+1) ·\nr c\u0000\ndL\n\u0000\n✓\nW (L) ·\nr c\u0000\ndL\u00001\n\u0000\n✓\nW (L\u00001) ···\nr c\u0000\nd1\n\u0000\n⇣\nW (1)x\n⌘◆◆\n,\nwhere W (L+1) 2 R1⇥dL is the weights in the ﬁnal layer, and ✓ =\n\u0000\nW (1),..., W (L+1)\u0000\nrepresents\nall the parameters in the network.\nWe initialize all the weights to be i.i.d. N (0, 1) random variables, and consider the limit of large\nhidden widths: d1,d 2,...,d L !1 . The scaling factor\np\nc\u0000 /dh in Equation (6) ensures that the\nnorm of g(h)(x) for each h 2 [L] is approximately preserved at initialization (see [ Du et al. , 2018b]).\nIn particular, for ReLU activation, we have E\nh\u0000\u0000g(h)(x)\n\u0000\u00002i\n= kxk2 (8h 2 [L]).\nRecall from [ Lee et al. , 2018] that in the inﬁnite width limit, the pre-activations f (h)(x) at every\nhidden layer h 2 [L] has all its coordinates tending to i.i.d. centered Gaussian processes of covariance\n⌃(h\u00001) : Rd ⇥ Rd ! R deﬁned recursively as: for h 2 [L],\n⌃(0)(x, x0)= x> x0,\n⇤(h)(x, x0)=\n✓\n⌃(h\u00001)(x, x)⌃ (h\u00001)(x, x0)\n⌃(h\u00001)(x0, x)⌃ (h\u00001)(x0, x0)\n◆\n2 R2⇥2,\n⌃(h)(x, x0)= c\u0000 E\n(u,v)⇠N (0,⇤(h) )\n[\u0000 (u) \u0000 (v)] .\n(7)\nTo give the formula of NTK, we also need to deﬁne a derivative covariance:\n˙⌃(h)(x, x0)= c\u0000 E\n(u,v)⇠N (0,⇤(h) )\n[˙\u0000(u)˙\u0000(v)] . (8)\nThe ﬁnal NTK expression for the fully-connected neural network is\n⇥(L)(x, x0)=\nL+1X\nh=1\n \n⌃(h\u00001)(x, x0) ·\nL+1Y\nh0=h\n˙⌃(h0)(x, x0)\n!\n, (9)\nwhere we let ˙⌃(L+1)(x, x0)=1 for convenience. We refer readers to Section D for the derivation of\nthis formula. Rigorously, for ReLU activation, we have the following theorem that gives a concrete\nbound on the hidden widths that is sufﬁcient for convergence to the NTK at initialization:\nTheorem 3.1 (Convergence to the NTK at initializatoin) . Fix ✏> 0 and \u0000 2 (0, 1). Suppose\n\u0000 (z) = max(0 ,z ) and minh2[L] dh \u0000 ⌦( L14\n✏4 log(L/\u0000)). Then for any inputs x, x02 Rd0 such that\nkxk 1, kx0k 1, with probability at least 1 \u0000 \u0000 we have:\n\u0000\u0000\u0000\u0000\n⌧ @f (✓, x)\n@✓ , @f (✓, x0)\n@✓\n\u0000\n\u0000 ⇥(L)(x, x0)\n\u0000\u0000\u0000\u0000 ✏.\nThe proof of Theorem 3.1 is given in Section E. Theorem 3.1 improves upon previous results [ Jacot\net al., 2018, Y ang, 2019] that also established similar convergence in the following sense:\n1. Previous results are asymptotic, i.e., they require the widths to go to inﬁnity, while Theorem 3.1\ngives a non-asymptotic bound on the required layer widths.\n2. Jacot et al. [2018] required sequential limit, i.e., d1,...,d L go to inﬁnity one by one, and Y ang\n[2019] let d1,...,d L go to inﬁnity at the same rate. On the other hand, Theorem 3.1 only requires\nminh2[L] dh to be sufﬁciently large, which is the weakest notion of limit.\nEquivalence between wide neural net and kernel regression with NTK. Built on Theorem 3.1,\nwe can further incorporate the training process and show the equivalence between a fully-trained\nsufﬁciently wide neural net and the kernel regression solution using the NTK, as described in\nLemma 3.1 and the discussion after it.\n5\nRecall that the training data are {(xi,y i)}n\ni=1 ⇢ Rd ⇥ R, and H⇤ 2 Rn⇥n is the NTK evaluated\non these training data, i.e., [H⇤]i,j =⇥ (L)(xi, xj ). Denote \u00000 = \u0000min (H⇤). For a testing point\nxte 2 Rd, we let kerntk(xte, X) 2 Rn be the kernel evaluated between the testing point and n\ntraining points, i.e., [kerntk(xte, X)]i =⇥ (L)(xte, xi). The prediction of kernel regression using\nNTK on this testing point is fntk (xte)=( k e rntk (xte, X))> (H⇤)\u00001 y.\nSince the above solution corresponds to the linear dynamics in Equation (4) with zero initialization, in\norder to establish equivalence between neural network and kernel regression, we would like the initial\noutput of the neural network to be small. Therefore, we apply a small multiplier > 0, and let the ﬁnal\noutput of the neural network be fnn(✓, x)= f (✓, x) . We let fnn(xte)=l i m t!1 fnn(✓(t), xte)\nbe the prediction of the neural network at the end of training.\nThe following theorem establishes the equivalence between the fully-trained wide neural network\nfnn and the kernel regression predictor fntk using the NTK.\nTheorem 3.2 (Equivalence between trained net and kernel regression) . Suppose \u0000 (z)=\nmax(0,z ), 1/ = poly(1 /✏,log(n/\u0000)) and d1 = d2 = ··· = dL = m with m \u0000\npoly(1/, L, 1/\u00000,n , log(1/\u0000)). Then for any xte 2 Rd with kxtek =1 , with probability at\nleast 1 \u0000 \u0000 over the random initialization, we have\n|fnn(xte) \u0000 fntk(xte)| ✏.\nThe proof of Theorem 3.2 is given in Section F. We remark that one can generalize our proof to more\nadvanced architectures, such as convolutinal neural network, ResNet, etc.\nTheorem 3.2 is, to our knowledge, the ﬁrst result that rigorously shows the equivalence between a\nfully-trained neural net and a deterministic kernel predictor. Compared with similar results by [ Jacot\net al., 2018, Lee et al. , 2019], our bound is non-asymptotic whereas theirs are asymptotic. Compared\nwith [Arora et al., 2019, Allen-Zhu et al., 2018b,a, Du et al., 2019, 2018b, Li and Liang, 2018, Zou\net al., 2018], our theorem is a more precise characterization of the learned neural network. That is, the\nprediction is essentially a kernel predictor. Therefore, to study the properties of over-parameterized\nnets, such as their generalization power, it is sufﬁcient to study the corresponding NTK.\nWhile this theorem only gives guarantee for a single point, using a union bound, we can show that\nthis guarantee holds for (exponentially many) ﬁnite testing points. Combing this with the standard\nanalysis of hold-out validation set, we can conclude that a fully-trained wide neural net enjoys the\nsame generalization ability as its corresponding NTK.\n4 Convolutional Neural Tangent Kernel\nIn this section we study convolutional neural nets (CNNs) and their corresponding CNTKs. We study\ntwo architectures, vanilla CNN and CNN with global average pooling (GAP). In this section we\ndeﬁne vanilla CNN and present its corresponding CNTK formula. The derivation of this formula is\ndeferred to Section G. We present the deﬁnition of CNN with GAP and its CNTK in Section H.\nTo formally deﬁne CNNs, we ﬁrst introduce some notation. We let P be the width and Q be the\nheight of the image. We use q 2 Z+ to denote the ﬁlter size. In practice, q =1 , 3, or 5. We use\nstandard zero padding and set stride size to be 1 to make sure the input of each layer has the same size.\nFor a convolutional ﬁlter w 2 Rq⇥q and an image x 2 RP ⇥Q, the convolution operator is deﬁned as\n[w ⇤x]ij =\nq\u0000 1\n2X\na=\u0000 q\u0000 1\n2\nq\u0000 1\n2X\nb=\u0000 q\u0000 1\n2\n[w]a+ q+1\n2 ,b+ q+1\n2\n[x]a+i,b+j for i 2 [P ],j 2 [Q]. (10)\nEquation (10) shows that patch [w ⇤x]ij depends on [x]i\u0000 q\u0000 1\n2 :i+ q\u0000 1\n2 ,j\u0000 q\u0000 1\n2 :j+ q\u0000 1\n2\n. Our CNTK\nformula also relies on this dependency. For (i, j, i0,j 0) 2 [P ] ⇥ [Q] ⇥ [P ] ⇥ [Q], deﬁne\nDij,i0j0\n= {(i + a, j + b, i0+ a0,j 0+ b0) 2 [P ] ⇥ [Q] ⇥ [P ] ⇥ [Q] |\u0000 (q \u0000 1)/2  a, b, a0,b 0 (q \u0000 1)/2} .\nLastly, for a tensor T 2 RP ⇥Q⇥P ⇥Q, we denote by [T ]Dij,i0j0 2 Rq⇥q⇥q⇥q a sub-tensor and we let\ntr (T )= P\ni,j Tij,ij .\n6\nA vanilla CNN consisting of L convolution layers and one fully-connected layer is formally deﬁned\nas follows:\n• Let x(0) = x 2 RP ⇥Q⇥C(0)\nbe the input image where C(0) is the number of channels.\n• For h =1 ,...,L , \u0000 =1 ,...,C (h), the intermediate outputs are deﬁned as\n˜x(h)\n(\u0000) =\nC(h\u0000 1)\nX\n↵ =1\nW (h)\n(↵ ),(\u0000) ⇤x(h\u00001)\n(↵ ) , x(h)\n(\u0000) =\nr c\u0000\nC(h) ⇥ q ⇥ q \u0000\n⇣\n˜x(h)\n(\u0000)\n⌘\n,\nwhere each W (h)\n(↵ ),(\u0000) 2 Rq⇥q is a ﬁlter with standard Gaussian initialization.\n• The ﬁnal output is deﬁned as f (✓, x)= P C(L)\n↵ =1\nD\nW (L+1)\n(↵ ) , x(L)\n(↵ )\nE\nwhere W (L+1)\n(↵ ) 2 RP ⇥Q is a\nweight matrix with standard Gaussian initialization.\nFor this architecture, using the same reasoning as in Section D, we obtain the following convolutional\nneural tangent kernel formula. The details are provided in Section G.\nCNTK formula. We let x, x0be two input images.\n• For ↵ =1 ,...,C (0), (i, j, i0,j 0) 2 [P ] ⇥ [Q] ⇥ [P ] ⇥ [Q], deﬁne\nK(0)\n(↵ ) (x, x0)= x(↵ ) ⌦ x0\n(↵ ) and\nh\n⌃(0)(x, x0)\ni\nij,i0j0\n=\nC(0)\nX\n↵ =1\ntr\n✓h\nK(0)\n(↵ )(x, x0)\ni\nDij,i0j0\n◆\n.\n• For h 2 [L],\n– For (i, j, i0,j 0) 2 [P ] ⇥ [Q] ⇥ [P ] ⇥ [Q], deﬁne\n⇤(h)\nij,i0j0(x, x0)=\n ⇥\n⌃(h\u00001)(x, x)\n⇤\nij,ij\n⇥\n⌃(h\u00001)(x, x0)\n⇤\nij,i0j0\n⇥\n⌃(h\u00001) (x0, x)\n⇤\ni0j0,ij\n⇥\n⌃(h\u00001) (x0, x0)\n⇤\ni0j0,i0j0\n!\n2 R2⇥2.\n– Deﬁne K(h)(x, x0), ˙K(h)(x, x0) 2 RP ⇥Q⇥P ⇥Q: for (i, j, i0,j 0) 2 [P ] ⇥ [Q] ⇥ [P ] ⇥ [Q],\nh\nK(h)(x, x0)\ni\nij,i0j0\n= c\u0000\nq2 · E\n(u,v)⇠N\n⇣\n0,⇤(h)\nij,i0j0(x,x0)\n⌘ [\u0000 (u) \u0000 (v)] , (11)\nh\n˙K(h)(x, x0)\ni\nij,i0j0\n= c\u0000\nq2 · E\n(u,v)⇠N\n⇣\n0,⇤(h)\nij,i0j0(x,x0)\n⌘ [˙\u0000 (u)˙\u0000 (v)] . (12)\n– Deﬁne ⌃(h)(x, x0) 2 RP ⇥Q⇥P ⇥Q: for (i, j, i0,j 0) 2 [P ] ⇥ [Q] ⇥ [P ] ⇥ [Q],\nh\n⌃(h)(x, x0)\ni\nij,i0j0\n=tr\n✓h\nK(h)(x, x0)\ni\nDij,i0j0\n◆\n.\nNote that ⌃(x, x0) and ˙⌃(x, x0) share similar structures as their NTK counterparts in Equations (7)\nand (8). The only difference is that we have one more step, taking the trace over patches. This step\nrepresents the convolution operation in the corresponding CNN. Next, we can use a recursion to\ncompute the CNTK:\n1. First, we deﬁne ⇥(0)(x, x0)= ⌃(0)(x, x0).\n2. For h =1 ,...,L \u0000 1 and (i, j, i0,j 0) 2 [P ] ⇥ [Q] ⇥ [P ] ⇥ [Q], we deﬁne\nh\n⇥(h)(x, x0)\ni\nij,i0j0\n=t r\n✓h\n˙K(h)(x, x0) \u0000 ⇥(h\u00001)(x, x0)+ K(h)(x, x0)\ni\nDij,i0j0\n◆\n.\n3. For h = L , we deﬁne ⇥(L)(x, x0)= ˙K(L)(x, x0) \u0000 ⇥(L\u00001)(x, x0)+ K(L)(x, x0).\n4. The ﬁnal CNTK value is deﬁned as tr\n\u0000\n⇥(L)(x, x0)\n\u0000\n.\nIn Section H we give the CNTK formula for CNNs with GAP , which is similar to vanilla CNNs. To\ncompute the CNTK matrix corresponding to a CNN with GAP that has L convolution layers and one\nfully-connected layer on n samples, the time complexity is O(n2P 2Q2L). Previous work assumed\nthat directly computing convolutional kernel (with pooling) exactly is computationally infeasible,\nand thus resorted to approximations like Monte Carlo sampling [ Novak et al., 2019]. We are able to\nscale the exact CNTK computation to the full CIFAR-10 dataset and 20-layer CNN with GAP . We\npresent our efﬁcient computation approach in Section I.\n7\nDepth CNN-V CNTK-V CNTK-V -2KCNN-GAP CNTK-GAP CNTK-GAP-2K\n3 59.97% 64.47% 40.94% 63.81% 70.47% 49.71%\n4 60.20% 65.52% 42.54% 80.93% 75.93% 51.06%\n6 64.11% 66.03% 43.43% 83.75% 76.73% 51.73%\n11 69.48% 65.90% 43.42% 82.92% 77.43% 51.92%\n21 75.57% 64.09% 42.53% 83.30% 77.08% 52.22%\nTable 1: Classiﬁcation accuracies of CNNs and CNTKs on the CIFAR-10 dataset. CNN-V represents\nvanilla CNN and CNTK-V represents the kernel corresponding to CNN-V . CNN-GAP represents\nCNN with GAP and CNTK-GAP represents the kernel correspondong to CNN-GAP . CNTK-V -2K\nand CNTK-GAP-2K represent training CNTKs with only 2,000 training data.\n5 Experiments\nWe evaluate the performances of CNNs and their corresponding CNTKs on the CIFAR-10 dataset.\nThe implementation details are in Section A. We also compare the performances between CNTKs and\ntheir corresponding random features. Due to space limit, we defer these results on random features to\nSection B.\nResults. We test two types of architectures, vanilla CNN and CNN with global average pooling\n(GAP), as described in Sections 4 and H. We also test CNTKs with only 2,000 training data to\nsee whether their performances are consistent with CNTKs and CNNs using the full training set.\nThe results are summarized in Table 1. Notice that in Table 1, depth is the total number of layers\n(including both convolution layers and fully-connected layers).\nSeveral comments are in sequel. First, CNTKs are very powerful kernels. The best kernel, 11-layer\nCNTK with GAP , achieves 77.43% classiﬁcation accuracy on CIFAR-10. This results in a signiﬁcant\nnew benchmark for performance of a pure kernel-based method on CIFAR-10, being 10% higher\nthan methods reported in [ Novak et al., 2019].\nSecond, we ﬁnd that for both CNN and CNTK, depth can affect the classiﬁcation accuracy. This\nobservation demonstrates that depth not only matters in deep neural networks but can also affect the\nperformance of CNTKs.\nThird, the global average pooling operation can signiﬁcantly increase the classiﬁcation accuracy by\n8% - 10% for both CNN and CNTK. Based on this ﬁnding, we expect that many techniques that\nimprove the performance of neural networks are in some sense universal, i.e., these techniques can\nalso beneﬁt kernel methods.\nFourth, we ﬁnd that there is still a 5% - 6% performance gap between CNTKs and CNNs. Since\nCNTKs exactly correspond to inﬁnitely wide CNNs, this performance gap implies that ﬁnite width\nhas its beneﬁts. Therefore, it is likely that recent theoretical work on over-parameterization that\noperates in the NTK regime cannot fully explain the success of neural networks yet, and we believe it\nis an interesting open problem to characterize this gap.\nPotential application in neural architecture search. Finally, we ﬁnd that performances of CNTK-\nV -2Ks and CNTK-GAP-2Ks are highly correlated to their CNN-V , CNTK-V , CNN-GAP and CNTK-\nGAP counterparts. Again we see CNTK-GAP-2Ks outperform CNTK-V -2Ks by a large margin\n(about 8% - 9%). One potential application of this observation is to guide neural architecture search.\nWe can compute the kernel on a small training data, test it on a validation set, and choose neural\nnetwork architectures based on the performance of this small kernel on the validation set. We leave\nlarge scale experiments of this idea for future work.\n6 Conclusion\nBy giving the ﬁrst practical algorithm for computing CNTKs exactly, this paper allows investigation\nof the behavior of inﬁnitely wide (hence inﬁnitely over-parametrized) deep nets, which turns out to\n8\nnot be much worse than that of their ﬁnite counterparts. We also give a fully rigorous proof that a\nsufﬁciently wide net is approximately equivalent to the kernel regression predictor, thus yielding a\npowerful new off-the-shelf kernel. We leave it as an open problem to understand the behavior of\ninﬁnitely wide nets with features such as Batch Normalization or Residual Layers. Of course, one\ncan also hope that the analysis of inﬁnite nets provides rigorous insight into ﬁnite ones.\nAcknowledgments\nWe thank Jason D. Lee, Haochuan Li and Xiyu Zhai for useful discussions. S. Arora, W. Hu and\nZ. Li are supported by NSF, ONR, Simons Foundation, Schmidt Foundation, Mozilla Research,\nAmazon Research, DARPA and SRC. R. Salakhutdinov and R. Wang are supported in part by NSF\nIIS-1763562, Ofﬁce of Naval Research grant N000141812861, and Nvidia NV AIL award. We thank\nAmazon Web Services for providing compute time for the experiments in this paper.\nReferences\nZeyuan Allen-Zhu, Y uanzhi Li, and Yingyu Liang. Learning and generalization in overparameterized\nneural networks, going beyond two layers. arXiv preprint arXiv:1811.04918, 2018a.\nZeyuan Allen-Zhu, Y uanzhi Li, and Zhao Song. A convergence theory for deep learning via over-\nparameterization. arXiv preprint arXiv:1811.03962, 2018b.\nSanjeev Arora, Simon S Du, Wei Hu, Zhiyuan Li, and Ruosong Wang. Fine-grained analysis of\noptimization and generalization for overparameterized two-layer neural networks. arXiv preprint\narXiv:1901.08584, 2019.\nStéphane Boucheron, Gábor Lugosi, and Pascal Massart. Concentration inequalities: A nonasymptotic\ntheory of independence. Oxford university press, 2013.\nY uan Cao and Quanquan Gu. A generalization theory of gradient descent for learning over-\nparameterized deep relu networks. arXiv preprint arXiv:1902.01384, 2019.\nLenaic Chizat and Francis Bach. A note on lazy training in supervised differentiable programming.\narXiv preprint arXiv:1812.07956, 2018.\nY oungmin Cho and Lawrence K Saul. Kernel methods for deep learning. In Advances in neural\ninformation processing systems, pages 342–350, 2009.\nAmit Daniely. SGD learns the conjugate kernel class of the network. In Advances in Neural\nInformation Processing Systems, pages 2422–2430, 2017.\nAmit Daniely, Roy Frostig, and Y oram Singer. Toward deeper understanding of neural networks:\nThe power of initialization and a dual view on expressivity. In Advances In Neural Information\nProcessing Systems, pages 2253–2261, 2016.\nSimon S Du and Wei Hu. Width provably matters in optimization for deep linear neural networks.\narXiv preprint arXiv:1901.08572, 2019.\nSimon S Du, Wei Hu, and Jason D Lee. Algorithmic regularization in learning deep homogeneous\nmodels: Layers are automatically balanced. In Advances in Neural Information Processing Systems\n31, pages 382–393. 2018a.\nSimon S Du, Jason D Lee, Haochuan Li, Liwei Wang, and Xiyu Zhai. Gradient descent ﬁnds global\nminima of deep neural networks. arXiv preprint arXiv:1811.03804, 2018b.\nSimon S. Du, Xiyu Zhai, Barnabas Poczos, and Aarti Singh. Gradient descent provably optimizes\nover-parameterized neural networks. In International Conference on Learning Representations ,\n2019.\nJianqing Fan, Cong Ma, and Yiqiao Zhong. A selective overview of deep learning. arXiv preprint\narXiv:1904.05526, 2019.\n9\nAdrià Garriga-Alonso, Carl Edward Rasmussen, and Laurence Aitchison. Deep convolutional\nnetworks as shallow gaussian processes. In International Conference on Learning Representations ,\n2019. URL https://openreview.net/forum?id=Bklfsi0cKm.\nTamir Hazan and Tommi Jaakkola. Steps toward deep kernel methods from inﬁnite neural networks.\narXiv preprint arXiv:1508.05133, 2015.\nArthur Jacot, Franck Gabriel, and Clément Hongler. Neural tangent kernel: Convergence and\ngeneralization in neural networks. arXiv preprint arXiv:1806.07572, 2018.\nJaehoon Lee, Jascha Sohl-dickstein, Jeffrey Pennington, Roman Novak, Sam Schoenholz, and\nY asaman Bahri. Deep neural networks as gaussian processes. In International Conference on\nLearning Representations, 2018. URL https://openreview.net/forum?id=B1EA-M-0Z.\nJaehoon Lee, Lechao Xiao, Samuel S Schoenholz, Y asaman Bahri, Jascha Sohl-Dickstein, and Jeffrey\nPennington. Wide neural networks of any depth evolve as linear models under gradient descent.\narXiv preprint arXiv:1902.06720, 2019.\nY uanzhi Li and Yingyu Liang. Learning overparameterized neural networks via stochastic gradient\ndescent on structured data. arXiv preprint arXiv:1808.01204, 2018.\nJulien Mairal, Piotr Koniusz, Zaid Harchaoui, and Cordelia Schmid. Convolutional kernel networks.\nIn Advances in neural information processing systems , pages 2627–2635, 2014.\nAlexander G de G Matthews, Mark Rowland, Jiri Hron, Richard E Turner, and Zoubin Ghahramani.\nGaussian process behaviour in wide deep neural networks. arXiv preprint arXiv:1804.11271, 2018.\nRadford M Neal. Priors for inﬁnite networks. In Bayesian Learning for Neural Networks , pages\n29–53. Springer, 1996.\nRoman Novak, Lechao Xiao, Y asaman Bahri, Jaehoon Lee, Greg Y ang, Daniel A. Abolaﬁa, Jeffrey\nPennington, and Jascha Sohl-dickstein. Bayesian deep convolutional networks with many channels\nare gaussian processes. In International Conference on Learning Representations , 2019. URL\nhttps://openreview.net/forum?id=B1g30j0qF7.\nNicolas Le Roux and Y oshua Bengio. Continuous neural networks. In Proceedings of the Eleventh\nInternational Conference on Artiﬁcial Intelligence and Statistics , volume 2 of Proceedings of\nMachine Learning Research, pages 404–411, San Juan, Puerto Rico, 2007.\nMark V an der Wilk, Carl Edward Rasmussen, and James Hensman. Convolutional gaussian processes.\nIn Advances in Neural Information Processing Systems , pages 2849–2858, 2017.\nChristopher KI Williams. Computing with inﬁnite networks. In Advances in neural information\nprocessing systems, pages 295–301, 1997.\nGreg Y ang. Scaling limits of wide neural networks with weight sharing: Gaussian process behavior,\ngradient independence, and neural tangent kernel derivation. arXiv preprint arXiv:1902.04760,\n2019.\nChiyuan Zhang, Samy Bengio, Moritz Hardt, Benjamin Recht, and Oriol Vinyals. Understanding\ndeep learning requires rethinking generalization. In Proceedings of the International Conference\non Learning Representations (ICLR), 2017 , 2017.\nDifan Zou, Y uan Cao, Dongruo Zhou, and Quanquan Gu. Stochastic gradient descent optimizes\nover-parameterized deep ReLU networks. arXiv preprint arXiv:1811.08888, 2018.\n10",
  "values": {
    "Interpretable (to users)": "Yes",
    "Critiqability": "Yes",
    "Transparent (to users)": "Yes",
    "Privacy": "Yes",
    "Autonomy (power to decide)": "Yes",
    "Respect for Law and public interest": "Yes",
    "Explicability": "Yes",
    "Justice": "Yes",
    "Not socially biased": "Yes",
    "Respect for Persons": "Yes",
    "Non-maleficence": "Yes",
    "User influence": "Yes",
    "Beneficence": "Yes",
    "Collective influence": "Yes",
    "Fairness": "Yes",
    "Deferral to humans": "Yes"
  }
}