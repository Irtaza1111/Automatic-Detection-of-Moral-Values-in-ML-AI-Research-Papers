{
  "pdf": "NeurIPS-2019-wide-neural-networks-of-any-depth-evolve-as-linear-models-under-gradient-descent-Paper",
  "title": "Wide Neural Networks of Any Depth Evolve as Linear Models Under Gradient Descent",
  "author": "Jaehoon Lee, Lechao Xiao, Samuel Schoenholz, Yasaman Bahri, Roman Novak, Jascha Sohl-Dickstein, Jeffrey Pennington",
  "paper_id": "NeurIPS-2019-wide-neural-networks-of-any-depth-evolve-as-linear-models-under-gradient-descent-Paper",
  "text": "Wide Neural Networks of Any Depth Evolve as\nLinear Models Under Gradient Descent\nJaehoon Lee⇤, Lechao Xiao ⇤, Samuel S. Schoenholz, Yasaman Bahri\nRoman Novak, Jascha Sohl-Dickstein, Jeffrey Pennington\nGoogle Brain\n{jaehlee, xlc, schsam, yasamanb, romann, jaschasd, jpennin}@google.com\nAbstract\nA longstanding goal in deep learning research has been to precisely characterize\ntraining and generalization. However, the often complex loss landscapes of neural\nnetworks have made a theory of learning dynamics elusive. In this work, we show\nthat for wide neural networks the learning dynamics simplify considerably and\nthat, in the inﬁnite width limit, they are governed by a linear model obtained from\nthe ﬁrst-order Taylor expansion of the network around its initial parameters. Fur-\nthermore, mirroring the correspondence between wide Bayesian neural networks\nand Gaussian processes, gradient-based training of wide neural networks with a\nsquared loss produces test set predictions drawn from a Gaussian process with a\nparticular compositional kernel. While these theoretical results are only exact in the\ninﬁnite width limit, we nevertheless ﬁnd excellent empirical agreement between\nthe predictions of the original network and those of the linearized version even\nfor ﬁnite practically-sized networks. This agreement is robust across different\narchitectures, optimization methods, and loss functions.\n1 Introduction\nMachine learning models based on deep neural networks have achieved unprecedented performance\nacross a wide range of tasks [ 1, 2, 3]. Typically, these models are regarded as complex systems for\nwhich many types of theoretical analyses are intractable. Moreover, characterizing the gradient-based\ntraining dynamics of these models is challenging owing to the typically high-dimensional non-convex\nloss surfaces governing the optimization. As is common in the physical sciences, investigating the\nextreme limits of such systems can often shed light on these hard problems. For neural networks,\none such limit is that of inﬁnite width, which refers either to the number of hidden units in a fully-\nconnected layer or to the number of channels in a convolutional layer. Under this limit, the output of\nthe network at initialization is a draw from a Gaussian process (GP); moreover, the network output\nremains governed by a GP after exact Bayesian training using squared loss [ 4, 5, 6, 7, 8]. Aside from\nits theoretical simplicity, the inﬁnite-width limit is also of practical interest as wider networks have\nbeen found to generalize better [ 5, 7, 9, 10, 11].\nIn this work, we explore the learning dynamics of wide neural networks under gradient descent and\nﬁnd that the weight-space description of the dynamics becomes surprisingly simple: as the width\nbecomes large, the neural network can be effectively replaced by its ﬁrst-order Taylor expansion with\nrespect to its parameters at initialization. For this linear model, the dynamics of gradient descent\nbecome analytically tractable. While the linearization is only exact in the inﬁnite width limit, we\nnevertheless ﬁnd excellent agreement between the predictions of the original network and those of\n⇤Both authors contributed equally to this work. Work done as a member of the Google AI Residency program\n(https://g.co/airesidency).\n33rd Conference on Neural Information Processing Systems (NeurIPS 2019), V ancouver, Canada.\nthe linearized version even for ﬁnite width conﬁgurations. The agreement persists across different\narchitectures, optimization methods, and loss functions.\nFor squared loss, the exact learning dynamics admit a closed-form solution that allows us to charac-\nterize the evolution of the predictive distribution in terms of a GP . This result can be thought of as an\nextension of “sample-then-optimize\" posterior sampling [ 12] to the training of deep neural networks.\nOur empirical simulations conﬁrm that the result accurately models the variation in predictions across\nan ensemble of ﬁnite-width models with different random initializations.\nHere we summarize our contributions:\n• Parameter space dynamics: We show that wide network training dynamics in parameter space\nare equivalent to the training dynamics of a model which is afﬁne in the collection of all network\nparameters, the weights and biases. This result holds regardless of the choice of loss function. For\nsquared loss, the dynamics admit a closed-form solution as a function of time.\n• Sufﬁcient conditions for linearization : We formally prove that there exists a threshold learning\nrate ⌘critical (see Theorem 2.1), such that gradient descent training trajectories with learning rate\nsmaller than ⌘critical stay in an O\n\u0000\nn\u00001/2\u0000\n-neighborhood of the trajectory of the linearized network\nwhen n, the width of the hidden layers, is sufﬁciently large.\n• Output distribution dynamics : We formally show that the predictions of a neural network\nthroughout gradient descent training are described by a GP as the width goes to inﬁnity (see\nTheorem 2.2), extending results from Jacot et al. [13]. We further derive explicit time-dependent\nexpressions for the evolution of this GP during training. Finally, we provide a novel interpretation\nof the result. In particular, it offers a quantitative understanding of the mechanism by which\ngradient descent differs from Bayesian posterior sampling of the parameters: while both methods\ngenerate draws from a GP , gradient descent does not generate samples from the posterior of any\nprobabilistic model.\n• Large scale experimental support : We empirically investigate the applicability of the theory in\nthe ﬁnite-width setting and ﬁnd that it gives an accurate characterization of both learning dynamics\nand posterior function distributions across a variety of conditions, including some practical network\narchitectures such as the wide residual network [ 14].\n• Parameterization independence: We note that linearization result holds both in standard and\nNTK parameterization (deﬁned in § 2.1), while previous work assumed the latter, emphasizing that\nthe effect is due to increase in width rather than the particular parameterization.\n• Analytic ReLU and erf neural tangent kernels: We compute the analytic neural tangent kernel\ncorresponding to fully-connected networks with ReLU or erf nonlinearities.\n• Source code: Example code investigating both function space and parameter space linearized\nlearning dynamics described in this work is released as open source code within [ 15].2 We also\nprovide accompanying interactive Colab notebooks for both parameter space 3 and function\nspace4 linearization.\n1.1 Related work\nWe build on recent work by Jacot et al. [13] that characterize the exact dynamics of network outputs\nthroughout gradient descent training in the inﬁnite width limit. Their results establish that full batch\ngradient descent in parameter space corresponds to kernel gradient descent in function space with\nrespect to a new kernel, the Neural Tangent Kernel (NTK). We examine what this implies about\ndynamics in parameter space, where training updates are actually made.\nDaniely et al. [16] study the relationship between neural networks and kernels at initialization. They\nbound the difference between the inﬁnite width kernel and the empirical kernel at ﬁnite width n,\nwhich diminishes as O(1/pn). Daniely [17] uses the same kernel perspective to study stochastic\ngradient descent (SGD) training of neural networks.\nSaxe et al. [18] study the training dynamics of deep linear networks, in which the nonlinearities\nare treated as identity functions. Deep linear networks are linear in their inputs, but not in their\n2Note that the open source library has been expanded since initial submission of this work.\n3colab.sandbox.google.com/github/google/neural-tangents/blob/master/notebooks/weight_space_linearization.ipynb\n4colab.sandbox.google.com/github/google/neural-tangents/blob/master/notebooks/function_space_linearization.ipynb\n2\nparameters. In contrast, we show that the outputs of sufﬁciently wide neural networks are linear in\nthe updates to their parameters during gradient descent, but not usually their inputs.\nDu et al. [19], Allen-Zhu et al. [20, 21], Zou et al. [22] study the convergence of gradient descent\nto global minima. They proved that for i.i.d. Gaussian initialization, the parameters of sufﬁciently\nwide networks move little from their initial values during SGD. This small motion of the parameters\nis crucial to the effect we present, where wide neural networks behave linearly in terms of their\nparameters throughout training.\nMei et al. [23], Chizat and Bach [24], Rotskoff and V anden-Eijnden[25], Sirignano and Spiliopoulos\n[26] analyze the mean ﬁeld SGD dynamics of training neural networks in the large-width limit. Their\nmean ﬁeld analysis describes distributional dynamics of network parameters via a PDE. However,\ntheir analysis is restricted to one hidden layer networks with a scaling limit (1/n) different from ours\n(1/pn), which is commonly used in modern networks [ 2, 27].\nChizat et al. [28]5 argued that inﬁnite width networks are in ‘lazy training’ regime and maybe too\nsimple to be applicable to realistic neural networks. Nonetheless, we empirically investigate the\napplicability of the theory in the ﬁnite-width setting and ﬁnd that it gives an accurate characterization\nof both the learning dynamics and posterior function distributions across a variety of conditions,\nincluding some practical network architectures such as the wide residual network [ 14].\n2 Theoretical results\n2.1 Notation and setup for architecture and training dynamics\nLet D✓ Rn0 ⇥ Rk denote the training set and X = {x :( x, y) 2D } and Y = {y :( x, y) 2D }\ndenote the inputs and labels, respectively. Consider a fully-connected feed-forward network with L\nhidden layers with widths nl, for l =1 ,. . . ,Land a readout layer with nL+1 = k. For each x 2 Rn0 ,\nwe use hl(x),x l(x) 2 Rnl to represent the pre- and post-activation functions at layer l with input x.\nThe recurrence relation for a feed-forward network is deﬁned as\n⇢hl+1 = xlW l+1 + bl+1\nxl+1 = \u0000\n\u0000\nhl+1\u0000 and\n(\nW l\ni,j = \u0000!pnl\n! l\nij\nbl\nj = \u0000b\u0000l\nj\n, (1)\nwhere \u0000 is a point-wise activation function, W l+1 2 Rnl ⇥nl+1 and bl+1 2 Rnl+1 are the weights and\nbiases, ! l\nij and bl\nj are the trainable variables, drawn i.i.d. from a standard Gaussian ! l\nij ,\u0000 l\nj ⇠N (0, 1)\nat initialization, and \u00002\n! and \u00002\nb are weight and bias variances. Note that this parametrization is non-\nstandard, and we will refer to it as the NTK parameterization. It has already been adopted in several\nrecent works [ 29, 30, 13, 19, 31]. Unlike the standard parameterization that only normalizes the\nforward dynamics of the network, the NTK-parameterization also normalizes its backward dynamics.\nWe note that the predictions and training dynamics of NTK-parameterized networks are identical\nto those of standard networks, up to a width-dependent scaling factor in the learning rate for each\nparameter tensor. As we derive, and support experimentally, in Supplementary Material (SM) § F\nand §G, our results (linearity in weights, GP predictions) also hold for networks with a standard\nparameterization.\nWe deﬁne ✓l ⌘ vec\n\u0000\n{W l,b l}\n\u0000\n, the ((nl\u00001 + 1)nl) ⇥ 1 vector of all parameters for layer l. ✓ =\nvec\n\u0000\n[L+1\nl=1 ✓l\u0000\nthen indicates the vector of all network parameters, with similar deﬁnitions for ✓l\nand ✓>l. Denote by ✓t the time-dependence of the parameters and by ✓0 their initial values. We\nuse ft(x) ⌘ hL+1(x) 2 Rk to denote the output (or logits) of the neural network at time t. Let\n`(ˆy, y): Rk ⇥ Rk ! R denote the loss function where the ﬁrst argument is the prediction and the\nsecond argument the true label. In supervised learning, one is interested in learning a ✓ that minimizes\nthe empirical loss6, L = P\n(x,y)2D `(ft(x, ✓),y ).\n5We note that this is a concurrent work and an expanded version of this note is presented in parallel at\nNeurIPS 2019.\n6To simplify the notation for later equations, we use the total loss here instead of the average loss, but for all\nplots in §3, we show the average loss.\n3\nLet ⌘ be the learning rate 7. Via continuous time gradient descent, the evolution of the parameters ✓\nand the logits f can be written as\n˙✓t = \u0000⌘r✓ft(X )T rft (X )L (2)\n˙ft(X )= r✓ft(X ) ˙✓t = \u0000⌘ ˆ⇥t(X , X )rft (X )L (3)\nwhere ft(X )=v e c\n\u0000\n[ft (x)]x2X\n\u0000\n, the k|D| ⇥ 1 vector of concatenated logits for all examples, and\nrft (X )L is the gradient of the loss with respect to the model’s output, ft(X ). ˆ⇥t ⌘ ˆ⇥t(X , X ) is the\ntangent kernel at time t, which is a k|D| ⇥ k|D| matrix\nˆ⇥t = r✓ft(X )r✓ft(X )T =\nL+1X\nl=1\nr✓l ft(X )r✓l ft(X )T . (4)\nOne can deﬁne the tangent kernel for general arguments, e.g. ˆ⇥t(x, X ) where x is test input. At\nﬁnite-width, ˆ⇥ will depend on the speciﬁc random draw of the parameters and in this context we\nrefer to it as the empirical tangent kernel.\nThe dynamics of discrete gradient descent can be obtained by replacing ˙✓t and ˙ft(X ) with (✓i+1 \u0000 ✓i)\nand (fi+1(X ) \u0000 fi(X )) above, and replacing e\u0000⌘ ˆ⇥0 t with (1 \u0000 (1 \u0000 ⌘ ˆ⇥0)i) below.\n2.2 Linearized networks have closed form training dynamics for parameters and outputs\nIn this section, we consider the training dynamics of the linearized network. Speciﬁcally, we replace\nthe outputs of the neural network by their ﬁrst order Taylor expansion,\nf lin\nt (x) ⌘ f0(x)+ r✓f0(x)|✓=✓0 ! t , (5)\nwhere ! t ⌘ ✓t \u0000 ✓0 is the change in the parameters from their initial values. Note that f lin\nt is the\nsum of two terms: the ﬁrst term is the initial output of the network, which remains unchanged during\ntraining, and the second term captures the change to the initial value during training. The dynamics\nof gradient ﬂow using this linearized function are governed by,\n˙! t = \u0000⌘r✓f0(X )T rf lin\nt (X )L (6)\n˙f lin\nt (x)= \u0000⌘ ˆ⇥0(x, X )rf lin\nt (X )L . (7)\nAs r✓f0(x) remains constant throughout training, these dynamics are often quite simple. In the case\nof an MSE loss, i.e., `(ˆy, y)= 1\n2 kˆy \u0000 yk2\n2, the ODEs have closed form solutions\n! t = \u0000r✓f0(X )T ˆ⇥\u00001\n0\n⇣\nI \u0000 e\u0000⌘ ˆ⇥0 t\n⌘\n(f0(X ) \u0000Y ) , (8)\nf lin\nt (X )=( I \u0000 e\u0000⌘ ˆ⇥0 t)Y + e\u0000⌘ ˆ⇥0 tf0(X ) . (9)\nFor an arbitrary point x, f lin\nt (x)= µt(x)+ \u0000t(x), where\nµt(x)= ˆ⇥0(x, X ) ˆ⇥\u00001\n0\n⇣\nI \u0000 e\u0000⌘ ˆ⇥0 t\n⌘\nY (10)\n\u0000t(x)= f0(x) \u0000 ˆ⇥0 (x, X ) ˆ⇥\u00001\n0\n⇣\nI \u0000e\u0000⌘ ˆ⇥0 t\n⌘\nf0(X ). (11)\nTherefore, we can obtain the time evolution of the linearized neural network without running gradient\ndescent. We only need to compute the tangent kernel ˆ⇥0 and the outputs f0 at initialization and use\nEquations 8, 10, and 11 to compute the dynamics of the weights and the outputs.\n2.3 Inﬁnite width limit yields a Gaussian process\nAs the width of the hidden layers approaches inﬁnity, the Central Limit Theorem (CLT) implies\nthat the outputs at initialization {f0(x)}x2X converge to a multivariate Gaussian in distribution.\n7Note that compared to the conventional parameterization, ⌘ is larger by factor of width [ 31]. The NTK\nparameterization allows usage of a universal learning rate scale irrespective of network width.\n4\nInformally, this occurs because the pre-activations at each layer are a sum of Gaussian random\nvariables (the weights and bias), and thus become a Gaussian random variable themselves. See\n[32, 33, 5, 34, 35] for more details, and [ 36, 7] for a formal treatment.\nTherefore, randomly initialized neural networks are in correspondence with a certain class of GPs\n(hereinafter referred to as NNGPs), which facilitates a fully Bayesian treatment of neural networks\n[5, 6]. More precisely, let f i\nt denote the i-th output dimension and K denote the sample-to-sample\nkernel function (of the pre-activation) of the outputs in the inﬁnite width setting,\nKi,j (x, x0)= l i m\nmin(n1 ,...,nL )!1\nE\nh\nf i\n0(x) · f j\n0 (x0)\ni\n, (12)\nthen f0(X ) ⇠N (0, K(X , X )), where Ki,j (x, x0) denotes the covariance between the i-th output\nof x and j-th output of x0, which can be computed recursively (see Lee et al. [5, §2.3 ] and SM\n§E). For a test input x 2X T , the joint output distribution f ([x, X ]) is also multivariate Gaussian.\nConditioning on the training samples 8, f (X )= Y, the distribution of f (x)|X , Y is also a Gaussian\nN (µ(x), ⌃(x)),\nµ(x)= K(x, X )K\u00001Y, ⌃(x)= K(x, x) \u0000K (x, X )K\u00001K(x, X )T , (13)\nand where K = K(X , X ). This is the posterior predictive distribution resulting from exact Bayesian\ninference in an inﬁnitely wide neural network.\n2.3.1 Gaussian processes from gradient descent training\nIf we freeze the variables ✓L after initialization and only optimize ✓L+1, the original network and its\nlinearization are identical. Letting the width approach inﬁnity, this particular tangent kernel ˆ⇥0 will\nconverge to K in probability and Equation 10 will converge to the posterior Equation 13 as t !1\n(for further details see SM § D). This is a realization of the “sample-then-optimize\" approach for\nevaluating the posterior of a Gaussian process proposed in Matthews et al. [ 12].\nIf none of the variables are frozen, in the inﬁnite width setting, ˆ⇥0 also converges in probability to\na deterministic kernel ⇥ [13, 37], which we sometimes refer to as the analytic kernel, and which\ncan also be computed recursively (see SM § E). For ReLU and erf nonlinearity, ⇥ can be exactly\ncomputed (SM § C) which we use in § 3. Letting the width go to inﬁnity, for any t, the output f lin\nt (x)\nof the linearized network is also Gaussian distributed because Equations 10 and 11 describe an afﬁne\ntransform of the Gaussian [f0(x),f 0(X )]. Therefore\nCorollary 1. F or every test points in x 2X T , and t \u0000 0, f lin\nt (x) converges in distribution as width\ngoes to inﬁnity to a Gaussian with mean and covariance given by 9\nµ(XT )=⇥ ( XT , X )⇥ \u0000 1\n⇣\nI \u0000 e\u0000 ⌘⇥t\n⌘\nY , (14)\n⌃(XT , XT )= K (XT , XT )+⇥ ( XT , X )⇥\u0000 1\n⇣\nI \u0000 e\u0000 ⌘⇥t\n⌘\nK\n⇣\nI \u0000 e\u0000 ⌘⇥t\n⌘\n⇥\u0000 1⇥( X , XT )\n\u0000\n⇣\n⇥(XT , X )⇥\u0000 1\n⇣\nI \u0000 e\u0000 ⌘⇥t\n⌘\nK (X , XT )+ h.c.\n⌘\n. (15)\nTherefore, over random initialization, limt!1 limn!1 f lin\nt (x) has distribution\nN\n\u0000\n⇥( XT , X )⇥ \u00001Y,\nK (XT , XT )+⇥ ( XT , X )⇥\u00001K⇥\u00001⇥( X , XT ) \u0000\n\u0000\n⇥(XT , X )⇥\u00001K (X , XT )+ h.c.\n\u0000\u0000\n. (16)\nUnlike the case when only ✓L+1 is optimized, Equations 14 and 15 do not admit an interpretation\ncorresponding to the posterior sampling of a probabilistic model. 10 We contrast the predictive\ndistributions from the NNGP , NTK-GP (i.e. Equations 14 and 15) and ensembles of NNs in Figure 2.\nInﬁnitely-wide neural networks open up ways to study deep neural networks both under fully Bayesian\ntraining through the Gaussian process correspondence, and under GD training through the lineariza-\ntion perspective. The resulting distributions over functions are inconsistent (the distribution resulting\n8 This imposes that hL+1 directly corresponds to the network predictions. In the case of softmax readout,\nvariational or sampling methods are required to marginalize over hL+1.\n9Here “+h.c. ” is an abbreviation for “plus the Hermitian conjugate”.\n10One possible exception is when the NNGP kernel and NTK are the same up to a scalar multiplication. This\nis the case when the activation function is the identity function and there is no bias term.\n5\nFigure 1: Relative Frobenius norm change during training. Three hidden layer ReLU net-\nworks trained with ⌘ =1 .0 on a subset of MNIST ( |D| = 128 ). We measure changes of (in-\nput/output/intermediary) weights, empirical ˆ⇥, and empirical ˆK after T =2 17 steps of gradient\ndescent updates for varying width. We see that the relative change in input/output weights scales as\n1/pn while intermediate weights scales as 1/n, this is because the dimension of the input/output\ndoes not grow with n. The change in ˆ⇥ and ˆK is upper bounded by O (1/pn) but is closer to\nO (1/n). See Figure S6 for the same experiment with 3-layer tanh and 1-layer ReLU networks. See\nFigures S9 and S10 for additional comparisons of ﬁnite width empirical and analytic kernels.\nfrom GD training does not generally correspond to a Bayesian posterior). We believe understand-\ning the biases over learned functions induced by different training schemes and architectures is a\nfascinating avenue for future work.\n2.4 Inﬁnite width networks are linearized networks\nEquation 2 and 3 of the original network are intractable in general, since ˆ⇥t evolves with time.\nHowever, for the mean squared loss, we are able to prove formally that, as long as the learning rate\n⌘<⌘ critical := 2(\u0000min(⇥) + \u0000max(⇥))\u00001, where \u0000min/max(⇥) is the min/max eigenvalue of ⇥, the\ngradient descent dynamics of the original neural network falls into its linearized dynamics regime.\nTheorem 2.1 (Informal). Let n1 = ··· = nL = n and assume \u0000min(⇥) > 0. Applying gradient\ndescent with learning rate ⌘<⌘ critical (or gradient ﬂow), for every x 2 Rn0 with kxk2  1, with\nprobability arbitrarily close to 1 over random initialization,\nsup\nt\u00000\n\u0000\u0000ft(x) \u0000 f lin\nt (x)\n\u0000\u0000\n2 , sup\nt\u00000\nk✓t \u0000 ✓0k2pn , sup\nt\u00000\n\u0000\u0000\u0000ˆ⇥t \u0000 ˆ⇥0\n\u0000\u0000\u0000\nF\n= O(n\u0000 1\n2 ), as n !1 . (17)\nTherefore, as n !1 , the distributions of ft(x) and f lin\nt (x) become the same. Coupling with\nCorollary 1, we have\nTheorem 2.2. If ⌘<⌘ critical, then for every x 2 Rn0 with kxk2  1, as n !1 , ft(x) converges\nin distribution to the Gaussian with mean and variance given by Equation 14 and Equation 15.\nWe refer the readers to Figure 2 for empirical veriﬁcation of this theorem. The proof of Theorem 2.1\nconsists of two steps. The ﬁrst step is to prove the global convergence of overparameterized neural\nnetworks [19, 20, 21, 22] and stability of the NTK under gradient descent (and gradient ﬂow); see\nSM §G. This stability was ﬁrst observed and proved in [ 13] in the gradient ﬂow and sequential limit\n(i.e. letting n1 !1 ,. . . , nL !1 sequentially) setting under certain assumptions about global\nconvergence of gradient ﬂow. In § G, we show how to use the NTK to provide a self-contained (and\ncleaner) proof of such global convergence and the stability of NTK simultaneously. The second step\nis to couple the stability of NTK with Grönwall’s type arguments [ 38] to upper bound the discrepancy\nbetween ft and f lin\nt , i.e. the ﬁrst norm in Equation 17. Intuitively, the ODE of the original network\n(Equation 3) can be considered as a kˆ⇥t \u0000 ˆ⇥0kF -ﬂuctuation from the linearized ODE (Equation 7).\nOne expects the difference between the solutions of these two ODEs to be upper bounded by some\nfunctional of kˆ⇥t \u0000 ˆ⇥0kF ; see SM § H. Therefore, for a large width network, the training dynamics\ncan be well approximated by linearized dynamics.\nNote that the updates for individual weights in Equation 6 vanish in the inﬁnite width limit, which for\ninstance can be seen from the explicit width dependence of the gradients in the NTK parameterization.\nIndividual weights move by a vanishingly small amount for wide networks in this regime of dynamics,\nas do hidden layer activations, but they collectively conspire to provide a ﬁnite change in the ﬁnal\noutput of the network, as is necessary for training. An additional insight gained from linearization\n6\nof the network is that the individual instance dynamics derived in [ 13] can be viewed as a random\nfeatures method,11 where the features are the gradients of the model with respect to its weights.\n2.5 Extensions to other optimizers, architectures, and losses\nOur theoretical analysis thus far has focused on fully-connected single-output architectures trained\nby full batch gradient descent. In SM § B we derive corresponding results for: networks with multi-\ndimensional outputs, training against a cross entropy loss, and gradient descent with momentum.\nIn addition to these generalizations, there is good reason to suspect the results to extend to much\nbroader class of models and optimization procedures. In particular, a wealth of recent literature\nsuggests that the mean ﬁeld theory governing the wide network limit of fully-connected models [ 32,\n33] extends naturally to residual networks [35], CNNs [34], RNNs [39], batch normalization [40], and\nto broad architectures [ 37]. We postpone the development of these additional theoretical extensions\nin favor of an empirical investigation of linearization for a variety of architectures.\nFigure 2: Dynamics of mean and variance of trained neural network outputs follow analytic\ndynamics from linearization . Black lines indicate the time evolution of the predictive output\ndistribution from an ensemble of 100 trained neural networks (NNs). The blue region indicates the\nanalytic prediction of the output distribution throughout training (Equations 14, 15). Finally, the red\nregion indicates the prediction that would result from training only the top layer, corresponding to an\nNNGP (Equations S22, S23). The trained network has 3 hidden layers of width 8192, tanh activation\nfunctions, \u00002\nw =1 .5, no bias, and ⌘ =0 .5. The output is computed for inputs interpolated between\ntwo training points (denoted with black dots) x(↵)= ↵x(1) +( 1 \u0000 ↵)x(2). The shaded region and\ndotted lines denote 2 standard deviations ( ⇠ 95% quantile) from the mean denoted in solid lines.\nTraining was performed with full-batch gradient descent with dataset size |D| = 128. For dynamics\nfor individual function initializations, see SM Figure S1.\n3 Experiments\nIn this section, we provide empirical support showing that the training dynamics of wide neural\nnetworks are well captured by linearized models. We consider fully-connected, convolutional, and\nwide ResNet architectures trained with full- and mini- batch gradient descent using learning rates\nsufﬁciently small so that the continuous time approximation holds well. We consider two-class\nclassiﬁcation on CIFAR-10 (horses and planes) as well as ten-class classiﬁcation on MNIST and\nCIFAR-10. When using MSE loss, we treat the binary classiﬁcation task as regression with one class\nregressing to +1 and the other to \u00001.\nExperiments in Figures 1, 4, S2, S3, S4, S5 and S6, were done in JAX [ 41]. The remaining experi-\nments used TensorFlow [42]. An open source implementation of this work providing tools to inves-\ntigate linearized learning dynamics is available at www.github.com/google/neural-tangents\n[15].\nPredictive output distribution: In the case of an MSE loss, the output distribution remains Gaussian\nthroughout training. In Figure 2, the predictive output distribution for input points interpolated\nbetween two training points is shown for an ensemble of neural networks and their corresponding\nGPs. The interpolation is given by x(↵)= ↵x(1) +( 1 \u0000 ↵)x(2) where x(1,2) are two training inputs\n11We thank Alex Alemi for pointing out a subtlety on correspondence to a random features method.\n7\nFigure 3: Full batch gradient descent on a model behaves similarly to analytic dynamics on\nits linearization, both for network outputs, and also for individual weights. A binary CIFAR\nclassiﬁcation task with MSE loss and a ReLU fully-connected network with 5 hidden layers of width\nn = 2048, ⌘ =0 .01, |D| = 256, k =1 , \u00002\nw =2 .0, and \u00002\nb =0 .1. Left two panes show dynamics for\na randomly selected subset of datapoints or parameters. Third pane shows that the dynamics of loss\nfor training and test points agree well between the original and linearized model. The last pane shows\nthe dynamics of RMSE between the two models on test points. We observe that the empirical kernel\nˆ⇥ gives more accurate dynamics for ﬁnite width networks.\nwith different classes. We observe that the mean and variance dynamics of neural network outputs\nduring gradient descent training follow the analytic dynamics from linearization well (Equations\n14, 15). Moreover the NNGP predictive distribution which corresponds to exact Bayesian inference,\nwhile similar, is noticeably different from the predictive distribution at the end of gradient descent\ntraining. For dynamics for individual function draws see SM Figure S1.\nComparison of training dynamics of linearized network to original network : For a particular\nrealization of a ﬁnite width network, one can analytically predict the dynamics of the weights and\noutputs over the course of training using the empirical tangent kernel at initialization. In Figures\n3, 4 (see also S2, S3), we compare these linearized dynamics (Equations 8, 9) with the result of\ntraining the actual network. In all cases we see remarkably good agreement. We also observe\nthat for ﬁnite networks, dynamics predicted using the empirical kernel ˆ⇥ better match the data\nthan those obtained using the inﬁnite-width, analytic, kernel ⇥. To understand this we note that\nkˆ⇥(n)\nT \u0000 ˆ⇥(n)\n0 kF = O(1/n) O (1/pn)= kˆ⇥(n)\n0 \u0000⇥kF , where ˆ⇥(n)\n0 denotes the empirical tangent\nkernel of width n network, as plotted in Figure 1.\nOne can directly optimize parameters of f lin instead of solving the ODE induced by the tangent\nkernel ˆ⇥. Standard neural network optimization techniques such as mini-batching, weight decay, and\ndata augmentation can be directly applied. In Figure 4 (S2, S3), we compared the training dynamics\nof the linearized and original network while directly training both networks.\nWith direct optimization of linearized model, we tested full ( |D| = 50, 000) MNIST digit classiﬁca-\ntion with cross-entropy loss, and trained with a momentum optimizer (Figure S3). For cross-entropy\nloss with softmax output, some logits at late times grow indeﬁnitely, in contrast to MSE loss where\nlogits converge to target value. The error between original and linearized model for cross entropy\nloss becomes much worse at late times if the two models deviate signiﬁcantly before the logits enter\ntheir late-time steady-growth regime (See Figure S4).\nLinearized dynamics successfully describes the training of networks beyond vanilla fully-connected\nmodels. To demonstrate the generality of this procedure we show we can predict the learning\ndynamics of subclass of Wide Residual Networks (WRNs) [ 14]. WRNs are a class of model that are\npopular in computer vision and leverage convolutions, batch normalization, skip connections, and\naverage pooling. In Figure 4, we show a comparison between the linearized dynamics and the true\ndynamics for a wide residual network trained with MSE loss and SGD with momentum, trained on\nthe full CIF AR-10 dataset. We slightly modiﬁed the block structure described in Table S1 so that\neach layer has a constant number of channels (1024 in this case), and otherwise followed the original\nimplementation. As elsewhere, we see strong agreement between the predicted dynamics and the\nresult of training.\nEffects of dataset size : The training dynamics of a neural network match those of its linearization\nwhen the width is inﬁnite and the dataset is ﬁnite. In previous experiments, we chose sufﬁciently\nwide networks to achieve small error between neural networks and their linearization for smaller\n8\nFigure 4: A wide residual network and its linearization behave similarly when both are trained\nby SGD with momentum on MSE loss on CIFAR-10. We adopt the network architecture\nfrom Zagoruyko and Komodakis [14]. We use N =1 , channel size 1024, ⌘ =1 .0, \u0000 =0 .9,\nk = 10 , \u00002\nw =1 .0, and \u00002\nb =0 .0. See Table S1 for details of the architecture. Both the linearized\nand original model are trained directly on full CIFAR-10 ( |D| = 50, 000), using SGD with batch size\n8. Output dynamics for a randomly selected subset of train and test points are shown in the ﬁrst two\npanes. Last two panes show training and accuracy curves for the original and linearized networks.\ndatasets. Overall, we observe that as the width grows the error decreases (Figure S5). Additionally,\nwe see that the error grows in the size of the dataset. Thus, although error grows with dataset this can\nbe counterbalanced by a corresponding increase in the model size.\n4 Discussion\nWe showed theoretically that the learning dynamics in parameter space of deep nonlinear neural\nnetworks are exactly described by a linearized model in the inﬁnite width limit. Empirical investiga-\ntion revealed that this agrees well with actual training dynamics and predictive distributions across\nfully-connected, convolutional, and even wide residual network architectures, as well as with different\noptimizers (gradient descent, momentum, mini-batching) and loss functions (MSE, cross-entropy).\nOur results suggest that a surprising number of realistic neural networks may be operating in the\nregime we studied. This is further consistent with recent experimental work showing that neural\nnetworks are often robust to re-initialization but not re-randomization of layers (Zhang et al. [ 43]).\nIn the regime we study, since the learning dynamics are fully captured by the kernel ˆ⇥ and the target\nsignal, studying the properties of ˆ⇥ to determine trainability and generalization are interesting future\ndirections. Furthermore, the inﬁnite width limit gives us a simple characterization of both gradient\ndescent and Bayesian inference. By studying properties of the NNGP kernel K and the tangent kernel\n⇥, we may shed light on the inductive bias of gradient descent.\nSome layers of modern neural networks may be operating far from the linearized regime. Preliminary\nobservations in Lee et al. [5] showed that wide neural networks trained with SGD perform similarly\nto the corresponding GPs as width increase, while GPs still outperform trained neural networks for\nboth small and large dataset size. Furthermore, in Novak et al. [7], it is shown that the comparison\nof performance between ﬁnite- and inﬁnite-width networks is highly architecture-dependent. In\nparticular, it was found that inﬁnite-width networks perform as well as or better than their ﬁnite-width\ncounterparts for many fully-connected or locally-connected architectures. However, the opposite was\nfound in the case of convolutional networks without pooling. It is still an open research question to\ndetermine the main factors that determine these performance gaps. We believe that examining the\nbehavior of inﬁnitely wide networks provides a strong basis from which to build up a systematic\nunderstanding of ﬁnite-width networks (and/or networks trained with large learning rates).\nAcknowledgements\nWe thank Greg Y ang and Alex Alemi for useful discussions and feedback. We are grateful to\nDaniel Freeman, Alex Irpan and anonymous reviewers for providing valuable feedbacks on the\ndraft. We thank the JAX team for developing a language which makes model linearization and NTK\ncomputation straightforward. We would like to especially thank Matthew Johnson for support and\ndebugging help.\n9\nReferences\n[1] Alex Krizhevsky, Ilya Sutskever, and Geoffrey E Hinton. Imagenet classiﬁcation with deep\nconvolutional neural networks. In Advances in Neural Information Processing Systems . 2012.\n[2] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image\nrecognition. In Conference on Computer Vision and Pattern Recognition , pages 770–778, 2016.\n[3] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. Bert: Pre-training of\ndeep bidirectional transformers for language understanding. arXiv preprint arXiv:1810.04805,\n2018.\n[4] Radford M. Neal. Priors for inﬁnite networks (tech. rep. no. crg-tr-94-1). University of Toronto,\n1994.\n[5] Jaehoon Lee, Y asaman Bahri, Roman Novak, Sam Schoenholz, Jeffrey Pennington, and Jascha\nSohl-dickstein. Deep neural networks as gaussian processes. In International Conference on\nLearning Representations, 2018.\n[6] Alexander G. de G. Matthews, Jiri Hron, Mark Rowland, Richard E. Turner, and Zoubin\nGhahramani. Gaussian process behaviour in wide deep neural networks. In International\nConference on Learning Representations, 4 2018. URL https://openreview.net/forum?\nid=H1-nGgWC-.\n[7] Roman Novak, Lechao Xiao, Jaehoon Lee, Y asaman Bahri, Greg Y ang, Jiri Hron, Daniel A.\nAbolaﬁa, Jeffrey Pennington, and Jascha Sohl-Dickstein. Bayesian deep convolutional net-\nworks with many channels are gaussian processes. In International Conference on Learning\nRepresentations, 2019.\n[8] Adrià Garriga-Alonso, Laurence Aitchison, and Carl Edward Rasmussen. Deep convolutional\nnetworks as shallow gaussian processes. In International Conference on Learning Representa-\ntions, 2019.\n[9] Behnam Neyshabur, Ryota Tomioka, and Nathan Srebro. In search of the real inductive bias:\nOn the role of implicit regularization in deep learning. In International Conference on Learning\nRepresentations workshop track, 2015.\n[10] Roman Novak, Y asaman Bahri, Daniel A. Abolaﬁa, Jeffrey Pennington, and Jascha Sohl-\nDickstein. Sensitivity and generalization in neural networks: an empirical study. In International\nConference on Learning Representations, 2018.\n[11] Behnam Neyshabur, Zhiyuan Li, Srinadh Bhojanapalli, Y ann LeCun, and Nathan Srebro. The\nrole of over-parametrization in generalization of neural networks. In International Conference\non Learning Representations, 2019.\n[12] Alexander G. de G. Matthews, Jiri Hron, Richard E. Turner, and Zoubin Ghahramani. Sample-\nthen-optimize posterior sampling for bayesian linear models. In NeurIPS Workshop on Advances\nin Approximate Bayesian Inference , 2017. URL http://approximateinference.org/\n2017/accepted/MatthewsEtAl2017.pdf.\n[13] Arthur Jacot, Franck Gabriel, and Clement Hongler. Neural tangent kernel: Convergence and\ngeneralization in neural networks. In Advances in Neural Information Processing Systems ,\n2018.\n[14] Sergey Zagoruyko and Nikos Komodakis. Wide residual networks. In British Machine Vision\nConference, 2016.\n[15] Roman Novak, Lechao Xiao, Jiri Hron, Jaehoon Lee, Jascha Sohl-Dickstein, and Samuel S.\nSchoenholz. Neural tangents: Fast and easy inﬁnite neural networks in python, 2019. URL\nhttp://github.com/google/neural-tangents.\n[16] Amit Daniely, Roy Frostig, and Y oram Singer. Toward deeper understanding of neural networks:\nThe power of initialization and a dual view on expressivity. In Advances In Neural Information\nProcessing Systems, 2016.\n10\n[17] Amit Daniely. SGD learns the conjugate kernel class of the network. In Advances in Neural\nInformation Processing Systems, 2017.\n[18] Andrew M Saxe, James L McClelland, and Surya Ganguli. Exact solutions to the nonlinear\ndynamics of learning in deep linear neural networks. In International Conference on Learning\nRepresentations, 2014.\n[19] Simon S Du, Jason D Lee, Haochuan Li, Liwei Wang, and Xiyu Zhai. Gradient descent ﬁnds\nglobal minima of deep neural networks. In International Conference on Machine Learning ,\n2019.\n[20] Zeyuan Allen-Zhu, Y uanzhi Li, and Zhao Song. A convergence theory for deep learning via\nover-parameterization. In International Conference on Machine Learning , 2019.\n[21] Zeyuan Allen-Zhu, Y uanzhi Li, and Zhao Song. On the convergence rate of training recurrent\nneural networks. arXiv preprint arXiv:1810.12065, 2018.\n[22] Difan Zou, Y uan Cao, Dongruo Zhou, and Quanquan Gu. Stochastic gradient descent optimizes\nover-parameterized deep relu networks. Machine Learning, 2019.\n[23] Song Mei, Andrea Montanari, and Phan-Minh Nguyen. A mean ﬁeld view of the landscape\nof two-layer neural networks. Proceedings of the National Academy of Sciences , 115(33):\nE7665–E7671, 2018.\n[24] Lenaic Chizat and Francis Bach. On the global convergence of gradient descent for over-\nparameterized models using optimal transport. In Advances in neural information processing\nsystems, 2018.\n[25] Grant M Rotskoff and Eric V anden-Eijnden. Parameters as interacting particles: long time\nconvergence and asymptotic error scaling of neural networks. In Advances in neural information\nprocessing systems, 2018.\n[26] Justin Sirignano and Konstantinos Spiliopoulos. Mean ﬁeld analysis of neural networks. arXiv\npreprint arXiv:1805.01053, 2018.\n[27] Xavier Glorot and Y oshua Bengio. Understanding the difﬁculty of training deep feedforward\nneural networks. In International Conference on Artiﬁcial Intelligence and Statistics , pages\n249–256, 2010.\n[28] Lenaic Chizat, Edouard Oyallon, and Francis Bach. On lazy training in differentiable program-\nming. arXiv preprint arXiv:1812.07956, 2018.\n[29] Twan van Laarhoven. L2 regularization versus batch and weight normalization. arXiv preprint\narXiv:1706.05350, 2017.\n[30] Tero Karras, Timo Aila, Samuli Laine, and Jaakko Lehtinen. Progressive growing of GANs for\nimproved quality, stability, and variation. In International Conference on Learning Representa-\ntions, 2018.\n[31] Daniel S. Park, Jascha Sohl-Dickstein, Quoc V . Le, and Samuel L. Smith. The effect of network\nwidth on stochastic gradient descent and generalization: an empirical study. In International\nConference on Machine Learning, 2019.\n[32] Ben Poole, Subhaneil Lahiri, Maithra Raghu, Jascha Sohl-Dickstein, and Surya Ganguli.\nExponential expressivity in deep neural networks through transient chaos. In Advances In\nNeural Information Processing Systems, pages 3360–3368, 2016.\n[33] Samuel S Schoenholz, Justin Gilmer, Surya Ganguli, and Jascha Sohl-Dickstein. Deep informa-\ntion propagation. International Conference on Learning Representations , 2017.\n[34] Lechao Xiao, Y asaman Bahri, Jascha Sohl-Dickstein, Samuel Schoenholz, and Jeffrey Penning-\nton. Dynamical isometry and a mean ﬁeld theory of CNNs: How to train 10,000-layer vanilla\nconvolutional neural networks. In International Conference on Machine Learning , 2018.\n11\n[35] Ge Y ang and Samuel Schoenholz. Mean ﬁeld residual networks: On the edge of chaos. In\nAdvances in Neural Information Processing Systems . 2017.\n[36] Alexander G de G Matthews, Mark Rowland, Jiri Hron, Richard E Turner, and Zoubin\nGhahramani. Gaussian process behaviour in wide deep neural networks. arXiv preprint\narXiv:1804.11271, 9 2018.\n[37] Greg Y ang. Scaling limits of wide neural networks with weight sharing: Gaussian pro-\ncess behavior, gradient independence, and neural tangent kernel derivation. arXiv preprint\narXiv:1902.04760, 2019.\n[38] Sever Silvestru Dragomir. Some Gronwall type inequalities and applications . Nova Science\nPublishers New Y ork, 2003.\n[39] Minmin Chen, Jeffrey Pennington, and Samuel Schoenholz. Dynamical isometry and a mean\nﬁeld theory of RNNs: Gating enables signal propagation in recurrent neural networks. In\nInternational Conference on Machine Learning , 2018.\n[40] Greg Y ang, Jeffrey Pennington, Vinay Rao, Jascha Sohl-Dickstein, and Samuel S. Schoen-\nholz. A mean ﬁeld theory of batch normalization. In International Conference on Learning\nRepresentations, 2019.\n[41] Roy Frostig, Peter Hawkins, Matthew Johnson, Chris Leary, and Dougal Maclaurin. JAX:\nAutograd and XLA. www.github.com/google/jax, 2018.\n[42] Martín Abadi, Paul Barham, Jianmin Chen, Zhifeng Chen, Andy Davis, Jeffrey Dean, Matthieu\nDevin, Sanjay Ghemawat, Geoffrey Irving, Michael Isard, et al. Tensorﬂow: A system for\nlarge-scale machine learning. In 12th USENIX Symposium on Operating Systems Design and\nImplementation (OSDI 16), 2016.\n[43] Chiyuan Zhang, Samy Bengio, and Y oram Singer. Are all layers created equal? arXiv preprint\narXiv:1902.01996, 2019.\n[44] Ning Qian. On the momentum term in gradient descent learning algorithms. Neural networks,\n12(1):145–151, 1999.\n[45] Weijie Su, Stephen Boyd, and Emmanuel Candes. A differential equation for modeling nes-\nterov’s accelerated gradient method: Theory and insights. In Advances in Neural Information\nProcessing Systems, pages 2510–2518, 2014.\n[46] Y oungmin Cho and Lawrence K Saul. Kernel methods for deep learning. In Advances in neural\ninformation processing systems, 2009.\n[47] Christopher KI Williams. Computing with inﬁnite networks. In Advances in neural information\nprocessing systems, pages 295–301, 1997.\n[48] Roman V ershynin. Introduction to the non-asymptotic analysis of random matrices. arXiv\npreprint arXiv:1011.3027, 2010.\n12",
  "values": {
    "Explicability": "No",
    "Critiqability": "No",
    "Interpretable (to users)": "No",
    "Transparent (to users)": "No",
    "Autonomy (power to decide)": "No",
    "Non-maleficence": "No",
    "Privacy": "No",
    "User influence": "No",
    "Respect for Persons": "No",
    "Collective influence": "No",
    "Fairness": "No",
    "Respect for Law and public interest": "No",
    "Not socially biased": "No",
    "Beneficence": "No",
    "Deferral to humans": "No",
    "Justice": "No"
  }
}