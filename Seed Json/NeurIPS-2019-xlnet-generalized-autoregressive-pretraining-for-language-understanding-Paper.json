{
  "pdf": "NeurIPS-2019-xlnet-generalized-autoregressive-pretraining-for-language-understanding-Paper",
  "title": "XLNet: Generalized Autoregressive Pretraining for Language Understanding",
  "author": "Zhilin Yang, Zihang Dai, Yiming Yang, Jaime Carbonell, Russ R. Salakhutdinov, Quoc V. Le",
  "paper_id": "NeurIPS-2019-xlnet-generalized-autoregressive-pretraining-for-language-understanding-Paper",
  "text": "XLNet: Generalized Autoregressive Pretraining\nfor Language Understanding\nZhilin Yang∗1, Zihang Dai∗12, Yiming Yang1, Jaime Carbonell 1,\nRuslan Salakhutdinov1, Quoc V . Le2\n1Carnegie Mellon University, 2Google AI Brain Team\n{zhiliny,dzihang,yiming,jgc,rsalakhu}@cs.cmu.edu, qvl@google.com\nAbstract\nWith the capability of modeling bidirectional contexts, denoising autoencoding\nbased pretraining like BERT achieves better performance than pretraining ap-\nproaches based on autoregressive language modeling. However, relying on corrupt-\ning the input with masks, BERT neglects dependency between the masked positions\nand suffers from a pretrain-ﬁnetune discrepancy. In light of these pros and cons, we\npropose XLNet, a generalized autoregressive pretraining method that (1) enables\nlearning bidirectional contexts by maximizing the expected likelihood over all\npermutations of the factorization order and (2) overcomes the limitations of BERT\nthanks to its autoregressive formulation. Furthermore, XLNet integrates ideas\nfrom Transformer-XL, the state-of-the-art autoregressive model, into pretraining.\nEmpirically, under comparable experiment setting, XLNet outperforms BERT on\n20 tasks, often by a large margin, including question answering, natural language\ninference, sentiment analysis, and document ranking.1.\n1 Introduction\nUnsupervised representation learning has been highly successful in the domain of natural language\nprocessing [7, 22, 27, 28, 10]. Typically, these methods ﬁrst pretrain neural networks on large-scale\nunlabeled text corpora, and then ﬁnetune the models or representations on downstream tasks. Under\nthis shared high-level idea, different unsupervised pretraining objectives have been explored in\nliterature. Among them, autoregressive (AR) language modeling and autoencoding (AE) have been\nthe two most successful pretraining objectives.\nAR language modeling seeks to estimate the probability distribution of a text corpus with an au-\ntoregressive model [7, 27, 28]. Speciﬁcally, given a text sequencex = (x1, · · ·,xT ), AR language\nmodeling factorizes the likelihood into a forward productp(x) =∏T\nt=1p(xt | x<t) or a backward\nonep(x) =∏1\nt=Tp(xt | x>t). A parametric model (e.g. a neural network) is trained to model each\nconditional distribution. Since an AR language model is only trained to encode a uni-directional con-\ntext (either forward or backward), it is not effective at modeling deep bidirectional contexts. On the\ncontrary, downstream language understanding tasks often require bidirectional context information.\nThis results in a gap between AR language modeling and effective pretraining.\nIn comparison, AE based pretraining does not perform explicit density estimation but instead aims to\nreconstruct the original data from corrupted input. A notable example is BERT [10], which has been\nthe state-of-the-art pretraining approach. Given the input token sequence, a certain portion of tokens\nare replaced by a special symbol [MASK], and the model is trained to recover the original tokens from\nthe corrupted version. Since density estimation is not part of the objective, BERT is allowed to utilize\n∗Equal contribution. Order determined by swapping the one in [9].\n1Pretrained models and code are available at https://github.com/zihangdai/xlnet\n33rd Conference on Neural Information Processing Systems (NeurIPS 2019), Vancouver, Canada.\nbidirectional contexts for reconstruction. As an immediate beneﬁt, this closes the aforementioned\nbidirectional information gap in AR language modeling, leading to improved performance. However,\nthe artiﬁcial symbols like [MASK] used by BERT during pretraining are absent from real data at\nﬁnetuning time, resulting in a pretrain-ﬁnetune discrepancy. Moreover, since the predicted tokens are\nmasked in the input, BERT is not able to model the joint probability using the product rule as in AR\nlanguage modeling. In other words, BERT assumes the predicted tokens are independent of each\nother given the unmasked tokens, which is oversimpliﬁed as high-order, long-range dependency is\nprevalent in natural language [9].\nFaced with the pros and cons of existing language pretraining objectives, in this work, we propose\nXLNet, a generalized autoregressive method that leverages the best of both AR language modeling\nand AE while avoiding their limitations.\n• Firstly, instead of using a ﬁxed forward or backward factorization order as in conventional AR mod-\nels, XLNet maximizes the expected log likelihood of a sequence w.r.t. all possible permutations\nof the factorization order . Thanks to the permutation operation, the context for each position can\nconsist of tokens from both left and right. In expectation, each position learns to utilize contextual\ninformation from all positions, i.e., capturing bidirectional context.\n• Secondly, as a generalized AR language model, XLNet does not rely on data corruption. Hence,\nXLNet does not suffer from the pretrain-ﬁnetune discrepancy that BERT is subject to. Meanwhile,\nthe autoregressive objective also provides a natural way to use the product rule for factorizing the\njoint probability of the predicted tokens, eliminating the independence assumption made in BERT.\nIn addition to a novel pretraining objective, XLNet improves architectural designs for pretraining.\n• Inspired by the latest advancements in AR language modeling, XLNet integrates the segment\nrecurrence mechanism and relative encoding scheme of Transformer-XL [9] into pretraining, which\nempirically improves the performance especially for tasks involving a longer text sequence.\n• Naively applying a Transformer(-XL) architecture to permutation-based language modeling does\nnot work because the factorization order is arbitrary and the target is ambiguous. As a solution, we\npropose to reparameterize the Transformer(-XL) network to remove the ambiguity.\nEmpirically, under comparable experiment setting, XLNet consistently outperforms BERT [10] on a\nwide spectrum of problems including GLUE language understanding tasks, reading comprehension\ntasks like SQuAD and RACE, text classiﬁcation tasks such as Yelp and IMDB, and the ClueWeb09-B\ndocument ranking task.\nRelated Work The idea of permutation-based AR modeling has been explored in [32, 12], but there\nare several key differences. Firstly, previous models aim to improve density estimation by baking\nan “orderless” inductive bias into the model while XLNet is motivated by enabling AR language\nmodels to learn bidirectional contexts. Technically, to construct a valid target-aware prediction\ndistribution, XLNet incorporates the target position into the hidden state via two-stream attention\nwhile previous permutation-based AR models relied on implicit position awareness inherent to their\nMLP architectures. Finally, for both orderless NADE and XLNet, we would like to emphasize that\n“orderless” does not mean that the input sequence can be randomly permuted but that the model\nallows for different factorization orders of the distribution.\nAnother related idea is to perform autoregressive denoising in the context of text generation [ 11],\nwhich only considers a ﬁxed order though.\n2 Proposed Method\n2.1 Background\nIn this section, we ﬁrst review and compare the conventional AR language modeling and BERT for\nlanguage pretraining. Given a text sequence x = [x1, · · ·,xT ], AR language modeling performs\npretraining by maximizing the likelihood under the forward autoregressive factorization:\nmax\nθ\nlogpθ(x) =\nT∑\nt=1\nlogpθ(xt | x<t) =\nT∑\nt=1\nlog exp\n(\nhθ(x1:t−1)⊤e(xt)\n)\n∑\nx′ exp (hθ(x1:t−1)⊤e(x′)), (1)\n2\nwherehθ(x1:t−1) is a context representation produced by neural models, such as RNNs or Transform-\ners, ande(x) denotes the embedding ofx. In comparison, BERT is based on denoising auto-encoding.\nSpeciﬁcally, for a text sequencex, BERT ﬁrst constructs a corrupted versionˆx by randomly setting\na portion (e.g. 15%) of tokens in x to a special symbol [MASK]. Let the masked tokens be ¯x. The\ntraining objective is to reconstruct ¯x from ˆx:\nmax\nθ\nlogpθ(¯x | ˆx) ≈\nT∑\nt=1\nmt logpθ(xt | ˆx) =\nT∑\nt=1\nmt log exp\n(\nHθ(ˆx)⊤\nt e(xt)\n)\n∑\nx′ exp\n(\nHθ(ˆx)⊤\nt e(x′)\n), (2)\nwheremt = 1 indicatesxt is masked, andHθ is a Transformer that maps a length-T text sequence x\ninto a sequence of hidden vectorsHθ(x) = [Hθ(x)1,Hθ(x)2, · · ·,Hθ(x)T ]. The pros and cons of\nthe two pretraining objectives are compared in the following aspects:\n• Independence Assumption: As emphasized by the ≈ sign in Eq. (2), BERT factorizes the joint\nconditional probabilityp(¯x | ˆx) based on an independence assumption that all masked tokens ¯x\nare separately reconstructed. In comparison, the AR language modeling objective (1) factorizes\npθ(x) using the product rule that holds universally without such an independence assumption.\n• Input noise : The input to BERT contains artiﬁcial symbols like [MASK] that never occur in\ndownstream tasks, which creates a pretrain-ﬁnetune discrepancy. Replacing[MASK] with original\ntokens as in [10] does not solve the problem because original tokens can be only used with a small\nprobability — otherwise Eq. (2) will be trivial to optimize. In comparison, AR language modeling\ndoes not rely on any input corruption and does not suffer from this issue.\n• Context dependency: The AR representation hθ(x1:t−1) is only conditioned on the tokens up\nto position t (i.e. tokens to the left), while the BERT representation Hθ(x)t has access to the\ncontextual information on both sides. As a result, the BERT objective allows the model to be\npretrained to better capture bidirectional context.\n2.2 Objective: Permutation Language Modeling\nAccording to the comparison above, AR language modeling and BERT possess their unique advan-\ntages over the other. A natural question to ask is whether there exists a pretraining objective that\nbrings the advantages of both while avoiding their weaknesses.\nBorrowing ideas from orderless NADE [32], we propose the permutation language modeling objective\nthat not only retains the beneﬁts of AR models but also allows models to capture bidirectional\ncontexts. Speciﬁcally, for a sequencex of lengthT , there areT ! different orders to perform a valid\nautoregressive factorization. Intuitively, if model parameters are shared across all factorization orders,\nin expectation, the model will learn to gather information from all positions on both sides.\nTo formalize the idea, let ZT be the set of all possible permutations of the length-T index sequence\n[1, 2,...,T ]. We usezt and z<t to denote thet-th element and the ﬁrstt−1 elements of a permutation\nz ∈ ZT . Then, our proposed permutation language modeling objective can be expressed as follows:\nmax\nθ\nEz∼ZT\n[ T∑\nt=1\nlogpθ(xzt | xz<t)\n]\n. (3)\nEssentially, for a text sequence x, we sample a factorization order z at a time and decompose the\nlikelihoodpθ(x) according to factorization order. Since the same model parameterθ is shared across\nall factorization orders during training, in expectation,xt has seen every possible elementxi ̸=xt in\nthe sequence, hence being able to capture the bidirectional context. Moreover, as this objective ﬁts\ninto the AR framework, it naturally avoids the independence assumption and the pretrain-ﬁnetune\ndiscrepancy discussed in Section 2.1.\nRemark on Permutation The proposed objective only permutes the factorization order, not the\nsequence order. In other words, we keep the original sequence order, use the positional encodings\ncorresponding to the original sequence, and rely on a proper attention mask in Transformers to\nachieve permutation of the factorization order. Note that this choice is necessary, since the model\nwill only encounter text sequences with the natural order during ﬁnetuning.\nTo provide an overall picture, we show an example of predicting the tokenx3 given the same input\nsequence x but under different factorization orders in the Appendix A.7 with Figure 4.\n3\n2.3 Architecture: Two-Stream Self-Attention for Target-Aware Representations\nSample a factorization order:\n3 à2 à4 à1\nAttention Masks\ne(x$)w e(x')w e(x()w e(x))w\nh$\n($)g$\n($) h'\n($)g'\n($) h(\n($)g(\n($) h)\n($)g)\n($)\nh$\n(')g$\n(') h'\n(')g'\n(') h(\n(')g(\n(')\nh)\n(')g)\n(')\nContent stream:\ncan see self\nQuery stream:\ncannot see self\nx$ x' x( x)\nMasked Two-stream Attention\nMasked Two-stream Attention\n(c)\nh$\n(,)g$\n(,) h'\n(,)g'\n(,) h(\n(,)g(\n(,) h)\n(,)g)\n(,)\nh$\n($)g$\n($)\nAttention\nQ K, V\nh$\n($)g$\n($)\nAttention\nQ K, V\n(b)\n(a)\nh$\n(,)g$\n(,) h'\n(,)g'\n(,) h(\n(,)g(\n(,) h)\n(,)g)\n(,)\nFigure 1: (a): Content stream attention, which is the same as the standard self-attention. (b): Query\nstream attention, which does not have access information about the contentxzt. (c): Overview of the\npermutation language modeling training with two-stream attention.\nWhile the permutation language modeling objective has desired properties, naive implementation with\nstandard Transformer parameterization may not work. To see the problem, assume we parameterize\nthe next-token distributionpθ(Xzt | xz<t) using the standard Softmax formulation, i.e.,pθ(Xzt =\nx | xz<t) =\nexp(e(x)⊤hθ(xz<t ))∑\nx′ exp(e(x′)⊤hθ(xz<t )), wherehθ(xz<t) denotes the hidden representation of xz<t\nproduced by the shared Transformer network after proper masking. Now notice that the representation\nhθ(xz<t) does not depend on which position it will predict, i.e., the value ofzt. Consequently, the\nsame distribution is predicted regardless of the target position, which is not able to learn useful\nrepresentations (see Appendix A.1 for a concrete example). To avoid this problem, we propose to\nre-parameterize the next-token distribution to be target position aware:\npθ(Xzt =x | xz<t) = exp\n(\ne(x)⊤gθ(xz<t,zt)\n)\n∑\nx′ exp (e(x′)⊤gθ(xz<t,zt)), (4)\nwheregθ(xz<t,zt) denotes a new type of representations which additionally take the target position\nzt as input.\nTwo-Stream Self-Attention While the idea of target-aware representations removes the ambiguity\nin target prediction, how to formulate gθ(xz<t,zt) remains a non-trivial problem. Among other\npossibilities, we propose to “stand” at the target position zt and rely on the position zt to gather\ninformation from the context xz<t through attention. For this parameterization to work, there are two\nrequirements that are contradictory in a standard Transformer architecture: (1) to predict the token\nxzt,gθ(xz<t,zt) should only use the positionzt and not the contentxzt, otherwise the objective\nbecomes trivial; (2) to predict the other tokensxzj withj >t,gθ(xz<t,zt) should also encode the\ncontentxzt to provide full contextual information. To resolve such a contradiction, we propose to use\ntwo sets of hidden representations instead of one:\n• The content representation hθ(xz≤t), or abbreviated as hzt, which serves a similar role to the\nstandard hidden states in Transformer. This representation encodes both the context andxzt itself.\n• The query representationgθ(xz<t,zt), or abbreviated asgzt, which only has access to the contex-\ntual information xz<t and the positionzt, but not the contentxzt, as discussed above.\nComputationally, the ﬁrst layer query stream is initialized with a trainable vector, i.e. g(0)\ni = w,\nwhile the content stream is set to the corresponding word embedding, i.e. h(0)\ni =e(xi). For each\nself-attention layerm = 1,...,M , the two streams of representations are schematically2 updated\n2To avoid clutter, we omit the implementation details including multi-head attention, residual connection,\nlayer normalization and position-wise feed-forward as used in Transformer(-XL). The details are included in\nAppendix A.2 for reference.\n4\nwith a shared set of parameters as follows (illustrated in Figures 1 (a) and (b)):\ng(m)\nzt ← Attention(Q =g(m−1)\nzt , KV = h(m−1)\nz<t ;θ), (query stream: usezt but cannot seexzt)\nh(m)\nzt ← Attention(Q =h(m−1)\nzt , KV = h(m−1)\nz≤t ;θ), (content stream: use bothzt andxzt).\nwhere Q, K, V denote the query, key, and value in an attention operation [33]. The update rule of the\ncontent representations is exactly the same as the standard self-attention, so during ﬁnetuning, we\ncan simply drop the query stream and use the content stream as a normal Transformer(-XL). Finally,\nwe can use the last-layer query representationg(M )\nzt to compute Eq. (4).\nPartial Prediction While the permutation language modeling objective (3) has several beneﬁts, it is\na much more challenging optimization problem due to the permutation and causes slow convergence\nin preliminary experiments. To reduce the optimization difﬁculty, we choose to only predict the last\ntokens in a factorization order. Formally, we split z into a non-target subsequence z≤c and a target\nsubsequence z>c, wherec is the cutting point. The objective is to maximize the log-likelihood of the\ntarget subsequence conditioned on the non-target subsequence, i.e.,\nmax\nθ\nEz∼ZT\n[\nlogpθ(xz>c | xz≤c)\n]\n= Ez∼ZT\n\n\n|z|∑\nt=c+1\nlogpθ(xzt | xz<t)\n\n. (5)\nNote that z>c is chosen as the target because it possesses the longest context in the sequence given the\ncurrent factorization order z. A hyperparameterK is used such that about 1/K tokens are selected\nfor predictions; i.e., |z|/(|z| −c) ≈K. For unselected tokens, their query representations need not\nbe computed, which saves speed and memory.\n2.4 Incorporating Ideas from Transformer-XL\nSince our objective function ﬁts in the AR framework, we incorporate the state-of-the-art AR\nlanguage model, Transformer-XL [9], into our pretraining framework, and name our method after it.\nWe integrate two important techniques in Transformer-XL, namely the relative positional encoding\nscheme and the segment recurrence mechanism. We apply relative positional encodings based on the\noriginal sequence as discussed earlier, which is straightforward. Now we discuss how to integrate the\nrecurrence mechanism into the proposed permutation setting and enable the model to reuse hidden\nstates from previous segments. Without loss of generality, suppose we have two segments taken from\na long sequence s; i.e., ˜x = s1:T and x = sT +1:2T . Let ˜z and z be permutations of [1 · · ·T ] and\n[T + 1 · · ·2T ] respectively. Then, based on the permutation ˜z, we process the ﬁrst segment, and then\ncache the obtained content representations ˜h(m) for each layerm. Then, for the next segment x, the\nattention update with memory can be written as\nh(m)\nzt ← Attention(Q =h(m−1)\nzt , KV =\n[\n˜h(m−1), h(m−1)\nz≤t\n]\n;θ)\nwhere [.,. ] denotes concatenation along the sequence dimension. Notice that positional encodings\nonly depend on the actual positions in the original sequence. Thus, the above attention update is\nindependent of ˜z once the representations ˜h(m) are obtained. This allows caching and reusing the\nmemory without knowing the factorization order of the previous segment. In expectation, the model\nlearns to utilize the memory over all factorization orders of the last segment. The query stream can\nbe computed in the same way. Finally, Figure 1 (c) presents an overview of the proposed permutation\nlanguage modeling with two-stream attention (see Appendix A.7 for more detailed illustration).\n2.5 Modeling Multiple Segments\nMany downstream tasks have multiple input segments, e.g., a question and a context paragraph in\nquestion answering. We now discuss how we pretrain XLNet to model multiple segments in the\nautoregressive framework. During the pretraining phase, following BERT, we randomly sample two\nsegments (either from the same context or not) and treat the concatenation of two segments as one\nsequence to perform permutation language modeling. We only reuse the memory that belongs to\nthe same context. Speciﬁcally, the input to our model is the same as BERT: [CLS, A, SEP, B, SEP],\nwhere “SEP” and “CLS” are two special symbols and “A” and “B” are the two segments. Although\n5\nwe follow the two-segment data format, XLNet-Large does not use the objective of next sentence\nprediction [10] as it does not show consistent improvement in our ablation study (see Section 3.4).\nRelative Segment Encodings Architecturally, different from BERT that adds an absolute segment\nembedding to the word embedding at each position, we extend the idea of relative encodings from\nTransformer-XL to also encode the segments. Given a pair of positionsi andj in the sequence, if\ni andj are from the same segment, we use a segment encoding sij = s+ or otherwise sij = s−,\nwhere s+ and s− are learnable model parameters for each attention head. In other words, we only\nconsider whether the two positions are within the same segment, as opposed to considering which\nspeciﬁc segments they are from. This is consistent with the core idea of relative encodings; i.e., only\nmodeling the relationships between positions. When i attends toj, the segment encoding sij is used\nto compute an attention weightaij = (qi + b)⊤sij, where qi is the query vector as in a standard\nattention operation and b is a learnable head-speciﬁc bias vector. Finally, the valueaij is added to\nthe normal attention weight. There are two beneﬁts of using relative segment encodings. First, the\ninductive bias of relative encodings improves generalization [9]. Second, it opens the possibility of\nﬁnetuning on tasks that have more than two input segments, which is not possible using absolute\nsegment encodings.\n2.6 Discussion\nComparing Eq. (2) and (5), we observe that both BERT and XLNet perform partial prediction, i.e.,\nonly predicting a subset of tokens in the sequence. This is a necessary choice for BERT because if all\ntokens are masked, it is impossible to make any meaningful predictions. In addition, for both BERT\nand XLNet, partial prediction plays a role of reducing optimization difﬁculty by only predicting\ntokens with sufﬁcient context. However, the independence assumption discussed in Section 2.1\ndisables BERT to model dependency between targets.\nTo better understand the difference, let’s consider a concrete example [New, York, is, a, city]. Suppose\nboth BERT and XLNet select the two tokens [New, York] as the prediction targets and maximize\nlogp(New York | is a city). Also suppose that XLNet samples the factorization order [is, a, city,\nNew, York]. In this case, BERT and XLNet respectively reduce to the following objectives:\nJBERT = logp(New | is a city) + logp(York | is a city),\nJXLNet = logp(New | is a city) + logp(York | New, is a city).\nNotice that XLNet is able to capture the dependency between the pair (New, York), which is omitted\nby BERT. Although in this example, BERT learns some dependency pairs such as (New, city) and\n(York, city), it is obvious that XLNet always learns more dependency pairs given the same target and\ncontains “denser” effective training signals.\nFor more formal analysis and further discussion, please refer to Appendix A.5.\n3 Experiments\n3.1 Pretraining and Implementation\nFollowing BERT [10], we use the BooksCorpus [40] and English Wikipedia as part of our pretraining\ndata, which have 13GB plain text combined. In addition, we include Giga5 (16GB text) [ 26],\nClueWeb 2012-B (extended from [5]), and Common Crawl [6] for pretraining. We use heuristics\nto aggressively ﬁlter out short or low-quality articles for ClueWeb 2012-B and Common Crawl,\nwhich results in 19GB and 110GB text respectively. After tokenization with SentencePiece [17], we\nobtain 2.78B, 1.09B, 4.75B, 4.30B, and 19.97B subword pieces for Wikipedia, BooksCorpus, Giga5,\nClueWeb, and Common Crawl respectively, which are 32.89B in total.\nOur largest model XLNet-Large has the same architecture hyperparameters as BERT-Large, which\nresults in a similar model size. During pretraining, we always use a full sequence length of 512.\nFirstly, to provide a fair comparison with BERT (section 3.2), we also trained XLNet-Large-wikibooks\non BooksCorpus and Wikipedia only, where we reuse all pretraining hyper-parameters as in the\noriginal BERT. Then, we scale up the training of XLNet-Large by using all the datasets described\nabove. Speciﬁcally, we train on 512 TPU v3 chips for 500K steps with an Adam weight decay\noptimizer, linear learning rate decay, and a batch size of 8192, which takes about 5.5 days. It was\n6\nobserved that the model still underﬁts the data at the end of training. Finally, we perform ablation\nstudy (section 3.4) based on the XLNet-Base-wikibooks.\nSince the recurrence mechanism is introduced, we use a bidirectional data input pipeline where each\nof the forward and backward directions takes half of the batch size. For training XLNet-Large, we set\nthe partial prediction constantK as 6 (see Section 2.3). Our ﬁnetuning procedure follows BERT [10]\nexcept otherwise speciﬁed3. We employ an idea of span-based prediction, where we ﬁrst sample a\nlengthL ∈ [1, · · ·, 5], and then randomly select a consecutive span ofL tokens as prediction targets\nwithin a context of (KL) tokens.\nWe use a variety of natural language understanding datasets to evaluate the performance of our\nmethod. Detailed descriptions of the settings for all the datasets can be found in Appendix A.3.\n3.2 Fair Comparison with BERT\nModel SQuAD1.1 SQuAD2.0 RACE MNLI QNLI QQP RTE SST-2 MRPC CoLA STS-B\nBERT-Large\n(Best of 3)\n86.7/92.8 82.8/85.5 75.1 87.3 93.0 91.4 74.0 94.0 88.7 63.7 90.2\nXLNet-Large-\nwikibooks\n88.2/94.0 85.1/87.8 77.4 88.4 93.9 91.8 81.2 94.4 90.0 65.2 91.1\nTable 1: Fair comparison with BERT. All models are trained using the same data and hyperparameters as in\nBERT. We use the best of 3 BERT variants for comparison; i.e., the original BERT, BERT with whole word\nmasking, and BERT without next sentence prediction.\nHere, we ﬁrst compare the performance of BERT and XLNet in a fair setting to decouple the effects\nof using more data and the improvement from BERT to XLNet. In Table 1, we compare (1) best\nperformance of three different variants of BERT and (2) XLNet trained with the same data and\nhyperparameters. As we can see, trained on the same data with an almost identical training recipe,\nXLNet outperforms BERT by a sizable margin on all the considered datasets.\n3.3 Results After Scaling Up\nRACE Accuracy Middle High Model NDCG@20 ERR@20\nGPT [28] 59.0 62.9 57.4 DRMM [13] 24.3 13.8\nBERT [25] 72.0 76.6 70.1 KNRM [8] 26.9 14.9\nBERT+DCMN∗ [38] 74.1 79.5 71.8 Conv [8] 28.7 18.1\nRoBERTa [21] 83.2 86.5 81.8 BERT† 30.53 18.67\nXLNet 85.4 88.6 84.0 XLNet 31.10 20.28\nTable 2: Comparison with state-of-the-art results on the test set of RACE, a reading comprehension task, and on\nClueWeb09-B, a document ranking task.∗ indicates using ensembles.† indicates our implementations. “Middle”\nand “High” in RACE are two subsets representing middle and high school difﬁculty levels. All BERT, RoBERTa,\nand XLNet results are obtained with a 24-layer architecture with similar model sizes (aka BERT-Large).\nAfter the initial publication of our manuscript, a few other pretrained models were released such as\nRoBERTa [21] and ALBERT [19]. Since ALBERT involves increasing the model hidden size from\n1024 to 2048/4096 and thus substantially increases the amount of computation in terms of FLOPs, we\nexclude ALBERT from the following results as it is hard to lead to scientiﬁc conclusions. To obtain\nrelatively fair comparison with RoBERTa, the experiment in this section is based on full data and\nreuses the hyper-parameters of RoBERTa, as described in section 3.1.\nThe results are presented in Tables 2 (reading comprehension & document ranking), 3 (question\nanswering), 4 (text classiﬁcation) and 5 (natural language understanding), where XLNet generally\noutperforms BERT and RoBERTa. In addition, we make two more interesting observations:\n3Hyperparameters for pretraining and ﬁnetuning are in Appendix A.4.\n7\nSQuAD2.0 EM F1 SQuAD1.1 EM F1\nDev set results (single model)\nBERT [10] 78.98 81.77 BERT† [10] 84.1 90.9\nRoBERTa [21] 86.5 89.4 RoBERTa [21] 88.9 94.6\nXLNet 87.9 90.6 XLNet 89.7 95.1\nTest set results on leaderboard (single model, as of Dec 14, 2019)\nBERT∗ [10] 80.005 83.061\nRoBERTa [21] 86.820 89.795\nXLNet 87.926 90.689\nTable 3: Results on SQuAD, a reading comprehension dataset.† marks our runs with the ofﬁcial code. We are\nnot able to obtain the test results on SQuAD1.1 from the organizers after submitting our result for more than one\nmonth.\nModel IMDB Y elp-2 Y elp-5 DBpedia AG Amazon-2 Amazon-5\nCNN [15] - 2.90 32.39 0.84 6.57 3.79 36.24\nDPCNN [15] - 2.64 30.58 0.88 6.87 3.32 34.81\nMixed V AT [31, 23] 4.32 - - 0.70 4.95 - -\nULMFiT [14] 4.6 2.16 29.98 0.80 5.01 - -\nBERT [35] 4.51 1.89 29.32 0.64 - 2.63 34.17\nXLNet 3.20 1.37 27.05 0.60 4.45 2.11 31.67\nTable 4: Comparison with state-of-the-art error rates on the test sets of several text classiﬁcation datasets. All\nBERT and XLNet results are obtained with a 24-layer architecture with similar model sizes (aka BERT-Large).\nModel MNLI QNLI QQP RTE SST-2 MRPC CoLA STS-B WNLI\nSingle-task single models on dev\nBERT [2] 86.6/- 92.3 91.3 70.4 93.2 88.0 60.6 90.0 -\nRoBERTa [21] 90.2/90.2 94.7 92.2 86.6 96.4 90.9 68.0 92.4 -\nXLNet 90.8/90.8 94.9 92.3 85.9 97.0 90.8 69.0 92.5 -\nMulti-task ensembles on test (from leaderboard as of Oct 28, 2019)\nMT-DNN∗ [20] 87.9/87.4 96.0 89.9 86.3 96.5 92.7 68.4 91.1 89.0\nRoBERTa∗ [21] 90.8/90.2 98.9 90.2 88.2 96.7 92.3 67.8 92.2 89.0\nXLNet∗ 90.9/90.9† 99.0† 90.4† 88.5 97.1 † 92.9 70.2 93.0 92.5\nTable 5: Results on GLUE.∗ indicates using ensembles, and† denotes single-task results in a multi-task row.\nAll dev results are the median of 10 runs. The upper section shows direct comparison on dev data and the lower\nsection shows comparison with state-of-the-art results on the public leaderboard.\n• For explicit reasoning tasks like SQuAD and RACE that involve longer context, the performance\ngain of XLNet is usually larger. This superiority at dealing with longer context could come from\nthe Transformer-XL backbone in XLNet.\n• For classiﬁcation tasks that already have abundant supervised examples such as MNLI (>390K),\nYelp (>560K) and Amazon (>3M), XLNet still lead to substantial gains.\n3.4 Ablation Study\nWe perform an ablation study to understand the importance of each design choice based on four\ndatasets with diverse characteristics. Speciﬁcally, there are three main aspects we hope to study:\n• The effectiveness of the permutation language modeling objective alone, especially compared to\nthe denoising auto-encoding objective used by BERT.\n• The importance of using Transformer-XL as the backbone neural architecture.\n• The necessity of some implementation details including span-based prediction, the bidirectional\ninput pipeline, and next-sentence prediction.\nWith these purposes in mind, in Table 6, we compare 6 XLNet-Base variants with different implemen-\ntation details (rows 3 - 8), the original BERT-Base model (row 1), and an additional Transformer-XL\n8\nbaseline trained with the denoising auto-encoding (DAE) objective used in BERT but with the bidi-\nrectional input pipeline (row 2). For fair comparison, all models are based on a 12-layer architecture\nwith the same model hyper-parameters as BERT-Base and are trained on only Wikipedia and the\nBooksCorpus. All results reported are the median of 5 runs.\n# Model RACE SQuAD2.0 MNLI SST-2\nF1 EM m/mm\n1 BERT-Base 64.3 76.30 73.66 84.34/84.65 92.78\n2 DAE + Transformer-XL 65.03 79.56 76.80 84.88/84.45 92.60\n3 XLNet-Base ( K = 7) 66.05 81.33 78.46 85.84/85.43 92.66\n4 XLNet-Base ( K = 6) 66.66 80.98 78.18 85.63/85.12 93.35\n5 - memory 65.55 80.15 77.27 85.32/85.05 92.78\n6 - span-based pred 65.95 80.61 77.91 85.49/85.02 93.12\n7 - bidirectional data 66.34 80.65 77.87 85.31/84.99 92.66\n8 + next-sent pred 66.76 79.83 76.94 85.32/85.09 92.89\nTable 6: The results of BERT on RACE are taken from [38]. We run BERT on the other datasets using the\nofﬁcial implementation and the same hyperparameter search space as XLNet.K is a hyperparameter to control\nthe optimization difﬁculty (see Section 2.3).\nExamining rows 1 - 4 of Table 6, we can see both Transformer-XL and the permutation LM clearly\ncontribute the superior performance of XLNet over BERT. Moreover, if we remove the memory\ncaching mechanism (row 5), the performance clearly drops, especially for RACE which involves the\nlongest context among the 4 tasks. In addition, rows 6 - 7 show that both span-based prediction and\nthe bidirectional input pipeline play important roles in XLNet. Finally, we unexpectedly ﬁnd the the\nnext-sentence prediction objective proposed in the original BERT does not necessarily lead to an\nimprovement in our setting. Hence, we exclude the next-sentence prediction objective from XLNet.\nFinally, we also perform a qualitative study of the attention patterns, which is included in Appendix\nA.6 due to page limit.\n4 Conclusions\nXLNet is a generalized AR pretraining method that uses a permutation language modeling objective\nto combine the advantages of AR and AE methods. The neural architecture of XLNet is developed to\nwork seamlessly with the AR objective, including integrating Transformer-XL and the careful design\nof the two-stream attention mechanism. XLNet achieves substantial improvement over previous\npretraining objectives on various tasks.\nAcknowledgments\nThe authors would like to thank Qizhe Xie and Adams Wei Yu for providing useful feedback on the\nproject, Jamie Callan for providing the ClueWeb dataset, Youlong Cheng, Yanping Huang and Shibo\nWang for providing ideas to improve our TPU implementation, Chenyan Xiong and Zhuyun Dai\nfor clarifying the setting of the document ranking task. ZY and RS were supported by the Ofﬁce of\nNaval Research grant N000141812861, the National Science Foundation (NSF) grant IIS1763562,\nthe Nvidia fellowship, and the Siebel scholarship. ZD and YY were supported in part by NSF under\nthe grant IIS-1546329 and by the DOE-Ofﬁce of Science under the grant ASCR #KJ040201.\nReferences\n[1] Rami Al-Rfou, Dokook Choe, Noah Constant, Mandy Guo, and Llion Jones. Character-level\nlanguage modeling with deeper self-attention. arXiv preprint arXiv:1808.04444, 2018.\n[2] Anonymous. Bam! born-again multi-task networks for natural language understanding. anony-\nmous preprint under review, 2018.\n[3] Alexei Baevski and Michael Auli. Adaptive input representations for neural language modeling.\narXiv preprint arXiv:1809.10853, 2018.\n[4] Yoshua Bengio and Samy Bengio. Modeling high-dimensional discrete data with multi-layer\nneural networks. In Advances in Neural Information Processing Systems, pages 400–406, 2000.\n9\n[5] Jamie Callan, Mark Hoy, Changkuk Yoo, and Le Zhao. Clueweb09 data set, 2009.\n[6] Common Crawl. Common crawl. URl: http://http://commoncrawl. org, 2019.\n[7] Andrew M Dai and Quoc V Le. Semi-supervised sequence learning. In Advances in neural\ninformation processing systems, pages 3079–3087, 2015.\n[8] Zhuyun Dai, Chenyan Xiong, Jamie Callan, and Zhiyuan Liu. Convolutional neural networks\nfor soft-matching n-grams in ad-hoc search. In Proceedings of the eleventh ACM international\nconference on web search and data mining, pages 126–134. ACM, 2018.\n[9] Zihang Dai, Zhilin Yang, Yiming Yang, William W Cohen, Jaime Carbonell, Quoc V Le,\nand Ruslan Salakhutdinov. Transformer-xl: Attentive language models beyond a ﬁxed-length\ncontext. arXiv preprint arXiv:1901.02860, 2019.\n[10] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. Bert: Pre-training of\ndeep bidirectional transformers for language understanding. arXiv preprint arXiv:1810.04805,\n2018.\n[11] William Fedus, Ian Goodfellow, and Andrew M Dai. Maskgan: better text generation via ﬁlling\nin the_. arXiv preprint arXiv:1801.07736, 2018.\n[12] Mathieu Germain, Karol Gregor, Iain Murray, and Hugo Larochelle. Made: Masked autoencoder\nfor distribution estimation. In International Conference on Machine Learning, pages 881–889,\n2015.\n[13] Jiafeng Guo, Yixing Fan, Qingyao Ai, and W Bruce Croft. A deep relevance matching model for\nad-hoc retrieval. In Proceedings of the 25th ACM International on Conference on Information\nand Knowledge Management, pages 55–64. ACM, 2016.\n[14] Jeremy Howard and Sebastian Ruder. Universal language model ﬁne-tuning for text classiﬁca-\ntion. arXiv preprint arXiv:1801.06146, 2018.\n[15] Rie Johnson and Tong Zhang. Deep pyramid convolutional neural networks for text catego-\nrization. In Proceedings of the 55th Annual Meeting of the Association for Computational\nLinguistics (Volume 1: Long Papers), pages 562–570, 2017.\n[16] Vid Kocijan, Ana-Maria Cretu, Oana-Maria Camburu, Yordan Yordanov, and Thomas\nLukasiewicz. A surprisingly robust trick for winograd schema challenge. arXiv preprint\narXiv:1905.06290, 2019.\n[17] Taku Kudo and John Richardson. Sentencepiece: A simple and language independent subword\ntokenizer and detokenizer for neural text processing. arXiv preprint arXiv:1808.06226, 2018.\n[18] Guokun Lai, Qizhe Xie, Hanxiao Liu, Yiming Yang, and Eduard Hovy. Race: Large-scale\nreading comprehension dataset from examinations. arXiv preprint arXiv:1704.04683, 2017.\n[19] Zhenzhong Lan, Mingda Chen, Sebastian Goodman, Kevin Gimpel, Piyush Sharma, and Radu\nSoricut. Albert: A lite bert for self-supervised learning of language representations. arXiv\npreprint arXiv:1909.11942, 2019.\n[20] Xiaodong Liu, Pengcheng He, Weizhu Chen, and Jianfeng Gao. Multi-task deep neural networks\nfor natural language understanding. arXiv preprint arXiv:1901.11504, 2019.\n[21] Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike\nLewis, Luke Zettlemoyer, and Veselin Stoyanov. Roberta: A robustly optimized bert pretraining\napproach. arXiv preprint arXiv:1907.11692, 2019.\n[22] Bryan McCann, James Bradbury, Caiming Xiong, and Richard Socher. Learned in translation:\nContextualized word vectors. In Advances in Neural Information Processing Systems, pages\n6294–6305, 2017.\n[23] Takeru Miyato, Andrew M Dai, and Ian Goodfellow. Adversarial training methods for semi-\nsupervised text classiﬁcation. arXiv preprint arXiv:1605.07725, 2016.\n[24] Aaron van den Oord, Nal Kalchbrenner, and Koray Kavukcuoglu. Pixel recurrent neural\nnetworks. arXiv preprint arXiv:1601.06759, 2016.\n[25] Xiaoman Pan, Kai Sun, Dian Yu, Heng Ji, and Dong Yu. Improving question answering with\nexternal knowledge. arXiv preprint arXiv:1902.00993, 2019.\n10\n[26] Robert Parker, David Graff, Junbo Kong, Ke Chen, and Kazuaki Maeda. English gigaword\nﬁfth edition, linguistic data consortium. Technical report, Technical Report. Linguistic Data\nConsortium, Philadelphia, Tech. Rep., 2011.\n[27] Matthew E Peters, Mark Neumann, Mohit Iyyer, Matt Gardner, Christopher Clark, Ken-\nton Lee, and Luke Zettlemoyer. Deep contextualized word representations. arXiv preprint\narXiv:1802.05365, 2018.\n[28] Alec Radford, Karthik Narasimhan, Tim Salimans, and Ilya Sutskever. Improving language\nunderstanding by generative pre-training. URL https://s3-us-west-2. amazonaws. com/openai-\nassets/research-covers/languageunsupervised/language understanding paper. pdf, 2018.\n[29] Pranav Rajpurkar, Robin Jia, and Percy Liang. Know what you don’t know: Unanswerable\nquestions for squad. arXiv preprint arXiv:1806.03822, 2018.\n[30] Pranav Rajpurkar, Jian Zhang, Konstantin Lopyrev, and Percy Liang. Squad: 100,000+ questions\nfor machine comprehension of text. arXiv preprint arXiv:1606.05250, 2016.\n[31] Devendra Singh Sachan, Manzil Zaheer, and Ruslan Salakhutdinov. Revisiting lstm networks\nfor semi-supervised text classiﬁcation via mixed objective function. 2018.\n[32] Benigno Uria, Marc-Alexandre Côté, Karol Gregor, Iain Murray, and Hugo Larochelle. Neural\nautoregressive distribution estimation. The Journal of Machine Learning Research, 17(1):7184–\n7220, 2016.\n[33] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez,\nŁukasz Kaiser, and Illia Polosukhin. Attention is all you need. InAdvances in neural information\nprocessing systems, pages 5998–6008, 2017.\n[34] Alex Wang, Amanpreet Singh, Julian Michael, Felix Hill, Omer Levy, and Samuel R. Bowman.\nGLUE: A multi-task benchmark and analysis platform for natural language understanding. 2019.\nIn the Proceedings of ICLR.\n[35] Qizhe Xie, Zihang Dai, Eduard Hovy, Minh-Thang Luong, and Quoc V . Le. Unsupervised data\naugmentation. arXiv preprint arXiv:1904.12848, 2019.\n[36] Chenyan Xiong, Zhuyun Dai, Jamie Callan, Zhiyuan Liu, and Russell Power. End-to-end neural\nad-hoc ranking with kernel pooling. In Proceedings of the 40th International ACM SIGIR\nconference on research and development in information retrieval, pages 55–64. ACM, 2017.\n[37] Zhilin Yang, Zihang Dai, Ruslan Salakhutdinov, and William W Cohen. Breaking the softmax\nbottleneck: A high-rank rnn language model. arXiv preprint arXiv:1711.03953, 2017.\n[38] Shuailiang Zhang, Hai Zhao, Yuwei Wu, Zhuosheng Zhang, Xi Zhou, and Xiang Zhou. Dual co-\nmatching network for multi-choice reading comprehension. arXiv preprint arXiv:1901.09381,\n2019.\n[39] Xiang Zhang, Junbo Zhao, and Yann LeCun. Character-level convolutional networks for text\nclassiﬁcation. In Advances in neural information processing systems, pages 649–657, 2015.\n[40] Yukun Zhu, Ryan Kiros, Rich Zemel, Ruslan Salakhutdinov, Raquel Urtasun, Antonio Torralba,\nand Sanja Fidler. Aligning books and movies: Towards story-like visual explanations by\nwatching movies and reading books. In Proceedings of the IEEE international conference on\ncomputer vision, pages 19–27, 2015.\n11",
  "values": {
    "Transparent (to users)": "Yes",
    "Interpretable (to users)": "Yes",
    "Deferral to humans": "Yes",
    "Respect for Persons": "Yes",
    "Non-maleficence": "Yes",
    "Privacy": "Yes",
    "Critiqability": "Yes",
    "Respect for Law and public interest": "Yes",
    "Explicability": "Yes",
    "Not socially biased": "Yes",
    "Autonomy (power to decide)": "Yes",
    "User influence": "Yes",
    "Fairness": "Yes",
    "Collective influence": "Yes",
    "Justice": "Yes",
    "Beneficence": "Yes"
  }
}