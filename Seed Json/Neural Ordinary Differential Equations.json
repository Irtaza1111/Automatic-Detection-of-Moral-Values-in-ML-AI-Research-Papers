{
  "pdf": "Neural Ordinary Differential Equations",
  "title": "Neural Ordinary Differential Equations",
  "author": "Tian Qi Chen, Yulia Rubanova, Jesse Bettencourt, David K. Duvenaud",
  "paper_id": "Neural Ordinary Differential Equations",
  "text": "Neural Ordinary Differential Equations\nRicky T. Q. Chen*, Yulia Rubanova*, Jesse Bettencourt*, David Duvenaud\nUniversity of Toronto, V ector Institute\n���������� ��������� ���������� ������������������������\nAbstract\nWe introduce a new family of deep neural network models. Instead of specifying a\ndiscrete sequence of hidden layers, we parameterize the derivative of the hidden\nstate using a neural network. The output of the network is computed using a black-\nbox differential equation solver. These continuous-depth models have constant\nmemory cost, adapt their evaluation strategy to each input, and can explicitly trade\nnumerical precision for speed. We demonstrate these properties in continuous-depth\nresidual networks and continuous-time latent variable models. We also construct\ncontinuous normalizingﬂows, a generative model that can train by maximum\nlikelihood, without partitioning or ordering the data dimensions. For training, we\nshow how to scalably backpropagate through any ODE solver, without access to its\ninternal operations. This allows end-to-end training of ODEs within larger models.\n1 Introduction Residual Network ODE Network\n� � �\n�������������������\n�\n�\n�\n�\n�\n������\n� � �\n�������������������\n�\n�\n�\n�\n�\n������\nFigure 1:Left:A Residual network deﬁnes a\ndiscrete sequence ofﬁnite transformations.\nRight:A ODE network deﬁnes a vector\nﬁeld, which continuously transforms the state.\nBoth:Circles represent evaluation locations.\nModels such as residual networks, recurrent neural\nnetwork decoders, and normalizingﬂows build com-\nplicated transformations by composing a sequence of\ntransformations to a hidden state:\nht+1 =h t +f(h t,θ t)(1)\nwhere t∈{0. . . T} and ht ∈R D . These iterative\nupdates can be seen as an Euler discretization of a\ncontinuous transformation ( Lu et al. , 2017; Haber\nand Ruthotto, 2017; Ruthotto and Haber, 2018).\nWhat happens as we add more layers and take smaller\nsteps? In the limit, we parameterize the continuous\ndynamics of hidden units using an ordinary differen-\ntial equation (ODE) speciﬁed by a neural network:\ndh(t)\ndt =f(h(t), t,θ)(2)\nStarting from the input layer h(0), we can deﬁne the output layer h(T) to be the solution to this\nODE initial value problem at some time T . This value can be computed by a black-box differential\nequation solver, which evaluates the hidden unit dynamics f wherever necessary to determine the\nsolution with the desired accuracy. Figure 1 contrasts these two approaches.\nDeﬁning and evaluating models using ODE solvers has several beneﬁts:\nMemory efﬁciency In Section 2, we show how to compute gradients of a scalar-valued loss with\nrespect to all inputs of any ODE solver,without backpropagating through the operations of the solver.\nNot storing any intermediate quantities of the forward pass allows us to train our models with constant\nmemory cost as a function of depth, a major bottleneck of training deep models.\n32nd Conference on Neural Information Processing Systems (NeurIPS 2018), Montréal, Canada.\nAdaptive computation Euler’s method is perhaps the simplest method for solving ODEs. There\nhave since been more than 120 years of development of efﬁcient and accurate ODE solvers ( Runge,\n1895; Kutta, 1901; Hairer et al., 1987). Modern ODE solvers provide guarantees about the growth\nof approximation error, monitor the level of error, and adapt their evaluation strategy on theﬂy to\nachieve the requested level of accuracy. This allows the cost of evaluating a model to scale with\nproblem complexity. After training, accuracy can be reduced for real-time or low-power applications.\nParameter efﬁciency When the hidden unit dynamics are parameterized as a continuous function\nof time, the parameters of nearby “layers” are automatically tied together. In Section 3, we show that\nthis reduces the number of parameters required on a supervised learning task.\nScalable and invertible normalizingﬂows An unexpected side-beneﬁt of continuous transforma-\ntions is that the change of variables formula becomes easier to compute. In Section 4, we derive\nthis result and use it to construct a new class of invertible density models that avoids the single-unit\nbottleneck of normalizingﬂows, and can be trained directly by maximum likelihood.\nContinuous time-series models Unlike recurrent neural networks, which require discretizing\nobservation and emission intervals, continuously-deﬁned dynamics can naturally incorporate data\nwhich arrives at arbitrary times. In Section 5, we construct and demonstrate such a model.\n2 Reverse-mode automatic differentiation of ODE solutions\nThe main technical difﬁculty in training continuous-depth networks is performing reverse-mode\ndifferentiation (also known as backpropagation) through the ODE solver. Differentiating through\nthe operations of the forward pass is straightforward, but incurs a high memory cost and introduces\nadditional numerical error.\nWe treat the ODE solver as a black box, and compute gradients using theadjoint sensitivity\nmethod( Pontryagin et al. , 1962). This approach computes gradients by solving a second, aug-\nmented ODE backwards in time, and is applicable to all ODE solvers. This approach scales linearly\nwith problem size, has low memory cost, and explicitly controls numerical error.\nConsider optimizing a scalar-valued loss functionL(), whose input is the result of an ODE solver:\nL(z(t1)) =L\n�\nz(t0) +\n�t1\nt0\nf(z(t), t,θ)dt\n�\n=L(ODESolve(z(t 0), f, t0, t1,θ))(3)\n�������������\n�����\nFigure 2: Reverse-mode differentiation of an ODE\nsolution. The adjoint sensitivity method solves\nan augmented ODE backwards in time. The aug-\nmented system contains both the original state and\nthe sensitivity of the loss with respect to the state.\nIf the loss depends directly on the state at multi-\nple observation times, the adjoint state must be\nupdated in the direction of the partial derivative of\nthe loss with respect to each observation.\nTo optimize L, we require gradients with respect\nto θ. Theﬁrst step is to determining how the\ngradient of the loss depends on the hidden state\nz(t) at each instant. This quantity is called the\nadjoint a(t) = ∂L/∂z(t). Its dynamics are given\nby another ODE, which can be thought of as the\ninstantaneous analog of the chain rule:\nda(t)\ndt =−a(t) T ∂f(z(t), t,θ)\n∂z (4)\nWe can compute ∂L/∂z(t0) by another call to an\nODE solver. This solver must run backwards,\nstarting from the initial value of ∂L/∂z(t1 ). One\ncomplication is that solving this ODE requires\nthe knowing value of z(t) along its entire tra-\njectory. However, we can simply recompute\nz(t) backwards in time together with the adjoint,\nstarting from itsﬁnal valuez(t 1).\nComputing the gradients with respect to the pa-\nrameters θ requires evaluating a third integral,\nwhich depends on bothz(t)anda(t):\ndL\ndθ =\n�t0\nt1\na(t)T ∂f(z(t), t,θ)\n∂θ dt(5)\n2\nThe vector-Jacobian products a(t)T ∂f\n∂z and a(t)T ∂f\n∂θ in (4) and (5) can be efﬁciently evaluated by\nautomatic differentiation, at a time cost similar to that of evaluatingf. All integrals for solvingz,a\nand ∂L\n∂θ can be computed in a single call to an ODE solver, which concatenates the original state, the\nadjoint, and the other partial derivatives into a single vector. Algorithm 1 shows how to construct the\nnecessary dynamics, and call an ODE solver to compute all gradients at once.\nAlgorithm 1Reverse-mode derivative of an ODE initial value problem\nInput:dynamics parametersθ, start timet 0, stop timet 1,ﬁnal statez(t 1), loss gradient ∂L/∂z(t1 )\ns0 = [z(t1), ∂L\n∂z(t1 ) ,0 |θ| ] �Deﬁne initial augmented state\ndefaug_dynamics([z(t),a(t),·], t,θ):�Deﬁne dynamics on augmented state\nreturn[f(z(t), t,θ),−a(t) T ∂f\n∂z ,−a(t) T ∂f\n∂θ ]�Compute vector-Jacobian products\n[z(t0), ∂L\n∂z(t0 ) , ∂L\n∂θ ] =ODESolve(s 0,aug_dynamics, t 1, t0,θ)�Solve reverse-time ODE\nreturn ∂L\n∂z(t0 ) , ∂L\n∂θ �Return gradients\nMost ODE solvers have the option to output the state z(t) at multiple times. When the loss depends\non these intermediate states, the reverse-mode derivative must be broken into a sequence of separate\nsolves, one between each consecutive pair of output times (Figure 2). At each observation, the adjoint\nmust be adjusted in the direction of the corresponding partial derivative ∂L/∂z(ti ).\nThe results above extend those of Stapor et al. (2018, section 2.4.2). An extended version of\nAlgorithm 1 including derivatives w.r.t. t0 and t1 can be found in Appendix C. Detailed derivations\nare provided in Appendix B. Appendix D provides Python code which computes all derivatives for\n���������������������� by extending the �������� automatic differentiation package. This\ncode also supports all higher-order derivatives. We have since released a PyTorch ( Paszke et al. ,\n2017) implementation, including GPU-based implementations of several standard ODE solvers at\n������������������������������� .\n3 Replacing residual networks with ODEs for supervised learning\nIn this section, we experimentally investigate the training of neural ODEs for supervised learning.\nSoftware To solve ODE initial value problems numerically, we use the implicit Adams method\nimplemented in LSODE and VODE and interfaced through the ��������������� package. Being\nan implicit method, it has better guarantees than explicit methods such as Runge-Kutta but requires\nsolving a nonlinear optimization problem at every step. This setup makes direct backpropagation\nthrough the integrator difﬁcult. We implement the adjoint sensitivity method in Python’s ��������\nframework (Maclaurin et al. , 2015). For the experiments in this section, we evaluated the hidden\nstate dynamics and their derivatives on the GPU using Tensorﬂow, which were then called from the\nFortran ODE solvers, which were called from Python��������code.\nTable 1: Performance on MNIST. † From LeCun\net al. (1998).\nTest Error # Params Memory Time\n1-Layer MLP† 1.60% 0.24 M - -\nResNet 0.41% 0.60 MO(L)O(L)\nRK-Net 0.47% 0.22 MO( ˜L)O( ˜L)\nODE-Net 0.42% 0.22 MO(1)O( ˜L)\nModel Architectures We experiment with a\nsmall residual network which downsamples the\ninput twice then applies 6 standard residual\nblocks He et al. (2016b), which are replaced\nby an ODESolve module in the ODE-Net vari-\nant. We also test a network with the same archi-\ntecture but where gradients are backpropagated\ndirectly through a Runge-Kutta integrator, re-\nferred to as RK-Net. Table 1 shows test error, number of parameters, and memory cost. L denotes\nthe number of layers in the ResNet, and ˜L is the number of function evaluations that the ODE solver\nrequests in a single forward pass, which can be interpreted as an implicit number of layers.\nWeﬁnd that ODE-Nets and RK-Nets can achieve around the same performance as the ResNet, while\nusing fewer parameters. For reference, a neural net with a single hidden layer of 300 units has around\nthe same number of parameters as the ODE-Net and RK-Net architecture that we tested.\n3\nError Control in ODE-Nets ODE solvers can approximately ensure that the output is within a\ngiven tolerance of the true solution. Changing this tolerance changes the behavior of the network.\nWeﬁrst verify that error can indeed be controlled in Figure 3a. The time spent by the forward call is\nproportional to the number of function evaluations (Figure 3b), so tuning the tolerance gives us a\ntrade-off between accuracy and computational cost. One could train with high accuracy, but switch to\na lower accuracy at test time.\nFigure 3: Statistics of a trained ODE-Net. (NFE = number of function evaluations.)\nFigure 3c) shows a surprising result: the number of evaluations in the backward pass is roughly\nhalf of the forward pass. This suggests that the adjoint sensitivity method is not only more memory\nefﬁcient, but also more computationally efﬁcient than directly backpropagating through the integrator,\nbecause the latter approach will need to backprop through each function evaluation in the forward\npass.\nNetwork Depth It’s not clear how to deﬁne the ‘depth‘ of an ODE solution. A related quantity is\nthe number of evaluations of the hidden state dynamics required, a detail delegated to the ODE solver\nand dependent on the initial state or input. Figure 3d shows that he number of function evaluations\nincreases throughout training, presumably adapting to increasing complexity of the model.\n4 Continuous Normalizing Flows\nThe discretized equation (1) also appears in normalizingﬂows ( Rezende and Mohamed, 2015) and\nthe NICE framework ( Dinh et al. , 2014). These methods use the change of variables theorem to\ncompute exact changes in probability if samples are transformed through a bijective functionf:\nz1 =f(z 0) =⇒logp(z 1) = logp(z 0)−log\n����det ∂f\n∂z0\n���� (6)\nAn example is the planar normalizingﬂow ( Rezende and Mohamed, 2015):\nz(t+ 1) =z(t) +uh(w Tz(t) +b),logp(z(t+ 1)) = logp(z(t))−log\n����1 +u T ∂h\n∂z\n���� (7)\nGenerally, the main bottleneck to using the change of variables formula is computing of the deter-\nminant of the Jacobian ∂f/∂z, which has a cubic cost in either the dimension of z, or the number\nof hidden units. Recent work explores the tradeoff between the expressiveness of normalizingﬂow\nlayers and computational cost ( Kingma et al., 2016; Tomczak and Welling, 2016; Berg et al., 2018).\nSurprisingly, moving from a discrete set of layers to a continuous transformation simpliﬁes the\ncomputation of the change in normalizing constant:\nTheorem 1 (Instantaneous Change of V ariables). Let z(t) be aﬁnite continuous random variable\nwith probability p(z(t)) dependent on time. Let dz\ndt =f(z(t), t) be a differential equation describing\na continuous-in-time transformation of z(t). Assuming that f is uniformly Lipschitz continuous in z\nand continuous int, then the change in log probability also follows a differential equation,\n∂logp(z(t))\n∂t =−tr\n�d f\ndz(t)\n�\n(8)\nProof in Appendix A. Instead of the log determinant in (6), we now only require a trace operation.\nAlso unlike standardﬁniteﬂows, the differential equation f does not need to be bijective, since if\nuniqueness is satisﬁed, then the entire transformation is automatically bijective.\n4\nAs an example application of the instantaneous change of variables, we can examine the continuous\nanalog of the planarﬂow, and its change in normalization constant:\ndz(t)\ndt =uh(w Tz(t) +b), ∂logp(z(t))\n∂t =−u T ∂h\n∂z(t) (9)\nGiven an initial distribution p(z(0)), we can sample from p(z(t)) and evaluate its density by solving\nthis combined ODE.\nUsing multiple hidden units with linear cost While det is not a linear function, the trace function\nis, which implies tr(�\nn Jn) = �\nn tr(Jn). Thus if our dynamics is given by a sum of functions then\nthe differential equation for the log density is also a sum:\ndz(t)\ndt =\nM�\nn=1\nfn(z(t)), dlogp(z(t))\ndt =\nM�\nn=1\ntr\n�∂fn\n∂z\n�\n(10)\nThis means we can cheaply evaluateﬂow models having many hidden units, with a cost only linear in\nthe number of hidden units M . Evaluating such ‘wide’ﬂow layers using standard normalizingﬂows\ncostsO(M 3), meaning that standard NF architectures use many layers of only a single hidden unit.\nTime-dependent dynamics We can specify the parameters of aﬂow as a function of t, making the\ndifferential equation f(z(t), t) change with t. This is parameterization is a kind of hypernetwork ( Ha\net al., 2016). We also introduce a gating mechanism for each hidden unit, dz\ndt = �\nn σn(t)fn(z)\nwhere σn(t)∈(0,1) is a neural network that learns when the dynamic fn(z) should be applied. We\ncall these models continuous normalizingﬂows (CNF).\n4.1 Experiments with Continuous Normalizing Flows\nWeﬁrst compare continuous and discrete planarﬂows at learning to sample from a known distribution.\nWe show that a planar CNF with M hidden units can be at least as expressive as a planar NF with\nK=Mlayers, and sometimes much more expressive.\nDensity matching We conﬁgure the CNF as described above, and train for 10,000 iterations\nusing Adam ( Kingma and Ba , 2014). In contrast, the NF is trained for 500,000 iterations using\nRMSprop (Hinton et al. , 2012), as suggested by Rezende and Mohamed (2015). For this task, we\nminimize KL (q(x)�p(x)) as the loss function where q is theﬂow model and the target density p(·)\ncan be evaluated. Figure 4 shows that CNF generally achieves lower loss.\nMaximum Likelihood Training A useful property of continuous-time normalizingﬂows is that\nwe can compute the reverse transformation for about the same cost as the forward pass, which cannot\nbe said for normalizingﬂows. This lets us train theﬂow on a density estimation task by performing\nK=2 K=8 K=32 M=2 M=8 M=32\n1\n�� �� ��\n���\n��\n2\n�� �� ��\n���\n��\n3\n(a) Target\n (b) NF\n (c) CNF\n�� �� ��\n���\n�� (d) Loss vs. K/M\nFigure 4: Comparison of normalizingﬂows versus continuous normalizingﬂows. The model capacity\nof normalizingﬂows is determined by their depth (K), while continuous normalizingﬂows can also\nincrease capacity by increasing width (M), making them easier to train.\n5\nDensity\n5%\n 20%\n 40%\n 60%\n 80%\n 100%\nSamples\nNF\nTarget\n(a) Two Circles\nDensity\n5%\n 20%\n 40%\n 60%\n 80%\n 100%\nSamples\nNF\nTarget\n(b) Two Moons\nFigure 5: Visualizing the transformation from noise to data. Continuous-time normalizingﬂows\nare reversible, so we can train on a density estimation task and still be able to sample from the learned\ndensity efﬁciently.\nmaximum likelihood estimation, which maximizes Ep(x)[logq(x)] where q(·) is computed using\nthe appropriate change of variables theorem, then afterwards reverse the CNF to generate random\nsamples fromq(x).\nFor this task, we use 64 hidden units for CNF, and 64 stacked one-hidden-unit layers for NF. Figure 5\nshows the learned dynamics. Instead of showing the initial Gaussian distribution, we display the\ntransformed distribution after a small amount of time which shows the locations of the initial planar\nﬂows. Interestingly, toﬁt the Two Circles distribution, the CNF rotates the planarﬂows so that\nthe particles can be evenly spread into circles. While the CNF transformations are smooth and\ninterpretable, weﬁnd that NF transformations are very unintuitive and this model has difﬁcultyﬁtting\nthe two moons dataset in Figure 5b.\n5 A generative latent function time-series model\nApplying neural networks to irregularly-sampled data such as medical records, network trafﬁc, or\nneural spiking data is difﬁcult. Typically, observations are put into bins ofﬁxed duration, and the\nlatent dynamics are discretized in the same way. This leads to difﬁculties with missing data and ill-\ndeﬁned latent variables. Missing data can be addressed using generative time-series models ( Álvarez\nand Lawrence, 2011; Futoma et al. , 2017; Mei and Eisner , 2017; Soleimani et al. , 2017a) or data\nimputation ( Che et al. , 2018). Another approach concatenates time-stamp information to the input of\nan RNN (Choi et al., 2016; Lipton et al., 2016; Du et al., 2016; Li, 2017).\nWe present a continuous-time, generative approach to modeling time series. Our model represents\neach time series by a latent trajectory. Each trajectory is determined from a local initial state, zt0 , and\na global set of latent dynamics shared across all time series. Given observation times t0, t1, . . . , tN\nand an initial state zt0 , an ODE solver produces zt1 , . . . ,ztN , which describe the latent state at each\nobservation.We deﬁne this generative model formally through a sampling procedure:\nzt0 ∼p(z t0 ) (11)\nzt1 ,z t2 , . . . ,ztN =ODESolve(z t0 , f,θ f , t0, . . . , tN )(12)\neachx ti ∼p(x|z ti ,θ x) (13)\nFunction f is a time-invariant function that takes the value z at the current time step and outputs the\ngradient: ∂z(t)/∂t =f(z(t),θ f ). We parametrize this function using a neural net. Because f is time-\ninvariant, given any latent state z(t), the entire latent trajectory is uniquely deﬁned. Extrapolating\nthis latent trajectory lets us make predictions arbitrarily far forwards or backwards in time.\nTraining and Prediction We can train this latent-variable model as a variational autoen-\ncoder (Kingma and Welling, 2014; Rezende et al., 2014), with sequence-valued observations. Our\nrecognition net is an RNN, which consumes the data sequentially backwards in time, and out-\nputs qφ(z0|x1,x 2, . . . ,xN ). A detailed algorithm can be found in Appendix E. Using ODEs as a\ngenerative model allows us to make predictions for arbitrary time points t1...tM on a continuous\ntimeline.\n6\nµ\nσ\nzt0\nzt1\n�����������\n������������\n����������\n�\nq(zt0|xt0...xtN)\nht0 ht1 htN\nODESolve(zt0, f,θf, t0, ..., tM)\nztM\n�\nztN\nztN+1\n�������� ����������\nx(t)\nt0 t1 tN\n����\ntN+1 tM\n���������� �������������\nt0 t1 tN tN+1 tM\nˆx(t)\nFigure 6: Computation graph of the latent ODE model.\nλ(t)\nt\nFigure 7: Fitting a latent ODE dy-\nnamics model with a Poisson pro-\ncess likelihood. Dots show event\ntimes. The line is the learned inten-\nsityλ(t)of the Poisson process.\nPoisson Process likelihoods The fact that an observation oc-\ncurred often tells us something about the latent state. For ex-\nample, a patient may be more likely to take a medical test if\nthey are sick. The rate of events can be parameterized by a\nfunction of the latent state: p(event at timet|z(t)) =λ(z(t)) .\nGiven this rate function, the likelihood of a set of indepen-\ndent observation times in the interval [tstart, tend] is given by an\ninhomogeneous Poisson process ( Palm, 1943):\nlogp(t 1 . . . tN |t start, tend) =\nN�\ni=1\nlogλ(z(t i))−\n�tend\ntstart\nλ(z(t))dt\nWe can parameterize λ(·) using another neural network. Con-\nveniently, we can evaluate both the latent trajectory and the\nPoisson process likelihood together in a single call to an ODE solver. Figure 7 shows the event rate\nlearned by such a model on a toy dataset.\nA Poisson process likelihood on observation times can be combined with a data likelihood to jointly\nmodel all observations and the times at which they were made.\n5.1 Time-series Latent ODE Experiments\nWe investigate the ability of the latent ODE model toﬁt and extrapolate time series. The recognition\nnetwork is an RNN with 25 hidden units. We use a 4-dimensional latent space. We parameterize the\ndynamics function f with a one-hidden-layer network with 20 hidden units. The decoder computing\np(xti |zti ) is another neural network with one hidden layer with 20 hidden units. Our baseline was a\nrecurrent neural net with 25 hidden units trained to minimize negative Gaussian log-likelihood. We\ntrained a second version of this RNN whose inputs were concatenated with the time difference to the\nnext observation to aid RNN with irregular observations.\nTable 2: Predictive RMSE on test set\n# Observations 30/100 50/100 100/100\nRNN 0.3937 0.3202 0.1813\nLatent ODE0.1642 0.1502 0.1346\nBi-directional spiral dataset We generated a\ndataset of 1000 2-dimensional spirals, each start-\ning at a different point, sampled at 100 equally-\nspaced timesteps. The dataset contains two\ntypes of spirals: half are clockwise while the\nother half counter-clockwise. To make the task\nmore realistic, we add gaussian noise to the observations.\nTime series with irregular time points To generate irregular timestamps, we randomly sample\npoints from each trajectory without replacement ( n={30,50,100} ). We report predictive root-\nmean-squared error (RMSE) on 100 time points extending beyond those that were used for training.\nTable 2 shows that the latent ODE has substantially lower predictive RMSE.\nFigure 8 shows examples of spiral reconstructions with 30 sub-sampled points. Reconstructions from\nthe latent ODE were obtained by sampling from the posterior over latent trajectories and decoding it\n7\nFigure 9: Data-space trajectories decoded from varying one dimension of zt0 . Color indicates\nprogression through time, starting at purple and ending at red. Note that the trajectories on the left\nare counter-clockwise, while the trajectories on the right are clockwise.\nto data-space. Examples with varying number of time points are shown in Appendix F. We observed\nthat reconstructions and extrapolations are consistent with the ground truth regardless of number of\nobserved points and despite the noise.\n(a) Recurrent Neural Network\n(b) Latent Neural Ordinary Differential Equation\n������������\n�����������\n����������\n�������������\n(c) Latent Trajectories\nFigure 8: ( a): Reconstruction and extrapolation\nof spirals with irregular time points by a recurrent\nneural network. ( b): Reconstructions and extrapo-\nlations by a latent neural ODE. Blue curve shows\nmodel prediction. Red shows extrapolation. ( c) A\nprojection of inferred 4-dimensional latent ODE\ntrajectories onto theirﬁrst two dimensions. Color\nindicates the direction of the corresponding trajec-\ntory. The model has learned latent dynamics which\ndistinguishes the two directions.\nLatent space interpolation Figure 8c shows\nlatent trajectories projected onto theﬁrst two\ndimensions of the latent space. The trajecto-\nries form two separate clusters of trajectories,\none decoding to clockwise spirals, the other to\ncounter-clockwise. Figure 9 shows that the la-\ntent trajectories change smoothly as a function\nof the initial point z(t0), switching from a clock-\nwise to a counter-clockwise spiral.\n6 Scope and Limitations\nMinibatching The use of mini-batches is less\nstraightforward than for standard neural net-\nworks. One can still batch together evaluations\nthrough the ODE solver by concatenating the\nstates of each batch element together, creating a\ncombined ODE with dimension D×K . In some\ncases, controlling error on all batch elements to-\ngether might require evaluating the combined\nsystem K times more often than if each system\nwas solved individually. However, in practice\nthe number of evaluations did not increase sub-\nstantially when using minibatches.\nUniqueness When do continuous dynamics\nhave a unique solution? Picard’s existence the-\norem (Coddington and Levinson , 1955) states\nthat the solution to an initial value problem ex-\nists and is unique if the differential equation is\nuniformly Lipschitz continuous in z and contin-\nuous in t. This theorem holds for our model if\nthe neural network hasﬁnite weights and uses\nLipshitz nonlinearities, such as����or����.\nSetting tolerances Our framework allows the user to trade off speed for precision, but requires\nthe user to choose an error tolerance on both the forward and reverse passes during training. For\nsequence modeling, the default value of ������ was used. In the classiﬁcation and density estimation\nexperiments, we were able to reduce the tolerance to ���� and ���� , respectively, without degrading\nperformance.\nReconstructing forward trajectories Reconstructing the state trajectory by running the dynamics\nbackwards can introduce extra numerical error if the reconstructed trajectory diverges from the\noriginal. This problem can be addressed by checkpointing: storing intermediate values of z on the\n8\nforward pass, and reconstructing the exact forward trajectory by re-integrating from those points. We\ndid notﬁnd this to be a practical problem, and we informally checked that reversing many layers of\ncontinuous normalizingﬂows with default tolerances recovered the initial states.\n7 Related Work\nThe use of the adjoint method for training continuous-time neural networks was previously pro-\nposed ( LeCun et al. , 1988; Pearlmutter, 1995), though was not demonstrated practically. The\ninterpretation of residual networks He et al. (2016a) as approximate ODE solvers spurred research\ninto exploiting reversibility and approximate computation in ResNets ( Chang et al., 2017; Lu et al.,\n2017). We demonstrate these same properties in more generality by directly using an ODE solver.\nAdaptive computation One can adapt computation time by training secondary neural networks\nto choose the number of evaluations of recurrent or residual networks ( Graves, 2016; Jernite et al.,\n2016; Figurnov et al., 2017; Chang et al. , 2018). However, this introduces overhead both at training\nand test time, and extra parameters that need to beﬁt. In contrast, ODE solvers offer well-studied,\ncomputationally cheap, and generalizable rules for adapting the amount of computation.\nConstant memory backprop through reversibility Recent work developed reversible versions\nof residual networks ( Gomez et al., 2017; Haber and Ruthotto, 2017; Chang et al., 2017), which gives\nthe same constant memory advantage as our approach. However, these methods require restricted\narchitectures, which partition the hidden units. Our approach does not have these restrictions.\nLearning differential equations Much recent work has proposed learning differential equations\nfrom data. One can train feed-forward or recurrent neural networks to approximate a differential\nequation ( Raissi and Karniadakis , 2018; Raissi et al. , 2018a; Long et al. , 2017), with applica-\ntions such asﬂuid simulation ( Wiewel et al., 2018). There is also signiﬁcant work on connecting\nGaussian Processes (GPs) and ODE solvers ( Schober et al. , 2014). GPs have been adapted toﬁt\ndifferential equations ( Raissi et al. , 2018b) and can naturally model continuous-time effects and\ninterventions (Soleimani et al., 2017b; Schulam and Saria, 2017). Ryder et al. (2018) use stochastic\nvariational inference to recover the solution of a given stochastic differential equation.\nDifferentiating through ODE solvers The ������ library (Farrell et al., 2013) implements adjoint\ncomputation for general ODE and PDE solutions, but only by backpropagating through the individual\noperations of the forward solver. The Stan library ( Carpenter et al. , 2015) implements gradient\nestimation through ODE solutions using forward sensitivity analysis. However, forward sensitivity\nanalysis is quadratic-time in the number of variables, whereas the adjoint sensitivity analysis is\nlinear ( Carpenter et al. , 2015; Zhang and Sandu , 2014). Melicher et al. (2017) used the adjoint\nmethod to train bespoke latent dynamic models.\nIn contrast, by providing a generic vector-Jacobian product, we allow an ODE solver to be trained\nend-to-end with any other differentiable model components. While use of vector-Jacobian products\nfor solving the adjoint method has been explored in optimal control ( Andersson, 2013; Andersson\net al., In Press, 2018), we highlight the potential of a general integration of black-box ODE solvers\ninto automatic differentiation ( Baydin et al., 2018) for deep learning and generative modeling.\n8 Conclusion\nWe investigated the use of black-box ODE solvers as a model component, developing new models\nfor time-series modeling, supervised learning, and density estimation. These models are evaluated\nadaptively, and allow explicit control of the tradeoff between computation speed and accuracy.\nFinally, we derived an instantaneous version of the change of variables formula, and developed\ncontinuous-time normalizingﬂows, which can scale to large layer sizes.\n9\n9 Acknowledgements\nWe thank Wenyi Wang and Geoff Roeder for help with proofs, and Daniel Duckworth, Ethan Fetaya,\nHossein Soleimani, Eldad Haber, Ken Caluwaerts, and Daniel Flam-Shepherd for feedback. We\nthank Chris Rackauckas, Dougal Maclaurin, and Matthew James Johnson for helpful discussions.\nReferences\nMauricio A Álvarez and Neil D Lawrence. Computationally efﬁcient convolved multiple output\nGaussian processes.Journal of Machine Learning Research, 12(May):1459–1500, 2011.\nBrandon Amos and J Zico Kolter. OptNet: Differentiable optimization as a layer in neural networks.\nInInternational Conference on Machine Learning, pages 136–145, 2017.\nJoel Andersson.A general-purpose software framework for dynamic optimization. PhD thesis, 2013.\nJoel A E Andersson, Joris Gillis, Greg Horn, James B Rawlings, and Moritz Diehl. CasADi – A\nsoftware framework for nonlinear optimization and optimal control.Mathematical Programming\nComputation, In Press, 2018.\nAtilim Gunes Baydin, Barak A Pearlmutter, Alexey Andreyevich Radul, and Jeffrey Mark Siskind.\nAutomatic differentiation in machine learning: a survey.Journal of machine learning research, 18\n(153):1–153, 2018.\nRianne van den Berg, Leonard Hasenclever, Jakub M Tomczak, and Max Welling. Sylvester\nnormalizingﬂows for variational inference.arXiv preprint arXiv:1803.05649, 2018.\nBob Carpenter, Matthew D Hoffman, Marcus Brubaker, Daniel Lee, Peter Li, and Michael Betan-\ncourt. The Stan math library: Reverse-mode automatic differentiation in c++.arXiv preprint\narXiv:1509.07164, 2015.\nBo Chang, Lili Meng, Eldad Haber, Lars Ruthotto, David Begert, and Elliot Holtham. Reversible\narchitectures for arbitrarily deep residual neural networks.arXiv preprint arXiv:1709.03698, 2017.\nBo Chang, Lili Meng, Eldad Haber, Frederick Tung, and David Begert. Multi-level residual networks\nfrom dynamical systems view. InInternational Conference on Learning Representations, 2018.\nURL ������������������������������������� ���� .\nZhengping Che, Sanjay Purushotham, Kyunghyun Cho, David Sontag, and Y an Liu. Recurrent neural\nnetworks for multivariate time series with missing values.Scientiﬁc Reports, 8(1):6085, 2018.\nURL ������������������������������� ���� ������ � .\nEdward Choi, Mohammad Taha Bahadori, Andy Schuetz, Walter F. Stewart, and Jimeng Sun.\nDoctor AI: Predicting clinical events via recurrent neural networks. InProceedings of the 1st\nMachine Learning for Healthcare Conference, volume 56 ofProceedings of Machine Learning\nResearch, pages 301–318. PMLR, 18–19 Aug 2016. URL �����������������������������\n��������������� .\nEarl A Coddington and Norman Levinson.Theory of ordinary differential equations. Tata McGraw-\nHill Education, 1955.\nLaurent Dinh, David Krueger, and Y oshua Bengio. NICE: Non-linear independent components\nestimation.arXiv preprint arXiv:1410.8516, 2014.\nNan Du, Hanjun Dai, Rakshit Trivedi, Utkarsh Upadhyay, Manuel Gomez-Rodriguez, and Le Song.\nRecurrent marked temporal point processes: Embedding event history to vector. InInternational\nConference on Knowledge Discovery and Data Mining, pages 1555–1564. ACM, 2016.\nPatrick Farrell, David Ham, Simon Funke, and Marie Rognes. Automated derivation of the adjoint of\nhigh-level transientﬁnite element programs.SIAM Journal on Scientiﬁc Computing, 2013.\nMichael Figurnov, Maxwell D Collins, Y ukun Zhu, Li Zhang, Jonathan Huang, Dmitry V etrov, and\nRuslan Salakhutdinov. Spatially adaptive computation time for residual networks.arXiv preprint,\n2017.\n10\nJ. Futoma, S. Hariharan, and K. Heller. Learning to Detect Sepsis with a Multitask Gaussian Process\nRNN Classiﬁer.ArXiv e-prints, 2017.\nAidan N Gomez, Mengye Ren, Raquel Urtasun, and Roger B Grosse. The reversible residual network:\nBackpropagation without storing activations. InAdvances in Neural Information Processing\nSystems, pages 2211–2221, 2017.\nAlex Graves. Adaptive computation time for recurrent neural networks.arXiv preprint\narXiv:1603.08983, 2016.\nDavid Ha, Andrew Dai, and Quoc V Le. Hypernetworks.arXiv preprint arXiv:1609.09106, 2016.\nEldad Haber and Lars Ruthotto. Stable architectures for deep neural networks.Inverse Problems, 34\n(1):014004, 2017.\nE. Hairer, S.P . Nørsett, and G. Wanner.Solving Ordinary Differential Equations I – Nonstiff Problems.\nSpringer, 1987.\nKaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image\nrecognition. InProceedings of the IEEE conference on computer vision and pattern recognition,\npages 770–778, 2016a.\nKaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Identity mappings in deep residual\nnetworks. InEuropean conference on computer vision, pages 630–645. Springer, 2016b.\nGeoffrey Hinton, Nitish Srivastava, and Kevin Swersky. Neural networks for machine learning lecture\n6a overview of mini-batch gradient descent, 2012.\nY acine Jernite, Edouard Grave, Armand Joulin, and Tomas Mikolov. V ariable computation in\nrecurrent neural networks.arXiv preprint arXiv:1611.06188, 2016.\nDiederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization.arXiv preprint\narXiv:1412.6980, 2014.\nDiederik P . Kingma and Max Welling. Auto-encoding variational Bayes.International Conference\non Learning Representations, 2014.\nDiederik P Kingma, Tim Salimans, Rafal Jozefowicz, Xi Chen, Ilya Sutskever, and Max Welling.\nImproved variational inference with inverse autoregressiveﬂow. InAdvances in Neural Information\nProcessing Systems, pages 4743–4751, 2016.\nW. Kutta. Beitrag zur näherungsweisen Integration totaler Differentialgleichungen.Zeitschrift für\nMathematik und Physik, 46:435–453, 1901.\nY ann LeCun, D Touresky, G Hinton, and T Sejnowski. A theoretical framework for back-propagation.\nInProceedings of the 1988 connectionist models summer school, volume 1, pages 21–28. CMU,\nPittsburgh, Pa: Morgan Kaufmann, 1988.\nY ann LeCun, Léon Bottou, Y oshua Bengio, and Patrick Haffner. Gradient-based learning applied to\ndocument recognition.Proceedings of the IEEE, 86(11):2278–2324, 1998.\nY ang Li. Time-dependent representation for neural event sequence prediction.arXiv preprint\narXiv:1708.00065, 2017.\nZachary C Lipton, David Kale, and Randall Wetzel. Directly modeling missing data in sequences with\nRNNs: Improved classiﬁcation of clinical time series. InProceedings of the 1st Machine Learning\nfor Healthcare Conference, volume 56 ofProceedings of Machine Learning Research, pages 253–\n270. PMLR, 18–19 Aug 2016. URL ���������������������������������������������� .\nZ. Long, Y . Lu, X. Ma, and B. Dong. PDE-Net: Learning PDEs from Data.ArXiv e-prints, 2017.\nYiping Lu, Aoxiao Zhong, Quanzheng Li, and Bin Dong. Beyondﬁnite layer neural networks:\nBridging deep architectures and numerical differential equations.arXiv preprint arXiv:1710.10121,\n2017.\n11\nDougal Maclaurin, David Duvenaud, and Ryan P Adams. Autograd: Reverse-mode differentiation of\nnative Python. InICML workshop on Automatic Machine Learning, 2015.\nHongyuan Mei and Jason M Eisner. The neural Hawkes process: A neurally self-modulating\nmultivariate point process. InAdvances in Neural Information Processing Systems, pages 6757–\n6767, 2017.\nV aldemar Melicher, Tom Haber, and Wim V anroose. Fast derivatives of likelihood functionals for\nODE based models using adjoint-state method.Computational Statistics, 32(4):1621–1643, 2017.\nConny Palm. Intensitätsschwankungen im fernsprechverker.Ericsson Technics, 1943.\nAdam Paszke, Sam Gross, Soumith Chintala, Gregory Chanan, Edward Y ang, Zachary DeVito,\nZeming Lin, Alban Desmaison, Luca Antiga, and Adam Lerer. Automatic differentiation in\npytorch. 2017.\nBarak A Pearlmutter. Gradient calculations for dynamic recurrent neural networks: A survey.IEEE\nTransactions on Neural networks, 6(5):1212–1228, 1995.\nLev Semenovich Pontryagin, EF Mishchenko, VG Boltyanskii, and RV Gamkrelidze. The mathemat-\nical theory of optimal processes. 1962.\nM. Raissi and G. E. Karniadakis. Hidden physics models: Machine learning of nonlinear partial\ndifferential equations.Journal of Computational Physics, pages 125–141, 2018.\nMaziar Raissi, Paris Perdikaris, and George Em Karniadakis. Multistep neural networks for data-\ndriven discovery of nonlinear dynamical systems.arXiv preprint arXiv:1801.01236, 2018a.\nMaziar Raissi, Paris Perdikaris, and George Em Karniadakis. Numerical Gaussian processes for\ntime-dependent and nonlinear partial differential equations.SIAM Journal on Scientiﬁc Computing,\n40(1):A172–A198, 2018b.\nDanilo J Rezende, Shakir Mohamed, and Daan Wierstra. Stochastic backpropagation and approximate\ninference in deep generative models. InProceedings of the 31st International Conference on\nMachine Learning, pages 1278–1286, 2014.\nDanilo Jimenez Rezende and Shakir Mohamed. V ariational inference with normalizingﬂows.arXiv\npreprint arXiv:1505.05770, 2015.\nC. Runge. Über die numerische Auﬂösung von Differentialgleichungen.Mathematische Annalen, 46:\n167–178, 1895.\nLars Ruthotto and Eldad Haber. Deep neural networks motivated by partial differential equations.\narXiv preprint arXiv:1804.04272, 2018.\nT. Ryder, A. Golightly, A. S. McGough, and D. Prangle. Black-box V ariational Inference for\nStochastic Differential Equations.ArXiv e-prints, 2018.\nMichael Schober, David Duvenaud, and Philipp Hennig. Probabilistic ODE solvers with Runge-Kutta\nmeans. InAdvances in Neural Information Processing Systems 25, 2014.\nPeter Schulam and Suchi Saria. What-if reasoning with counterfactual Gaussian processes.arXiv\npreprint arXiv:1703.10651, 2017.\nHossein Soleimani, James Hensman, and Suchi Saria. Scalable joint models for reliable uncertainty-\naware event prediction.IEEE transactions on pattern analysis and machine intelligence, 2017a.\nHossein Soleimani, Adarsh Subbaswamy, and Suchi Saria. Treatment-response models for coun-\nterfactual reasoning with continuous-time, continuous-valued interventions.arXiv preprint\narXiv:1704.02038, 2017b.\nJos Stam. Stableﬂuids. InProceedings of the 26th annual conference on Computer graphics and\ninteractive techniques, pages 121–128. ACM Press/Addison-Wesley Publishing Co., 1999.\n12\nPaul Stapor, Fabian Froehlich, and Jan Hasenauer. Optimization and uncertainty analysis of ODE\nmodels using second order adjoint sensitivity analysis.bioRxiv, page 272005, 2018.\nJakub M Tomczak and Max Welling. Improving variational auto-encoders using Householderﬂow.\narXiv preprint arXiv:1611.09630, 2016.\nSteffen Wiewel, Moritz Becher, and Nils Thuerey. Latent-space physics: Towards learning the\ntemporal evolution ofﬂuidﬂow.arXiv preprint arXiv:1802.10123, 2018.\nHong Zhang and Adrian Sandu. Fatode: a library for forward, adjoint, and tangent linear integration\nof ODEs.SIAM Journal on Scientiﬁc Computing, 36(5):C504–C523, 2014.\n13",
  "values": {
    "Deferral to humans": "No",
    "Interpretable (to users)": "No",
    "Transparent (to users)": "No",
    "Explicability": "No",
    "Non-maleficence": "No",
    "Critiqability": "No",
    "Collective influence": "No",
    "Fairness": "No",
    "Beneficence": "No",
    "Not socially biased": "No",
    "Respect for Persons": "No",
    "User influence": "No",
    "Justice": "No",
    "Privacy": "No",
    "Respect for Law and public interest": "No",
    "Autonomy (power to decide)": "No"
  }
}