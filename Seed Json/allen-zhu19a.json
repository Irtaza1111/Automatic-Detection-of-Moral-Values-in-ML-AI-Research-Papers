{
  "pdf": "allen-zhu19a",
  "title": "A Convergence Theory for Deep Learning via Over-Parameterization",
  "author": "Zeyuan Allen-Zhu, Yuanzhi Li, Zhao Song",
  "paper_id": "allen-zhu19a",
  "text": "A Convergence Theory for Deep Learning via Over-Parameterization\nZeyuan Allen-Zhu * 1 Yuanzhi Li* 2 3 Zhao Song * 4 5 6\nAbstract\nDeep neural networks (DNNs) have demon-\nstrated dominating performance in many ﬁelds;\nsince AlexNet, networks used in practice are go-\ning wider and deeper. On the theoretical side, a\nlong line of works have been focusing on why\nwe can train neural networks when there is only\none hidden layer. The theory of multi-layer net-\nworks remains unsettled. In this work, we prove\nsimple algorithms such as stochastic gradient de-\nscent (SGD) can ﬁndglobal minima on the train-\ning objective of DNNs in polynomial time. We\nonly make two assumptions: the inputs do not de-\ngenerate and the network is over-parameterized.\nThe latter means the number of hidden neurons\nis sufﬁciently large: polynomial in L, the num-\nber of DNN layers and in n, the number of train-\ning samples. As concrete examples, starting from\nrandomly initialized weights, we show that SGD\nattains 100% training accuracy in classiﬁcation\ntasks, or minimizes regression loss in linear con-\nvergence speed ε∝ e−Ω(T), with running time\npolynomial in n and L. Our theory applies to the\nwidely-used but non-smooth ReLU activation,\nand to any smooth and possibly non-convex loss\nfunctions. In terms of network architectures, our\ntheory at least applies to fully-connected neural\nnetworks, convolutional neural networks (CNN),\nand residual neural networks (ResNet).\n*Equal contribution . Full version and future updates are avail-\nable at https://arxiv.org/abs/1811.03962.\nThis paper is a follow up to the recurrent neural network (RNN)\npaper (Allen-Zhu et al., 2018b) by the same set of authors. Most\nof the techniques used in this paper were already discovered in the\nRNN paper, and this paper can be viewed as a simpliﬁcation (or\nto some extent a special case) of the RNN setting in order to reach\nout to a wider audience. We compare the difference and mention\nour additional contribution in Section 1.2.\n1Microsoft Research AI 2Stanford University\n3Princeton University 4UT-Austin 5University Wash-\nington 6Harvard University. Correspondence to:\nZeyuan Allen-Zhu <zeyuan@csail.mit.edu>, Yuanzhi Li\n<yuanzhil@stanford.edu>, Zhao Song<zhaos@utexas.edu>.\nProceedings of the 36th International Conference on Machine\nLearning, Long Beach, California, PMLR 97, 2019. Copyright\n2019 by the author(s).\n1 Introduction\nNeural networks have demonstrated a great success in\nnumerous machine-learning tasks (Amodei et al., 2016;\nGraves et al., 2013; He et al., 2016; Krizhevsky et al., 2012;\nLillicrap et al., 2015; Silver et al., 2016; 2017). One of the\nempirical ﬁndings is that neural networks, trained by ﬁrst-\norder methods from random initialization, have a remark-\nable ability of ﬁtting training data (Zhang et al., 2017).\nFrom a capacity perspective, the ability to ﬁt training data\nmay not be surprising: modern neural networks are always\nheavily over-parameterized — they have (much) more pa-\nrameters than the total number of training samples. Thus,\nthere exists parameter choices to achieve zero training error\nas long as data does not degenerate.\nYet, from an optimization perspective, the fact that ran-\ndomly initialized ﬁrst-order methods can ﬁnd optimal so-\nlutions on the training data is quite non-trivial: neural net-\nworks are often equipped with the ReLU activation, mak-\ning the training objective not only non-convex, but even\nnon-smooth. Even the general convergence for ﬁnding\napproximate critical points of a non-convex, non-smooth\nfunction is not fully understood (Burke et al., 2005), and\nappears to be a challenging question on its own. This is in\ndirect contrast to practice, in which ReLU networks trained\nby stochastic gradient descent (SGD) from random initial-\nization almost never face the problem of non-smoothness\nor non-convexity, and can converge to even a global min-\nimal over the training set quite easily. This was demon-\nstrated by Goodfellow et al. (2015) using experiments for a\nvariety of network architectures, and a theoretical justiﬁca-\ntion remains missing to explain this phenomenon.\nRecently, there are quite a few papers trying to understand\nthe success of neural networks from optimization perspec-\ntive. Many of them focus on the case when the inputs\nare random Gaussian, and work only for two-layer neural\nnetworks (Brutzkus & Globerson, 2017; Du et al., 2018b;\nGe et al., 2017; Li & Yuan, 2017; Panigrahy et al., 2018;\nSoltanolkotabi, 2017; Tian, 2017; Zhong et al., 2017a;b).\nIn Li & Liang (2018), it was shown that for a two-layer net-\nwork with ReLU activation, SGD ﬁnds nearly-global op-\ntimal (say, 99% classiﬁcation accuracy) solutions on the\ntraining data, as long as the network isover-parameterized,\nmeaning that when the number of neurons is polynomi-\nA Convergence Theory for Deep Learning via Over-Parameterization\nally large comparing to the input size. Moreover, if the\ndata is sufﬁciently structured (say, coming from mixtures of\nseparable distributions), this perfect accuracy extends also\nto test data. As a separate note, over-parameterization is\nsuggested as the possible key to avoid bad local minima\nby Safran & Shamir (2018) even for two-layer networks.\nThere are also results that go beyond two-layer neural net-\nworks but with limitations. Some consider deeplinear neu-\nral networks without any activation functions (Arora et al.,\n2018a; Bartlett et al., 2018; Hardt & Ma, 2017; Kawaguchi,\n2016). The result of Daniely (2017) applies to multi-layer\nneural network with ReLU activation, but is about the con-\nvex training process only with respect to the last layer.\nDaniely worked in a parameter regime where the weight\nchanges of all layers except the last one make negligible\ncontribution to the ﬁnal output (and they form the so-called\nconjugate kernel). The result of Soudry & Carmon (2016)\nshows that under over-parameterization and under random\ninput perturbation, there is bad local minima for multi-layer\nneural networks. Their work did not show any provable\nconvergence rate.\nIn this paper, we study the following fundamental ques-\ntion\nCan DNN be trained close to zero training error\nefﬁciently under mild assumptions?\nIf so, can the running time depend only polynomially in\nthe number of layers?\nMotivation. In 2012 AlexNet (Krizhevsky et al., 2012)\nwas born with 5 convolutional layers. Since then, the com-\nmon trend in the deep learning community is to build net-\nwork architectures that go deeper. In 2014, Simonyan &\nZisserman (2014) proposed a VGG network with19 layers.\nLater, Szegedy et al. (2015) proposed GoogleNet with 22\nlayers. In practice, we cannot make the network deeper by\nnaively stacking layers together due to the so-called vanish-\ning / exploding gradient issues. For this reason, in 2015, He\net al. (2016) proposed an ingenious deep network structure\ncalled Deep Residual Network (ResNet), with the capabil-\nity of handling at least 152 layers. For more overview and\nvariants of ResNet, we refer the readers to (Fung, 2017).\nCompared to the practical neural networks that go much\ndeeper, the existing theory has been mostly around two-\nlayer (thus one-hidden-layer) networks even just for the\ntraining process alone. It is natural to ask if we can the-\noretically understand how the training process has worked\nfor multi-layer neural networks.\n1.1 Our Result\nIn this paper, we extend the over-parameterization the-\nory to multi-layer neural networks. We show that over-\nparameterized neural networks can indeed be trained by\nregular ﬁrst-order methods to global minima (e.g. zero\ntraining error), as as long as the dataset is non-degenerate.\nWe say that the dataset is non-degenerate if the data points\nare distinct. This is a minimal requirement since a dataset\n{(x1, y1), (x2, y2)} with the same input x1 = x2 and dif-\nferent labels y1̸= y2 can not be trained to zero error. We\ndenote by δ the minimum (relative) distance between two\ntraining data points, and by n the number of samples in the\ntraining dataset.\nNow, consider an L-layer fully-connected feedforward\nneural network, each layer consisting of m neurons\nequipped with ReLU activation. We show that,\n• As long as m ≥ poly(n, L, δ−1), starting from ran-\ndom Gaussian initialized weights, gradient descent\n(GD) and stochastic gradient descent (SGD) ﬁnd ε-\nerror global minimum in ℓ2 regression using at most\nT = poly(n, L, δ−1) log 1\nε iterations. This is a linear\nconvergence rate.\n• Using the same network, if the task is multi-label clas-\nsiﬁcation, then GD and SGD ﬁnd an 100% accuracy\nclassiﬁer on the training set in T = poly(n, L, δ−1) it-\nerations.\n• Our result also applies to other Lipschitz-smooth loss\nfunctions, and some other network architectures includ-\ning convolutional neural networks (CNNs) and residual\nnetworks (ResNet).\nRemark. This paper does not cover the the generalization\nof over-parameterized neural networks to the test data. We\nrefer interested readers to some practical evidence (Sri-\nvastava et al., 2015; Zagoruyko & Komodakis, 2016) that\ndeeper (and wider) neural networks actually generalize bet-\nter. As for theoretical results, over-parameterized neural\nnetworks provably generalize at least for two-layer net-\nworks (Allen-Zhu et al., 2018a; Li & Liang, 2018) and for\nthree-layer networks (Allen-Zhu et al., 2018a).1\nA concurrent but different result. We acknowledge a\nconcurrent work of Du et al. (2018a) which has a similar\nabstract to this paper, but is different from us in many as-\npects. Since we noticed many readers cannot tell the two\nresults apart, we compare them carefully below. Du et al.\n(2018a) has two main results, one for fully-connected net-\nworks and the other for residual networks (ResNet).\nFor fully-connected networks, they only proved the training\ntime is no more than exponential in the number of layers,\nleading to a claim of the form “ResNet has an advantage be-\ncause ResNet is polynomial-time but fully-connected net-\n1If data is “well-structured” two-layer over-parameterized\nneural networks can learn it using SGD with polynomially many\nsamples (Li & Liang, 2018). If data is produced by some un-\nknown two-layer (resp. three-layer) neural network, then two-\nlayer (resp. three-layer) neural networks can also provably learn\nit using SGD and polynomially many samples (Allen-Zhu et al.,\n2018a).\nA Convergence Theory for Deep Learning via Over-Parameterization\nwork is (possibly) exponential-time.” As we prove in this\npaper, fully-connected networks do have polynomial train-\ning time, so their logic behind this claim is ungrounded.\nFor residual networks, their training time scales polynomial\nin 1\nλ0\n, a parameter that depends on the minimal singular\nvalue of a complicated, L-times recursively-deﬁned kernel\nmatrix. It is not clear whether 1\nλ0\nis small or even poly-\nnomial from their original writing. In their version 2, they\nhave sketched a possible proof to bound 1\nλ0\nin the special\ncase of residual networks.\nTheir result is different from us in many other aspects.\nTheir result only applies to the (signiﬁcantly simpler 2)\nsmooth activation functions and thus cannot apply to the\nstate-of-the-art ReLU activation. Their ResNet requires the\nvalue of weight initialization to be a function polynomial\nin λ (which is our δ); this can heavily depend on the input\ndata. Their result only applies to gradient descent but not\nto SGD. Their result only applies to ℓ2 loss but not others.\n1.2 Other Related Works\nLi & Liang (2018) originally proved their result for the\ncross-entropy loss. Later, the “training accuracy” (not the\ntesting accuracy) part of (Li & Liang, 2018) was extended\nto the ℓ2 loss (Du et al., 2018c).\nLinear networks without activation functions are impor-\ntant subjects on its own. Besides the already cited refer-\nences (Arora et al., 2018a; Bartlett et al., 2018; Hardt &\nMa, 2017; Kawaguchi, 2016), there are a number of works\nthat study linear dynamical systems, which can be viewed\nas the linear version of recurrent neural networks or rein-\nforcement learning. Recent works in this line of research\ninclude (Alaeddini et al., 2018; Arora et al., 2018b; Dean\net al., 2017; 2018; Hardt et al., 2018; Hazan et al., 2017;\n2018; Marecek & Tchrakian, 2018; Oymak & Ozay, 2018;\nSimchowitz et al., 2018).\nThere is sequence of work about one-hidden-layer (mul-\ntiple neurons) CNN (Brutzkus & Globerson, 2017; Du\net al., 2018b; Goel et al., 2018; Oymak, 2018; Zhong et al.,\n2017a). Whether the patches overlap or not plays a cru-\ncial role in analyzing algorithms for such CNN. One cate-\ngory of the results have required the patches to be disjoint\n(Brutzkus & Globerson, 2017; Du et al., 2018b; Zhong\net al., 2017a). The other category (Goel et al., 2018; Oy-\nmak, 2018) have ﬁgured out a weaker assumption or even\nremoved that patch-disjoint assumption. On input data dis-\ntribution, most relied on inputs being Gaussian (Brutzkus\n& Globerson, 2017; Du et al., 2018b; Oymak, 2018; Zhong\net al., 2017a), and some assumed inputs to be symmet-\n2For instance, we have to establish a semi-smoothness the-\norem for deep ReLU networks (see Theorem 4). If instead the\nactivation function is Lipscthiz smooth, and if one does not care\nabout exponential blow up in the number of layers L, then the\nnetwork is automatically 2O(L)-Lipschitz smooth.\nrically distributed with identity covariance and bounded-\nness (Goel et al., 2018).\nAs for ResNet, Li & Yuan (2017) proved that SGD\nlearns one-hidden-layer residual neural networks under\nGaussian input assumption. The techniques in (Zhong\net al., 2017a;b) can also be generalized to one-hidden-layer\nResNet under the Gaussian input assumption; they can\nshow that GD starting from good initialization point (via\ntensor initialization) learns ResNet. Hardt & Ma (2017)\ndeep linear residual networks have no spurious local op-\ntima.\nIf no assumption is allowed, neural networks have been\nshown hard in several different perspectives. Thirty years\nago, Blum & Rivest (1993) ﬁrst proved that learning the\nneural network is NP-complete. Stronger hardness results\nhave been proved over the last decade (Daniely, 2016;\nDaniely & Shalev-Shwartz, 2016; Goel et al., 2017; Kli-\nvans & Sherstov, 2009; Livni et al., 2014; Manurangsi &\nReichman, 2018; Song et al., 2017).\nAn over-parameterized RNN theory. For experts in\nDNN theory, one may view this present paper as a deeply-\nsimpliﬁed version of the recurrent neural network (RNN)\npaper (Allen-Zhu et al., 2018b) by the same set of authors.\nA recurrent neural network executed on input sequences\nwith time horizon L is very similar to a feedforward neu-\nral network with L layers. The main difference is that in\nfeedforward neural networks, weight matrices are differ-\nent across layers, and thus independently randomly initial-\nized; in contrast, in RNN, the same weight matrix is applied\nacross the entire time horizon so we do not have fresh new\nrandomness for proofs that involve in induction.\nSo, the over-parameterized convergence theory of DNN is\nmuch simpler than that of RNN.\nWe write this DNN result as a separate paper because: (1)\nnot all the readers can easily notice that DNN is easier to\nstudy than RNN; (2) we believe the convergence of DNN\nis important on its own; (3) the proof in this paper is much\nsimpler (30 vs 80 pages) and could reach out to a wider au-\ndience; (4) the simplicity of this paper allows us to tighten\nparameters in some non-trivial ways; and (5) the simplic-\nity of this paper allows us to also study convolutional net-\nworks, residual networks, as well as different loss functions\n(all of them were missing from (Allen-Zhu et al., 2018b)).\nWe also note that the techniques of this paper can be com-\nbined with (Allen-Zhu et al., 2018b) to show the conver-\ngence of over-parameterized deep RNN.\n2 Preliminaries\nWe use N (µ, σ) to denote the Gaussian distribution of\nmean µ and variance σ; andB(m, 1\n2) to denote the binomial\ndistribution with m trials and 1/2 success rate. We use∥v∥\nA Convergence Theory for Deep Learning via Over-Parameterization\nto denote Euclidean norms of vectors v, and∥M∥2,∥M∥F\nto denote spectral and Frobenius norms of matricesM. For\na tuple− →W = (W1, . . . ,WL) of matrices, we let∥− →W∥2 =\nmaxℓ∈[L]∥Wℓ∥2 and∥− →W∥F = (∑L\nℓ=1∥Wℓ∥2\nF )1/2.\nWe use φ(x) = max {0, x} to denote the ReLU func-\ntion, and extend it to vectors v∈ Rm by letting φ(v) =\n(φ(v1), . . . , φ(vm)). We use 1event to denote the indicator\nfunction for event.\nThe training data consist of vector pairs {(xi, y∗\ni )}i∈[n],\nwhere each xi∈ Rd is the feature vector and y∗\ni is the label\nof the i-th training sample. We assume without loss of gen-\nerality that data are normalized so that∥xi∥ = 1 and its last\ncoordinate (xi)d = 1√\n2.3 We make the following separable\nassumption on the training data (motivated by (Li & Liang,\n2018)):\nAssumption 2.1. For every pair i, j∈ [n], we have∥xi−\nxj∥≥ δ.\nTo present the simplest possible proof, the main body\nof this paper only focuses on depth- L feedforward fully-\nconnected neural networks with an ℓ2-regression task.\nTherefore, each y∗\ni∈ Rd is a target vector for the regression\ntask. We explain how to extend it to more general settings\nin Section 5 and the Appendix. For notational simplicity,\nwe assume all the hidden layers have the same number of\nneurons, and our results trivially generalize to each layer\nhaving different number of neurons. Speciﬁcally, we focus\non the following network\ngi,0 = Axi hi,0 = φ(Axi) for i∈ [n]\ngi,ℓ = Wℓhi,ℓ−1 hi,ℓ = φ(Wℓhi,ℓ−1) for i∈ [n], ℓ∈ [L]\nyi = Bhi,L for i∈ [n]\nwhere A∈ Rm×d is the weight matrix for the input layer,\nWℓ∈ Rm×m is the weight matrix for theℓ-th hidden layer,\nand B∈ Rd×m is the weight matrix for the output layer.\nFor notational convenience in the proofs, we may also use\nhi,−1 to denote xi and W0 to denote A.\nDeﬁnition 2.2 (diagonal sign matrix) . For each i∈ [n]\nand ℓ∈{ 0, 1, . . . , L}, we denote by Di,ℓ the diagonal sign\nmatrix where (Di,ℓ)k,k = 1(Wℓhi,ℓ−1)k≥0 for each k ∈\n[m].\nAs a result, we have hi,ℓ = Di,ℓWℓhi,ℓ−1 = Di,ℓgi,ℓ and\n(Di,ℓ)k,k = 1(gi,ℓ)k≥0.\n3Without loss of generality, one can re-scale and assume\n∥xi∥ ≤1/\n√\n2 for every i∈ [n]. Again, without loss of gen-\nerality, one can pad eachxi by an additional coordinate to ensure\n∥xi∥ = 1/\n√\n2. Finally, without loss of generality, one can pad\neachxi by an additional coordinate 1√\n2 to ensure∥xi∥ = 1. This\nlast coordinate 1√\n2 is equivalent to introducing a (random) bias\nterm, because A( y√\n2, 1√\n2 ) = A√\n2 (y, 0)+b whereb∼N (0, 1\nm I).\nIn our proofs, the speciﬁc constant 1√\n2 does not matter.\nWe make the following standard choices of random initial-\nization:\nDeﬁnition 2.3. We say that− →W = (W1, . . . ,WL), A and\nB are at random initialization if\n• [Wℓ]i,j∼N (0, 2\nm) for every i, j∈ [m] and ℓ∈ [L];\n• Ai,j∼N (0, 2\nm) for every (i, j)∈ [m]× [d]; and\n• Bi,j∼N (0, 1\nd) for every (i, j)∈ [d]× [m].\nAssumption 2.4. Throughout this paper we assume m≥\nΩ\n(\npoly(n, L, δ−1)· d\n)\nfor some sufﬁciently large polyno-\nmial. To present the simplest proof, we did not try to im-\nprove such polynomial factors.\n2.1 Objective and Gradient\nOur regression objective is\nF (− →W) :=\nn∑\ni=1\nFi(− →W) where\nFi(− →W) := 1\n2∥Bhi,L− y∗\ni∥2 for each i∈ [n]\nWe also denote by lossi := Bhi,L− y∗\ni the loss vector for\nsample i. For simplicity, we only focus on training − →W in\nthis paper and thus leave A and B at random initialization.\nOur techniques can be extended to the case whenA, B and− →W are jointly trained.\nDeﬁnition 2.5. For each ℓ ∈ {1, 2,··· , L}, we deﬁne\nBacki,ℓ := BDi,LWL··· Di,ℓWℓ∈ Rd×m and for ℓ =\nL + 1, we deﬁne Backi,ℓ = B∈ Rd×m.\nUsing this notation, one can calculate the gradient of\nF (− →W) as follows.\nFact 2.6. The gradient with respect to the k-th row of\nWℓ∈ Rm×m is\n∇[Wℓ]k F (− →W)\n=\nn∑\ni=1\n(Back⊤\ni,ℓ+1lossi)k· hi,ℓ−1· 1⟨[Wℓ]k,hi,ℓ−1⟩≥0\nThe gradient with respect to Wℓ is\n∇Wℓ F (− →W) = ∑n\ni=1 Di,ℓ(Back⊤\ni,ℓ+1lossi)h⊤\ni,ℓ−1\nWe denote by\n∇F (− →W) =\n(\n∇W1 F (− →W), . . . ,∇WL F (− →W)\n)\n.\n3 Our Results and Techniques\nTo present our result in the simplest possible way, we\nchoose to mainly focus on fully-connected L-layer neural\nnetworks with the ℓ2 regression loss. We shall extend it to\nmore general settings (such as convolutional and residual\nnetworks and other losses) in Section 5. Our main results\ncan be stated as follows:\nTheorem 1 (gradient descent) . Suppose m ≥\n˜Ω\n(\npoly(n, L, δ−1)· d\n)\n. Starting from random initializa-\nA Convergence Theory for Deep Learning via Over-Parameterization\ntion, with probability at least 1− e−Ω(log2m), gradient de-\nscent with learning rate η = Θ\n( dδ\npoly(n,L)·m\n)\nﬁnds a point\nF (− →W)≤ ε in\nT = Θ\n(poly(n, L)\nδ2 · log ε−1)\niterations.\nThis is known as the linear convergence rate because ε\ndrops exponentially fast in T . We have not tried to im-\nprove the polynomial factors in m and T , and are aware of\nseveral ways to improve these factors (but at the expense\nof complicating the proof). We note that d is the data input\ndimension and our result is independent of d.\nRemark. In our version 1, for simplicity, we also put a\nlog2(1/ε) factor in the amount of over-parameterization m\nin Theorem 1. Since some readers have raised concerns re-\ngarding this Du et al. (2018a), we have now removed it at\nthe expense of changing half a line of the proof.\nTheorem 2 (SGD). Suppose b ∈ [n] and m ≥\n˜Ω\n(poly(n,L,δ−1)·d\nb\n)\n. Starting from random initialization,\nwith probability at least 1− e−Ω(log2m), SGD with learn-\ning rate η = Θ( bδd\npoly(n,L)m log2m) and mini-batch size b\nﬁnds F (− →W)≤ ε in\nT = Θ\n(poly(n, L)· log2 m\nδ2b · log ε−1)\niterations.\nThis is again a linear convergence rate because T∝ log 1\nε .\nThe reason for the additional log2 m factor comparing to\nTheorem 1 is because we have a 1− e−Ω(log2m) high con-\nﬁdence bound.\nRemark. For experts in optimization theory, one may im-\nmediately question the accuracy of Theorem 2, because\nSGD is known to converge at a slower rate T ∝ 1\npoly(ε)\neven for convex functions. There is no contradiction here.\nImaging a strongly convex function f(x) = ∑n\ni=1 fi(x)\nthat has a common minimizer x∗∈ arg minx{fi(x)} for\nevery i∈ [n], then SGD is known to converge in a linear\nconvergence rate.\n3.1 Technical Theorems\nThe main difﬁculty of this paper is to prove the following\ntwo technical theorems. The ﬁrst one is about the gradient\nbounds for points that are sufﬁciently close to the random\ninitialization:\nTheorem 3 (no critical point) . With probability≥ 1−\ne−Ω(m/poly(n,L,δ−1)) over randomness− →W(0), A, B, it sat-\nisﬁes for every ℓ∈ [L], every i∈ [n], and every− →W with\n∥− →W−− →W(0)∥2≤ 1\npoly(n,L,δ−1),\n∥∇F (− →W)∥2\nF≤ O\n(\nF (− →W)× Lnm\nd\n)\n∥∇F (− →W)∥2\nF≥ Ω\n(\nF (− →W)× δm\ndn2\n)\n.\nMost notably, the second property of Theorem 3 says that\nas long as the objective is large, the gradient norm is also\nlarge. (See also Figure 1.) This means, when we are sufﬁ-\nciently close to the random initialization, there is no saddle\npoint or critical point of any order. This gives us hope to\nﬁndglobal minima of the objective F (− →W).\nUnfortunately, Theorem 3 itself is enough. Even if we fol-\nlow the negative gradient direction of F (− →W), how can we\nguarantee that the objective truly decreases? Classically\nin optimization theory, one relies on the smoothness prop-\nerty (e.g. Lipscthiz smoothness (Nesterov, 2004)) to derive\nsuch objective-decrease guarantee. Unfortunately, smooth-\nness property at least requires the objective to be twice dif-\nferentiable, but ReLU activation is not.\nTo deal with this issue, we prove the following “semi-\nsmoothness” property of the objective.\nTheorem 4 (semi-smoothness). With probability at\nleast 1− e−Ω(m/poly(L,logm)) over the randomness of− →W(0), A, B, we have :\nfor every ˘− →W∈ (Rm×m)L with\n∥ ˘− →W−− →W(0)∥2≤ 1\npoly(L, log m) ,\nand for every− →W′∈ (Rm×m)L with\n∥− →W′∥2≤ 1\npoly(L, log m) ,\nthe following inequality holds\nF ( ˘− →W +− →W′)≤ F ( ˘− →W) +⟨∇F ( ˘− →W),− →W′⟩\n+ O\n(nL2m\nd\n)\n∥− →W′∥2\n2\n+ poly(L)√nm log m√\nd\n·∥− →W′∥2\n(\nF ( ˘− →W)\n)1/2\nQuite different from classical smoothness, we still have a\nﬁrst-order term∥− →W′∥2 on the right hand side, but classi-\ncal smoothness only has a second-order term ∥− →W′∥2\n2. As\none can see in our ﬁnal proofs, as m goes larger (so when\nwe over-parameterize), the effect of the ﬁrst-order term be-\ncomes smaller and smaller comparing to the second-order\nterm. This brings Theorem 4 closer and closer, but still not\nidentical, to the classical Lipschitz smoothness.\nThe derivation of our main Theorem 1 and 2 from technical\nTheorem 3 and 4 is quite straightforward, and can be found\nin Section F and G.\nRemark. In our proofs, we show that GD and SGD can\nconverge fast enough and thus the weights stay close to\nrandom initialization by spectral norm bound 1\npoly(n,L,δ−1).\n(This ensures Theorem 3 and 4 both apply.) This bound\nseems extremely small, but in fact, is large enough to to-\nA Convergence Theory for Deep Learning via Over-Parameterization\n1 21 41 61 81 101 121 141 161\n0\n0.05\n0.1\n0.15\n0.2\n0.25\n0.3\n0.35\n0\n0.2\n0.4\n0.6\n0.8\n1\n1 21 41 61 81 101 121 141 161\nGRADIENT NORM\nOBJECTIVE VALUE\n# OF EPOCHS\nObjValue\nGradNorm\nCIF AR-10 dataset,\nvgg19bn architecture\nFigure 1: Landscapes of the CIFAR10 image-classiﬁcation training objectiveF(W) near pointsW = Wt on the SGD training trajectory. The x andy axes represent the\ngradient direction∇F(Wt) and the most negatively curved direction of the Hessian after smoothing (approximately found by Oja’s method (Allen-Zhu & Li, 2017;\n2018)). Thez axis represents the objective value.\nObservation. As far as minimizing objective is concerned, the (negative) gradient direction sufﬁciently decreases the training objective. This is consistent with our\nmain ﬁndings Theorem 3 and 4. Using second-order information gives little help.\nRemark 1. Gradient norm does not tend to zero because cross-entropy loss is not strongly convex (see Section 5).\nRemark 2. The task is CIFAR10 (for CIFAR100 or CIFAR10 with noisy label, see Figure 2 through 7 in appendix).\nRemark 3. Architecture is ResNet with 32 layers (for VGG19 or ResNet-110, see Figure 2 through 7 in appendix).\nRemark 4. The six plots correspond to epoch 5, 40, 90, 120, 130 and 160. We start with learning rate0.1, and decrease it to0.01 at epoch 81, and to0.001 at epoch\n122. SGD with momentum 0.9 is used. The training code is unchanged from (Yang, 2018) and we only write new code for plotting such landscapes.\ntally change the outputs and ﬁt the training data, because\nweights are randomly initialized (per entry) at around 1√m\nfor m being large.\nIn practice, we acknowledge that one often goes beyond\nthis theory-predicted spectral-norm boundary. However,\nquite interestingly, we still observe Theorem 3 and 4 hap-\npen in practice at least for image classiﬁcation tasks. In\nFigure 1, we show the typical landscape near a point − →W\non the SGD training trajectory. The gradient is sufﬁciently\nlarge and going in its direction can indeed decrease the ob-\njective; in contrast, though the objective is non-convex, the\nnegative curvature of its “Hessian” is not signiﬁcant com-\nparing to gradient. From Figure 1 we also see that the ob-\njective function is sufﬁciently smooth (at least in the two\ninterested dimensions that we plot).\n4 Main Techniques\nProof to Theorem 3 and 4 consist of the following steps.\nStep 1: properties at random initialization. Let− →W =− →W(0) be at random initialization and hi,ℓ and Di,ℓ be de-\nﬁned with respect to− →W. We ﬁrst show that forward propa-\ngation neither explode or vanish. That is,\n∥hi,ℓ∥≈ 1 for all i∈ [n] and ℓ∈ [L].\nThis is basically because for a ﬁxed y, we have ∥Wy∥2\nis around 2, and if its signs are sufﬁciently random, then\nReLU activation kills half of the norm, that is∥φ(Wy)∥≈\n1. Then applying induction ﬁnishes the proof.\nAnalyzing forward propagation is not enough. We also\nneed spectral norm bounds on the backward matrix\n∥BDi,LWL··· Di,aWa∥2≤ O(\n√\nm/d) ,\nand on the intermediate matrix\n∥Di,aWa··· Di,bWb∥2≤ O(\n√\nL)\nfor every a, b∈ [L]. Note that if one naively bounds the\nspectral norm by induction, then ∥Di,aWa∥2≈ 2 and it\nwill exponentially blow up! Our careful analysis ensures\nthat even when L layers are stacked together, there is no\nexponential blow up in L.\nThe ﬁnal lemma in this step proves that, as long as ∥xi−\nA Convergence Theory for Deep Learning via Over-Parameterization\nxj∥≥ δ, then\n∥hi,ℓ− hj,ℓ∥≥ Ω(δ) for each layer ℓ∈ [L].\nThis can be proved by a careful induction. Details are in\nSection A.\nStep 2: stability after adversarial perturbation. We\nshow that for every − →W that is “close” to initialization,\nmeaning∥Wℓ− W(0)\nℓ ∥2 ≤ ω for every ℓ and for some\nω≤ 1\npoly(L), then\n(a) the number of sign changes ∥Di,ℓ− D(0)\ni,ℓ∥0 is at most\nO(mω2/3L), and\n(b) the perturbation amount∥hi,ℓ− h(0)\ni,ℓ∥≤ O(ωL5/2).\nWe call this “forward stability”, and it is the most technical\nproof of this paper.\nRemark. Intuitively, both “(a) implies (b)” and “(b) implies\n(a)” are not hard to prove. If the number of sign changes\nis bounded in all layers, then hi,ℓ and h(0)\ni,ℓ cannot be too\nfar away by applying matrix concentration; and reversely,\nif hi,ℓ is not far from h(0)\ni,ℓ in all layers, then the number of\nsign changes per layer must be small. Unfortunately, one\ncannot apply such derivation with induction, because con-\nstants will blow up exponentially in the number of layers.\nRemark. In the ﬁnal proof, − →W is a point obtained by\nGD/SGD starting from− →W(0), and thus− →W may depend on\nthe randomness of − →W(0). Since we cannot control how\nsuch randomness correlates, we argue for the above two\nproperties against all possible− →W.\nAnother main result in this step is to show that the back-\nward matrix BDi,LWL··· Di,aWa does not change by\nmore than O(ω1/3L2√\nm/d) in spectral norm. Recall that\nin the Step 1 we shown that this matrix is of spectral norm\nO(\n√\nm/d); thus as long as ω1/3L2 ≪ 1, this change is\nsomewhat negligible. Details are in Section B.\nStep 3: gradient bound. The hard part of Theorem 3\nis to show gradient lower bound. For this purpose, re-\ncall from Fact 2.6 that each sample i ∈ [n] contributes\nto the full gradient matrix by Di,ℓ(Back⊤\ni,ℓ+1lossi)h⊤\ni,ℓ−1,\nwhere the backward matrix is applied to a loss vector\nlossi. To show this is large, intuitively, one wishes to show\n(Back⊤\ni,ℓ+1lossi) and hi,ℓ−1 are both vectors with large Eu-\nclidean norm.\nThanks to Step 1 and 2, this is not hard for a single sam-\nple i ∈ [n]. For instance, ∥h(0)\ni,ℓ−1∥ ≈1 by Step 1 and\nwe know∥hi,ℓ−1− h(0)\ni,ℓ−1∥≤ o(1) from Step 2. One can\nalso argue for Back⊤\ni,ℓ+1lossi but this is a bit harder. In-\ndeed, when moving from random initialization− →W(0) to− →W,\nthe loss vector lossi can change completely. Fortunately,\nlossi∈ Rd is a low-dimensional vector, so one can calcu-\nlate∥Back⊤\ni,ℓ+1u∥ for every ﬁxedu and then apply ε-net.\nFinally, how to combine the above argument with mul-\ntiple samples i ∈ [n]? These matrices are clearly not\nindependent and may (in principle) sum up to zero. To\ndeal with this, we use ∥hi,ℓ − hj,ℓ∥ ≥ Ω(δ) from\nStep 1. In other words, even if the contribution matrix\nDi,ℓ(Back⊤\ni,ℓ+1lossi)h⊤\ni,ℓ−1 with respect to one sample i is\nﬁxed, the contribution matrix with respect to other sam-\nples j∈ [n]\\{ i} are still sufﬁciently random. Thus, the\nﬁnal gradient matrix will still be large. This idea comes\nfrom the prior work (Li & Liang, 2018), and helps us prove\nTheorem 3. Details in Appendix C and D.\nStep 4: smoothness. In order to prove Theorem 4, one\nneeds to argue, if we are currently at ˘− →W and perturb it by− →W′, then how much does the objective change in second\nand higher order terms. This is different from our sta-\nbility theory in Step 2, because Step 2 is regarding hav-\ning a perturbation on− →W(0); in contrast, in Theorem 4 we\nneed a (small) perturbation − →W′ on top of ˘− →W, which may\nalready be a point perturbed from − →W(0). Nevertheless,\nwe still manage to show that, if ˘hi,ℓ is calculated on ˘− →W\nand hi,ℓ is calculated on ˘− →W +− →W′, then∥hi,ℓ− ˘hi,ℓ∥≤\nO(L1.5)∥W′∥2. This, along with other properties to prove,\nensures semi-smoothness. This explains Theorem 4 and\ndetails are in Section E.\nRemark. In other words, the amount of changes to each\nhidden layer (i.e., hi,ℓ− ˘hi,ℓ) is proportional to the amount\nof perturbation∥W′∥2. This may sound familiar to some\nreaders: a ReLU function is Lipschitz continuous |φ(a)−\nφ(b)|≤| a− b|, and composing Lipschitz functions still\nyield Lipschitz functions. What is perhaps surprising here\nis that this “composition” does not create exponential blow-\nup in the Lipschitz continuity parameter, as long as the\namount of over-parameterization is sufﬁcient and ˘− →W is\nclose to initialization.\n5 Notable Extensions\nOur Step 1 through Step 4 in Section 4 in fact give rise to\na general plan for proving the training convergence of any\nneural network (at least with respect to the ReLU activa-\ntion). Thus, it is expected that it can be generalized to many\nother settings. Not only we can have different number of\nneurons each layer, our theorems can be extended at least\nin the following three major directions.4\nDifferent loss functions. There is absolutely no need to\nrestrict only to ℓ2 regression loss. We prove in Appendix H\n4In principle, each such proof may require a careful rewriting\nof the main body of this paper. We choose to sketch only the\nproof difference (in the appendix) in order to keep this paper short.\nIf there is sufﬁcient interest from the readers, we can consider\nadding the full proofs in the future revision of this paper.\nA Convergence Theory for Deep Learning via Over-Parameterization\nthat, for any Lipschitz-smooth loss function f:\nTheorem 5 (arbitrary loss). From random initialization,\nwith probability at least 1− e−Ω(log2m), gradient descent\nwith appropriate learning rate satisfy the following.\n• If f is nonconvex but σ-gradient dominant (a.k.a.\nPolyak-Łojasiewicz), GD ﬁnds ε-error minimizer in5\nT = ˜O\n(poly(n,L)\nσδ 2 · log 1\nε\n)\niterations\nas long as m≥ ˜Ω\n(\npoly(n, L, δ−1)· dσ−2)\n.\n• If f is convex, then GD ﬁnds ε-error minimizer in\nT = ˜O\n(poly(n,L)\nδ2 · 1\nε\n)\niterations\nas long as m≥ ˜Ω\n(\npoly(n, L, δ−1)· d log ε−1)\n.\n• If f is non-convex, then SGD ﬁnds a point with∥∇f∥≤\nε in at most6\nT = ˜O\n(poly(n,L)\nδ2 · 1\nε2\n)\niterations\nas long as m≥ ˜Ω\n(\npoly(n, L, δ−1)· dε−1)\n.\n• If f is cross-entropy for multi-label classiﬁcation, then\nGD attains 100% training accuracy in at most7.\nT = ˜O\n(poly(n,L)\nδ2\n)\niterations\nas long as m≥ ˜Ω\n(\npoly(n, L, δ−1)· d\n)\n.\nWe remark here that theℓ2 loss is 1-gradient dominant so it\nfalls into the above general Theorem 5. One can also derive\nsimilar bounds for (mini-batch) SGD so we do not repeat\nthe statements here.\nConvolutional neural networks (CNN). There are lots\nof different ways to design CNN and each of them may\nrequire somewhat different proofs. In Appendix I, we study\nthe case when A, W1, . . . ,WL−1 are convolutional while\nWL and B are fully connected. We assume for notational\nsimplicity that each hidden layer has d points each with m\nchannels. (In vision tasks, a point is a pixel). In the most\ngeneral setting, these valuesd and m can vary across layers.\nWe prove the following theorem:\nTheorem 6 (CNN). As long as m≥ ˜Ω\n(\npoly(n, L, d, δ−1)·\nd\n)\n, with high probability, GD and SGD ﬁnd an ε-error so-\nlution for ℓ2 regression in\nT = ˜O\n(poly(n, L, d)\nδ2 · log ε−1)\n5Note that the loss function when combined with the neural\nnetwork togetherf (Bhi,L) is not gradient dominant. Therefore,\none cannot apply classical theory on gradient dominant functions\nto derive our same result.\n6Again, this cannot be derived from classical theory of ﬁnd-\ning approximate saddle points for non-convex functions, because\nweights− →W with small∥∇f (Bhi,L)∥ is a very different (usually\nmuch harder) task comparing to having small gradient with re-\nspect to− →W for the entire composite functionf (Bhi,L).\n7This is because attaining constant objective errorε = 1/4 for\nthe cross-entropy loss sufﬁces to imply perfect training accuracy.\niterations for CNN.\nOf course, one can replace ℓ2 loss with other loss functions\nin Theorem 5 to get different types of convergence rates.\nWe do not repeat them here.\nResidual neural networks (ResNet). There are lots of\ndifferent ways to design ResNet and each of them may\nrequire somewhat different proofs. In symbols, between\ntwo layers, one may study hℓ = φ(hℓ−1 + Whℓ−1),\nhℓ = φ(hℓ−1 + W2φ(W1hℓ−1)), or even hℓ = φ(hℓ−1 +\nW3φ(W2φ(W1hℓ−1))). Since the main purpose here is\nto illustrate the generality of our techniques but not to at-\ntack each speciﬁc setting, in Appendix J, we choose to con-\nsider the simplest residual setting hℓ = φ(hℓ−1 + Whℓ−1)\n(that was also studied for instance by theoretical work\n(Hardt & Ma, 2017)). With appropriately chosen random\ninitialization, we prove the following theorem:\nTheorem 7 (ResNet). As long as m≥ ˜Ω\n(\npoly(n, L, δ−1)·\nd\n)\n, with high probability, GD and SGD ﬁnd an ε-error so-\nlution for ℓ2 regression in\nT = ˜O\n(poly(n, L)\nδ2 · log ε−1)\niterations for ResNet.\nOf course, one can replace ℓ2 loss with other loss functions\nin Theorem 5 to get different types of convergence rates.\nWe do not repeat them here.\n6 Conclusion\nIn this paper we demonstrate for state-of-the-art network\narchitectures such as fully-connected neural networks, con-\nvolutional networks (CNN), or residual networks (ResNet),\nassuming there are n training samples without duplication,\nas long as the number of parameters is polynomial inn and\nL, ﬁrst-order methods such as GD/SGD can ﬁnd global op-\ntima of the training objective efﬁciently, that is, with run-\nning time only polynomially dependent on the total number\nof parameters of the network.\nFigure 1 illustrates our main technical contribution. With\nthe help of over-parameterization, near the GD/SGD train-\ning trajectory, there is no local minima and the objective\nis semi-smooth. The former means as long as the train-\ning objective is large, the objective gradient is also large.\nThe latter means simply following the (opposite) gradi-\nent direction can sufﬁciently decrease the objective. They\ntwo together means GD/SGD ﬁnds global minima on over-\nparameterized feedforward neural networks.\nThere are plenty of open directions following our work, es-\npecially how to extend our result to other types of deep\nlearning tasks and/or proving generalization. There is al-\nready generalization theory (Allen-Zhu et al., 2018a) for\nover-parameterized three-layer neural networks, so can we\ngo any deeper?\nA Convergence Theory for Deep Learning via Over-Parameterization\nReferences\nAlaeddini, A., Alemzadeh, S., Mesbahi, A., and Mesbahi,\nM. Linear model regression on time-series data: Non-\nasymptotic error bounds and applications.arXiv preprint\narXiv:1807.06611, 2018.\nAllen-Zhu, Z. and Li, Y . Follow the Compressed Leader:\nFaster Online Learning of Eigenvectors and Faster\nMMWU. In ICML, 2017. Full version available at\nhttp://arxiv.org/abs/1701.01722.\nAllen-Zhu, Z. and Li, Y . Neon2: Finding Local Minima\nvia First-Order Oracles. In NeurIPS, 2018. Full ver-\nsion available at http://arxiv.org/abs/1711.\n06673.\nAllen-Zhu, Z., Li, Y ., and Liang, Y . Learning and Gener-\nalization in Overparameterized Neural Networks, Going\nBeyond Two Layers. arXiv preprint arXiv:1811.04918,\nNovember 2018a.\nAllen-Zhu, Z., Li, Y ., and Song, Z. On the convergence\nrate of training recurrent neural networks.arXiv preprint\narXiv:1810.12065, October 2018b.\nAmodei, D., Ananthanarayanan, S., Anubhai, R., Bai, J.,\nBattenberg, E., Case, C., Casper, J., Catanzaro, B.,\nCheng, Q., Chen, G., et al. Deep speech 2: End-to-end\nspeech recognition in English and Mandarin. In Inter-\nnational Conference on Machine Learning (ICML) , pp.\n173–182, 2016.\nArora, S., Cohen, N., Golowich, N., and Hu, W. A conver-\ngence analysis of gradient descent for deep linear neural\nnetworks. arXiv preprint arXiv:1810.02281, 2018a.\nArora, S., Hazan, E., Lee, H., Singh, K., Zhang, C., and\nZhang, Y . Towards provable control for unknown linear\ndynamical systems. 2018b.\nBartlett, P., Helmbold, D., and Long, P. Gradient descent\nwith identity initialization efﬁciently learns positive def-\ninite linear transformations. In International Conference\non Machine Learning (ICML), pp. 520–529, 2018.\nBlum, A. L. and Rivest, R. L. Training a 3-node neural net-\nwork is np-complete. In Machine learning: From theory\nto applications (A preliminary version of this paper was\nappeared in NIPS 1989), pp. 9–28. Springer, 1993.\nBrutzkus, A. and Globerson, A. Globally optimal gradi-\nent descent for a convnet with gaussian inputs. In In-\nternational Conference on Machine Learning (ICML) .\nhttp://arxiv.org/abs/1702.07966, 2017.\nBurke, J. V ., Lewis, A. S., and Overton, M. L. A robust gra-\ndient sampling algorithm for nonsmooth, nonconvex op-\ntimization. SIAM Journal on Optimization , 15(3):751–\n779, 2005.\nDaniely, A. Complexity theoretic limitations on learning\nhalfspaces. In Proceedings of the forty-eighth annual\nACM symposium on Theory of Computing (STOC) , pp.\n105–117. ACM, 2016.\nDaniely, A. SGD learns the conjugate kernel class of the\nnetwork. In Advances in Neural Information Processing\nSystems (NeurIPS), pp. 2422–2430, 2017.\nDaniely, A. and Shalev-Shwartz, S. Complexity theoretic\nlimitations on learning dnfs. In Conference on Learning\nTheory (COLT), pp. 815–830, 2016.\nDean, S., Mania, H., Matni, N., Recht, B., and Tu, S. On\nthe sample complexity of the linear quadratic regulator.\narXiv preprint arXiv:1710.01688, 2017.\nDean, S., Tu, S., Matni, N., and Recht, B. Safely learning to\ncontrol the constrained linear quadratic regulator. arXiv\npreprint arXiv:1809.10121, 2018.\nDu, S. S., Lee, J. D., Li, H., Wang, L., and Zhai, X. Gradi-\nent descent ﬁnds global minima of deep neural networks.\narXiv preprint arXiv:1811.03804, November 2018a.\nDu, S. S., Lee, J. D., Tian, Y ., P ´oczos, B., and Singh,\nA. Gradient descent learns one-hidden-layer CNN:\ndon’t be afraid of spurious local minima. In In-\nternational Conference on Machine Learning (ICML) .\nhttp://arxiv.org/abs/1712.00779, 2018b.\nDu, S. S., Zhai, X., Poczos, B., and Singh, A. Gradient\nDescent Provably Optimizes Over-parameterized Neural\nNetworks. ArXiv e-prints, 2018c.\nFung, V . An overview of resnet and its vari-\nants. https://towardsdatascience.com/an-overview-of-\nresnet-and-its-variants-5281e2f56035, 2017.\nGe, R., Lee, J. D., and Ma, T. Learning one-hidden-layer\nneural networks with landscape design. In ICLR, 2017.\nURL http://arxiv.org/abs/1711.00501.\nGoel, S., Kanade, V ., Klivans, A., and Thaler, J. Reliably\nlearning the ReLU in polynomial time. InConference on\nLearning Theory (COLT), 2017.\nGoel, S., Klivans, A., and Meka, R. Learning one con-\nvolutional layer with overlapping patches. In Interna-\ntional Conference on Machine Learning (ICML) . arXiv\npreprint arXiv:1802.02547, 2018.\nGoodfellow, I. J., Vinyals, O., and Saxe, A. M. Qualita-\ntively characterizing neural network optimization prob-\nlems. In ICLR, 2015.\nGraves, A., Mohamed, A.-r., and Hinton, G. Speech recog-\nnition with deep recurrent neural networks. In IEEE In-\nternational Conference on Acoustics, Speech and Signal\nProcessing (ICASSP), pp. 6645–6649. IEEE, 2013.\nA Convergence Theory for Deep Learning via Over-Parameterization\nHardt, M. and Ma, T. Identity matters in deep learning.\nIn ICLR, 2017. URL http://arxiv.org/abs/\n1611.04231.\nHardt, M., Ma, T., and Recht, B. Gradient descent learns\nlinear dynamical systems. Journal of Machine Learning\nResearch (JMLR), 19(29):1–44, 2018.\nHazan, E., Singh, K., and Zhang, C. Learning linear dy-\nnamical systems via spectral ﬁltering. In Advances in\nNeural Information Processing Systems (NeurIPS) , pp.\n6702–6712, 2017.\nHazan, E., Lee, H., Singh, K., Zhang, C., and Zhang, Y .\nSpectral ﬁltering for general linear dynamical systems.\nIn Advances in Neural Information Processing Systems\n(NINPS), 2018.\nHe, K., Zhang, X., Ren, S., and Sun, J. Deep residual learn-\ning for image recognition. In Proceedings of the IEEE\nconference on computer vision and pattern recognition ,\npp. 770–778, 2016.\nKawaguchi, K. Deep learning without poor local minima.\nIn Advances in Neural Information Processing Systems,\npp. 586–594, 2016.\nKlivans, A. R. and Sherstov, A. A. Cryptographic hard-\nness for learning intersections of halfspaces. Journal of\nComputer and System Sciences, 75(1):2–12, 2009.\nKrizhevsky, A., Sutskever, I., and Hinton, G. E. Imagenet\nclassiﬁcation with deep convolutional neural networks.\nIn Advances in neural information processing systems ,\npp. 1097–1105, 2012.\nLi, Y . and Liang, Y . Learning overparameterized neural\nnetworks via stochastic gradient descent on structured\ndata. In Advances in Neural Information Processing Sys-\ntems (NeurIPS), 2018.\nLi, Y . and Yuan, Y . Convergence analysis of two-layer\nneural networks with ReLU activation. In Advances\nin Neural Information Processing Systems (NeurIPS) .\nhttp://arxiv.org/abs/1705.09886, 2017.\nLillicrap, T. P., Hunt, J. J., Pritzel, A., Heess, N., Erez,\nT., Tassa, Y ., Silver, D., and Wierstra, D. Continuous\ncontrol with deep reinforcement learning. arXiv preprint\narXiv:1509.02971, 2015.\nLivni, R., Shalev-Shwartz, S., and Shamir, O. On the\ncomputational efﬁciency of training neural networks.\nIn Advances in Neural Information Processing Systems\n(NeurIPS), pp. 855–863, 2014.\nManurangsi, P. and Reichman, D. The computa-\ntional complexity of training ReLU(s). arXiv preprint\narXiv:1810.04207, 2018.\nMarecek, J. and Tchrakian, T. Robust spectral ﬁltering and\nanomaly detection. arXiv preprint arXiv:1808.01181 ,\n2018.\nNesterov, Y . Introductory Lectures on Convex Program-\nming Volume: A Basic course , volume I. Kluwer Aca-\ndemic Publishers, 2004. ISBN 1402075537.\nOymak, S. Learning compact neural networks with regu-\nlarization. arXiv preprint arXiv:1802.01223, 2018.\nOymak, S. and Ozay, N. Non-asymptotic identiﬁcation\nof LTI systems from a single trajectory. arXiv preprint\narXiv:1806.05722, 2018.\nPanigrahy, R., Rahimi, A., Sachdeva, S., and Zhang, Q.\nConvergence results for neural networks via electrody-\nnamics. In ITCS, 2018.\nSafran, I. and Shamir, O. Spurious local minima are\ncommon in two-layer ReLU neural networks. In In-\nternational Conference on Machine Learning (ICML) .\nhttp://arxiv.org/abs/1712.08968, 2018.\nShamir, O. A variant of azuma’s inequality for martingales\nwith subgaussian tails. ArXiv e-prints, abs/1110.2392,\n10 2011.\nSilver, D., Huang, A., Maddison, C. J., Guez, A., Sifre, L.,\nVan Den Driessche, G., Schrittwieser, J., Antonoglou, I.,\nPanneershelvam, V ., Lanctot, M., et al. Mastering the\ngame of Go with deep neural networks and tree search.\nNature, 529(7587):484–489, 2016.\nSilver, D., Schrittwieser, J., Simonyan, K., Antonoglou, I.,\nHuang, A., Guez, A., Hubert, T., Baker, L., Lai, M.,\nBolton, A., et al. Mastering the game of Go without\nhuman knowledge. Nature, 550(7676):354, 2017.\nSimchowitz, M., Mania, H., Tu, S., Jordan, M. I.,\nand Recht, B. Learning without mixing: Towards a\nsharp analysis of linear system identiﬁcation. In Con-\nference on Learning Theory (COLT) . arXiv preprint\narXiv:1802.08334, 2018.\nSimonyan, K. and Zisserman, A. Very deep convolu-\ntional networks for large-scale image recognition. arXiv\npreprint arXiv:1409.1556, 2014.\nSoltanolkotabi, M. Learning ReLUs via gradient descent.\nCoRR, abs/1705.04591, 2017. URL http://arxiv.\norg/abs/1705.04591.\nSong, L., Vempala, S., Wilmes, J., and Xie, B. On the\ncomplexity of learning neural networks. In Advances in\nNeural Information Processing Systems (NeurIPS) , pp.\n5514–5522, 2017.\nA Convergence Theory for Deep Learning via Over-Parameterization\nSoudry, D. and Carmon, Y . No bad local minima: Data in-\ndependent training error guarantees for multilayer neural\nnetworks. arXiv preprint arXiv:1605.08361, 2016.\nSrivastava, R. K., Greff, K., and Schmidhuber, J. Training\nvery deep networks. In Advances in neural information\nprocessing systems (NeurIPS), pp. 2377–2385, 2015.\nSzegedy, C., Liu, W., Jia, Y ., Sermanet, P., Reed, S.,\nAnguelov, D., Erhan, D., Vanhoucke, V ., and Rabi-\nnovich, A. Going deeper with convolutions. In Pro-\nceedings of the IEEE conference on computer vision and\npattern recognition, pp. 1–9, 2015.\nTian, Y . An analytical formula of population gradi-\nent for two-layered ReLU network and its applica-\ntions in convergence and critical point analysis. In In-\nternational Conference on Machine Learning (ICML) .\nhttp://arxiv.org/abs/1703.00560, 2017.\nYang, W. Classiﬁcation on CIFAR-10/100 and Ima-\ngeNet with PyTorch, 2018. URL https://github.\ncom/bearpaw/pytorch-classification. Ac-\ncessed: 2018-04.\nZagoruyko, S. and Komodakis, N. Wide residual networks.\narXiv preprint arXiv:1605.07146, 2016.\nZhang, C., Bengio, S., Hardt, M., Recht, B., and Vinyals,\nO. Understanding deep learning requires rethinking gen-\neralization. In International Conference on Learning\nRepresentations (ICLR), 2017.\nZhang, J., Lin, Y ., Song, Z., and Dhillon, I. S. Learning\nlong term dependencies via Fourier recurrent units. In\nInternational Conference on Machine Learning (ICML).\narXiv preprint arXiv:1803.06585, 2018.\nZhong, K., Song, Z., and Dhillon, I. S. Learning non-\noverlapping convolutional neural networks with multiple\nkernels. arXiv preprint arXiv:1711.03440, 2017a.\nZhong, K., Song, Z., Jain, P., Bartlett, P. L., and Dhillon,\nI. S. Recovery guarantees for one-hidden-layer neu-\nral networks. In International Conference on Machine\nLearning (ICML) . arXiv preprint arXiv:1706.03175,\n2017b.",
  "values": {
    "Non-maleficence": "No",
    "Deferral to humans": "No",
    "Autonomy (power to decide)": "No",
    "Interpretable (to users)": "No",
    "Respect for Law and public interest": "No",
    "Justice": "No",
    "Collective influence": "No",
    "Fairness": "No",
    "Privacy": "No",
    "Transparent (to users)": "No",
    "Explicability": "No",
    "User influence": "No",
    "Critiqability": "No",
    "Respect for Persons": "No",
    "Beneficence": "No",
    "Not socially biased": "No"
  }
}