{
  "pdf": "arora18b",
  "title": "icml_main.1.1",
  "author": "Sanjeev Arora, Rong Ge, Behnam Neyshabur, Yi Zhang",
  "paper_id": "arora18b",
  "text": "Stronger Generalization Bounds for Deep Nets via a Compression Approach\nSanjeev Arora 1 Rong Ge 2 Behnam Neyshabur 3 Yi Zhang 1\nAbstract\nDeep nets generalize well despite having more\nparameters than the number of training samples.\nRecent works try to give an explanation using\nPAC-Bayes and Margin-based analyses, but do\nnot as yet result in sample complexity bounds\nbetter than naive parameter counting. The cur-\nrent paper shows generalization bounds that are\norders of magnitude better in practice. These\nrely upon new succinct reparametrizations of the\ntrained net — a compression that is explicit and\nefﬁcient. These yield generalization bounds via a\nsimple compression-based framework introduced\nhere. Our results also provide some theoretical\njustiﬁcation for widespread empirical success in\ncompressing deep nets. Analysis of correctness\nof our compression relies upon some newly iden-\ntiﬁed “noise stability”properties of trained deep\nnets, which are also experimentally veriﬁed. The\nstudy of these properties and resulting general-\nization bounds are also extended to convolutional\nnets, which had eluded earlier attempts on proving\ngeneralization.\n1. Introduction\nA mystery about deep nets is that they generalize (i.e., pre-\ndict well on unseen data) despite having far more parameters\nthan the number of training samples. One commonly voiced\nexplanation is that regularization during training –whether\nimplicit via use of SGD (Neyshabur et al., 2015c; Hardt\net al., 2016) or explicit via weight decay, dropout (Srivas-\ntava et al., 2014), batch normalization (Ioffe and Szegedy,\n2015), etc. –reduces the effective capacity of the net. But\nZhang et al. (2017) questioned this received wisdom and\nAuthors listed in alphabetical order 1Princeton University, Com-\nputer Science Department 2Duke University, Computer Science\nDepartment 3Institute for Advanced Study, School of Mathe-\nmatics. Correspondence to: Rong Ge <rongge@cs.duke.edu>,\nBehnam Neyshabur <bneyshabur@ias.edu>, Yi Zhang\n<y.zhang@cs.princeton.edu>.\nProceedings of the 35th International Conference on Machine\nLearning, Stockholm, Sweden, PMLR 80, 2018. Copyright 2018\nby the author(s).\nfueled research in this area by showing experimentally that\nstandard architectures using SGD and regularization can\nstill reach low training error on randomly labeled examples\n(which clearly won’t generalize).\nClearly, deep nets trained on real-life data have some proper-\nties that reduce effective capacity, but identifying them has\nproved difﬁcult —at least in a quantitative way that yields\nsample size upper bounds similar to classical analyses in\nsimpler models such as SVMs (Bartlett and Mendelson,\n2002; Evgeniou et al., 2000; Smola et al., 1998) or matrix\nfactorization (Fazel et al., 2001; Srebro et al., 2005).\nQualitatively (Hochreiter and Schmidhuber, 1997; Hinton\nand Van Camp, 1993) suggested that nets that generalize\nwell are ﬂat minima in the optimization landscape of the\ntraining loss. Recently Keskar et al. (2016) show using\nexperiments with different batch-sizes that sharp minima\ndo correlate with higher generalization error. A quanti-\ntative version of “ﬂatness” was suggested in (Langford\nand Caruana, 2001): the net’s output is stable to noise\nadded to the net’s trainable parameters. Using PAC-Bayes\nbound (McAllester, 1998; 1999) this noise stability yielded\ngeneralization bounds for fully connected nets of depth 2.\nThe theory has been extended to multilayer fully connected\nnets (Neyshabur et al., 2017b), although thus far yields sam-\nple complexity bounds much worse than naive parameter\ncounting. (Same holds for the earlier Bartlett and Mendel-\nson (2002); Neyshabur et al. (2015b); Bartlett et al. (2017);\nNeyshabur et al. (2017a); Golowich et al. (2017); see Fig-\nure 3). Another notion of noise stability —closely related to\ndropout and batch normalization—is stability of the output\nwith respect to the noise injected at the nodes of the network,\nwhich was recently shown experimentally (Morcos et al.,\n2018) to improve in tandem with generalization ability dur-\ning training, and to be absent in nets trained on random data.\nChaudhari et al. (2016) suggest adding noise to gradient\ndescent to bias it towards ﬁnding ﬂat minima.\nWhile study of generalization may appear a bit academic —\nheld-out data easily establishes generalization in practice—\nthe ultimate hope is that it will help identify simple, measur-\nable and intuitive properties of well-trained deep nets, which\nin turn may fuel superior architectures and faster training.\nWe hope the detailed study —theoretical and empirical—in\nthe current paper advances this goal.\nStronger Generalization Bounds for Deep Nets via a Compression Approach\nContributions of this paper.\n1. A simple compression framework (Section 2) for prov-\ning generalization bounds, perhaps a more explicit and\nintuitive form of the PAC-Bayes work. It also yields\nelementary short proofs of recent generalization re-\nsults (Section 2.2).\n2. Identifying new form of noise-stability for deep nets:\nthe stability of each layer’s computation to noise in-\njected at lower layers. (Earlier papers worked only\nwith stability of the output layer.) Figure 1 visualizes\nthe stability of network w.r.t. Gaussian injected noise.\nFormal statements require a string of other properties\n(Section 3). All are empirically studied, including their\ncorrelation with generalization (Section 6).\n3. Using the above properties to derive efﬁcient and prov-\nably correct algorithms that reduce the effective num-\nber of parameters in the nets, yielding generalization\nbounds that: (a) are better than naive parameter count-\ning (Section 6) (b) depend on simple, intuitive and\nmeasurable properties of the network (Section 4) (c)\napply also to convolutional nets (Section 5) (d) empiri-\ncally correlate with generalization (Section 6).\nThe main idea is to show that noise stability allows individ-\nual layers to be compressed via a linear-algebraic procedure\nAlgorithm 1. This results in new error in the output of the\nlayer. This added error is “Gaussian-like” and tends to get\nattenuated as it propagates to higher layers.\nFigure 1. Attenuation of injected noise on a VGG-19 net trained\non CIFAR-10. The x-axis is the index of layers and y-axis denote\nthe relative error due to the noise (∥ˆxi−xi∥/∥xi∥). A curve starts\nat the layer where a scaled Gaussian noise is injected to its input,\nwhoseℓ2 norm is set to 10% of the norm of its original input. As\nit propagates up, the injected noise has rapidly decreasing effect\non higher layers. This property is shown to imply compressibility.\nOther related works. Dziugaite and Roy (2017) use non-\nconvex optimization to optimize the PAC-Bayes bound and\nget a non-vacuous sample bound on MNIST. While very\ncreative, this provides little insight into favorable properties\nof networks. Liang et al. (2017) have suggested Fisher-\nRao metric, a regularization based on the Fisher matrix and\nshowed that this metric correlate with generalization. Un-\nfortunately, they could only apply their method to linear\nnetworks. Recently Kawaguchi et al. (2017) connects Path-\nNorm (Neyshabur et al., 2015a) to generalization. However,\nthe proved generalization bound depends on the distribution\nand measuring it requires vector operations on exponentially\nhigh dimensional vectors. Other works have designed exper-\niments to empirically evaluate potential properties of the net-\nwork that helps generalization(Arpit et al., 2017; Neyshabur\net al., 2017b; Dinh et al., 2017). The idea of compressing\ntrained deep nets is very popular for low-power applications;\nfor a survey see Cheng et al. (2018).\nFinally, note that the terms compression and stability are\ntraditionally used in a different sense in generalization the-\nory (Littlestone and Warmuth, 1986; Kearns and Ron, 1999;\nShalev-Shwartz et al., 2010). Our framework is compared\nto other notions in the remarks after Theorem 2.1.\nNotation: We use standard formalization of multiclass clas-\nsiﬁcation, where data consists of sample x and its label y\n(an integer from1 to k). A multiclass classiﬁer f maps input\nx to f(x)∈ Rk and the maximum coordinate of f(x) is\nthe predicted label. The classiﬁcation loss for any distribu-\ntionD is deﬁned as P(x,y)∼D [f(x)[y] < maxi̸=y f(x)[j]]\nwhere f(x)[y] is the y-th coordinate of f(x). If γ > 0 is\nsome desired margin, then the expected margin loss is\nLγ(f) = P(x,y)∼D\n[\nf(x)[y]≤ γ + max\ni̸=y\nf(x)[j]\n]\n(Notice, the classiﬁcation loss corresponds to γ = 0.) Let\nˆLγ denote empirical estimate of the margin loss. General-\nization error is the difference between the two.\nFor most of the paper we assume that deep nets have fully\nconnected layers, and use ReLU activations. We treat con-\nvolutional nets in Section 5. If the net has d layers, we\nlabel the vector before activation at these layers by x0, x1,\nxd for the d layers where x0 is the input to the net, also\ndenoted simply x. So xi = Aiφ(xi−1) where Ai is the\nweight matrix of the ith layer. (Here φ(x) if x is a vector\napplies the ReLU component-wise. The ReLU is allowed a\ntrainable bias parameter, which is omitted from the notation\nbecause it has no effect on any calculations below.) We\ndenote the number of hidden units in layer i by hi and set\nh = maxd\ni=1 hi. Let fA(x) be the function calculated by\nthe above network.\nStable rank of a matrix B is∥B∥2\nF /∥B∥2\n2, where∥·∥ F\ndenotes Frobenius norm and∥·∥ 2 denotes spectral norm.\nNote that stable rank is at most (linear algebraic) rank.\nFor any two layer i≤ j, denote by Mi,j the operator for\ncomposition of these layers and Ji,j\nx be the Jacobian of\nthis operator at input x (a matrix whose p, q is the partial\nderivative of the pth output coordinate with respect to the\nq’th input input). Therefore, we have xj = Mi,j(xi). Fur-\nthermore, since the activation functions are ReLU, we have\nStronger Generalization Bounds for Deep Nets via a Compression Approach\nMi,j(xi) = Ji,j\nxi xi.\n2. Compression and Generalization\nOur compression framework rests on the following obvious\nfact. Suppose the training data contains m samples, and f\nis a classiﬁer from a complicated class (e.g., deep nets with\nmuch more than m parameters) that incurs very low empir-\nical loss. We are trying to understand if it will generalize.\nNow suppose we can compute a classiﬁer g with discrete\ntrainable parameters much less than m and which incurs\nsimilar loss on the training data as f. Then g must incur\nlow classiﬁcation error on the full distribution. This frame-\nwork has the advantage of staying with intuitive parameter\ncounting and to avoid explicitly dealing with the hypothesis\nclass that includes f (see note after Theorem 2.1). Notice,\nthe mapping from f to g merely needs to exist, not to be\nefﬁciently computable. But in all our examples the map-\nping will be explicit and fairly efﬁcient. Now we formalize\nthe notions. The proofs are elementary via concentration\nbounds and appear in the appendix.\nDeﬁnition 1 ((γ,S)-compressible). For any setA of param-\neter values, let f be a classiﬁer and GA ={gA|A∈A} be\na class of classiﬁers. We say f is (γ, S)-compressible via\nGA if there exists A∈A such that for any x∈ S, we have\nfor all y\n|f(x)[y]− gA(x)[y]|≤ γ.\nWe also consider a different setting where the compression\nalgorithm is allowed a“helper string” s, which is arbitrary\nbut ﬁxed before looking at the training samples. Often s\nwill contain random numbers. A simple example is to let s\nbe the random initialization used for training the deep net\nand then compress the difference between the ﬁnal weights\nand s. This can give better generalization bounds, similar\nto (Dziugaite and Roy, 2017). Other nontrivial examples\nappear later.\nDeﬁnition 2 ((γ,S)-compressible using helper string s).\nSuppose GA,s ={gA,s|A∈A} is a class of classiﬁers\nindexed by trainable parameters A and ﬁxed strings s. A\nclassiﬁer f is (γ, S)-compressible with respect to GA,s us-\ning helper string s if there exists A∈A such that for any\nx∈ S, we have for all y\n|f(x)[y]− gA,s(x)[y]|≤ γ.\nTheorem 2.1. Suppose GA,s ={gA,s|A∈A} where A is\na set of q parameters each of which can have at most r dis-\ncrete values and s is a helper string. Let S be a training set\nwith m samples. For any margin γ > 0, if the trained clas-\nsiﬁer f is (γ, S)-compressible via GA,s with helper string\ns, then there exists A∈A such that with high probability\nover the training set,\nL0(gA)≤ ˆLγ(f) + O\n(√\nq log r\nm\n)\n.\nRemarks: (1) The framework proves the generalization not\nof f but of its compression gA. (An exception is if the two\nare shown to have similar loss at every point in the domain,\nnot just the training set. This is the case in Theorem 2.2.)\n(2) The previous item highlights how our framework steps\naway from uniform convergence framework, e.g., covering\nnumber arguments (Dudley, 2010; Anthony and Bartlett,\n2009). There, one needs to ﬁx a hypothesis class indepen-\ndent of the training set. By contrast we have no hypothesis\nclass, only a single neural net that has some speciﬁc prop-\nerties (described in Section 3) on a single ﬁnite training\nset. But if we can compress this speciﬁc neural net to a\nsimpler neural nets with fewer parameters then we can use\ncovering number argument on this simpler class to get the\ngeneralization of the compressed net.\n(3) Issue (1) exists also in standard PAC-Bayes framework\nfor deep nets (see tongue-in-cheek title of Langford and\nCaruana (2001)). They yield generalization bounds not for\nf but for a noised version of f (i.e., net given by W + η,\nwhere W is parameter vector of f and η is a noise vector).\n(4) As we will see later, our compression which is achieved\nvia a randomized algorithm seems “non-destructive” and\nshould not overﬁt to the training set more than the original\nnetwork. Moreover, for us issue (1) could be ﬁxed by show-\ning that if f satisﬁes the properties of Section 3 on training\ndata then it satisﬁes them on the entire domain. This is left\nfor future work.\n2.1. Example 1: Linear classiﬁers with margin\nTo illustrate the above compression method and its connec-\ntion to noise stability, we use linear classiﬁers with high\nmargins. Let c∈ Rh(∥c∥ = 1) be a classiﬁer for binary\nclassiﬁcation whose output on input x is sgn(c· x). Let\nD be a distribution on inputs (x, y) where∥x∥ = 1 and\ny∈{± 1}. Say c has margin γ if for all (x, y) in the train-\ning set we have y(c⊤x)≥ γ.\nIf we add Gaussian noise vector η with coordinate-wise\nvariance σ2 to c, then E[x· (c + η)] is c· x and the variance\nis σ2. (A similar analysis applies to noising of x instead of\nc.) Thus the margin is large if and only if the classiﬁer’s\noutput is somewhat noise-stable.\nA classiﬁer with margin γ can be compressed to one that has\nonly O(1/γ2) non-zero entries. For each coordinate i, toss a\ncoin with Pr[heads] = 8c2\ni /γ2 and if it comes up heads set\nthe coordinate to equal toγ2/8ci (see Algorithm 2 in supple-\nmentary material). This yields a vector ˆc with only O(1/γ2)\nnonzero entries such that for any vector u, with reasonable\nStronger Generalization Bounds for Deep Nets via a Compression Approach\nprobability|ˆc⊤u− c⊤u|≤ γ, so ˆc and c will make the same\nprediction. We can then apply Theorem 2.1 on a discretized\nversion of ˆc to show that the sparsiﬁed classiﬁer has good\ngeneralization with O(log d/γ2) samples.\nThis compressed classiﬁer works correctly for a ﬁxed input\nx with good probability but not high probability. To ﬁx\nthis, one can recourse to the “compression with ﬁxed string”\nmodel. The ﬁxed string is a random linear transformation.\nWhen applied to unit vector x, it tends to equalize all coor-\ndinates and the guarantee|ˆc⊤u− c⊤u|≤ γ can hold with\nhigh probability. This random linear transformation can be\nﬁxed before seeing the training data. See Section A.2 in\nsupplementary material for details.\n2.2. Example 2: Existing generalization bounds\nOur compression framework gives easy and short proof of\nthe generalization bounds of a recent paper; see appendix\nfor slightly stronger result of Bartlett et al. (2017).\nTheorem 2.2. ((Neyshabur et al., 2017a)) For any deep\nnet with layers A1, A2, . . . Ad and output margin γ on a\ntraining set S, the generalization error can be bounded by\n˜O\n\n\n√ hd2 maxx∈S∥x∥ ∏d\ni=1∥Ai∥2\n2\n∑d\ni=1\n∥Ai∥2\nF\n∥Ai∥2\n2\nγ2m\n\n .\nThe second part of this expression (∑d\ni=1\n∥Ai∥2\nF\n∥Ai∥2\n2\n) is sum of\nstable ranks of the layers, a natural measure of their true\nparameter count. The ﬁrst part ( ∏d\ni=1∥Ai∥2\n2) is related to\nthe Lipschitz constant of the network, namely, the maximum\nnorm of the vector it can produce if the input is a unit vector.\nThe Lipschitz constant of a matrix operator B is just its\nspectral norm∥B∥2. Since the network applies a sequence\nof matrix operations interspersed with ReLU, and ReLU is\n1-Lipschitz we conclude that the Lipschitz constant of the\nfull network is at most ∏d\ni=1∥Ai∥2.\nTo prove Theorem 2.2 we use the following lemma to com-\npress the matrix at each layer to a matrix of smaller rank.\nSince a matrix of rank r can be expressed as the product of\ntwo matrices of inner dimension r, it has 2hr parameters\n(instead of the trivial h2). (Furthermore, the parameters can\nbe discretized via trivial rounding to get a compression with\ndiscrete parameters as needed by Deﬁnition 1.)\nLemma 1. For any matrix A∈ Rm×n, let ˆA be the trun-\ncated version of A where singular values that are smaller\nthan δ∥A∥2 are removed. Then∥ ˆA− A∥2≤ δ∥A∥2 and ˆA\nhas rank at most∥A∥2\nF /(δ2∥A∥2\n2).\nProof. Let r be the rank of ˆA. By construction, the max-\nimum singular value of ˆA− A is at most δ∥A∥2. Since\nthe remaining singular values are at least δ∥A∥2, we have\n∥A∥F≥∥ ˆA∥F≥√rδ∥A∥2.\nFor each i replace layer i by its compression using the above\nlemma, with δ = γ(3∥x∥d ∏d\ni=1∥Ai∥2)−1. How much\nerror does this introduce at each layer and how much does\nit affect the output after passing through the intermediate\nlayers (and getting magniﬁed by their Lipschitz constants)?\nSince A− ˆAi has spectral norm (i.e., Lipschitz constant)\nat most δ, the error at the output due to changing layer i in\nisolation is at most δ∥xi∥ ∏d\nj=1∥Aj∥2≤ γ/3d.\nA simple induction (see (Neyshabur et al., 2017a) if needed)\ncan now show the total error incurred in all layers is bounded\nby γ. The generalization bound follows immediately from\nTheorem 2.1.\n3. Noise Stability Properties of Deep Nets\nThis section introduces noise stability properties of deep\nnets that imply better compression (and hence generaliza-\ntion). They help overcome the pessimistic error analysis of\nour proof of Theorem 2.2: when a layer was compressed,\nthe resulting error was assumed to blow up in a worst-case\nmanner according to the Lipschitz constant (namely, prod-\nuct of spectral norms of layers). This hurt the amount of\ncompression achievable. The new noise stability properties\nroughly amount to saying that noise injected at a layer has\nvery little effect on the higher layers. Our formalization\nstarts with noise sensitivity, which captures how an operator\ntransmits noise vs signal.\nDeﬁnition 3. If M is a mapping from real-valued vectors to\nreal-valued vectors, andN is some noise distribution then\nnoise sensitivity of M at x with respect toN , is\nψN (M, x) = Eη∈N\n[∥M(x + η∥x∥)− M(x)∥2\n∥M(x)∥2\n]\n,\nThe noise sensitivity of M with respect toN on a set of\ninputs S, denoted ψN,S(M), is the maximum of ψN (M, x)\nover all inputs x in S.\nTo illustrate, we examine noise sensitivity of a matrix (i.e.,\nlinear mapping) with respect to Gaussian distribution. Low\nsensitivity turns out to imply that the matrix has some large\nsingular values (i.e., low stable rank), which give directions\nthat can preferentially carry the “signal”x whereas noise η\nattenuates because it distributes uniformly across directions.\nProposition 3.1. The noise sensitivity of a matrix M at any\nvector x̸= 0 with respect to Gaussian distributionN (0, I)\nis exactly∥M∥2\nF∥x∥2/∥M x∥2, and at least its stable rank.\nStronger Generalization Bounds for Deep Nets via a Compression Approach\nProof. Using E[ηη⊤] = I, we bound the numerator by\nEη[∥M (x +η∥x∥)−Mx∥2] = Eη[∥x∥2∥Mη∥2]\n= Eη[∥x∥2tr(Mηη ⊤M ⊤)] =∥x∥2tr(MM ⊤) =∥M∥2\nF∥x∥2.\nThus noise sensitivityψ at x is∥M∥2\nF∥x∥2/∥M x∥2, which\nis at least the stable rank ∥M∥2\nF /∥M∥2\n2 since∥M x∥ ≤\n∥M∥2∥x∥.\nThe above proposition suggests that if a vector x is aligned\nto a matrix M (i.e. correlated with high singular directions\nof M), then matrix M becomes less sensitive to noise at x.\nThis intuition will be helpful in understanding the properties\nwe deﬁne later to formalize noise stability.\nThe above discussion motivates the following approach.\nWe compress each layer i by an appropriate randomized\ncompression algorithm, such that the noise/error in its output\nis “Gaussian-like”. If layers i + 1 and higher have low\nsensitivity to this new noise, then the compression can be\nmore extreme produce much higher noise. We formalize\nthis idea using Jacobian Ji,j, which describes instantaneous\nchange of Mi,j(x) under inﬁnitesimal perturbation of x.\n3.1. Formalizing Error-resilience\nNow we formalize the error-resilience properties. Section 6\nreports empirical ﬁndings about these properties. The ﬁrst\nis cushion, to be thought of roughly as reciprocal of noise\nsensitivity. We ﬁrst formalize it for single layer.\nDeﬁnition 4 (layer cushion). The layer cushion of layer i\nis similarly deﬁned to be the largest number µi such that for\nany x∈ S, µi∥Ai∥F∥φ(xi−1)∥≤∥ Aiφ(xi−1)∥.\nIntuitively, cushion considers how much smaller the\noutput Aiφ(xi−1) is compared to the upper bound\n∥Ai∥F∥φ(xi−1)∥. Using argument similar to Proposi-\ntion 3.1, we can see that1/µ2\ni is equal to the noise sensitivity\nof matrix Ai at input φ(xi−1) with respect to Gaussian noise\nη∼N (0, I).\nOf course, for nonlinear operators the deﬁnition of error\nresilience is less clean. Let’s denote by Mi,j : Rhi\n→ Rhj\nthe operator corresponding to the portion of the deep net\nfrom layer i to layer j, and by Ji,j its Jacobian. If inﬁnitesi-\nmal noise is injected before level i then Mi,j passes it like\nJi,j, a linear operator. When the noise is small but not in-\nﬁnitesimal then one hopes that Mi,j still behaves roughly\nlinearly (recall that ReLU nets are piecewise linear). To\nformalize this, we deﬁne Interlayer Cushion (Deﬁnition 5)\nthat captures the local linear approximation of the operator\nM.\nDeﬁnition 5 (Interlayer Cushion). For any two layersi≤ j,\nwe deﬁne the interlayer cushion µi,j as the largest number\nsuch that for any x∈ S:\nµi,j∥Ji,j\nxi∥F∥xi∥≤∥ Ji,j\nxi xi∥\nFurthermore, for any layer i we deﬁne the mini-\nmal interlayer cushion as µi→ = min i≤j≤d µi,j =\nmin{1/\n√\nhi, mini<j≤d µi,j}1.\nSince Ji,j\nx is a linear transformation, a calculation similar\nto Proposition 3.1 shows that its noise sensitivity at xi with\nrespect to Gaussian distributionN (0, I) is at most 1\nµ2\nij\n.\nThe next property quantiﬁes the intuitive observation on the\nlearned networks that for any training data, almost half of\nthe ReLU activations at each layer are active. If the input to\nthe activations is well-distributed and the activations do not\ncorrelate with the magnitude of the input, then one would\nexpect that on average, the effect of applying activations at\nany layer is to decrease the norm of the pre-activation vector\nby at most some small constant factor.\nDeﬁnition 6 (Activation Contraction). The activation con-\ntraction c is deﬁned as the smallest number such that for any\nlayer i and any x∈ S,\n∥φ(xi)∥≥∥ xi∥/c.\nWe discussed how the interlayer cushion captures noise-\nresilience of the network if behaves linearly, namely, when\nthe set of activated ReLU gates does not change upon in-\njecting noise. In general the activations do change, but the\ndeviation from linear behavior is bounded for small noise\nvectors, as quantiﬁed next.\nDeﬁnition 7 (Interlayer Smoothness). Let η be the noise\ngenerated as a result of substituting weights in some of\nthe layers before layer i using Algorithm 1. We deﬁne\ninterlayer smoothness ρδ to be the largest number such that\nwith probability 1− δ over noise η for any two layers i < j\nany x∈ S:\n∥Mi,j(xi + η)− Ji,j\nxi (xi + η)∥≤ ∥η∥∥xj∥\nρδ∥xi∥ .\nFor a single layer,ρδ captures the ratio of input/weight align-\nment to noise/weight alignment. Since the noise behaves\nsimilar to Gaussian, one expects this number to be greater\nthan one for a single layer. When j > i + 1, the weights\nand activations create more dependencies. However, since\nthese dependences are applied on both noise and input, we\nagain expect that if the input is more aligned to the weights\nthan noise, this should not change in higher layers. In Sec-\ntion 6, we show that the interlayer smoothness is indeed\ngood: 1/ρδ is a small constant. Please see Appendix A.4\nfor a more detailed discussion on interlayer smoothness.\n4. Fully Connected Networks\nWe prove generalization bounds using for fully connected\nmultilayer nets. Details appear in Appendix Section B.\n1Note thatJi,i\nxi =I andµi,i = 1/\n√\nhi\nStronger Generalization Bounds for Deep Nets via a Compression Approach\nTheorem 4.1. For any fully connected network fA with\nρδ ≥ 3d, any probability 0 < δ ≤ 1 and any margin γ,\nAlgorithm 1 generates weights ˜A for the network f ˜A such\nthat with probability 1− δ over the training set and f ˜A, the\nexpected error L0(f ˜A) is bounded by\nˆLγ(fA) + ˜O\n\n\n√ c2d2 maxx∈S∥fA(x)∥2\n2\n∑d\ni=1\n1\nµ2\niµ2\ni→\nγ2m\n\n\nwhere µi, µi→, c and ρδ are layer cushion, interlayer cush-\nion, activation contraction and interlayer smoothness de-\nﬁned in Deﬁnitions 4,5,6 and 7 respectively.\nTo prove this we describe a compression of the net with re-\nspect to a ﬁxed (random) string. In contrast to the determinis-\ntic compression of Lemma 1, this randomized compression\nensures that the resulting error in the output behaves like a\nGaussian. The proofs are similar to standard JL dimension\nreduction.\nAlgorithm 1 Matrix-Project (A, ε, η)\nRequire: Layer matrix A∈ Rh1×h2, error parameter ε, η.\nEnsure: Returns ˆA s.t.∀ ﬁxed vectors u, v,\nPr[|u⊤ ˆAv− u⊤Av∥≥ ε∥A∥F∥u∥∥v∥]≤ η.\nSample k = log(1/η)/ε2 random matrices M1, . . . , Mk\nwith entries i.i.d.±1 (“helper string”)\nfor k′ = 1 to k do\nLet Zk′ =⟨A, Mk′⟩Mk′.\nend for\nLet ˆA = 1\nk\n∑k\nk′=1 Zk′\nNote that the helper string of random matrices Mi’s were\nchosen and ﬁxed before training set S was picked. Each\nweight matrix is thus represented as only k real numbers\n⟨A, Mi⟩ for i = 1, 2, ..., k.\nLemma 2. For any0 < δ, ε≤ 1, let G ={(Ui, xi)}m\ni=1 be\na set of matrix/vector pairs of sizem where U∈ Rn×h1 and\nx∈ Rh2, let ˆA∈ Rh1×h2 be the output of Algorithm 1 with\nη = δ/mn and ∆ = ˆA− A. With probability at least 1− δ\nwe have for any (U, x)∈ G,∥U∆x∥≤ ε∥A∥F∥U∥F∥x∥.\nNext Lemma bounds the number of parameters of the com-\npressed network resulting from applying Algorithm 1 to all\nthe layer matrices of the net. The proof does induction on\nthe layers and bounds the effect of the error on the output of\nthe network using properties deﬁned in Section 3.1.\nLemma 3. For any fully connected network fA with ρδ≥\n3d, any probability 0 < δ ≤ 1 and any error 0 < ε ≤\n1, Algorithm 1 generates weights ˜A for a network with\n72c2d2 log(mdh/δ)\nε2 · ∑d\ni=1\n1\nµ2\niµ2\ni→\ntotal parameters such that\nwith probability 1− δ/2 over the generated weights ˜A, for\nany x∈ S:\n∥fA(x)− f ˜A(x)∥≤ ε∥fA(x)∥.\nwhere µi, µi→, c and ρδ are layer cushion, interlayer cush-\nion, activation contraction and interlayer smoothness de-\nﬁned in Deﬁnitions 4,5,6 and 7 respectively.\nSome obvious improvements: (i) Empirically it has been\nobserved that deep net training introduces fairly small\nchanges to parameters as compared to the (random) ini-\ntial weights (Dziugaite and Roy, 2017). We can exploit this\nby incorporating the random initial weights into the helper\nstring and do the entire proof above not with the layer matri-\nces Ai but only the difference from the initial starting point.\nExperiments in Section 6 show this improves the bounds.\n(ii) Cushions and other quantities deﬁned earlier are data-\ndependent, and required to hold for the entire training set.\nHowever, the proofs go through if we remove say ζ fraction\nof outliers that violate the deﬁnitions; this allows us to use\nmore favorable values for cushion etc. and lose an additive\nfactor ζ in the generalization error.\n5. Convolutional Neural Networks\nNow we sketch how to provably compress convolutional\nnets. (Details appear in Section C of supplementary.) In-\ntuitively, this feels harder because the weights are already\ncompressed— they’re shared across patches!\nTheorem 5.1. For any convolutional neural network fA\nwith ρδ≥ 3d, any probability 0 < δ≤ 1 and any margin γ,\nAlgorithm 4 generates weights ˜A for the network f ˜A such\nthat with probability 1− δ over the training set and f ˜A:\nL0(f ˜A)≤ ˆLγ(fA)\n+ ˜O\n\n\n√ c2d2 maxx∈S∥fA(x)∥2\n2\n∑d\ni=1\nβ2(⌈κi/si⌉)2\nµ2\niµ2\ni→\nγ2m\n\n\nwhere µi, µi→, c, ρδ and β are layer cushion, interlayer\ncushion, activation contraction, interlayer smoothness and\nwell-distributed Jacobian deﬁned in Deﬁnitions 4,8,6, 7 and\n9 respectively. Furthermore, si and κi are stride and ﬁlter\nwidth in layer i.\nLet’s realize that obvious extensions of earlier sections fail.\nSuppose layer i of the neural network is an image of dimen-\nsion ni\n1× ni\n2 and each pixel has hi channels, the size of the\nﬁlter at layer i is κi× κi with stride si. The convolutional\nﬁlter has dimension hi−1× hi× κi× κi. Applying ma-\ntrix compression (Algorithm 1) independently to each copy\nof a convolutional ﬁlter makes number of new parameters\nproportional to ni\n1ni\n2, a big blowup.\nStronger Generalization Bounds for Deep Nets via a Compression Approach\nCompressing a convolutional ﬁlter once and reusing it in\nall patches doesn’t work because the interlayer analysis im-\nplicitly requires the noise generated by the compression\nbehave similar to a spherical Gaussian, but the shared ﬁl-\nters introduce correlations. Quantitatively, using the fully\nconnected analysis would require the error to be less than\ninterlayer cushion value µi→ (Deﬁnition 5) which is at most\n1/\n√\nhini\n1ni\n2, and this can never be achieved from compress-\ning matrices that are far smaller than ni\n1× ni\n2 to begin with.\nWe end up with a solution in between fully independent\nand fully dependent: p-wise independence. The algorithm\ngenerates p-wise independent compressed ﬁlters ˆA(a,b) for\neach convolution location (a, b)∈ [ni\n1]× [ni\n2]. It results in\np times more parameters than a single compression. If p\ngrows logarithmically with relevant parameters, the ﬁlters\nbehave like fully independent ﬁlters. Using this idea we\ncan generalize the deﬁnition of interlayer margin to the\nconvolution setting:\nDeﬁnition 8 (Interlayer Cushion, Convolution Setting). For\nany two layers i≤ j, we deﬁne the interlayer cushion µi,j\nas the largest number such that for any x∈ S:\nµi,j· 1√\nni\n1ni\n2\n∥Ji,j\nxi∥F∥xi∥≤∥ Ji,j\nxi xi∥\nFurthermore, for any layer i we deﬁne the mini-\nmal interlayer cushion as µi→ = min i≤j≤d µi,j =\nmin{1/\n√\nhi, mini<j≤d µi,j}2.\nRecall that interlayer cushion is related to the noise sen-\nsitivity of Ji,j\nxi at xi with respect to Gaussian distribu-\ntionN (0, I). When we consider Ji,j\nxi applied to a noise\nη, if different pixels in η are independent Gaussian ran-\ndom variables, then we can indeed expect ∥Ji,j\nxi η∥ ≈\n1√\nhini\n1ni\n2\n∥Ji,j\nxi∥∥η∥, which explains the extra 1√\nni\n1ni\n2\nfac-\ntor in Deﬁnition 8 compared to Deﬁnition 5. The proof also\nneeds to assume —in line with intuition behind convolution\narchitecture— that information from the entire image ﬁeld\nis incorporated somewhat uniformly across pixels. It is for-\nmalized using the Jacobian which gives the partial derivative\nof the output with respect to pixels at previous layer.\nDeﬁnition 9 (Well-distributed Jacobian) . Let Ji,j\nx be\nthe Jacobian of Mi,j at x, we know Ji,j\nx ∈\nRhi×ni\n1×ni\n2×hj×nj\n1×nj\n2. We say the Jacobian is β well-\ndistributed if for any x∈ S, any i, j, any (a, b)∈ [ni\n1× ni\n2],\n∥[Ji,j\nx ]:,a,b,:,:,:∥F≤ β√\nni\n1ni\n2\n∥Ji,j\nx ∥F\n6. Empirical Evaluation\nWe study noise stability properties (deﬁned in Section 3)\nof an actual trained deep net, and compute a generalization\n2Note thatJi,i\nxi =I andµi,i = 1/\n√\nhi\nbound from Theorem 5.1. Experiments were performed\nby training a VGG-19 architecture (Simonyan and Zisser-\nman, 2014) and a AlexNet (Krizhevsky et al., 2012) for\nmulti-class classiﬁcation task on CIFAR-10 dataset. Opti-\nmization used SGD with mini-batch size 128, weight decay\n5e-4, momentum 0.9 and initial learning rate 0.05, but de-\ncayed by factor 2 every 30 epochs. Drop-out was used\nin fully-connected layers. We trained both networks for\n299 epochs and the ﬁnal VGG-19 network achieved 100%\ntraining and 92.45% validation accuracy while the AlexNet\nachieved 100% training and 77.22% validation accuracy. To\ninvestigate the effect of corrupted label, we trained another\nAlexNet, which 100% training and 9.86% validation accu-\nracy, on CIFAR-10 dataset with randomly shufﬂed labels.\nOur estimate of the sample complexity bound used exact\ncomputation of norms of weight matrices (or tensors) in\nall bounds(||A||1,∞,||A||1,2,||A||2,||A||F ). Like previous\nbounds in generalization theory, ours also depend upon nui-\nsance parameters like depth d, logarithm of h, etc. which\nprobably are an artifact of the proof. These are ignored in\nthe computation (also in computing earlier bounds) for sim-\nplicity. Even the generalization based on parameter counting\narguments does have an extra dependence on depth (Bartlett\net al., 2017). A recent work, (Golowich et al., 2017) showed\nthat many such depth dependencies can be improved.\n6.1. Empirical investigation of noise stability properties\nSection 3 identiﬁes four properties in the networks that con-\ntribute to noise-stability: layer cushion, interlayer cushion,\ncontraction, interlayer smoothness. Figure 2 plots the dis-\ntribution of over different data points in the training set and\ncompares to a Gaussian random network and then scaled\nproperly. The layer cushion, which quantiﬁes its noise sta-\nbility, is drastically improved during the training, especially\nfor the higher layers (8 and higher) where most parameters\nlive. Moreover, we observe that interlayer cushion, activa-\ntion contraction and interlayer smoothness behave nicely\neven after training. These plots suggest that the driver of\nthe generalization phenomenon is layer cushion. The other\nproperties are being maintained in the network and prevent\nthe network from falling prey to pessimistic assumptions\nthat causes the other older generalization bounds to be very\nhigh. The assumptions made in section 3 (also in B.1) are\nveriﬁed on the VGG-19 net in appendix D.1 by histogram-\nming the distribution of layer cushion, interlayer cushion,\ncontraction, interlayer smoothness, and well-distributedness\nof the Jacobians of each layer of the net on each data point\nin the training set. Some examples are shown in Figure 2.\n6.2. Correlation to generalization error\nWe evaluate our generalization bound during the training,\nsee Figure 3, Right. After 120 epochs, the training error is\nStronger Generalization Bounds for Deep Nets via a Compression Approach\n0.0 0.1 0.2 0.3\na) layer cushion µi\nrandom init\ntrained\n0.2 0.4 0.6\nb) minimal inter-layer cushion µi!\nrandom init\ntrained\n1.0 1.2 1.4\nc) contraction c\nrandom init\ntrained\n0.00 0.02 0.04 0.06\nd) interlayer smoothness 1/⇢\nrandom init\ntrained\nFigure 2. Distribution of a) layer cushion, b) (unclipped) mini-\nmal interlayer cushion, c) activation contraction and d) interlayer\nsmoothness of the 13-th layer of VGG-19 nets on on training set.\nThe distributions on a randomly-initialized and a trained net are\nshown in blue and orange. Note that after clipping, the minimal\ninterlayer cushion is set to 1/√hi for all layers except the ﬁrst one,\nsee appendix D.1.\nalmost zero but the test error continues to improve in later\nepochs. Our generalization bound continues to improve,\nthough not to the same level. Thus our generalization bound\ncaptures part of generalization phenomenon, not all. Still,\nthis suggests that SGD somehow improves our generaliza-\ntion measure implicitly. Making this rigorous is a good topic\nfor further research.\nFurthermore, we investigate effect of training with normal\ndata and corrupted data by training two AlexNets respec-\ntively on original and corrupted CIFAR-10 with randomly\nshufﬂed labels. We identify two key properties that differ\nsigniﬁcantly between the two networks: layer cushion and\nactivation contraction, see D.2. Since our bound predicts\nlarger cushion and lower contraction indicates better gen-\neralization, our bound is consistent w with the fact that the\nnet trained on normal data generalizes (77.22% validation\naccuracy).\n6.3. Comparison to other generalization bounds\nFigure 3 compares our proposed bound to other neural-net\ngeneralization bounds on the VGG-19 net and compares\nto naive VC dimension bound (which of course is too pes-\nsimistic). All previous generalization bounds are orders\nof magnitude worse than ours; the closest one is spectral\nnorms times average ℓ1,2 of the layers (Bartlett et al., 2017)\nwhich is still about 1018, far greater than VC dimension.\n(As mentioned we’re ignoring nuisance factors like depth\nVC-dim\n120 200 280\n0.075\n0.08\n0.085\n0.09\n0.095\nFigure 3. Left) Comparing neural net generalization bounds. See\nAppendix D.3 for details. Right) Comparing our bound to empiri-\ncal generalization error during training. Our bound is rescaled to\nbe within the same range as the generalization error.\nand log h which make the comparison to VC dimension a\nbit unfair, but the comparison to previous bounds is fair.)\nThis should not be surprising as all other bounds are based\non product of norms is pessimistic (see note at the start of\nSection 3) which we avoid due to the noise stability analysis\nresulting in a bound that has more dependence on the data.\nTable 1 shows the compressibility of various layers accord-\ning to the bounds given by our theorem. Again, this is a\nqualitative due to ignoring nuisance factors, but it gives an\nidea of which layers are important in the calculation.\nlayer c2\niβ2\ni⌈κi/si⌉2\nµ2\niµ2\ni→\nactual # param compression ( %)\n1 1644.87 1728 95.18\n4 644654.14 147456 437.18\n6 3457882.42 589824 586.25\n9 36920.60 1179648 3.129\n12 22735.09 2359296 0.963\n15 26583.81 2359296 1.126\n18 5052.15 262144 1.927\nTable 1. Effective number of parameters identiﬁed by our bound.\nCompression rates can be as low as 1% in later layers (from 9 to\n19) whereas earlier layers are not so compressible. Dependence on\ndepthd, log factors, constants are ignored as mentioned in the text.\n7. Conclusions\nWith a new compression-based approach, the paper has\nmade progress on several open issues regarding general-\nization properties of deep nets. The approach also adapts\nspecially to convolutional nets. The empirical veriﬁcation of\nthe theory in Section 6 shows a rich set of new properties sat-\nisﬁed by deep nets trained on realistic data, which we hope\nwill fuel further theory work on deep learning, including\nhow these properties play into optimization and expressiv-\nity. Another possibility is a more rigorous understanding of\ndeep net compression, which sees copious empirical work\nmotivated by low-power applications. Perhaps our p-wise in-\ndependence idea used for compressing convnets (Section 5)\nhas practical implications.\nStronger Generalization Bounds for Deep Nets via a Compression Approach\nAcknowledgments\nThis research was done with support from NSF, ONR, Darpa,\nSRC, Simons Foundation, Mozilla Research, and Schmidt\nFoundation.\nReferences\nMartin Anthony and Peter L Bartlett. Neural network learn-\ning: Theoretical foundations. cambridge university press,\n2009.\nDevansh Arpit, Stanislaw Jastrzebski, Nicolas Ballas, David\nKrueger, Emmanuel Bengio, Maxinder S Kanwal, Tegan\nMaharaj, Asja Fischer, Aaron Courville, Yoshua Bengio,\net al. A closer look at memorization in deep networks.\narXiv preprint arXiv:1706.05394, 2017.\nPeter Bartlett, Dylan J Foster, and Matus Telgarsky.\nSpectrally-normalized margin bounds for neural net-\nworks. arXiv preprint arXiv:1706.08498, 2017.\nPeter L Bartlett and Shahar Mendelson. Rademacher and\ngaussian complexities: Risk bounds and structural results.\nJournal of Machine Learning Research, 3(Nov):463–482,\n2002.\nPratik Chaudhari, Anna Choromanska, Stefano Soatto, and\nYann LeCun. Entropy-sgd: Biasing gradient descent into\nwide valleys. arXiv preprint arXiv:1611.01838, 2016.\nYu Cheng, Duo Wang, Pan Zhou, and Tao Zhang. Model\ncompression and acceleration for deep neural networks:\nThe principles, progress, and challenges. IEEE Signal\nProc. Magazine, 35, Jan 2018.\nLaurent Dinh, Razvan Pascanu, Samy Bengio, and Yoshua\nBengio. Sharp minima can generalize for deep nets.arXiv\npreprint arXiv:1703.04933, 2017.\nRichard M Dudley. Universal donsker classes and metric\nentropy. In Selected Works of RM Dudley, pages 345–365.\nSpringer, 2010.\nGintare Karolina Dziugaite and Daniel M Roy. Computing\nnonvacuous generalization bounds for deep (stochastic)\nneural networks with many more parameters than training\ndata. arXiv preprint arXiv:1703.11008, 2017.\nTheodoros Evgeniou, Massimiliano Pontil, and Tomaso\nPoggio. Regularization networks and support vector ma-\nchines. Advances in computational mathematics, 13(1):1,\n2000.\nMaryam Fazel, Haitham Hindi, and Stephen P Boyd. A rank\nminimization heuristic with application to minimum order\nsystem approximation. In American Control Conference,\n2001. Proceedings of the 2001, volume 6, pages 4734–\n4739. IEEE, 2001.\nNoah Golowich, Alexander Rakhlin, and Ohad Shamir. Size-\nindependent sample complexity of neural networks.arXiv\npreprint arXiv:1712.06541, 2017.\nMoritz Hardt, Benjamin Recht, and Yoram Singer. Train\nfaster, generalize better: Stability of stochastic gradient\ndescent. In ICML, 2016.\nGeoffrey E Hinton and Drew Van Camp. Keeping the neu-\nral networks simple by minimizing the description length\nof the weights. In Proceedings of the sixth annual con-\nference on Computational learning theory, pages 5–13.\nACM, 1993.\nSepp Hochreiter and J ¨urgen Schmidhuber. Flat minima.\nNeural Computation, 9(1):1–42, 1997.\nSergey Ioffe and Christian Szegedy. Batch normalization:\nAccelerating deep network training by reducing internal\ncovariate shift. In ICML, 2015.\nKenji Kawaguchi, Leslie Pack Kaelbling, and Yoshua Ben-\ngio. Generalization in deep learning. arXiv preprint\narXiv:1710.05468, 2017.\nMichael Kearns and Dana Ron. Algorithmic stability and\nsanity-check bounds for leave-one-out cross-validation.\nNeural computation, 11(6):1427–1453, 1999.\nNitish Shirish Keskar, Dheevatsa Mudigere, Jorge Nocedal,\nMikhail Smelyanskiy, and Ping Tak Peter Tang. On large-\nbatch training for deep learning: Generalization gap and\nsharp minima. arXiv preprint arXiv:1609.04836, 2016.\nAlex Krizhevsky, Ilya Sutskever, and Geoffrey E Hinton.\nImagenet classiﬁcation with deep convolutional neural\nnetworks. In Advances in neural information processing\nsystems, pages 1097–1105, 2012.\nJohn Langford and Rich Caruana. (not) bounding the true\nerror. In Proceedings of the 14th International Confer-\nence on Neural Information Processing Systems: Natural\nand Synthetic, pages 809–816. MIT Press, 2001.\nTengyuan Liang, Tomaso Poggio, Alexander Rakhlin, and\nJames Stokes. Fisher-rao metric, geometry, and complex-\nity of neural networks. arXiv preprint arXiv:1711.01530,\n2017.\nNick Littlestone and Manfred Warmuth. Relating data com-\npression and learnability. Technical report, Technical\nreport, University of California, Santa Cruz, 1986.\nDavid A McAllester. Some PAC-Bayesian theorems. In\nProceedings of the eleventh annual conference on Com-\nputational learning theory, pages 230–234. ACM, 1998.\nStronger Generalization Bounds for Deep Nets via a Compression Approach\nDavid A McAllester. PAC-Bayesian model averaging. In\nProceedings of the twelfth annual conference on Compu-\ntational learning theory, pages 164–170. ACM, 1999.\nAri Morcos, David GT Barrett, Matthew Botvinick, and\nNeil Rabinowitz. On the importance of single di-\nrections for generalization. In Proceeding of the In-\nternational Conference on Learning Representations ,\n2018. URL https://openreview.net/forum?\nid=r1iuQjxCZ&noteId=r1iuQjxCZ.\nBehnam Neyshabur, Ruslan R Salakhutdinov, and Nati Sre-\nbro. Path-sgd: Path-normalized optimization in deep\nneural networks. In Advances in Neural Information\nProcessing Systems, pages 2422–2430, 2015a.\nBehnam Neyshabur, Ryota Tomioka, and Nathan Srebro.\nNorm-based capacity control in neural networks. In\nProceeding of the 28th Conference on Learning Theory\n(COLT), 2015b.\nBehnam Neyshabur, Ryota Tomioka, and Nathan Srebro.\nIn search of the real inductive bias: On the role of im-\nplicit regularization in deep learning. Proceeding of the\nInternational Conference on Learning Representations\nworkshop track, 2015c.\nBehnam Neyshabur, Srinadh Bhojanapalli, David\nMcAllester, and Nathan Srebro. A pac-bayesian\napproach to spectrally-normalized margin bounds for\nneural networks. arXiv preprint arXiv:1707.09564 ,\n2017a.\nBehnam Neyshabur, Srinadh Bhojanapalli, David\nMcAllester, and Nati Srebro. Exploring generalization\nin deep learning. In Advances in Neural Information\nProcessing Systems, pages 5949–5958, 2017b.\nChristos Pelekis and Jan Ramon. Hoeffding’s inequality\nfor sums of weakly dependent random variables. arXiv\npreprint arXiv:1507.06871, 2015.\nShai Shalev-Shwartz, Ohad Shamir, Nathan Srebro, and\nKarthik Sridharan. Learnability, stability and uniform\nconvergence. Journal of Machine Learning Research, 11\n(Oct):2635–2670, 2010.\nKaren Simonyan and Andrew Zisserman. Very deep con-\nvolutional networks for large-scale image recognition.\narXiv preprint arXiv:1409.1556, 2014.\nAlex J Smola, Bernhard Sch ¨olkopf, and Klaus-Robert\nM¨uller. The connection between regularization opera-\ntors and support vector kernels. Neural networks, 11(4):\n637–649, 1998.\nNathan Srebro, Jason Rennie, and Tommi S Jaakkola.\nMaximum-margin matrix factorization. In Advances in\nneural information processing systems, pages 1329–1336,\n2005.\nNitish Srivastava, Geoffrey Hinton, Alex Krizhevsky, Ilya\nSutskever, and Ruslan Salakhutdinov. Dropout: A simple\nway to prevent neural networks from overﬁtting. The\nJournal of Machine Learning Research, 15(1):1929–1958,\n2014.\nChiyuan Zhang, Samy Bengio, Moritz Hardt, Benjamin\nRecht, and Oriol Vinyals. Understanding deep learning\nrequires rethinking generalization. In International Con-\nference on Learning Representations, 2017.",
  "values": {
    "Fairness": "No",
    "Non-maleficence": "No",
    "Collective influence": "No",
    "User influence": "No",
    "Explicability": "No",
    "Critiqability": "No",
    "Respect for Law and public interest": "No",
    "Justice": "No",
    "Privacy": "No",
    "Beneficence": "No",
    "Deferral to humans": "No",
    "Interpretable (to users)": "No",
    "Respect for Persons": "No",
    "Transparent (to users)": "No",
    "Not socially biased": "No",
    "Autonomy (power to decide)": "No"
  }
}