{
  "pdf": "arora19a",
  "title": "Fine-Grained Analysis of Optimization and Generalization for Overparameterized Two-Layer Neural Networks",
  "author": "Unknown",
  "paper_id": "arora19a",
  "text": "Fine-Grained Analysis of Optimization and Generalization for\nOverparameterized Two-Layer Neural Networks\nSanjeev Arora *12 Simon S. Du *3 Wei Hu *1 Zhiyuan Li *1 Ruosong Wang *3\nAbstract\nRecent works have cast some light on the mys-\ntery of why deep nets ﬁt any data and generalize\ndespite being very overparametrized. This paper\nanalyzes training and generalization for a simple\n2-layer ReLU net with random initialization, and\nprovides the following improvements over recent\nworks:\n(i) Using a tighter characterization of training\nspeed than recent papers, an explanation for\nwhy training a neural net with random la-\nbels leads to slower training, as originally\nobserved in [Zhang et al. ICLR’17].\n(ii) Generalization bound independent of net-\nwork size, using a data-dependent complex-\nity measure. Our measure distinguishes\nclearly between random labels and true la-\nbels on MNIST and CIFAR, as shown by\nexperiments. Moreover, recent papers re-\nquire sample complexity to increase (slowly)\nwith the size, while our sample complexity is\ncompletely independent of the network size.\n(iii) Learnability of a broad class of smooth func-\ntions by 2-layer ReLU nets trained via gradi-\nent descent.\nThe key idea is to track dynamics of training and\ngeneralization via properties of a related kernel.\n1. Introduction\nThe well-known work of Zhang et al. (2017) highlighted\nintriguing experimental phenomena about deep net train-\ning – speciﬁcally, optimization and generalization – and\nasked whether theory could explain them. They showed\n*Alphabetical order 1Princeton University, Princeton, NJ, USA\n2Institute for Advanced Study, Princeton, NJ, USA 3Carnegie\nMellon University, Pittsburgh, PA, USA. Correspondence to: Wei\nHu <huwei@cs.princeton.edu>.\nProceedings of the 36 th International Conference on Machine\nLearning, Long Beach, California, PMLR 97, 2019. Copyright\n2019 by the author(s).\nthat sufﬁciently powerful nets (with vastly more parameters\nthan number of training samples) can attain zero training\nerror, regardless of whether the data is properly labeled or\nrandomly labeled. Obviously, training with randomly la-\nbeled data cannot generalize, whereas training with properly\nlabeled data generalizes. See Figure 2 replicating some of\nthese results.\nRecent papers have begun to provide explanations, showing\nthat gradient descent can allow an overparametrized multi-\nlayer net to attain arbitrarily low training error on fairly\ngeneric datasets (Du et al., 2018a;c; Li & Liang, 2018; Allen-\nZhu et al. , 2018b; Zou et al. , 2018), provided the amount\nof overparametrization is a high polynomial of the relevant\nparameters (i.e. vastly more than the overparametrization in\n(Zhang et al. , 2017)). Under further assumptions it can also\nbe shown that the trained net generalizes ( Allen-Zhu et al.,\n2018a). But some issues were not addressed in these papers,\nand the goal of the current paper is to address them.\nFirst, the experiments in ( Zhang et al. , 2017) show that\nthough the nets attain zero training error on even random\ndata, the convergence rate is much slower. See Figure 1.\nQuestion 1. Why do true labels give faster convergence\nrate than random labels for gradient descent?\nThe above papers do not answer this question, since their\nproof of convergence does not distinguish between good\nand random labels.\nThe next issue is about generalization: clearly, some prop-\nerty of properly labeled data controls generalization, but\nwhat? Classical measures used in generalization theory\nsuch as VC-dimension and Rademacher complexity are\nmuch too pessimistic. A line of research proposed norm-\nbased (e.g. ( Bartlett et al., 2017a)) and compression-based\nbounds ( Arora et al. , 2018). But the sample complexity\nupper bounds obtained are still far too weak. Furthermore\nthey rely on some property of the trained net that is re-\nvealed/computed at the end of training. There is no property\nof data alone that determine upfront whether the trained net\nwill generalize. A recent paper ( Allen-Zhu et al. , 2018a)\nassumed that there exists an underlying (unknown) neural\nnetwork that achieves low error on the data distribution, and\nthe amount of data available is quite a bit more than the min-\nFine-Grained Analysis of Optimization and Generalization for Overparameterized Two-Layer Neural Networks\nimum number of samples needed to learn this underlying\nneural net. Under this condition, the overparametrized net\n(which has way more parameters) can learn in a way that\ngeneralizes. However, it is hard to verify from data whether\nthis assumption is satisﬁed, even after the larger net has\nﬁnished training. 1 Thus the assumption is in some sense\nunveriﬁable.\nQuestion 2. Is there an easily veriﬁable complexity measure\nthat can differentiate true labels and random labels?\nWithout explicit regularization, to attack this problem, one\nmust resort to algorithm-dependent generalization analysis.\nOne such line of work established that ﬁrst-order methods\ncan automatically ﬁnd minimum-norm/maximum-margin\nsolutions that ﬁt the data in the settings of logistic regres-\nsion, deep linear networks, and symmetric matrix factor-\nization ( Soudry et al. , 2018; Gunasekar et al. , 2018a;b;\nJi & Telgarsky , 2018; Li et al. , 2018b). However, how\nto extend these results to non-linear neural networks re-\nmains unclear ( Wei et al., 2018). Another line of algorithm-\ndependent analysis of generalization ( Hardt et al. , 2015;\nMou et al., 2017; Chen et al., 2018) used stability of speciﬁc\noptimization algorithms that satisfy certain generic proper-\nties like convexity, smoothness, etc. However, as the number\nof epochs becomes large, these generalization bounds are\nvacuous.\nOur results. We give a new analysis that provides answers\nto Questions 1 and 2 for overparameterized two-layer neural\nnetworks with ReLU activation trained by gradient descent\n(GD), when the number of neurons in the hidden layer is\nsufﬁciently large. In this setting, Du et al. (2018c) have\nproved that GD with random initialization can achieve zero\ntraining error for any non-degenerate data. We give a more\nreﬁned analysis of the trajectory of GD which enables us to\nprovide answers to Questions 1 and 2. In particular:\n• In Section 4, using the trajectory of the network pre-\ndictions on the training data during optimization, we\naccurately estimate the magnitude of training loss in\neach iteration. Our key ﬁnding is that the number of\niterations needed to achieve a target accuracy depends\non the projections of data labels on the eigenvectors\nof a certain Gram matrix to be deﬁned in Equation (3).\nOn MNIST and CIFAR datasets, we ﬁnd that such pro-\njections are signiﬁcantly different for true labels and\nrandom labels, and as a result we are able to answer\nQuestion 1.\n• In Section 5, we give a generalization bound for the\nsolution found by GD, based on accurate estimates of\nhow much the network parameters can move during\noptimization (in suitable norms). Our generalization\n1In Section 2, we discuss the related works in more details.\nbound depends on a data-dependent complexity mea-\nsure (c.f. Equation (10)), and notably, is completely in-\ndependent of the number of hidden units in the network.\nAgain, we test this complexity measure on MNIST and\nCIFAR, and ﬁnd that the complexity measures for true\nand random labels are signiﬁcantly different, which\nthus answers Question 2.\nNotice that because zero training error is achieved by\nthe solution found by GD, a generalization bound is an\nupper bound on the error on the data distribution (test\nerror). We also remark that our generalization bound is\nvalid for any data labels – it does not require the exis-\ntence of a small ground-truth network as in ( Allen-Zhu\net al., 2018a). Moreover, our bound can be efﬁciently\ncomputed for any data labels.\n• In Section 6, we further study what kind of functions\ncan be provably learned by two-layer ReLU networks\ntrained by GD. Combining the optimization and gener-\nalization results, we uncover a broad class of learnable\nfunctions, including linear functions, two-layer neural\nnetworks with polynomial activation \u0000(z)= z2l or co-\nsine activation, etc. Our requirement on the smoothness\nof learnable functions is weaker than that in (Allen-Zhu\net al., 2018a).\nFinally, we note that the intriguing generalization phenom-\nena in deep learning were observed in kernel methods as\nwell (Belkin et al. , 2018). The analysis in the current pa-\nper is also related to a kernel from the ReLU activation\n(c.f. Equation ( 3)).\n2. Related Work\nIn this section we survey previous works on optimization\nand generalization aspects of neural networks.\nOptimization. Many papers tried to characterize geomet-\nric landscapes of objective functions ( Safran & Shamir ,\n2017; Zhou & Liang , 2017; Freeman & Bruna , 2016; Hardt\n& Ma , 2016; Nguyen & Hein , 2017; Kawaguchi, 2016;\nV enturi et al., 2018; Soudry & Carmon , 2016; Du & Lee ,\n2018; Soltanolkotabi et al., 2018; Haeffele & Vidal, 2015).\nThe hope is to leverage recent advance in ﬁrst-order algo-\nrithms ( Ge et al. , 2015; Lee et al. , 2016; Jin et al. , 2017)\nwhich showed that if the landscape satisﬁes (1) all local\nminima are global and (2) all saddle points are strict (i.e.,\nthere exists a negative curvature), then ﬁrst-order methods\ncan escape all saddle points and ﬁnd a global minimum.\nUnfortunately, these desired properties do not hold even for\nsimple non-linear shallow neural networks (Y un et al., 2018)\nor 3-layer linear neural networks ( Kawaguchi, 2016).\nAnother approach is to directly analyze trajectory of the op-\ntimization method and to show convergence to global mini-\nFine-Grained Analysis of Optimization and Generalization for Overparameterized Two-Layer Neural Networks\nmum. A series of papers made strong assumptions on input\ndistribution as well as realizability of labels, and showed\nglobal convergence of (stochastic) gradient descent for some\nshallow neural networks ( Tian, 2017; Soltanolkotabi, 2017;\nBrutzkus & Globerson , 2017; Du et al. , 2017a;b; Li &\nY uan, 2017). Some local convergence results have also been\nproved (Zhong et al. , 2017; Zhang et al. , 2018). However,\nthese assumptions are not satisﬁed in practice.\nFor two-layer neural networks, a line of papers used mean\nﬁeld analysis to establish that for inﬁnitely wide neural\nnetworks, the empirical distribution of the neural network\nparameters can be described as a Wasserstein gradient\nﬂow (Mei et al., 2018; Chizat & Bach, 2018a; Sirignano &\nSpiliopoulos, 2018; Rotskoff & V anden-Eijnden, 2018; Wei\net al., 2018). However, it is unclear whether this framework\ncan explain the behavior of ﬁrst-order methods on ﬁnite-size\nneural networks.\nRecent breakthroughs were made in understanding opti-\nmization of overparameterized neural networks through\nthe trajectory-based approach. They proved global poly-\nnomial time convergence of (stochastic) gradient descent on\nnon-linear neural networks for minimizing empirical risk.\nTheir proof techniques can be roughly classiﬁed into two\ncategories. Li & Liang (2018); Allen-Zhu et al. (2018b);\nZou et al. (2018) analyzed the trajectory of parameters and\nshowed that on the trajectory, the objective function satisﬁes\ncertain gradient dominance property. On the other hand,\n(Du et al., 2018a;c) analyzed the trajectory of network pre-\ndictions on training samples and showed that it enjoys a\nstrongly-convex-like property.\nGeneralization. It is well known that the VC-dimension\nof neural networks is at least linear in the number of pa-\nrameters (Bartlett et al. , 2017b), and therefore classical VC\ntheory cannot explain the generalization ability of mod-\nern neural networks with more parameters than training\nsamples. Researchers have proposed norm-based general-\nization bounds ( Bartlett & Mendelson, 2002; Bartlett et al.,\n2017a; Neyshabur et al. , 2015; 2017; 2019; Konstantinos\net al. , 2017; Golowich et al. , 2017; Li et al. , 2018a) and\ncompression-based bounds (Arora et al., 2018). Dziugaite\n& Roy (2017); Zhou et al. (2019) used the PAC-Bayes ap-\nproach to compute non-vacuous generalization bounds for\nMNIST and ImageNet, respectively. All these bounds are\nposterior in nature – they depend on certain properties of\nthe trained neural networks. Therefore, one has to ﬁnish\ntraining a neural network to know whether it can general-\nize. Comparing with these results, our generalization bound\nonly depends on training data and can be calculated without\nactually training the neural network.\nAnother line of work assumed the existence of a true model,\nand showed that the (regularized) empirical risk minimizer\nhas good generalization with sample complexity that de-\npends on the true model ( Du et al., 2018b; Ma et al., 2018;\nImaizumi & Fukumizu , 2018). These papers ignored the\ndifﬁculty of optimization, while we are able to prove gener-\nalization of the solution found by gradient descent. Further-\nmore, our generic generalization bound does not assume the\nexistence of any true model.\nOur paper is closely related to ( Allen-Zhu et al. , 2018a)\nwhich showed that two-layer overparametrized neural net-\nworks trained by randomly initialized stochastic gradient\ndescent can learn a class of inﬁnite-order smooth functions.\nIn contrast, our generalization bound depends on a data-\ndependent complexity measure that can be computed for\nany dataset, without assuming any ground-truth model. Fur-\nthermore, as a consequence of our generic bound, we also\nshow that two-layer neural networks can learn a class of\ninﬁnite-order smooth functions, with a less strict require-\nment for smoothness. Allen-Zhu et al. (2018a) also studied\nthe generalization performance of three-layer neural nets.\nLastly, our work is related to kernel methods, especially\nrecent discoveries of the connection between deep learn-\ning and kernels ( Jacot et al., 2018; Chizat & Bach, 2018b;\nDaniely et al., 2016; Daniely, 2017). Our analysis utilized\nseveral properties of a related kernel from the ReLU activa-\ntion (c.f. Equation ( 3)).\n3. Preliminaries and Overview of Results\nNotation. We use bold-faced letters for vectors and matri-\nces. For a matrix A, let Aij be its (i, j)-th entry. We use\nk·k2 to denote the Euclidean norm of a vector or the spectral\nnorm of a matrix, and use k·kF to denote the Frobenius norm\nof a matrix. Denote by \u0000min(A) the minimum eigenvalue of\na symmetric matrix A. Let vec (A) be the vectorization of a\nmatrix A in column-ﬁrst order. Let I be the identity matrix\nand [n]= {1, 2,...,n }. Denote by N (µ, ⌃) the Gaussian\ndistribution with mean µ and covariance ⌃. Denote by \u0000 (·)\nthe ReLU function \u0000 (z) = max {z, 0}. Denote by I{E}\nthe indicator function for an event E.\n3.1. Setting: Two-Layer Neural Network Trained by\nRandomly Initialized Gradient Descent\nWe consider a two-layer ReLU activated neural network\nwith m neurons in the hidden layer:\nfW,a(x)= 1pm\nmX\nr=1\nar \u0000\n\u0000\nw>\nr x\n\u0000\n,\nwhere x 2 Rd is the input, w1,..., wm 2 Rd are weight\nvectors in the ﬁrst layer, a1,...,a m 2 R are weights\nin the second layer. For convenience we denote W =\n(w1,..., wm) 2 Rd⇥m and a =( a1,...,a m)> 2 Rm.\nFine-Grained Analysis of Optimization and Generalization for Overparameterized Two-Layer Neural Networks\nWe are given n input-label samples S = {(xi,y i)}n\ni=1\ndrawn i.i.d. from an underlying data distribution D over\nRd ⇥ R. We denote X =( x1,..., xn) 2 Rd⇥n and\ny =( y1,...,y n)> 2 Rn. For simplicity, we assume that\nfor (x,y ) sampled from D, we have kxk2 =1 and |y| 1.\nWe train the neural network by randomly initialized gradient\ndescent (GD) on the quadratic loss over data S. In particular,\nwe ﬁrst initialize the parameters randomly:\nwr (0) ⇠N (0, 2I),a r ⇠ unif ({\u00001, 1}) , 8r 2 [m],\n(1)\nwhere 0 <  1 controls the magnitude of initialization,\nand all randomnesses are independent. We then ﬁx the\nsecond layer a and optimize the ﬁrst layer W through GD\non the following objective function:\n\u0000(W)= 1\n2\nnX\ni=1\n(yi \u0000 fW,a(xi))2 . (2)\nThe GD update rule can be written as: 2\nwr (k + 1) \u0000 wr (k)= \u0000⌘ @\u0000(W(k))\n@wr\n= \u0000 ⌘ arpm\nnX\ni=1\n(fW(k),a(xi) \u0000 yi)I\n\u0000\nwr (k)>xi \u0000 0\n \nxi,\nwhere ⌘> 0 is the learning rate.\n3.2. The Gram Matrix from ReLU Kernel\nGiven {xi}n\ni=1, we deﬁne the following Gram matrix H1 2\nRn⇥n as follows:\nH1\nij = Ew⇠N (0,I)\n⇥\nx>\ni xj I\n\u0000\nw>xi \u0000 0, w>xj \u0000 0\n ⇤\n= x>\ni xj\n\u0000\n⇡ \u0000 arccos(x>\ni xj )\n\u0000\n2⇡ , 8i, j 2 [n].\n(3)\nThis matrix can be viewed as a Gram matrix from a kernel\nassociated with the ReLU function, and has been studied\nin (Xie et al. , 2017; Tsuchida et al. , 2017; Du et al. , 2018c).\nIn our setting of training a two-layer ReLU network, Du\net al. (2018c) showed that if H1 is positive deﬁnite, GD\nconverges to 0 training loss if m is sufﬁciently large:\nTheorem 3.1 ((Du et al. , 2018c)3). Assume \u00000 =\n\u0000min(H1 ) > 0. F or \u0000 2 (0, 1), if m =⌦\n⇣\nn6\n\u00004\n0 2 \u00003\n⌘\nand\n⌘ = O\n\u0000\u00000\nn2\n\u0000\n, then with probability at least 1 \u0000 \u0000 over the\nrandom initialization (1), we have:\n• \u0000(W(0)) = O(n/\u0000);\n2Since ReLU is not differentiable at 0, we just deﬁne “gradient”\nusing this formula, and this is indeed what is used in practice.\n3Du et al. (2018c) only considered the case  =1 , but it is\nstraightforward to generalize their result to general  at the price\nof an extra 1/2 factor in m.\n• \u0000(W(k + 1)) \n⇣\n1 \u0000 ⌘\u0000 0\n2\n⌘\n\u0000(W(k)), 8k \u0000 0.\nOur results on optimization and generalization also crucially\ndepend on this matrix H1 .\n3.3. Overview of Our Results\nNow we give an informal description of our main results.\nIt assumes that the initialization magnitude  is sufﬁciently\nsmall and the network width m is sufﬁciently large (to be\nquantiﬁed later).\nThe following theorem gives a precise characterization of\nhow the objective decreases to 0. It says that this process\nis essentially determined by a power method for matrix\nI \u0000 ⌘H1 applied on the label vector y.\nTheorem 3.2 (Informal version of Theorem 4.1). With high\nprobability we have:\n\u0000(W(k)) ⇡ 1\n2\n\u0000\u0000(I \u0000 ⌘H1 )ky\n\u0000\u00002\n2 , 8k \u0000 0.\nAs a consequence, we are able to distinguish the conver-\ngence rates for different labels y, which can be determined\nby the projections of y on the eigenvectors of H1 . This\nallows us to obtain an answer to Question 1. See Section 4\nfor details.\nOur main result for generalization is the following:\nTheorem 3.3 (Informal version of Theorem 5.1). F or any\n1-Lipschitz loss function, the generalization error of the\ntwo-layer ReLU network found by GD is at most\nr\n2y>(H1 )\u00001y\nn . (4)\nNotice that our generalization bound (4) can be computed\nfrom data {(xi,y i)}n\ni=1, and is completely independent of\nthe network width m. We observe that this bound can clearly\ndistinguish true labels and random labels, thus providing an\nanswer to Question 2. See Section 5 for details.\nFinally, using Theorem 3.3, we prove that we can use our\ntwo-layer ReLU network trained by GD to learn a broad\nclass of functions, including linear functions, two-layer neu-\nral networks with polynomial activation \u0000(z)= z2l or co-\nsine activation, etc. See Section 6 for details.\n3.4. Additional Notation\nWe introduce some additional notation that will be used.\nDeﬁne ui = fW,a(xi), i.e., the network’s prediction on the\ni-th input. We also use u =( u1,...,u n)> 2 Rn to denote\nall n predictions. Then we have \u0000(W)= 1\n2 ky \u0000 uk2\n2 and\nFine-Grained Analysis of Optimization and Generalization for Overparameterized Two-Layer Neural Networks\nthe gradient of \u0000 can be written as:\n@\u0000(W)\n@wr\n= 1pm ar\nnX\ni=1\n(ui \u0000 yi)Ir,ixi, 8r 2 [m], (5)\nwhere Ir,i = I\n\u0000\nw>\nr xi \u0000 0\n \n.\nWe deﬁne two matrices Z and H which will play a key role\nin our analysis of the GD trajectory:\nZ = 1pm\n0\nB@\nI1,1a1x1 ··· I1,na1xn\n... . . . ...\nIm,1amx1 ··· Im,namxn\n1\nCA 2 Rmd⇥n,\nand H = Z>Z. Note that\nHij = x>\ni xj\nm\nmX\nr=1\nIr,iIr,j, 8i, j 2 [n].\nWith this notation we have a more compact form of the\ngradient (5):\nvec (r\u0000(W)) = Z(u \u0000 y).\nThen the GD update rule is:\nvec (W(k + 1)) = vec (W(k)) \u0000 ⌘Z(k)(u(k) \u0000 y), (6)\nfor k =0 , 1,... . Throughout the paper, we use k as the itera-\ntion number, and also use k to index all variables that depend\non W(k). For example, we have ui(k)= fW(k),a(xi),\nIr,i(k)= I\n\u0000\nwr (k)>xi \u0000 0\n \n, etc.\n4. Analysis of Convergence Rate\nAlthough Theorem 3.1 already predicts linear convergence\nof GD to 0 loss, it only provides an upper bound on the\nloss and does not distinguish different types of labels. In\nparticular, it cannot answer Question 1. In this section we\ngive a ﬁne-grained analysis of the convergence rate.\nRecall the loss function \u0000(W)= 1\n2 ky \u0000 uk2\n2. Thus, it\nis equivalent to study how fast the sequence {u(k)}1\nk=0\nconverges to y. Key to our analysis is the observation that\nwhen the size of initialization  is small and the network\nwidth m is large, the sequence {u(k)}1\nk=0 stays close to\nanother sequence {˜u(k)}1\nk=0 which has a linear update\nrule:\n˜u(0) = 0,\n˜u(k + 1) = ˜u(k) \u0000 ⌘H1 (˜u(k) \u0000 y) , (7)\nwhere H1 is the Gram matrix deﬁned in ( 3).\nWrite the eigen-decomposition H1 = P n\ni=1 \u0000iviv>\ni ,\nwhere v1,..., vn 2 Rn are orthonormal eigenvectors of\nH1 and \u00001,...,\u0000 n are corresponding eigenvalues. Our\nmain theorem in this section is the following:\nTheorem 4.1. Suppose \u00000 = \u0000min(H1 ) > 0,  =\nO\n⇣\n✏\u0000pn\n⌘\n, m =⌦\n⇣\nn7\n\u00004\n0 2 \u00004 ✏2\n⌘\nand ⌘ = O\n\u0000\u00000\nn2\n\u0000\n. Then with\nprobability at least 1 \u0000 \u0000 over the random initialization, for\nall k =0 , 1, 2,... we have:\nky \u0000 u(k)k2 =\nvuut\nnX\ni=1\n(1 \u0000 ⌘\u0000i)2k \u0000\nv>\ni y\n\u00002\n± ✏. (8)\nThe proof of Theorem 4.1 is given in Appendix C.\nIn fact, the dominating term\nq P n\ni=1(1 \u0000 ⌘\u0000i)2k \u0000\nv>\ni y\n\u00002\nis exactly equal to ky \u0000 ˜u(k)k2, which we prove in Sec-\ntion 4.1.\nIn light of (8), it sufﬁces to understand how fast P n\ni=1(1 \u0000\n⌘\u0000i)2k \u0000\nv>\ni y\n\u00002\nconverges to 0 as k grows. Deﬁne\n⇠i(k)=( 1 \u0000 ⌘\u0000i)2k(v>\ni y)2, and notice that each se-\nquence {⇠i(k)}1\nk=0 is a geometric sequence which starts\nat ⇠i(0) = ( v>\ni y)2 and decreases at ratio (1 \u0000 ⌘\u0000i)2. In\nother words, we can think of decomposing the label vec-\ntor y into its projections onto all eigenvectors vi of H1 :\nkyk2\n2 = P n\ni=1(v>\ni y)2 = P n\ni=1 ⇠i(0), and the i-th portion\nshrinks exponentially at ratio (1 \u0000 ⌘\u0000i)2. The larger \u0000i is,\nthe faster {⇠i(k)}1\nk=0 decreases to 0, so in order to have\nfaster convergence we would like the projections of y onto\ntop eigenvectors to be larger. Therefore we obtain the fol-\nlowing intuitive rule to compare the convergence rates on\ntwo sets of labels in a qualitative manner (for ﬁxed kyk2):\n• For a set of labels y, if they align with the top eigen-\nvectors, i.e., (v>\ni y)2 is large for large \u0000i, then gradient\ndescent converges quickly.\n• For a set of labels y, if the projections on eigenvectors\n{\n\u0000\nv>\ni y\n\u00002\n}n\ni=1 are uniform, or labels align with eigen-\nvectors with respect to small eigenvalues, then gradient\ndescent converges with a slow rate.\nAnswer to Question 1. We now use this reasoning to\nanswer Question 1. In Figure 1(b), we compute the eigen-\nvalues of H1 (blue curve) for the MNIST dataset. The plot\nshows the eigenvalues of H1 admit a fast decay. We further\ncompute the projections {\n\u0000\u0000v>\ni y\n\u0000\u0000}n\ni=1 of true labels (red) and\nrandom labels (cyan). We observe that there is a signiﬁcant\ndifference between the projections of true labels and random\nlabels: true labels align well with top eigenvectors whereas\nprojections of random labels are close to being uniform.\nFurthermore, according to our theory, if a set of labels align\nwith the eigenvector associated with the least eigenvalue,\nthe convergence rate of gradient descent will be extremely\nslow. We construct such labels and in Figure 1(a) we indeed\nobserve slow convergence. We repeat the same experiments\non CIFAR and have similar observations (Figures 1(c) and\nFine-Grained Analysis of Optimization and Generalization for Overparameterized Two-Layer Neural Networks\n(a) Convergence Rate, MNIST.(b) Eigenval & Projections, MNIST.(c) Convergence Rate, CIFAR.(d) Eigenval & Projections, CIFAR.\nFigure 1: In Figures 1(a) and 1(c), we compare convergence rates of gradient descent between using true labels, random\nlabels and the worst case labels (normalized eigenvector of H1 corresponding to \u0000min(H1 ). In Figures 1(b) and 1(d), we\nplot the eigenvalues of H1 as well as projections of true, random, and worst case labels on different eigenvectors of H1 .\nThe experiments use gradient descent on data from two classes of MNIST or CIFAR. The plots clearly demonstrate that true\nlabels have much better alignment with top eigenvectors, thus enjoying faster convergence.\n1(d)). These empirical ﬁndings support our theory on the\nconvergence rate of gradient descent. See Appendix A for\nimplementation details.\n4.1. Proof Sketch of Theorem 4.1\nNow we prove ky \u0000 ˜u(k)k2\n2 = P n\ni=1(1 \u0000 ⌘\u0000i)2k \u0000\nv>\ni y\n\u00002\n.\nThe entire proof of Theorem 4.1 is given in Appendix C,\nwhich relies on the fact that the dynamics of {u(k)}1\nk=0 is\nessentially a perturbed version of ( 7).\nFrom (7) we have ˜u(k + 1) \u0000 y =( I \u0000 ⌘H1 )( ˜u(k) \u0000 y),\nwhich implies ˜u(k) \u0000 y =( I \u0000 ⌘H1 )k (˜u(0) \u0000 y)=\n\u0000(I \u0000 ⌘H1 )ky. Note that (I \u0000 ⌘H1 )k has eigen-\ndecomposition (I \u0000 ⌘H1 )k = P n\ni=1(1 \u0000 ⌘\u0000i)kviv>\ni and\nthat y can be decomposed as y = P n\ni=1(v>\ni y)vi. Then\nwe have ˜u(k) \u0000 y = \u0000 P n\ni=1(1 \u0000 ⌘\u0000i)k(v>\ni y)vi, which\nimplies k˜u(k) \u0000 yk2\n2 = P n\ni=1(1 \u0000 ⌘\u0000i)2k(v>\ni y)2.\n5. Analysis of Generalization\nIn this section, we study the generalization ability of the\ntwo-layer neural network fW(k),a trained by GD.\nFirst, in order for optimization to succeed, i.e., zero training\nloss is achieved, we need a non-degeneracy assumption on\nthe data distribution, deﬁned below:\nDeﬁnition 5.1. A distribution D over Rd ⇥ R is (\u00000,\u0000 ,n)-\nnon-degenerate, if for n i.i.d. samples {(xi,y i)}n\ni=1 from D,\nwith probability at least 1 \u0000 \u0000 we have \u0000min(H1 ) \u0000 \u00000 >\n0.\nRemark 5.1. Note that as long as no two xi and xj are\nparallel to each other , we have \u0000min(H1 ) > 0. (See ( Du\net al., 2018c)). F or most real-world distributions, any two\ntraining inputs are not parallel.\nOur main theorem is the following:\nTheorem 5.1. Fix a failure probability \u0000 2 (0, 1). Sup-\npose our data S = {(xi,y i)}n\ni=1 are i.i.d. samples from\na (\u00000,\u0000 /3,n )-non-degenerate distribution D, and  =\nO\n\u0000\u00000 \u0000\nn\n\u0000\n,m \u0000 \u00002poly\n\u0000\nn, \u0000\u00001\n0 ,\u0000 \u00001\u0000\n. Consider any loss\nfunction ` : R ⇥ R ! [0, 1] that is 1-Lipschitz in the ﬁrst\nargument such that `(y, y)=0 . Then with probability at\nleast 1 \u0000 \u0000 over the random initialization and the training\nsamples, the two-layer neural network fW(k),a trained by\nGD for k \u0000 ⌦\n⇣\n1\n⌘\u0000 0\nlog n\n\u0000\n⌘\niterations has population loss\nLD (fW(k),a)= E(x,y)⇠D\n⇥\n`(fW(k),a(x),y )\n⇤\nbounded as:\nLD (fW(k),a) \ns\n2y> (H1 )\u00001 y\nn + O\n0\n@\ns\nlog n\n\u00000 \u0000\nn\n1\nA .\n(9)\nThe proof of Theorem 5.1 is given in Appendix D and we\nsketch the proof in Section 5.1.\nNote that in Theorem 5.1 there are three sources of possi-\nble failures: (i) failure of satisfying \u0000min(H1 ) \u0000 \u00000, (ii)\nfailure of random initialization, and (iii) failure in the data\nsampling procedure (c.f. Theorem B.1). We ensure that all\nthese failure probabilities are at most \u0000/3 so that the ﬁnal\nfailure probability is at most \u0000.\nAs a corollary of Theorem 5.1, for binary classiﬁcation\nproblems (i.e., labels are ±1), we can show that (9) also\nbounds the population classiﬁcation error of the learned\nclassiﬁer. See Appendix D for the proof.\nCorollary 5.2. Under the same assumptions as in The-\norem 5.1 and additionally assuming that y 2 {± 1}\nfor (x,y ) ⇠D , with probability at least 1 \u0000 \u0000,\nthe population classiﬁcation error L01\nD (fW(k),a)=\nPr(x,y)⇠D\n⇥\nsign\n\u0000\nfW(k),a(x)\n\u0000\n6= y\n⇤\nis bounded as:\nL01\nD (fW(k),a) \ns\n2y> (H1 )\u00001 y\nn + O\n0\n@\ns\nlog n\n\u00000 \u0000\nn\n1\nA .\nFine-Grained Analysis of Optimization and Generalization for Overparameterized Two-Layer Neural Networks\n(a) MNIST Data. (b) CIFAR Data.\nFigure 2: Generalization error ( `1 loss and classiﬁcation\nerror) v.s. our complexity measure when different portions\nof random labels are used. We apply GD on data from two\nclasses of MNIST or CIFAR until convergence. Our com-\nplexity measure almost matches the trend of generalization\nerror as the portion of random labels increases. Note that `1\nloss is always an upper bound on the classiﬁcation error.\nNow we discuss our generalization bound. The dominating\nterm in (9) is: s\n2y> (H1 )\u00001 y\nn . (10)\nThis can be viewed as a complexity measure of data that\none can use to predict the test accuracy of the learned neural\nnetwork. Our result has the following advantages: (i) our\ncomplexity measure (10) can be directly computed given\ndata {(xi,y i)}n\ni=1, without the need of training a neural\nnetwork or assuming a ground-truth model; (ii) our bound\nis completely independent of the network width m.\nEvaluating our completixy measure (10). To illustrate\nthat the complexity measure in (10) effectively determines\ntest error, in Figure 2 we compare this complexity measure\nversus the test error with true labels and random labels (and\nmixture of true and random labels). Random and true labels\nhave signiﬁcantly different complexity measures, and as the\nportion of random labels increases, our complexity measure\nalso increases. See Appendix A for implementation details.\n5.1. Proof Sketch of Theorem 5.1\nThe main ingredients in the proof of Theorem 5.1 are Lem-\nmas 5.3 and 5.4. We defer the proofs of these lemmas as\nwell as the full proof of Theorem 5.1 to Appendix D.\nOur proof is based on a careful characterization of the tra-\njectory of {W(k)}1\nk=0 during GD. In particular, we bound\nits distance to initialization as follows:\nLemma 5.3. Suppose m \u0000 \u00002poly\n\u0000\nn, \u0000\u00001\n0 ,\u0000 \u00001\u0000\nand\n⌘ = O\n\u0000\u00000\nn2\n\u0000\n. Then with probability at least 1 \u0000 \u0000 over the\nrandom initialization, we have for all k \u0000 0:\n• kwr (k) \u0000 wr (0)k2 = O\n⇣\nnpm\u00000\np\n\u0000\n⌘\n(8r 2 [m]), and\n• kW(k) \u0000 W(0)kF \nq\ny> (H1 )\u00001 y + O\n⇣\nn\n\u00000 \u0000\n⌘\n+\npoly(n,\u0000\u00001\n0 ,\u0000\u00001 )\nm1/4 1/2 .\nThe bound on the movement of each wr was proved in\n(Du et al. , 2018c). Our main contribution is the bound on\nkW(k) \u0000 W(0)kF which corresponds to the total move-\nment of all neurons . The main idea is to couple the\ntrajectory of {W(k)}1\nk=0 with another simpler trajectoryn\nfW(k)\no1\nk=0\ndeﬁned as:\nfW(0) = 0,\nvec\n⇣\nfW(k + 1)\n⌘\n= vec\n⇣\nfW(k)\n⌘\n(11)\n\u0000 ⌘Z(0)\n⇣\nZ(0)>vec\n⇣\nfW(k)\n⌘\n\u0000 y\n⌘\n.\nWe prove\n\u0000\u0000\u0000fW(1 ) \u0000 fW(0)\n\u0000\u0000\u0000\nF\n=\np\ny>H(0)\u00001y in Sec-\ntion 5.2.4 The actually proof of Lemma 5.3 is essentially a\nperturbed version of this.\nLemma 5.3 implies that the learned function fW(k),a from\nGD is in a restricted class of neural nets whose weights are\nclose to initialization W(0). The following lemma bounds\nthe Rademacher complexity of this function class:\nLemma 5.4. Given R> 0, with probability at least 1 \u0000 \u0000\nover the random initialization ( W(0), a), simultaneously\nfor every B> 0, the following function class\nFW(0),a\nR,B = {fW,a : kwr \u0000 wr (0)k2  R (8r 2 [m]),\nkW \u0000 W(0)kF  B}\nhas empirical Rademacher complexity bounded as:\nRS\n⇣\nFW(0),a\nR,B\n⌘\n= 1\nn E\"2{±1}n\n2\n4 sup\nf 2FW(0),a\nR,B\nnX\ni=1\n\"if (xi)\n3\n5\n Bp\n2n\n \n1+\n✓2 log 2\n\u0000\nm\n◆1/4!\n+ 2R2pm\n + R\nr\n2 log 2\n\u0000.\nFinally, combining Lemmas 5.3 and 5.4, we are able to\nconclude that the neural network found by GD belongs\nto a function class with Rademacher complexity at mostp\ny>(H1 )\u00001y/(2n) (plus negligible errors). This gives\nus the generalization bound in Theorem 5.1 using the theory\nof Rademacher complexity (Appendix B).\n5.2. Analysis of the Auxiliary Sequence\nn\nfW(k)\no1\nk=0\nNow we give a proof of\n\u0000\u0000\u0000fW(1 ) \u0000 fW(0)\n\u0000\u0000\u0000\nF\n=\np\ny>H(0)\u00001y as an illustration for the proof of Lemma 5.3.\n4Note that we have H(0) ⇡ H1 from standard concentration.\nSee Lemma C.3.\nFine-Grained Analysis of Optimization and Generalization for Overparameterized Two-Layer Neural Networks\nDeﬁne v(k)= Z(0)>vec\n⇣\nfW(k)\n⌘\n2 Rn. Then from (11)\nwe have v(0) = 0 and v(k+1) = v(k)\u0000⌘H(0)(v(k)\u0000y),\nyielding v(k) \u0000 y = \u0000(I \u0000 ⌘H(0))ky. Plugging this\nback to (11) we get vec\n⇣\nfW(k + 1)\n⌘\n\u0000 vec\n⇣\nfW(k)\n⌘\n=\n⌘Z(0)(I \u0000 ⌘H(0))ky. Then taking a sum over k =0 , 1,...\nwe have\nvec\n⇣\nfW(1 )\n⌘\n\u0000 vec\n⇣\nfW(0)\n⌘\n=\n1X\nk=0\n⌘Z(0)(I \u0000 ⌘H(0))ky\n= Z(0)H(0)\u00001y.\nThe desired result thus follows:\u0000\u0000\u0000fW(1 ) \u0000 fW(0)\n\u0000\u0000\u0000\n2\nF\n= y>H(0)\u00001Z(0)>Z(0)H(0)\u00001y\n= y>H(0)\u00001y.\n6. Provable Learning using Two-Layer ReLU\nNeural Networks\nTheorem 5.1 determines that\nq\n2y>(H1 )\u00001 y\nn controls the\ngeneralization error. In this section, we study what functions\ncan be provably learned in this setting. We assume the data\nsatisfy yi = g(xi) for some underlying function g : Rd !\nR. A simple observation is that if we can prove\ny>(H1 )\u00001y  Mg\nfor some quantity Mg that is independent of the number\nof samples n, then Theorem 5.1 implies we can provably\nlearn the function g on the underlying data distribution using\nO\n⇣\nMg +log(1/\u0000)\n✏2\n⌘\nsamples. The following theorem shows\nthat this is indeed the case for a broad class of functions.\nTheorem 6.1. Suppose we have\nyi = g(xi)= ↵\n\u0000\n\u0000>xi\n\u0000p\n, 8i 2 [n],\nwhere p =1 or p =2 l (l 2 N+), \u0000 2 Rd and ↵ 2 R. Then\nwe have q\ny>(H1 )\u00001y  3p |↵ |·k \u0000kp\n2 .\nThe proof of Theorem 6.1 is given in Appendix E.\nNotice that for two label vectors y(1) and y(2), we have\nq\n(y(1) + y(2))>(H1 )\u00001 \u0000\ny(1) + y(2)\u0000\n\nq\n(y(1))>(H1 )\u00001y(1) +\nq\n(y(2))>(H1 )\u00001y(2).\nThis implies that the sum of learnable functions is also\nlearnable. Therefore, the following is a direct corollary of\nTheorem 6.1:\nCorollary 6.2. Suppose we have\nyi = g(xi)=\nX\nj\n↵ j\n\u0000\n\u0000>\nj xi\n\u0000pj\n, 8i 2 [n], (12)\nwhere for each j, pj 2{ 1, 2, 4, 6, 8,... }, \u0000j 2 Rd and\n↵ j 2 R. Then we have\nq\ny>(H1 )\u00001y  3\nX\nj\npj |↵ j |·k \u0000j kpj\n2 . (13)\nCorollary 6.2 shows that overparameterized two-layer ReLU\nnetwork can learn any function of the form (12) for which\n(13) is bounded. One can view (12) as two-layer neural\nnetworks with polynomial activation \u0000(z)= zp, where\n{\u0000j } are weights in the ﬁrst layer and {↵ j } are the second\nlayer. Below we give some speciﬁc examples.\nExample 6.1 (Linear functions) . F org(x)= \u0000>x, we\nhave Mg = O(k\u0000k2\n2).\nExample 6.2 (Quadratic functions) . F org(x)= x>Ax\nwhere A 2 Rd⇥d is symmetric, we can write down the eigen-\ndecomposition A = P d\nj=1 ↵ j \u0000j \u0000>\nj . Then we have g(x)=\nP d\nj=1 ↵ j (\u0000>\nj x)2, so Mg = O\n⇣P d\ni=1 |↵ j |\n⌘\n= O(kAk⇤).5\nThis is also the class of two-layer neural networks with\nquadratic activation.\nExample 6.3 (Cosine activation) . Suppose g(x)=\ncos(\u0000>x) \u0000 1 for some \u0000 2 Rd. Using Taylor series\nwe know g(x)= P 1\nj=1\n(\u00001)j (\u0000 >x)2j\n(2j)! . Thus we have\nMg = O\n⇣P 1\nj=1\nj\n(2j)! k\u0000k2j\n2\n⌘\n= O (k\u0000k2 · sinh(k\u0000k2)).\nFinally, we note that our “smoothness” requirement (13) is\nweaker than that in ( Allen-Zhu et al. , 2018a), as illustrated\nin the following example.\nExample 6.4 (A not-so-smooth function). Suppose g(x)=\n\u0000(\u0000>x), where \u0000(z)= z · arctan( z\n2 ) and k\u0000k2 \n1. We have g(x)= P 1\nj=1\n(\u00001)j\u00001 21\u00002j\n2j\u00001\n\u0000\n\u0000>x\n\u00002j\nsince\n\u0000\u0000\u0000>x\n\u0000\u0000  1. Thus Mg = O\n⇣P 1\nj=1\nj·21\u00002j\n2j\u00001 k\u0000k2j\n2\n⌘\n\nO\n⇣P 1\nj=1 21\u00002j k\u0000k2j\n⌘\n= O\n⇣\nk\u0000k2\n2\n⌘\n, so our result im-\nplies that this function is learnable by 2-layer ReLU nets.\nHowever, Allen-Zhu et al. (2018a)’s generalization theorem\nwould require P 1\nj=1\n⇣\nC\np\nlog(1/✏)\n⌘2j\n21\u00002j\n2j\u00001 to be bounded,\nwhere C is a large constant and ✏ is the target generalization\nerror. This is clearly not satisﬁed.\n7. Conclusion\nThis paper shows how to give a ﬁne-grained analysis of\nthe optimization trajectory and the generalization ability\nof overparameterized two-layer neural networks trained by\ngradient descent. We believe that our approach can also be\nuseful in analyzing overparameterized deep neural networks\nand other machine learning models.\n5kAk⇤ is the trace-norm of A.\nFine-Grained Analysis of Optimization and Generalization for Overparameterized Two-Layer Neural Networks\nAcknowledgments\nSA, WH and ZL acknowledge support from NSF, ONR,\nSimons Foundation, Schmidt Foundation, Mozilla Research,\nAmazon Research, DARPA and SRC. SSD acknowledges\nsupport from AFRL grant FA8750-17-2-0212 and DARPA\nD17AP00001. RW acknowledges support from ONR grant\nN00014-18-1-2562. Part of the work was done while SSD\nand RW were visiting the Simons Institute.\nReferences\nAllen-Zhu, Z., Li, Y ., and Liang, Y . Learning and generaliza-\ntion in overparameterized neural networks, going beyond\ntwo layers. arXiv preprint arXiv:1811.04918, 2018a.\nAllen-Zhu, Z., Li, Y ., and Song, Z. A convergence theory for\ndeep learning via over-parameterization. arXiv preprint\narXiv:1811.03962, 2018b.\nArora, S., Ge, R., Neyshabur, B., and Zhang, Y . Stronger\ngeneralization bounds for deep nets via a compression\napproach. arXiv preprint arXiv:1802.05296, 2018.\nBartlett, P . L. and Mendelson, S. Rademacher and gaussian\ncomplexities: Risk bounds and structural results. Journal\nof Machine Learning Research, 3(Nov):463–482, 2002.\nBartlett, P . L., Foster, D. J., and Telgarsky, M. J. Spectrally-\nnormalized margin bounds for neural networks. In Ad-\nvances in Neural Information Processing Systems , pp.\n6241–6250, 2017a.\nBartlett, P . L., Harvey, N., Liaw, C., and Mehrabian, A.\nNearly-tight VC-dimension and pseudodimension bounds\nfor piecewise linear neural networks. arXiv preprint\narXiv:1703.02930, 2017b.\nBelkin, M., Ma, S., and Mandal, S. To understand deep\nlearning we need to understand kernel learning. arXiv\npreprint arXiv:1802.01396, 2018.\nBrutzkus, A. and Globerson, A. Globally optimal gradi-\nent descent for a ConvNet with gaussian inputs. arXiv\npreprint arXiv:1702.07966, 2017.\nChen, Y ., Jin, C., and Y u, B. Stability and convergence trade-\noff of iterative optimization algorithms. arXiv preprint\narXiv:1804.01619, 2018.\nChizat, L. and Bach, F. On the global convergence of gradi-\nent descent for over-parameterized models using optimal\ntransport. arXiv preprint arXiv:1805.09545, 2018a.\nChizat, L. and Bach, F. A note on lazy training in su-\npervised differentiable programming. arXiv preprint\narXiv:1812.07956, 2018b.\nDaniely, A. SGD learns the conjugate kernel class of the\nnetwork. arXiv preprint arXiv:1702.08503, 2017.\nDaniely, A., Frostig, R., and Singer, Y . Toward deeper under-\nstanding of neural networks: The power of initialization\nand a dual view on expressivity. In Advances In Neural\nInformation Processing Systems, pp. 2253–2261, 2016.\nDu, S. S. and Lee, J. D. On the power of over-\nparametrization in neural networks with quadratic ac-\ntivation. arXiv preprint arXiv:1803.01206, 2018.\nDu, S. S., Lee, J. D., and Tian, Y . When is a convolutional\nﬁlter easy to learn? arXiv preprint arXiv:1709.06129 ,\n2017a.\nDu, S. S., Lee, J. D., Tian, Y ., Poczos, B., and Singh, A.\nGradient descent learns one-hidden-layer CNN: Don’t\nbe afraid of spurious local minima. arXiv preprint\narXiv:1712.00779, 2017b.\nDu, S. S., Lee, J. D., Li, H., Wang, L., and Zhai, X. Gradient\ndescent ﬁnds global minima of deep neural networks.\narXiv preprint arXiv:1811.03804, 2018a.\nDu, S. S., Wang, Y ., Zhai, X., Balakrishnan, S., Salakhutdi-\nnov, R. R., and Singh, A. How many samples are needed\nto estimate a convolutional neural network? In Advances\nin Neural Information Processing Systems , pp. 371–381,\n2018b.\nDu, S. S., Zhai, X., Poczos, B., and Singh, A. Gradient\ndescent provably optimizes over-parameterized neural\nnetworks. arXiv preprint arXiv:1810.02054, 2018c.\nDziugaite, G. K. and Roy, D. M. Computing nonvacuous\ngeneralization bounds for deep (stochastic) neural net-\nworks with many more parameters than training data.\narXiv preprint arXiv:1703.11008, 2017.\nFreeman, C. D. and Bruna, J. Topology and geometry\nof half-rectiﬁed network optimization. arXiv preprint\narXiv:1611.01540, 2016.\nGe, R., Huang, F., Jin, C., and Y uan, Y . Escaping from\nsaddle points \u0000 online stochastic gradient for tensor de-\ncomposition. In Proceedings of The 28th Conference on\nLearning Theory, pp. 797–842, 2015.\nGolowich, N., Rakhlin, A., and Shamir, O. Size-independent\nsample complexity of neural networks. arXiv preprint\narXiv:1712.06541, 2017.\nGunasekar, S., Lee, J., Soudry, D., and Srebro, N. Charac-\nterizing implicit bias in terms of optimization geometry.\narXiv preprint arXiv:1802.08246, 2018a.\nFine-Grained Analysis of Optimization and Generalization for Overparameterized Two-Layer Neural Networks\nGunasekar, S., Lee, J., Soudry, D., and Srebro, N. Implicit\nbias of gradient descent on linear convolutional networks.\narXiv preprint arXiv:1806.00468, 2018b.\nHaeffele, B. D. and Vidal, R. Global optimality in tensor\nfactorization, deep learning, and beyond. arXiv preprint\narXiv:1506.07540, 2015.\nHardt, M. and Ma, T. Identity matters in deep learning.\narXiv preprint arXiv:1611.04231, 2016.\nHardt, M., Recht, B., and Singer, Y . Train faster, generalize\nbetter: Stability of stochastic gradient descent. arXiv\npreprint arXiv:1509.01240, 2015.\nImaizumi, M. and Fukumizu, K. Deep neural networks\nlearn non-smooth functions effectively. arXiv preprint\narXiv:1802.04474, 2018.\nJacot, A., Gabriel, F., and Hongler, C. Neural tangent kernel:\nConvergence and generalization in neural networks. arXiv\npreprint arXiv:1806.07572, 2018.\nJi, Z. and Telgarsky, M. Gradient descent aligns the layers of\ndeep linear networks. arXiv preprint arXiv:1810.02032,\n2018.\nJin, C., Ge, R., Netrapalli, P ., Kakade, S. M., and Jordan,\nM. I. How to escape saddle points efﬁciently. In Proceed-\nings of the 34th International Conference on Machine\nLearning, pp. 1724–1732, 2017.\nKawaguchi, K. Deep learning without poor local minima.\nIn Advances In Neural Information Processing Systems ,\npp. 586–594, 2016.\nKonstantinos, P ., Davies, M., and V andergheynst,\nP . PAC-Bayesian margin bounds for convolutional\nneural networks-technical report. arXiv preprint\narXiv:1801.00171, 2017.\nKrizhevsky, A. and Hinton, G. Learning multiple layers\nof features from tiny images. Technical report, Citeseer,\n2009.\nLeCun, Y ., Bottou, L., Bengio, Y ., and Haffner, P . Gradient-\nbased learning applied to document recognition. Proceed-\nings of the IEEE , 86(11):2278–2324, 1998.\nLee, J. D., Simchowitz, M., Jordan, M. I., and Recht, B.\nGradient descent only converges to minimizers. In Con-\nference on Learning Theory , pp. 1246–1257, 2016.\nLi, X., Lu, J., Wang, Z., Haupt, J., and Zhao, T. On tighter\ngeneralization bound for deep neural networks: CNNs,\nResNets, and beyond. arXiv preprint arXiv:1806.05159,\n2018a.\nLi, Y . and Liang, Y . Learning overparameterized neural\nnetworks via stochastic gradient descent on structured\ndata. arXiv preprint arXiv:1808.01204, 2018.\nLi, Y . and Y uan, Y . Convergence analysis of two-layer\nneural networks with ReLU activation. arXiv preprint\narXiv:1705.09886, 2017.\nLi, Y ., Ma, T., and Zhang, H. Algorithmic regularization in\nover-parameterized matrix sensing and neural networks\nwith quadratic activations. In Conference On Learning\nTheory, pp. 2–47, 2018b.\nMa, C., Wu, L., et al. A priori estimates of the generaliza-\ntion error for two-layer neural networks. arXiv preprint\narXiv:1810.06397, 2018.\nMei, S., Montanari, A., and Nguyen, P .-M. A mean ﬁeld\nview of the landscape of two-layers neural networks.\narXiv preprint arXiv:1804.06561, 2018.\nMohri, M., Rostamizadeh, A., and Talwalkar, A. Founda-\ntions of machine learning. MIT Press, 2012.\nMou, W., Wang, L., Zhai, X., and Zheng, K. Generalization\nbounds of SGLD for non-convex learning: Two theoreti-\ncal viewpoints. arXiv preprint arXiv:1707.05947, 2017.\nNeyshabur, B., Tomioka, R., and Srebro, N. Norm-based\ncapacity control in neural networks. In Conference on\nLearning Theory, pp. 1376–1401, 2015.\nNeyshabur, B., Bhojanapalli, S., McAllester, D., and Srebro,\nN. A PAC-Bayesian approach to spectrally-normalized\nmargin bounds for neural networks. arXiv preprint\narXiv:1707.09564, 2017.\nNeyshabur, B., Li, Z., Bhojanapalli, S., LeCun, Y ., and\nSrebro, N. The role of over-parametrization in gener-\nalization of neural networks. In International Confer-\nence on Learning Representations , 2019. URL https:\n//openreview.net/forum?id=BygfghAcYX.\nNguyen, Q. and Hein, M. The loss surface of deep and wide\nneural networks. arXiv preprint arXiv:1704.08045, 2017.\nPaszke, A., Gross, S., Chintala, S., Chanan, G., Y ang, E.,\nDeVito, Z., Lin, Z., Desmaison, A., Antiga, L., and Lerer,\nA. Automatic differentiation in pytorch. 2017.\nRotskoff, G. M. and V anden-Eijnden, E. Neural networks as\ninteracting particle systems: Asymptotic convexity of the\nloss landscape and universal scaling of the approximation\nerror. arXiv preprint arXiv:1805.00915, 2018.\nSafran, I. and Shamir, O. Spurious local minima are com-\nmon in two-layer relu neural networks. arXiv preprint\narXiv:1712.08968, 2017.\nFine-Grained Analysis of Optimization and Generalization for Overparameterized Two-Layer Neural Networks\nSirignano, J. and Spiliopoulos, K. Mean ﬁeld analysis of\nneural networks. arXiv preprint arXiv:1805.01053, 2018.\nSoltanolkotabi, M. Learning ReLUs via gradient descent.\narXiv preprint arXiv:1705.04591, 2017.\nSoltanolkotabi, M., Javanmard, A., and Lee, J. D. Theo-\nretical insights into the optimization landscape of over-\nparameterized shallow neural networks. IEEE Transac-\ntions on Information Theory , 2018.\nSoudry, D. and Carmon, Y . No bad local minima: Data in-\ndependent training error guarantees for multilayer neural\nnetworks. arXiv preprint arXiv:1605.08361, 2016.\nSoudry, D., Hoffer, E., Nacson, M. S., Gunasekar, S., and\nSrebro, N. The implicit bias of gradient descent on sep-\narable data. Journal of Machine Learning Research, 19\n(70), 2018.\nTian, Y . An analytical formula of population gradient\nfor two-layered ReLU network and its applications in\nconvergence and critical point analysis. arXiv preprint\narXiv:1703.00560, 2017.\nTsuchida, R., Roosta-Khorasani, F., and Gallagher, M. In-\nvariance of weight distributions in rectiﬁed mlps. arXiv\npreprint arXiv:1711.09090, 2017.\nV enturi, L., Bandeira, A., and Bruna, J. Neural networks\nwith ﬁnite intrinsic dimension have no spurious valleys.\narXiv preprint arXiv:1802.06384, 2018.\nWei, C., Lee, J. D., Liu, Q., and Ma, T. On the margin\ntheory of feedforward neural networks. arXiv preprint\narXiv:1810.05369, 2018.\nXie, B., Liang, Y ., and Song, L. Diverse neural network\nlearns true target functions. In Artiﬁcial Intelligence and\nStatistics, pp. 1216–1224, 2017.\nY un, C., Sra, S., and Jadbabaie, A. A critical view\nof global optimality in deep learning. arXiv preprint\narXiv:1802.03487, 2018.\nZhang, C., Bengio, S., Hardt, M., Recht, B., and Vinyals, O.\nUnderstanding deep learning requires rethinking general-\nization. In Proceedings of the International Conference\non Learning Representations (ICLR), 2017 , 2017.\nZhang, X., Y u, Y ., Wang, L., and Gu, Q. Learning one-\nhidden-layer relu networks via gradient descent. arXiv\npreprint arXiv:1806.07808, 2018.\nZhong, K., Song, Z., Jain, P ., Bartlett, P . L., and Dhillon,\nI. S. Recovery guarantees for one-hidden-layer neural\nnetworks. arXiv preprint arXiv:1706.03175, 2017.\nZhou, W., V eitch, V ., Austern, M., Adams, R. P ., and Or-\nbanz, P . Non-vacuous generalization bounds at the ima-\ngenet scale: a PAC-bayesian compression approach. In\nInternational Conference on Learning Representations ,\n2019. URL https://openreview.net/forum?\nid=BJgqqsAct7.\nZhou, Y . and Liang, Y . Critical points of neural networks:\nAnalytical forms and landscape properties. arXiv preprint\narXiv:1710.11205, 2017.\nZou, D., Cao, Y ., Zhou, D., and Gu, Q. Stochastic gra-\ndient descent optimizes over-parameterized deep ReLU\nnetworks. arXiv preprint arXiv:1811.08888, 2018.",
  "values": {
    "Not socially biased": "Yes",
    "Beneficence": "Yes",
    "Transparent (to users)": "Yes",
    "Interpretable (to users)": "Yes",
    "Respect for Law and public interest": "Yes",
    "Critiqability": "Yes",
    "Respect for Persons": "Yes",
    "Explicability": "Yes",
    "Non-maleficence": "Yes",
    "Autonomy (power to decide)": "Yes",
    "Deferral to humans": "Yes",
    "Privacy": "Yes",
    "Fairness": "Yes",
    "Justice": "Yes",
    "User influence": "Yes",
    "Collective influence": "Yes"
  }
}