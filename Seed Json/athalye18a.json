{
  "pdf": "athalye18a",
  "title": "Obfuscated Gradients Give a False Sense of Security: Circumventing Defenses to Adversarial Examples",
  "author": "Anish Athalye, Nicholas Carlini, David Wagner",
  "paper_id": "athalye18a",
  "text": "Obfuscated Gradients Give a False Sense of Security:\nCircumventing Defenses to Adversarial Examples\nAnish Athalye * 1 Nicholas Carlini * 2 David Wagner2\nAbstract\nWe identify obfuscated gradients, a kind of gradi-\nent masking, as a phenomenon that leads to a false\nsense of security in defenses against adversarial\nexamples. While defenses that cause obfuscated\ngradients appear to defeat iterative optimization-\nbased attacks, we ﬁnd defenses relying on this\neffect can be circumvented. We describe charac-\nteristic behaviors of defenses exhibiting the effect,\nand for each of the three types of obfuscated gra-\ndients we discover, we develop attack techniques\nto overcome it. In a case study, examining non-\ncertiﬁed white-box-secure defenses at ICLR 2018,\nwe ﬁnd obfuscated gradients are a common occur-\nrence, with 7 of 9 defenses relying on obfuscated\ngradients. Our new attacks successfully circum-\nvent 6 completely, and 1 partially, in the original\nthreat model each paper considers.\n1. Introduction\nIn response to the susceptibility of neural networks to adver-\nsarial examples (Szegedy et al., 2013; Biggio et al., 2013),\nthere has been signiﬁcant interest recently in constructing de-\nfenses to increase the robustness of neural networks. While\nprogress has been made in understanding and defending\nagainst adversarial examples in the white-box setting, where\nthe adversary has full access to the network, a complete\nsolution has not yet been found.\nAs benchmarking against iterative optimization-based at-\ntacks (e.g., Kurakin et al. (2016a); Madry et al. (2018);\nCarlini & Wagner (2017c)) has become standard practice in\nevaluating defenses, new defenses have arisen that appear to\nbe robust against these powerful optimization-based attacks.\nWe identify one common reason why many defenses provide\n*Equal contribution 1Massachusetts Institute of Technol-\nogy 2University of California, Berkeley. Correspondence\nto: Anish Athalye <aathalye@mit.edu>, Nicholas Carlini\n<npc@cs.berkeley.edu>.\nProceedings of the 35 th International Conference on Machine\nLearning, Stockholm, Sweden, PMLR 80, 2018. Copyright 2018\nby the author(s).\napparent robustness against iterative optimization attacks:\nobfuscated gradients, a term we deﬁne as a special case of\ngradient masking (Papernot et al., 2017). Without a good\ngradient, where following the gradient does not successfully\noptimize the loss, iterative optimization-based methods can-\nnot succeed. We identify three types of obfuscated gradients:\nshattered gradients are nonexistent or incorrect gradients\ncaused either intentionally through non-differentiable op-\nerations or unintentionally through numerical instability;\nstochastic gradients depend on test-time randomness; and\nvanishing/exploding gradients in very deep computation\nresult in an unusable gradient.\nWe propose new techniques to overcome obfuscated gradi-\nents caused by these three phenomena. We address gradient\nshattering with a new attack technique we call Backward\nPass Differentiable Approximation, where we approximate\nderivatives by computing the forward pass normally and\ncomputing the backward pass using a differentiable approx-\nimation of the function. We compute gradients of random-\nized defenses by applying Expectation Over Transforma-\ntion (Athalye et al., 2017). We solve vanishing/exploding\ngradients through reparameterization and optimize over a\nspace where gradients do not explode/vanish.\nTo investigate the prevalence of obfuscated gradients and\nunderstand the applicability of these attack techniques, we\nuse as a case study the ICLR 2018 non-certiﬁed defenses\nthat claim white-box robustness. We ﬁnd that obfuscated\ngradients are a common occurrence, with 7 of 9 defenses\nrelying on this phenomenon. Applying the new attack tech-\nniques we develop, we overcome obfuscated gradients and\ncircumvent 6 of them completely, and 1 partially, under the\noriginal threat model of each paper. Along with this, we\noffer an analysis of the evaluations performed in the papers.\nAdditionally, we hope to provide researchers with a common\nbaseline of knowledge, description of attack techniques,\nand common evaluation pitfalls, so that future defenses can\navoid falling vulnerable to these same attack approaches.\nTo promote reproducible research, we release our re-\nimplementation of each of these defenses, along with imple-\nmentations of our attacks for each. 1\n1 https://github.com/anishathalye/obfuscated-gradients\nObfuscated Gradients Give a False Sense of Security: Circumventing Defenses to Adversarial Examples\n2. Preliminaries\n2.1. Notation\nWe consider a neural network f (·) used for classiﬁcation\nwhere f (x)i represents the probability that image x cor-\nresponds to label i. We classify images, represented as\nx∈ [0, 1]w·h·c for ac-channel image of widthw and height\nh. We usefj(·) to refer to layerj of the neural network, and\nf 1..j(·) the composition of layers 1 throughj. We denote\nthe classiﬁcation of the network asc(x) = arg maxif (x)i,\nandc∗(x) denotes the true label.\n2.2. Adversarial Examples\nGiven an imagex and classiﬁerf (·), an adversarial example\n(Szegedy et al., 2013)x′ satisﬁes two properties:D(x,x′)\nis small for some distance metric D, and c(x′)̸= c∗(x).\nThat is, for images,x andx′ appear visually similar butx′\nis classiﬁed incorrectly.\nIn this paper, we use theℓ∞ andℓ2 distortion metrics to mea-\nsure similarity. Two images which have a small distortion\nunder either of these metrics will appear visually identical.\nWe reportℓ∞ distance in the normalized [0, 1] space, so that\na distortion of 0.031 corresponds to 8/256, andℓ2 distance\nas the total root-mean-square distortion normalized by the\ntotal number of pixels (as is done in prior work).\n2.3. Datasets & Models\nWe evaluate these defenses on the same datasets on which\nthey claim robustness.\nIf a defense argues security on MNIST and any other dataset,\nwe only evaluate the defense on the larger dataset. On\nMNIST and CIFAR-10, we evaluate defenses over the en-\ntire test set and generate untargeted adversarial examples.\nOn ImageNet, we evaluate over 1000 randomly selected\nimages in the test set, construct targeted adversarial exam-\nples with randomly selected target classes, and report attack\nsuccess rate in addition to model accuracy. Generating tar-\ngeted adversarial examples is a strictly harder problem that\nwe believe is a more meaningful metric for evaluating at-\ntacks. 2 Conversely, for a defender, the harder task is to\nargue robustness to untargeted attacks.\nWe use standard models for each dataset. For MNIST we\nuse a standard 5-layer convolutional neural network which\nreaches 99.3% accuracy. On CIFAR-10 we train a wide\nResNet (Zagoruyko & Komodakis, 2016; He et al., 2016)\nto 95% accuracy. For ImageNet we use the InceptionV3\n(Szegedy et al., 2016) network which reaches 78.0% top-1\nand 93.9% top-5 accuracy.\n2Misclassiﬁcation is a less meaningful metric on ImageNet,\nwhere a misclassiﬁcation of closely related classes (e.g., a German\nshepherd classiﬁed as a Doberman) may not be meaningful.\n2.4. Threat Models\nPrior work considers adversarial examples in white-box and\nblack-box threat models. In this paper, we consider defenses\ndesigned for the white-box setting, where the adversary has\nfull access to the neural network classiﬁer (architecture and\nweights) and defense, but not test-time randomness (only\nthe distribution). We evaluate each defense under the threat\nmodel under which it claims to be secure (e.g., boundedℓ∞\ndistortion of ϵ = 0.031). It often easy to ﬁnd impercepti-\nbly perturbed adversarial examples by violating the threat\nmodel, but by doing so under the original threat model, we\nshow that the original evaluations were inadequate and the\nresultant claims of security were incorrect.\n2.5. Attack Methods\nWe construct adversarial examples with iterative\noptimization-based methods. For a given instance\nx, these attacks attempt to search for a δ such that\nc(x +δ)̸= c∗(x) either minimizing∥δ∥, or maximizing\nthe classiﬁcation loss onf (x +δ). To generateℓ∞ bounded\nadversarial examples we use Projected Gradient Descent\n(PGD) conﬁned to a speciﬁedℓ∞ ball; for ℓ2, we use the\nLagrangian relaxation of Carlini & Wagner (2017c). We\nuse between 100 and 10,000 iterations of gradient descent,\nas needed to obtain convergance. The speciﬁc choice of\noptimizer is far less important than choosing to use iterative\noptimization-based methods (Madry et al., 2018).\n3. Obfuscated Gradients\nA defense is said to cause gradient masking if it “does\nnot have useful gradients” for generating adversarial exam-\nples (Papernot et al., 2017); gradient masking is known to\nbe an incomplete defense to adversarial examples (Papernot\net al., 2017; Tram`er et al., 2018). Despite this, we observe\nthat 7 of the ICLR 2018 defenses rely on this effect.\nTo contrast from previous defenses which cause gradient\nmasking by learning to break gradient descent (e.g., by learn-\ning to make the gradients point the wrong direction (Tram`er\net al., 2018)), we refer to the case where defenses are de-\nsigned in such a way that the constructed defense necessarily\ncauses gradient masking as obfuscated gradients. We dis-\ncover three ways in which defenses obfuscate gradients (we\nuse this word because in these cases, it is the defense creator\nwho has obfuscated the gradient information); we brieﬂy\ndeﬁne and discuss each of them.\nShattered Gradients are caused when a defense is non-\ndifferentiable, introduces numeric instability, or otherwise\ncauses a gradient to be nonexistent or incorrect. Defenses\nthat cause gradient shattering can do so unintentionally,\nby using differentiable operations but where following the\ngradient does not maximize classiﬁcation loss globally.\nObfuscated Gradients Give a False Sense of Security: Circumventing Defenses to Adversarial Examples\nStochastic Gradients are caused by randomized defenses,\nwhere either the network itself is randomized or the input\nis randomly transformed before being fed to the classiﬁer,\ncausing the gradients to become randomized. This causes\nmethods using a single sample of the randomness to incor-\nrectly estimate the true gradient.\nExploding & Vanishing Gradientsare often caused by de-\nfenses that consist of multiple iterations of neural network\nevaluation, feeding the output of one computation as the\ninput of the next. This type of computation, when unrolled,\ncan be viewed as an extremely deep neural network evalua-\ntion, which can cause vanishing/exploding gradients.\n3.1. Identifying Obfuscated & Masked Gradients\nSome defenses intentionally break gradient descent and\ncause obfuscated gradients. However, others defenses unin-\ntentionally break gradient descent, but the cause of gradient\ndescent being broken is a direct result of the design of the\nneural network. We discuss below characteristic behaviors\nof defenses which cause this to occur. These behaviors may\nnot perfectly characterize all cases of masked gradients.\nOne-step attacks perform better than iterative attacks.\nIterative optimization-based attacks applied in a white-box\nsetting are strictly stronger than single-step attacks and\nshould give strictly superior performance. If single-step\nmethods give performance superior to iterative methods, it\nis likely that the iterative attack is becoming stuck in its\noptimization search at a local minimum.\nBlack-box attacks are better than white-box attacks.\nThe black-box threat model is a strict subset of the white-\nbox threat model, so attacks in the white-box setting should\nperform better; if a defense is obfuscating gradients, then\nblack-box attacks (which do not use the gradient) often per-\nform better than white-box attacks (Papernot et al., 2017).\nUnbounded attacks do not reach 100% success. With\nunbounded distortion, any classiﬁer should have0% robust-\nness to attack. If an attack does not reach 100% success\nwith sufﬁciently large distortion bound, this indicates the\nattack is not performing optimally against the defense, and\nthe attack should be improved.\nRandom sampling ﬁnds adversarial examples. Brute-\nforce random search (e.g., randomly sampling 105 or more\npoints) within someϵ-ball should not ﬁnd adversarial exam-\nples when gradient-based attacks do not.\nIncreasing distortion bound does not increase success.\nA larger distortion bound should monotonically increase\nattack success rate; signiﬁcantly increasing distortion bound\nshould result in signiﬁcantly higher attack success rate.\n4. Attack Techniques\nGenerating adversarial examples through optimization-\nbased methods requires useful gradients obtained through\nbackpropagation (Rumelhart et al., 1986). Many defenses\ntherefore either intentionally or unintentionally cause gradi-\nent descent to fail because of obfuscated gradients caused\nby gradient shattering, stochastic gradients, or vanish-\ning/exploding gradients. We discuss a number of techniques\nthat we develop to overcome obfuscated gradients.\n4.1. Backward Pass Differentiable Approximation\nShattered gradients, caused either unintentionally, e.g. by\nnumerical instability, or intentionally, e.g. by using non-\ndifferentiable operations, result in nonexistent or incorrect\ngradients. To attack defenses where gradients are not readily\navailable, we introduce a technique we call Backward Pass\nDifferentiable Approximation (BPDA) 3.\n4.1.1. A S PECIAL CASE :\nTHE STRAIGHT -T HROUGH ESTIMATOR\nAs a special case, we ﬁrst discuss what amounts to the\nstraight-through estimator (Bengio et al., 2013) applied to\nconstructing adversarial examples.\nMany non-differentiable defenses can be expressed as fol-\nlows: given a pre-trained classiﬁerf (·), construct a prepro-\ncessorg(·) and let the secured classiﬁer ˆf (x) = f (g(x))\nwhere the preprocessorg(·) satisﬁesg(x)≈x (e.g., such a\ng(·) may perform image denoising to remove the adversar-\nial perturbation, as in Guo et al. (2018)). Ifg(·) is smooth\nand differentiable, then computing gradients through the\ncombined network ˆf is often sufﬁcient to circumvent the\ndefense (Carlini & Wagner, 2017b). However, recent work\nhas constructed functions g(·) which are neither smooth\nnor differentiable, and therefore can not be backpropagated\nthrough to generate adversarial examples with a white-box\nattack that requires gradient signal.\nBecauseg is constructed with the property that g(x)≈x,\nwe can approximate its derivative as the derivative of the\nidentity function:∇xg(x)≈∇ xx = 1. Therefore, we can\napproximate the derivative off (g(x)) at the point ˆx as:\n∇xf (g(x))|x=ˆx≈∇ xf (x)|x=g(ˆx)\nThis allows us to compute gradients and therefore mount a\nwhite-box attack. Conceptually, this attack is simple. We\nperform forward propagation through the neural network as\nusual, but on the backward pass, we replace g(·) with the\nidentity function. In practice, the implementation can be ex-\npressed in an even simpler way: we approximate∇xf (g(x))\nby evaluating∇xf (x) at the point g(x). This gives us an\n3The BPDA approach can be used on an arbitrary network,\neven if it is already differentiable, to obtain a more useful gradient.\nObfuscated Gradients Give a False Sense of Security: Circumventing Defenses to Adversarial Examples\napproximation of the true gradient, and while not perfect, is\nsufﬁciently useful that when averaged over many iterations\nof gradient descent still generates an adversarial example.\nThe math behind the validity of this approach is similar to\nthe special case.\n4.1.2. G ENERALIZED ATTACK : BPDA\nWhile the above attack is effective for a simple class of\nnetworks expressible asf (g(x)) wheng(x)≈ x, it is not\nfully general. We now generalize the above approach into\nour full attack, which we call Backward Pass Differentiable\nApproximation (BPDA).\nLetf (·) = f 1...j(·) be a neural network, and letfi(·) be a\nnon-differentiable (or not usefully-differentiable) layer. To\napproximate∇xf (x), we ﬁrst ﬁnd a differentiable approxi-\nmationg(x) such thatg(x)≈fi(x). Then, we can approxi-\nmate∇xf (x) by performing the forward pass throughf (·)\n(and in particular, computing a forward pass throughfi(x)),\nbut on the backward pass, replacingfi(x) withg(x). Note\nthat we perform this replacement only on the backward pass.\nAs long as the two functions are similar, we ﬁnd that the\nslightly inaccurate gradients still prove useful in construct-\ning an adversarial example. Applying BPDA often requires\nmore iterations of gradient descent than without because\neach individual gradient descent step is not exactly correct.\nWe have found applying BPDA is often necessary: replacing\nfi(·) withg(·) on both the forward and backward pass is\neither completely ineffective (e.g. with Song et al. (2018)) or\nmany times less effective (e.g. with Buckman et al. (2018)).\n4.2. Attacking Randomized Classiﬁers\nStochastic gradients arise when using randomized transfor-\nmations to the input before feeding it to the classiﬁer or\nwhen using a stochastic classiﬁer. When using optimization-\nbased attacks on defenses that employ these techniques, it is\nnecessary to estimate the gradient of the stochastic function.\nExpectation over Transformation. For defenses that em-\nploy randomized transformations to the input, we apply Ex-\npectation over Transformation (EOT) (Athalye et al., 2017)\nto correctly compute the gradient over the expected trans-\nformation to the input.\nWhen attacking a classiﬁerf (·) that ﬁrst randomly trans-\nforms its input according to a functiont(·) sampled from a\ndistribution of transformationsT , EOT optimizes the expec-\ntation over the transformation Et∼Tf (t(x)). The optimiza-\ntion problem can be solved by gradient descent, noting that\n∇Et∼Tf (t(x)) = Et∼T∇f (t(x)), differentiating through\nthe classiﬁer and transformation, and approximating the\nexpectation with samples at each gradient descent step.\n4.3. Reparameterization\nWe solve vanishing/exploding gradients by reparameteriza-\ntion. Assume we are given a classiﬁerf (g(x)) whereg(·)\nperforms some optimization loop to transform the inputx\nto a new input ˆx. Often times, this optimization loop means\nthat differentiating throughg(·), while possible, yields ex-\nploding or vanishing gradients.\nTo resolve this, we make a change-of-variable x = h(z)\nfor some function h(·) such that g(h(z)) = h(z) for all\nz, but h(·) is differentiable. For example, if g(·) projects\nsamples to some manifold in a speciﬁc manner, we might\nconstructh(z) to return points exclusively on the manifold.\nThis allows us to compute gradients through f (h(z)) and\nthereby circumvent the defense.\n5. Case Study: ICLR 2018 Defenses\nAs a case study for evaluating the prevalence of obfuscated\ngradients, we study the ICLR 2018 non-certiﬁed defenses\nthat argue robustness in a white-box threat model. Each of\nthese defenses argues a high robustness to adaptive, white-\nbox attacks. We ﬁnd that seven of these nine defenses rely\non this phenomenon, and we demonstrate that our tech-\nniques can completely circumvent six of those (and partially\ncircumvent one) that rely on obfuscated gradients. We omit\ntwo defenses with provable security claims (Raghunathan\net al., 2018; Sinha et al., 2018) and one that only argues\nblack-box security (Tram`er et al., 2018). We include one\npaper, Ma et al. (2018), that was not proposed as a defense\nper se, but suggests a method to detect adversarial examples.\nThere is an asymmetry in attacking defenses versus con-\nstructing robust defenses: to show a defense can be by-\npassed, it is only necessary to demonstrate one way to do\nso; in contrast, a defender must show no attack can succeed.\nTable 1 summarizes our results. Of the 9 accepted papers,\n7 rely on obfuscated gradients. Two of these defenses\nargue robustness on ImageNet, a much harder task than\nCIFAR-10; and one argues robustness on MNIST, a much\neasier task than CIFAR-10. As such, comparing defenses\nacross datasets is difﬁcult.\n5.1. Non-obfuscated Gradients\n5.1.1. A DVERSARIAL TRAINING\nDefense Details. Originally proposed by Goodfellow\net al. (2014b), adversarial training solves a min-max game\nthrough a conceptually simple process: train on adversarial\nexamples until the model learns to classify them correctly.\nGiven training dataX and loss functionℓ(·), standard train-\ning chooses network weightsθ as\nθ∗ = arg min\nθ\nE\n(x,y)∈X\nℓ(x;y;Fθ).\nObfuscated Gradients Give a False Sense of Security: Circumventing Defenses to Adversarial Examples\nDefense Dataset Distance Accuracy\nBuckman et al. (2018) CIFAR 0.031 (ℓ∞) 0%∗\nMa et al. (2018) CIFAR 0.031 (ℓ∞) 5%\nGuo et al. (2018) ImageNet 0.005 (ℓ2) 0%∗\nDhillon et al. (2018) CIFAR 0.031 (ℓ∞) 0%\nXie et al. (2018) ImageNet 0.031 (ℓ∞) 0%∗\nSong et al. (2018) CIFAR 0.031 (ℓ∞) 9%∗\nSamangouei et al.\n(2018)\nMNIST 0.005 (ℓ2) 55%∗∗\nMadry et al. (2018) CIFAR 0.031 (ℓ∞) 47%\nNa et al. (2018) CIFAR 0.015 (ℓ∞) 15%\nTable 1. Summary of Results: Seven of nine defense techniques\naccepted at ICLR 2018 cause obfuscated gradients and are vulner-\nable to our attacks. Defenses denoted with∗ propose combining\nadversarial training; we report here the defense alone, see§5 for\nfull numbers. The fundamental principle behind the defense de-\nnoted with∗∗ has 0% accuracy; in practice, imperfections cause\nthe theoretically optimal attack to fail, see§5.4.2 for details.\nWe study the adversarial training approach of Madry et al.\n(2018) which for a givenϵ-ball solves\nθ∗ = arg min\nθ\nE\n(x,y)∈X\n[\nmax\nδ∈[−ϵ,ϵ]N\nℓ(x +δ;y;Fθ)\n]\n.\nTo approximately solve this formulation, the authors solve\nthe inner maximization problem by generating adversarial\nexamples using projected gradient descent.\nDiscussion. We believe this approach does not cause ob-\nfuscated gradients: our experiments with optimization-\nbased attacks do succeed with some probability (but do\nnot invalidate the claims in the paper). Further, the authors’\nevaluation of this defense performs all of the tests for charac-\nteristic behaviors of obfuscated gradients that we list. How-\never, we note that (1) adversarial retraining has been shown\nto be difﬁcult at ImageNet scale (Kurakin et al., 2016b),\nand (2) training exclusively on ℓ∞ adversarial examples\nprovides only limited robustness to adversarial examples\nunder other distortion metrics (Sharma & Chen, 2017).\n5.1.2. C ASCADE ADVERSARIAL TRAINING\nCascade adversarial machine learning (Na et al., 2018) is\nclosely related to the above defense. The main difference\nis that instead of using iterative methods to generate ad-\nversarial examples at each mini-batch, the authors train a\nﬁrst model, generate adversarial examples (with iterative\nmethods) on that model, add these to the training set, and\nthen train a second model on the augmented dataset only\nsingle-step methods for efﬁciency. Additionally, the authors\nconstruct a “uniﬁed embedding” and enforce that the clean\nand adversarial logits are close under some metric.\nDiscussion. Again, as above, we are unable to reduce the\nclaims made by the authors. However, these claims are\nweaker than other defenses (because the authors correctly\nperformed a strong optimization-based attack (Carlini &\nWagner, 2017c)): 16% accuracy withϵ =.015, compared\nto over 70% at the same perturbation budget with adversarial\ntraining as in Madry et al. (2018).\n5.2. Gradient Shattering\n5.2.1. T HERMOMETER ENCODING\nDefense Details. In contrast to prior work (Szegedy et al.,\n2013) which viewed adversarial examples as “blind spots”\nin neural networks, Goodfellow et al. (2014b) argue that the\nreason adversarial examples exist is that neural networks be-\nhave in a largely linear manner. The purpose of thermometer\nencoding is to break this linearity.\nGiven an imagex, for each pixel colorxi,j,c, thel-level ther-\nmometer encodingτ (xi,j,c) is al-dimensional vector where\nτ (xi,j,c)k = 1 if if xi,j,c >k/l , and 0 otherwise (e.g., for\na 10-level thermometer encoding,τ (0.66) = 1111110000 ).\nDue to the discrete nature of thermometer encoded val-\nues, it is not possible to directly perform gradient descent\non a thermometer encoded neural network. The authors\ntherefore construct Logit-Space Projected Gradient Ascent\n(LS-PGA) as an attack over the discrete thermometer en-\ncoded inputs. Using this attack, the authors perform the\nadversarial training of Madry et al. (2018) on thermometer\nencoded networks.\nOn CIFAR-10, just performing thermometer encoding was\nfound to give 50% accuracy within ϵ = 0.031 underℓ∞\ndistortion. By performing adversarial training with 7 steps\nof LS-PGA, robustness increased to 80%.\nDiscussion. While the intention behind this defense is to\nbreak the local linearity of neural networks, we ﬁnd that\nthis defense in fact causes gradient shattering. This can\nbe observed through their black-box attack evaluation: ad-\nversarial examples generated on a standard adversarially\ntrained model transfer to a thermometer encoded model re-\nducing the accuracy to 67%, well below the 80% robustness\nto the white-box iterative attack.\nEvaluation. We use the BPDA approach from §4.1.2,\nwhere we letf (x) = τ (x). Observe that if we deﬁne\nˆτ (xi,j,c)k = min(max(xi,j,c−k/l, 0), 1)\nthen\nτ (xi,j,c)k = ﬂoor (ˆτ (xi,j,c)k)\nso we can letg(x) = ˆτ (x) and replace the backwards pass\nwith the functiong(·).\nObfuscated Gradients Give a False Sense of Security: Circumventing Defenses to Adversarial Examples\nLS-PGA only reduces model accuracy to 50% on a\nthermometer-encoded model trained without adversarial\ntraining (bounded by ϵ = 0.031). In contrast, we achieve\n1% model accuracy with the lowerϵ = 0.015 (and 0% with\nϵ = 0.031). This shows no measurable improvement from\nstandard models, trained without thermometer encoding.\nWhen we attack a thermometer-encoded adversarially\ntrained model 4, we are able to reproduce the 80% accu-\nracy at ϵ = 0.031 claim against LS-PGA. However, our\nattack reduces model accuracy to 30%. This is signiﬁcantly\nweaker than the original Madry et al. (2018) model that\ndoes not use thermometer encoding. Because this model is\ntrained against the (comparatively weak) LS-PGA attack, it\nis unable to adapt to the stronger attack we present above.\n5.2.2. I NPUT TRANSFORMATIONS\nDefense Details. Guo et al. (2018) propose ﬁve input\ntransformations to counter adversarial examples.\nAs a baseline, the authors evaluate image cropping and\nrescaling, bit-depth reduction , and JPEG compression .\nThen the authors suggest two new transformations: (a) ran-\ndomly drop pixels and restore them by performing total\nvariance minimization; and (b) image quilting: reconstruct\nimages by replacing small patches with patches from “clean”\nimages, using minimum graph cuts in overlapping boundary\nregions to remove edge artifacts.\nThe authors explore different combinations of input trans-\nformations along with different underlying ImageNet classi-\nﬁers, including adversarially trained models. They ﬁnd that\ninput transformations provide protection even with a vanilla\nclassiﬁer.\nDiscussion. The authors ﬁnd that a ResNet-50 classiﬁer\nprovides a varying degree of accuracy for each of the ﬁve\nproposed input transformations under the strongest attack\nwith a normalizedℓ2 dissimilarity of 0.01, with the strongest\ndefenses achieving over 60% top-1 accuracy. We reproduce\nthese results when evaluating an InceptionV3 classiﬁer.\nThe authors do not succeed in white-box attacks, credit-\ning lack of access to test-time randomness as “particularly\ncrucial in developing strong defenses” (Guo et al., 2018). 5\nEvaluation. It is possible to bypass each defense inde-\npendently (and ensembles of defenses usually are not much\nstronger than the strongest sub-component (He et al., 2017)).\nWe circumvent image cropping and rescaling with a direct\n4That is, a thermometer encoded model that is trained using\nthe approach of (Madry et al., 2018).\n5This defense may be stronger in a threat model where the\nadversary does not have complete information about the exact\nquilting process used (personal communication with authors).\napplication of EOT. To circumvent bit-depth reduction and\nJPEG compression, we use BPDA and approximate the\nbackward pass with the identity function. To circumvent\ntotal variance minimization and image quilting, which are\nboth non-differentiable and randomized, we apply EOT and\nuse BPDA to approximate the gradient through the transfor-\nmation. With our attack, we achieve 100% targeted attack\nsuccess rate and accuracy drops to 0% for the strongest de-\nfense under the smallest perturbation budget considered in\nGuo et al. (2018), a root-mean-square perturbation of 0.05\n(and a “normalized” ℓ2 perturbation as deﬁned in Guo et al.\n(2018) of 0.01).\n5.2.3. L OCAL INTRINSIC DIMENSIONALITY (LID)\nLID is a general-purpose metric that measures the distance\nfrom an input to its neighbors. Ma et al. (2018) propose\nusing LID to characterize properties of adversarial examples.\nThe authors emphasize that this classiﬁeris not intended as a\ndefense against adversarial examples 6, however the authors\nargue that it is a robust method for detecting adversarial\nexamples that is not easy to evade by attempting their own\nadaptive attack and showing it fails.\nAnalysis Overview. Instead of actively attacking the de-\ntection method, we ﬁnd that LID is not able to detect high\nconﬁdence adversarial examples (Carlini & Wagner, 2017a),\neven in the unrealistic threat model where the adversary is\nentirely oblivious to the defense and generates adversarial\nexamples on the original classiﬁer. A full discussion of this\nattack is given in Supplement Section 1.\n5.3. Stochastic Gradients\n5.3.1. S TOCHASTIC ACTIVATION PRUNING (SAP)\nDefense Details. SAP (Dhillon et al., 2018) introduces\nrandomness into the evaluation of a neural network to de-\nfend against adversarial examples. SAP randomly drops\nsome neurons of each layer fi to 0 with probability pro-\nportional to their absolute value. That is, SAP essentially\napplies dropout at each layer where instead of dropping with\nuniform probability, nodes are dropped with a weighted dis-\ntribution. Values which are retained are scaled up (as is\ndone in dropout) to retain accuracy. Applying SAP de-\ncreases clean classiﬁcation accuracy slightly, with a higher\ndrop probability decreasing accuracy, but increasing robust-\nness. We study various levels of drop probability and ﬁnd\nthey lead to similar robustness numbers.\nDiscussion. The authors only evaluate SAP by taking a\nsingle step in the gradient direction (Dhillon et al., 2018).\nWhile taking a single step in the direction of the gradient\n6Personal communication with authors.\nObfuscated Gradients Give a False Sense of Security: Circumventing Defenses to Adversarial Examples\ncan be effective on non-randomized neural networks, when\nrandomization is used, computing the gradient with respect\nto one sample of the randomness is ineffective.\nEvaluation. To resolve this difﬁculty, we estimate the gra-\ndients by computing the expectation over instantiations of\nrandomness. At each iteration of gradient descent, instead\nof taking a step in the direction of∇xf (x) we move in the\ndirection of ∑k\ni=1∇xf (x) where each invocation is ran-\ndomized with SAP. We have found that choosingk = 10\nprovides useful gradients. We additionally had to resolve\na numerical instability when computing gradients: this de-\nfense caused computing a backward pass to cause exploding\ngradients due to division by numbers very close to 0.\nWith these approaches, we are able to reduce SAP model\naccuracy to 9% at ϵ = .015, and 0% at ϵ = 0.031. If\nwe consider an attack successful only when an example\nis classiﬁed incorrectly10 times out of 10 (and consider it\ncorrectly classiﬁed if it is ever classiﬁed as the correct label),\nmodel accuracy is below 10% withϵ = 0.031.\n5.3.2. M ITIGATING THROUGH RANDOMIZATION\nDefense Details. Xie et al. (2018) propose to defend\nagainst adversarial examples by adding a randomization\nlayer before the input to the classiﬁer. For a classiﬁer that\ntakes a 299× 299 input, the defense ﬁrst randomly rescales\nthe image to ar×r image, withr∈ [299, 331), and then\nrandomly zero-pads the image so that the result is331×331.\nThe output is then fed to the classiﬁer.\nDiscussion. The authors consider three attack scenarios:\nvanilla attack (an attack on the original classiﬁer), single-\npattern attack (an attack assuming some ﬁxed randomization\npattern), and ensemble-pattern attack (an attack over a small\nensemble of ﬁxed randomization patterns). The authors\nstrongest attack reduces InceptionV3 model accuracy to\n32.8% top-1 accuracy (over images that were originally\nclassiﬁed correctly).\nThe authors dismiss a stronger attack over larger choices\nof randomness, stating that it would be “computationally\nimpossible” (emphasis ours) and that such an attack “may\nnot even converge” (Xie et al., 2018).\nEvaluation. We ﬁnd the authors’ ensemble attack overﬁts\nto the ensemble with ﬁxed randomization. We bypass this\ndefense by applying EOT, optimizing over the (in this case,\ndiscrete) distribution of transformations.\nUsing this attack, even if we consider the attack successful\nonly when an example is classiﬁed incorrectly10 times out\nof 10, we achieve 100% targeted attack success rate and\nreduce the accuracy of the classiﬁer from 32.8% to 0.0%\nwith a maximumℓ∞ perturbation ofϵ = 0.031.\n5.4. Vanishing & Exploding Gradients\n5.4.1. P IXEL DEFEND\nDefense Details. Song et al. (2018) propose using a\nPixelCNN generative model to project a potential adver-\nsarial example back onto the data manifold before feeding\nit into a classiﬁer. The authors argue that adversarial ex-\namples mainly lie in the low-probability region of the data\ndistribution. PixelDefend “puriﬁes” adversarially perturbed\nimages prior to classiﬁcation by using a greedy decoding\nprocedure to approximate ﬁnding the highest probability\nexample within anϵ-ball of the input image.\nDiscussion. The authors evaluate PixelDefend on\nCIFAR-10 over various classiﬁers and perturbation budgets.\nWith a maximumℓ∞ perturbation ofϵ = 0.031, PixelDe-\nfend claims 46% accuracy (with a vanilla ResNet classiﬁer).\nThe authors dismiss the possibility of end-to-end attacks on\nPixelDefend due to the difﬁculty of differentiating through\nan unrolled version of PixelDefend due to vanishing\ngradients and computation cost.\nEvaluation. We sidestep the problem of computing gradi-\nents through an unrolled version of PixelDefend by approxi-\nmating gradients with BPDA, and we successfully mount\nan end-to-end attack using this technique 7. With this attack,\nwe can reduce the accuracy of a naturally trained classiﬁer\nwhich achieves 95% accuracy to 9% with a maximumℓ∞\nperturbation ofϵ = 0.031. We ﬁnd that combining adversar-\nial training (Madry et al., 2018) with PixelDefend provides\nno additional robustness over just using the adversarially\ntrained classiﬁer.\n5.4.2. D EFENSE -GAN\nDefense-GAN (Samangouei et al., 2018) uses a Generative\nAdversarial Network (Goodfellow et al., 2014a) to project\nsamples onto the manifold of the generator before classi-\nfying them. That is, the intuition behind this defense is\nnearly identical to PixelDefend, but using a GAN instead\nof a PixelCNN. We therefore summarize results here and\npresent the full details in Supplement Section 2.\nAnalysis Overview. Defense-GAN is not argued secure\non CIFAR-10, so we use MNIST. We ﬁnd that adversarial\nexamples exist on the manifold deﬁned by the generator.\nThat is, we show that we are able to construct an adversarial\nexamplex′ = G(z) so that x′≈ x butc(x)̸= c(x′). As\nsuch, a perfect projector would not modify this examplex′\nbecause it exists on the manifold described by the generator.\nHowever, while this attack would defeat a perfect projector\n7In place of a PixelCNN, due to the availability of a pre-trained\nmodel, we use a PixelCNN++ (Salimans et al., 2017) and discretize\nthe mixture of logistics to produce a 256-way softmax.\nObfuscated Gradients Give a False Sense of Security: Circumventing Defenses to Adversarial Examples\nmappingx to its nearest point onG(z), the imperfect gradi-\nent descent based approach taken by Defense-GAN does not\nperfectly preserve points on the manifold. We therefore con-\nstruct a second attack using BPDA to evade Defense-GAN,\nalthough at only a 45% success rate.\n6. Discussion\nHaving demonstrated attacks on these seven defenses, we\nnow take a step back and discuss the method of evaluating a\ndefense against adversarial examples.\nThe papers we study use a variety of approaches in eval-\nuating robustness of the proposed defenses. We list what\nwe believe to be the most important points to keep in mind\nwhile building and evaluating defenses. Much of what we\ndescribe below has been discussed in prior work (Carlini &\nWagner, 2017a; Madry et al., 2018); we repeat these points\nhere and offer our own perspective for completeness.\n6.1. Deﬁne a (realistic) threat model\nA threat model speciﬁes the conditions under which a de-\nfense argues security: a precise threat model allows for an\nexact understanding of the setting under which the defense is\nmeant to work. Prior work has used words including white-\nbox, grey-box, black-box, and no-box to describe slightly\ndifferent threat models, often overloading the same word.\nInstead of attempting to, yet again, redeﬁne the vocabulary,\nwe enumerate the various aspects of a defense that might\nbe revealed to the adversary or held secret to the defender:\nmodel architecture and model weights; training algorithm\nand training data; test time randomness (either the values\nchosen or the distribution); and, if the model weights are\nheld secret, whether query access is allowed (and if so, the\ntype of output, e.g. logits or only the top label).\nWhile there are some aspects of a defense that might be\nheld secret, threat models should not contain unrealistic\nconstraints. We believe any compelling threat model should\nat the very least grant knowledge of the model architecture,\ntraining algorithm, and allow query access.\nIt is not meaningful to restrict the computational power of\nan adversary artiﬁcially (e.g., to fewer than several thousand\nattack iterations). If two defenses are equally robust but gen-\nerating adversarial examples on one takes one second and\nanother takes ten seconds, the robustness has not increased.\n6.2. Make speciﬁc, testable claims\nSpeciﬁc, testable claims in a clear threat model precisely\nconvey the claimed robustness of a defense. For example, a\ncomplete claim might be: “We achieve 90% accuracy when\nbounded byℓ∞ distortion withϵ = 0.031, when the attacker\nhas full white-box access.”\nIn this paper, we study all papers under the threat model\nthe authors deﬁne. However, if a paper is evaluated under\na different threat model, explicitly stating so makes it clear\nthat the original paper’s claims are not being violated.\nA defense being speciﬁed completely, with all hyperpa-\nrameters given, is a prerequisite for claims to be testable.\nReleasing source code and a pre-trained model along with\nthe paper describing a speciﬁc threat model and robustness\nclaims is perhaps the most useful method of making testable\nclaims. At the time of writing this paper, four of the defenses\nwe study made complete source code available (Madry et al.,\n2018; Ma et al., 2018; Guo et al., 2018; Xie et al., 2018).\n6.3. Evaluate against adaptive attacks\nA strong defense is robust not only against existing attacks,\nbut also against future attacks within the speciﬁed threat\nmodel. A necessary component of any defense proposal is\ntherefore an attempt at an adaptive attack.\nAn adaptive attack is one that is constructed after a defense\nhas been completely speciﬁed, where the adversary takes ad-\nvantage of knowledge of the defense and is only restricted by\nthe threat model. One useful attack approach is to perform\nmany attacks and report the mean over the best attackper im-\nage. That is, for a set of attacksa∈A instead of reporting\nthe value min\na∈A\nmean\nx∈A\nf (a(x)) report mean\nx∈A\nmin\na∈A\nf (a(x)).\nIf a defense is modiﬁed after an evaluation, an adaptive\nattack is one that considers knowledge of the new defense.\nIn this way, concluding an evaluation with a ﬁnal adaptive\nattack can be seen as analogous to evaluating a model on\nthe test data.\n7. Conclusion\nConstructing defenses to adversarial examples requires de-\nfending against not only existing attacks but also future\nattacks that may be developed. In this paper, we identify\nobfuscated gradients, a phenomenon exhibited by certain\ndefenses that makes standard gradient-based methods fail\nto generate adversarial examples. We develop three attack\ntechniques to bypass three different types of obfuscated gra-\ndients. To evaluate the applicability of our techniques, we\nuse the ICLR 2018 defenses as a case study, circumventing\nseven of nine accepted defenses.\nMore generally, we hope that future work will be able to\navoid relying on obfuscated gradients (and other methods\nthat only prevent gradient descent-based attacks) for per-\nceived robustness, and use our evaluation approach to detect\nwhen this occurs. Defending against adversarial examples\nis an important area of research and we believe performing\na careful, thorough evaluation is a critical step that can not\nbe overlooked when designing defenses.\nObfuscated Gradients Give a False Sense of Security: Circumventing Defenses to Adversarial Examples\nAcknowledgements\nWe are grateful to Aleksander Madry, Andrew Ilyas, and\nAditi Raghunathan for helpful comments on an early draft\nof this paper. We thank Bo Li, Xingjun Ma, Laurens van der\nMaaten, Aurko Roy, Yang Song, and Cihang Xie for useful\ndiscussion and insights on their defenses.\nThis work was partially supported by the National Science\nFoundation through award CNS-1514457, Qualcomm, and\nthe Hewlett Foundation through the Center for Long-Term\nCybersecurity.\nReferences\nAthalye, A., Engstrom, L., Ilyas, A., and Kwok, K. Syn-\nthesizing robust adversarial examples. arXiv preprint\narXiv:1707.07397, 2017.\nBengio, Y ., L´eonard, N., and Courville, A. Estimating or\npropagating gradients through stochastic neurons for con-\nditional computation. arXiv preprint arXiv:1308.3432 ,\n2013.\nBiggio, B., Corona, I., Maiorca, D., Nelson, B., ˇSrndi´c, N.,\nLaskov, P., Giacinto, G., and Roli, F. Evasion attacks\nagainst machine learning at test time. In Joint European\nConference on Machine Learning and Knowledge Dis-\ncovery in Databases, pp. 387–402. Springer, 2013.\nBuckman, J., Roy, A., Raffel, C., and Goodfellow, I. Ther-\nmometer encoding: One hot way to resist adversarial ex-\namples. International Conference on Learning Represen-\ntations, 2018. URL https://openreview.net/\nforum?id=S18Su--CW. accepted as poster.\nCarlini, N. and Wagner, D. Adversarial examples are not\neasily detected: Bypassing ten detection methods. AISec,\n2017a.\nCarlini, N. and Wagner, D. Magnet and “efﬁcient defenses\nagainst adversarial attacks” are not robust to adversarial\nexamples. arXiv preprint arXiv:1711.08478, 2017b.\nCarlini, N. and Wagner, D. Towards evaluating the robust-\nness of neural networks. In IEEE Symposium on Security\n& Privacy, 2017c.\nDhillon, G. S., Azizzadenesheli, K., Bernstein, J. D., Kos-\nsaiﬁ, J., Khanna, A., Lipton, Z. C., and Anandkumar,\nA. Stochastic activation pruning for robust adversarial\ndefense. International Conference on Learning Represen-\ntations, 2018. URL https://openreview.net/\nforum?id=H1uR4GZRZ. accepted as poster.\nGoodfellow, I., Pouget-Abadie, J., Mirza, M., Xu, B.,\nWarde-Farley, D., Ozair, S., Courville, A., and Bengio,\nY . Generative adversarial nets. In Advances in neural\ninformation processing systems, pp. 2672–2680, 2014a.\nGoodfellow, I. J., Shlens, J., and Szegedy, C. Explain-\ning and harnessing adversarial examples. arXiv preprint\narXiv:1412.6572, 2014b.\nGuo, C., Rana, M., Cisse, M., and van der Maaten, L. Coun-\ntering adversarial images using input transformations.\nInternational Conference on Learning Representations ,\n2018. URL https://openreview.net/forum?\nid=SyJ7ClWCb. accepted as poster.\nHe, K., Zhang, X., Ren, S., and Sun, J. Deep residual learn-\ning for image recognition. In Proceedings of the IEEE\nconference on computer vision and pattern recognition ,\npp. 770–778, 2016.\nHe, W., Wei, J., Chen, X., Carlini, N., and Song, D. Adver-\nsarial example defenses: Ensembles of weak defenses are\nnot strong. arXiv preprint arXiv:1706.04701, 2017.\nKurakin, A., Goodfellow, I., and Bengio, S. Adversar-\nial examples in the physical world. arXiv preprint\narXiv:1607.02533, 2016a.\nKurakin, A., Goodfellow, I. J., and Bengio, S. Ad-\nversarial machine learning at scale. arXiv preprint\narXiv:1611.01236, 2016b.\nMa, X., Li, B., Wang, Y ., Erfani, S. M., Wijewickrema, S.,\nSchoenebeck, G., Houle, M. E., Song, D., and Bailey, J.\nCharacterizing adversarial subspaces using local intrinsic\ndimensionality. International Conference on Learning\nRepresentations, 2018. URL https://openreview.\nnet/forum?id=B1gJ1L2aW. accepted as oral pre-\nsentation.\nMadry, A., Makelov, A., Schmidt, L., Tsipras, D., and\nVladu, A. Towards deep learning models resistant to ad-\nversarial attacks. International Conference on Learning\nRepresentations, 2018. URL https://openreview.\nnet/forum?id=rJzIBfZAb. accepted as poster.\nNa, T., Ko, J. H., and Mukhopadhyay, S. Cascade adver-\nsarial machine learning regularized with a uniﬁed embed-\nding. In International Conference on Learning Represen-\ntations, 2018. URL https://openreview.net/\nforum?id=HyRVBzap-.\nPapernot, N., McDaniel, P., Goodfellow, I., Jha, S., Ce-\nlik, Z. B., and Swami, A. Practical black-box attacks\nagainst machine learning. In Proceedings of the 2017\nACM on Asia Conference on Computer and Communica-\ntions Security, ASIA CCS ’17, pp. 506–519, New York,\nNY , USA, 2017. ACM. ISBN 978-1-4503-4944-4. doi:\n10.1145/3052973.3053009. URL http://doi.acm.\norg/10.1145/3052973.3053009.\nObfuscated Gradients Give a False Sense of Security: Circumventing Defenses to Adversarial Examples\nRaghunathan, A., Steinhardt, J., and Liang, P. Certiﬁed de-\nfenses against adversarial examples.International Confer-\nence on Learning Representations, 2018. URL https:\n//openreview.net/forum?id=Bys4ob-Rb.\nRumelhart, D. E., Hinton, G. E., and Williams, R. J. Learn-\ning representations by back-propagating errors. Nature,\n323:533–536, 1986.\nSalimans, T., Karpathy, A., Chen, X., and Kingma, D. P.\nPixelcnn++: A pixelcnn implementation with discretized\nlogistic mixture likelihood and other modiﬁcations. In\nICLR, 2017.\nSamangouei, P., Kabkab, M., and Chellappa, R. Defense-\ngan: Protecting classiﬁers against adversarial attacks\nusing generative models. International Conference\non Learning Representations , 2018. URL https://\nopenreview.net/forum?id=BkJ3ibb0-. ac-\ncepted as poster.\nSharma, Y . and Chen, P.-Y . Attacking the madry de-\nfense model withL1-based adversarial examples. arXiv\npreprint arXiv:1710.10733, 2017.\nSinha, A., Namkoong, H., and Duchi, J. Certiﬁable distri-\nbutional robustness with principled adversarial training.\nInternational Conference on Learning Representations ,\n2018. URL https://openreview.net/forum?\nid=Hk6kPgZA-.\nSong, Y ., Kim, T., Nowozin, S., Ermon, S., and Kush-\nman, N. Pixeldefend: Leveraging generative models\nto understand and defend against adversarial examples.\nInternational Conference on Learning Representations ,\n2018. URL https://openreview.net/forum?\nid=rJUYGxbCW. accepted as poster.\nSzegedy, C., Zaremba, W., Sutskever, I., Bruna, J., Erhan,\nD., Goodfellow, I., and Fergus, R. Intriguing properties\nof neural networks. ICLR, 2013.\nSzegedy, C., Vanhoucke, V ., Ioffe, S., Shlens, J., and Wojna,\nZ. Rethinking the inception architecture for computer\nvision. In Proceedings of the IEEE Conference on Com-\nputer Vision and Pattern Recognition , pp. 2818–2826,\n2016.\nTram`er, F., Kurakin, A., Papernot, N., Goodfellow, I.,\nBoneh, D., and McDaniel, P. Ensemble adversarial train-\ning: Attacks and defenses. International Conference\non Learning Representations , 2018. URL https://\nopenreview.net/forum?id=rkZvSe-RZ. ac-\ncepted as poster.\nXie, C., Wang, J., Zhang, Z., Ren, Z., and Yuille, A. Mit-\nigating adversarial effects through randomization. In-\nternational Conference on Learning Representations ,\n2018. URL https://openreview.net/forum?\nid=Sk9yuql0Z. accepted as poster.\nZagoruyko, S. and Komodakis, N. Wide residual networks.\narXiv preprint arXiv:1605.07146, 2016.",
  "values": {
    "Critiqability": "No",
    "Explicability": "No",
    "Privacy": "No",
    "Collective influence": "No",
    "Beneficence": "No",
    "Fairness": "No",
    "User influence": "No",
    "Deferral to humans": "No",
    "Interpretable (to users)": "No",
    "Not socially biased": "No",
    "Transparent (to users)": "No",
    "Justice": "No",
    "Respect for Persons": "No",
    "Non-maleficence": "No",
    "Autonomy (power to decide)": "No",
    "Respect for Law and public interest": "No"
  }
}