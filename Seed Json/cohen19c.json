{
  "pdf": "cohen19c",
  "title": "cohen19c",
  "author": "Unknown",
  "paper_id": "cohen19c",
  "text": "Certiﬁed Adversarial Robustness via Randomized Smoothing\nJeremy Cohen 1 Elan Rosenfeld 1 J. Zico Kolter 1 2\nAbstract\nWe show how to turn any classiﬁer that classiﬁes\nwell under Gaussian noise into a new classiﬁer\nthat is certiﬁably robust to adversarial perturba-\ntions under theℓ2 norm. While this “randomized\nsmoothing” technique has been proposed before\nin the literature, we are the ﬁrst to provide a tight\nanalysis, which establishes a close connection\nbetweenℓ2 robustness and Gaussian noise. We\nuse the technique to train an ImageNet classiﬁer\nwith e.g. a certiﬁed top-1 accuracy of 49% un-\nder adversarial perturbations with ℓ2 norm less\nthan 0.5 (=127/255). Smoothing is the only ap-\nproach to certiﬁably robust classiﬁcation which\nhas been shown feasible on full-resolution Im-\nageNet. On smaller-scale datasets where com-\npeting approaches to certiﬁedℓ2 robustness are\nviable, smoothing delivers higher certiﬁed accura-\ncies. The empirical success of the approach sug-\ngests that provable methods based on randomiza-\ntion at prediction time are a promising direction\nfor future research into adversarially robust classi-\nﬁcation. Code and models are available athttp:\n//github.com/locuslab/smoothing.\n1. Introduction\nModern image classiﬁers achieve high accuracy on i.i.d.\ntest sets but are not robust to small, adversarially-chosen\nperturbations of their inputs (Szegedy et al., 2014; Biggio\net al., 2013). Given an imagex correctly classiﬁed by, say,\na neural network, an adversary can usually engineer an ad-\nversarial perturbationδ so small thatx +δ looks just like\nx to the human eye, yet the network classiﬁes x +δ as a\ndifferent, incorrect class. Many works have proposed heuris-\ntic methods for training classiﬁers intended to be robust to\nadversarial perturbations. However, most of these heuristics\nhave been subsequently shown to fail against suitably pow-\n1Carnegie Mellon University 2Bosch Center for AI. Correspon-\ndence to: Jeremy Cohen<jeremycohen@cmu.edu>.\nProceedings of the 36 th International Conference on Machine\nLearning, Long Beach, California, PMLR 97, 2019. Copyright\n2019 by the author(s).\nx\npA\npB\nFigure 1. Evaluating the smoothed classiﬁer at an inputx. Left:\nthe decision regions of the base classiﬁer f are drawn in differ-\nent colors. The dotted lines are the level sets of the distribution\nN (x,σ 2I). Right: the distributionf(N (x,σ 2I)). As discussed\nbelow,pA is a lower bound on the probability of the top class and\npB is an upper bound on the probability of each other class. Here,\ng(x) is “blue.”\n.\nerful adversaries (Carlini & Wagner, 2017; Athalye et al.,\n2018; Uesato et al., 2018). In response, a line of work on\ncertiﬁable robustness studies classiﬁers whose prediction at\nany pointx is veriﬁably constant within some set aroundx\n(e.g. Wong & Kolter, 2018; Raghunathan et al., 2018a). In\nmost of these works, the robust classiﬁer takes the form of a\nneural network. Unfortunately, all existing approaches for\ncertifying the robustness of neural networks have trouble\nscaling to networks that are large and expressive enough to\nsolve problems like ImageNet.\nOne workaround is to look for robust classiﬁers that are not\nneural networks. In this paper, we analyze an operation we\ncall randomized smoothing1 which transforms any arbitrary\nbase classiﬁerf into a new “smoothed classiﬁer”g that is\ncertiﬁably robust inℓ2 norm. Letf be an arbitrary classiﬁer\nwhich maps inputs Rd to classes Y. For any input x, the\nsmoothed classiﬁer’s predictiong(x) is deﬁned to be the\nclass which f is most likely to classify the random vari-\nable N (x,σ 2I) as. That is,g(x) returns the most probable\nprediction byf of random Gaussian corruptions ofx.\nIf the base classiﬁerf is most likely to classify N (x,σ 2I)\nasx’s correct class, then the smoothed classiﬁerg will be\n1We adopt this term because it has been used to describe as\nsimilar technique in a different context (Duchi et al., 2012).\nCertiﬁed Adversarial Robustness via Randomized Smoothing\ncorrect atx. But the smoothed classiﬁerg will also possess\na desirable property that the base classiﬁer may lack: one\ncan verify thatg’s prediction is constant within anℓ2 ball\naround any inputx, simply by estimating the probabilities\nwith whichf classiﬁesN (x,σ 2I) as each class. The higher\nthe probability with which f classiﬁes N (x,σ 2I) as the\nmost probable class, the larger the ℓ2 radius around x in\nwhichg provably returns that class.\nLecuyer et al. (2019) proposed randomized smoothing as\na provable adversarial defense, and used it to train the ﬁrst\ncertiﬁably robust classiﬁer for ImageNet. Subsequently, Li\net al. (2018) proved a stronger robustness guarantee. How-\never, both of these guarantees are loose, in the sense that\nthe smoothed classiﬁerg is provably always more robust\nthan the guarantee indicates. In this paper, we prove the\nﬁrst tight robustness guarantee for randomized smoothing.\nOur analysis reveals that smoothing with Gaussian noise\nnaturally induces certiﬁable robustness under theℓ2 norm.\nWe suspect that other, as-yet-unknown noise distributions\nmight induce robustness to other perturbation sets such as\ngeneralℓp norm balls.\nRandomized smoothing has one major drawback. If f is\na neural network, it is not possible to exactly compute the\nprobabilities with which f classiﬁes N (x,σ 2I) as each\nclass. Therefore, it is not possible to exactly evaluate the\nsmoothed classiﬁerg or to exactly compute the radius in\nwhichg is robust. Instead, we present Monte Carlo algo-\nrithms for both tasks that are guaranteed to succeed with\narbitrarily high probability.\nDespite this drawback, randomized smoothing enjoys sev-\neral compelling advantages over other certiﬁably robust\nclassiﬁers proposed in the literature: it makes no assump-\ntions about the base classiﬁer’s architecture, it is simple to\nimplement and understand, and, most importantly, it per-\nmits the use of arbitrarily large neural networks as the base\nclassiﬁer. In contrast, other certiﬁed defenses do not cur-\nTable 1. Approximate certiﬁed accuracy on ImageNet. Each row\nshows a radius r, the best hyperparameter σ for that radius, the\napproximate certiﬁed accuracy at radiusr of the corresponding\nsmoothed classiﬁer, and the standard accuracy of the corresponding\nsmoothed classiﬁer. To give a sense of scale, a perturbation with\nℓ2 radius 1.0 could change one pixel by 255, ten pixels by 80, 100\npixels by 25, or 1000 pixels by 8. Random guessing on ImageNet\nwould attain 0.1% accuracy.\nℓ2 RADIUS BEST σ CERT. A CC (%) S TD. A CC(%)\n0.5 0.25 49 67\n1.0 0.50 37 57\n2.0 0.50 19 57\n3.0 1.00 12 44\nFigure 2. The smoothed classiﬁer’s prediction at an inputx (left)\nis deﬁned as the most likely prediction by the base classiﬁer on\nrandom Gaussian corruptions ofx (right;σ = 0.5). Note that this\nGaussian noise is much larger in magnitude than the adversarial\nperturbations to which g is provably robust. One interpretation\nof randomized smoothing in high dimension is that these large\nrandom perturbations “drown out” small adversarial perturbations.\nrently scale to large networks. Indeed, smoothing is the only\ncertiﬁed adversarial defense which has been shown feasible\non the full-resolution ImageNet classiﬁcation task.\nWe use randomized smoothing to train state-of-the-art certi-\nﬁablyℓ2-robust ImageNet classiﬁers; for example, one of\nthem achieves 49% provable top-1 accuracy under adver-\nsarial perturbations withℓ2 norm less than 127/255 (Table\n1). We also demonstrate that on smaller-scale datasets like\nCIFAR-10 and SHVN, where competing approaches to cer-\ntiﬁedℓ2 robustness are feasible, randomized smoothing can\ndeliver better certiﬁed accuracies, both because it enables\nthe use of larger networks and because it does not constrain\nthe expressivity of the base classiﬁer.\n2. Related Work\nMany works have proposed classiﬁers intended to be ro-\nbust to adversarial perturbations. These approaches can\nbe broadly divided into empirical defenses, which empiri-\ncally seem robust to known adversarial attacks, andcertiﬁed\ndefenses, which are provably robust to certain kinds of ad-\nversarial perturbations.\nEmpirical defenses The most successful empirical de-\nfense to date is adversarial training (Goodfellow et al.,\n2015; Kurakin et al., 2017; Madry et al., 2018), in which\nadversarial examples are found during training (often using\nprojected gradient descent) and added to the training set.\nUnfortunately, it is typically impossible to tell whether a\nprediction by an empirically robust classiﬁer is truly robust\nto adversarial perturbations; the most that can be said is that\na speciﬁc attack was unable to ﬁnd any. In fact, many heuris-\ntic defenses proposed in the literature were later “broken”\nby stronger adversaries (Carlini & Wagner, 2017; Athalye\net al., 2018; Uesato et al., 2018; Athalye & Carlini, 2018).\nCertiﬁed Adversarial Robustness via Randomized Smoothing\nAiming to escape this cat-and-mouse game, a growing body\nof work has focused on defenses with formal guarantees.\nCertiﬁed defenses A classiﬁer is said to becertiﬁably ro-\nbust if for any inputx, one can easily obtain a guarantee that\nthe classiﬁer’s prediction is constant within some set around\nx, often an ℓ2 orℓ∞ ball. In most work in this area, the\ncertiﬁably robust classiﬁer is a neural network. Some works\npropose algorithms for certifying the robustness of generi-\ncally trained networks, while others (Wong & Kolter, 2018;\nRaghunathan et al., 2018a) propose both a robust training\nmethod and a complementary certiﬁcation mechanism.\nCertiﬁcation methods are eitherexact (a.k.a “complete”) or\nconservative (a.k.a “sound but incomplete”). In the context\nof ℓp norm-bounded perturbations, exact methods take a\nclassiﬁerg, input x, and radius r, and report whether or\nnot there exists a perturbationδ within ∥δ∥ ≤ r for which\ng(x) ̸=g(x +δ). In contrast, conservative methods either\ncertify that no such perturbation exists or decline to make a\ncertiﬁcation; they may decline even when it is true that no\nsuch perturbation exists. Exact methods are usually based\non Satisﬁability Modulo Theories (Katz et al., 2017; Carlini\net al., 2017; Ehlers, 2017; Huang et al., 2017) or mixed\ninteger linear programming (Cheng et al., 2017; Lomuscio\n& Maganti, 2017; Dutta et al., 2017; Fischetti & Jo, 2018;\nBunel et al., 2018). Unfortunately, no exact methods have\nbeen shown to scale beyond moderate-sized (100,000 acti-\nvations) networks (Tjeng et al., 2019), and networks of that\nsize can only be veriﬁed when they are trained in a manner\nthat impairs their expressivity.\nConservative certiﬁcation is more scalable. Some conser-\nvative methods bound the global Lipschitz constant of the\nneural network (Gouk et al., 2018; Tsuzuku et al., 2018;\nAnil et al., 2019; Cisse et al., 2017), but these approaches\ntend to be very loose on expressive networks. Others mea-\nsure the local smoothness of the network in the vicinity of a\nparticular inputx. In theory, one could obtain a robustness\nguarantee via an upper bound on the local Lipschitz con-\nstant of the network (Hein & Andriushchenko, 2017), but\ncomputing this quantity is intractable for general neural net-\nworks. Instead, a panoply of practical solutions have been\nproposed in the literature (Wong & Kolter, 2018; Wang et al.,\n2018a;b; Raghunathan et al., 2018a;b; Wong et al., 2018;\nDvijotham et al., 2018b;a; Croce et al., 2019; Gehr et al.,\n2018; Mirman et al., 2018; Singh et al., 2018; Gowal et al.,\n2018; Weng et al., 2018a; Zhang et al., 2018). Two themes\nstand out. Some approaches cast veriﬁcation as an opti-\nmization problem and import tools such as relaxation and\nduality from the optimization literature to provide conserva-\ntive guarantees (Wong & Kolter, 2018; Wong et al., 2018;\nRaghunathan et al., 2018a;b; Dvijotham et al., 2018b;a).\nOthers step through the network layer by layer, maintaining\nat each layer an outer approximation of the set of activations\nreachable by a perturbed input (Mirman et al., 2018; Singh\net al., 2018; Gowal et al., 2018; Weng et al., 2018a; Zhang\net al., 2018). None of these local certiﬁcation methods have\nbeen shown to be feasible on networks that are large and\nexpressive enough to solve modern machine learning prob-\nlems like the ImageNet classiﬁcation task. Also, all method\neither assume speciﬁc network architectures (e.g. ReLU\nactivations or a layered feedforward structure) or require\nextensive customization for new network architectures.\nRelated work involving noise Prior works have proposed\nusing a network’s robustness to Gaussian noise as a proxy\nfor its robustness to adversarial perturbations (Weng et al.,\n2018b; Ford et al., 2019), and have suggested that Gaussian\ndata augmentation could supplement or replace adversar-\nial training (Zantedeschi et al., 2017; Kannan et al., 2018).\nSmilkov et al. (2017) observed that averaging a classiﬁer’s\ninput gradients over Gaussian corruptions of an image yields\nvery interpretable saliency maps. The robustness of neural\nnetworks to random noise has been analyzed both theo-\nretically (Fawzi et al., 2016; Franceschi et al., 2018) and\nempirically (Dodge & Karam, 2017). Finally, Webb et al.\n(2019) proposed a statistical technique for estimating the\nnoise robustness of a classiﬁer more efﬁciently than naive\nMonte Carlo simulation; we did not use this technique since\nit appears to lack formal high-probability guarantees. While\nthese works hypothesized relationships between a neural net-\nwork’s robustness to random noise and the same network’s\nrobustness to adversarial perturbations, randomized smooth-\ning instead uses a classiﬁer’s robustness to random noiseto\ncreate a new classiﬁer robust to adversarial perturbations.\nRandomized smoothing Randomized smoothing has\nbeen studied previously for adversarial robustness. Sev-\neral works (Liu et al., 2018; Cao & Gong, 2017) proposed\nsimilar techniques as heuristic defenses, but did not prove\nany guarantees. Lecuyer et al. (2019) used inequalities\nfrom the differential privacy literature to prove an ℓ2 and\nℓ1 robustness guarantee for smoothing with Gaussian and\nLaplace noise, respectively. Subsequently, Li et al. (2018)\nused tools from information theory to prove a strongerℓ2 ro-\nbustness guarantee for Gaussian noise. However, all of these\nrobustness guarantees are loose. In contrast, we prove a tight\nrobustness guarantee inℓ2 norm for randomized smoothing\nwith Gaussian noise.\n3. Randomized smoothing\nConsider a classiﬁcation problem from Rd to classes Y.\nAs discussed above, randomized smoothing is a method for\nconstructing a new, “smoothed” classiﬁerg from an arbitrary\nbase classiﬁerf. When queried atx, the smoothed classiﬁer\ng returns whichever class the base classiﬁerf is most likely\nCertiﬁed Adversarial Robustness via Randomized Smoothing\nto return whenx is perturbed by isotropic Gaussian noise:\ng(x) = arg max\nc∈Y\nP(f(x +ε) =c) (1)\nwhere ε ∼ N (0,σ 2I)\nAn equivalent deﬁnition is that g(x) returns the class c\nwhose pre-image {x′ ∈ Rd : f(x′) = c} has the largest\nprobability measure under the distribution N (x,σ 2I). The\nnoise levelσ is a hyperparameter of the smoothed classiﬁer\ng which controls a robustness/accuracy tradeoff; it does not\nchange with the inputx. We leave undeﬁned the behavior\nofg when the argmax is not unique.\nWe will ﬁrst present our robustness guarantee for the\nsmoothed classiﬁer g. Then, since it is not possible to\nexactly evaluate the prediction ofg atx or to certify the ro-\nbustness ofg aroundx, we will give Monte Carlo algorithms\nfor both tasks that succeed with arbitrarily high probability.\n3.1. Robustness guarantee\nSuppose that when the base classiﬁerf classiﬁesN (x,σ 2I),\nthe most probable classcA is returned with probabilitypA,\nand the “runner-up” class is returned with probability pB.\nOur main result is that smoothed classiﬁerg is robust around\nx within theℓ2 radiusR = σ\n2 (Φ−1(pA)−Φ−1(pB)), where\nΦ−1 is the inverse of the standard Gaussian CDF. This result\nalso holds if we replacepA with a lower boundpA and we\nreplacepB with an upper boundpB.\nTheorem 1. Let f : Rd → Y be any deterministic or\nrandom function, and letε ∼ N (0,σ 2I). Letg be deﬁned\nas in (1). SupposecA ∈ Y andpA,pB ∈ [0, 1] satisfy:\nP(f(x +ε) =cA)≥pA≥pB≥ max\nc̸=cA\nP(f(x +ε) =c) (2)\nTheng(x +δ) =cA for all ∥δ∥2 <R , where\nR = σ\n2 (Φ−1(pA) − Φ−1(pB)) (3)\nWe now make several observations about Theorem 1:\n• Theorem 1 assumes nothing about f. This is crucial\nsince it is unclear which well-behavedness assump-\ntions, if any, are satisﬁed by modern deep architectures.\n• The certiﬁed radiusR is large when: (1) the noise level\nσ is high, (2) the probability of the top classcA is high,\nand (3) the probability of each other class is low.\n• The certiﬁed radius R goes to ∞ as pA → 1 and\npB → 0. This should sound reasonable: the Gaussian\ndistribution is supported on all of Rd, so the only way\nthatf(x +ε) = cA with probability 1 is if f = cA\nalmost everywhere.\nBoth Lecuyer et al. (2019) and Li et al. (2018) proved ℓ2\nrobustness guarantees for the same setting as Theorem 1, but\nwith different, smaller expressions for the certiﬁed radius.\nHowever, ourℓ2 robustness guarantee is tight: if (2) is all\nthat is known aboutf, then it is impossible to certify anℓ2\nball with radius larger than R. In fact, it is impossible to\ncertify any superset of theℓ2 ball with radiusR:\nTheorem 2. AssumepA +pB ≤ 1. For any perturbation\nδ with ∥δ∥2 >R , there exists a base classiﬁerf consistent\nwith the class probabilities (2) for whichg(x +δ) ̸=cA.\nTheorem 2 shows that Gaussian smoothing naturally in-\nducesℓ2 robustness: if we make no assumptions on the base\nclassiﬁer beyond the class probabilities (2), then the set of\nperturbations to which a Gaussian-smoothed classiﬁer is\nprovably robust is exactly anℓ2 ball.\nThe complete proofs of Theorems 1 and 2 are in Appendix\nA. We now sketch the proofs in the special case when there\nare only two classes.\nTheorem 1 (binary case). SupposepA ∈ ( 1\n2, 1] satisﬁes\nP(f(x +ε) = cA) ≥ pA. Then g(x +δ) = cA for all\n∥δ∥2 <σ Φ−1(pA).\nProof sketch. Fix a perturbation δ ∈ Rd. To guarantee\nthatg(x +δ) = cA, we need to show that f classiﬁes the\ntranslated Gaussian N (x +δ,σ 2I) ascA with probability\n> 1\n2. However, all we know about f is that f classiﬁes\nN (x,σ 2I) as cA with probability ≥ pA. This raises the\nquestion: out of all possible base classiﬁersf which classify\nN (x,σ 2I) as cA with probability ≥ pA, which one f∗\nclassiﬁesN (x+δ,σ 2I) ascA with the smallest probability?\nOne can show using an argument similar to the Neyman-\nPearson lemma (Neyman & Pearson, 1933) that this “worst-\ncase”f∗ is a linear classiﬁer whose decision boundary is\nnormal to the perturbationδ (Figure 3):\nf∗(x′) =\n{\ncA ifδT (x′ −x) ≤σ∥δ∥2Φ−1(pA)\ncB otherwise (4)\nThis “worst-case” f∗ classiﬁesN (x +δ,σ 2I) ascA with\nprobability Φ\n(\nΦ−1(pA) −∥δ∥2\nσ\n)\n. Therefore, to ensure that\neven the “worst-case”f∗ classiﬁesN (x+δ,σ 2I) ascA with\nprobability> 1\n2, we solve for thoseδ for which\nΦ\n(\nΦ−1(pA) − ∥δ∥2\nσ\n)\n> 1\n2\nwhich is equivalent to the condition∥δ∥2 <σ Φ−1(pA).\nTheorem 2 is a simple consequence: for anyδ with ∥δ∥2 >\nR, the base classiﬁerf∗ deﬁned in (4) is consistent with (2);\nyet iff∗ is the base classiﬁer, theng(x +δ) =cB.\nCertiﬁed Adversarial Robustness via Randomized Smoothing\nx +δ\nx\nx +δ\nx\nFigure 3. Illustration of f∗ in two dimensions. The concentric\ncircles are the density contours ofN (x,σ 2I) andN (x +δ,σ 2I).\nOut of all base classiﬁersf which classifyN (x,σ 2I) ascA (blue)\nwith probability≥ pA, such as both classiﬁers depicted above,\nthe “worst-case” f∗, which classiﬁesN (x +δ,σ 2I) ascA with\nminimal probability, is the classiﬁer depicted on the right: a linear\nclassiﬁer with decision boundary normal to the perturbationδ.\nFigure 5 (left) plots our ℓ2 robustness guarantee against\nthe guarantees derived in prior work. Observe that our\nR is much larger than that of Lecuyer et al. (2019) and\nmoderately larger than that of Li et al. (2018). Appendix I\nderives the other two guarantees using this paper’s notation.\nLinear base classiﬁer A two-class linear classiﬁer\nf(x) = sign(wTx +b) is already certiﬁable: the distance\nfrom any inputx to the decision boundary is|wTx+b|/∥w∥,\nand no perturbationδ withℓ2 norm less than this distance\ncan possibly changef’s prediction. In Appendix B we show\nthat iff is linear, then the smoothed classiﬁerg is identical\nto the base classiﬁerf. Moreover, we show that our bound\n(3) will certify the true robust radius |wTx +b|/∥w∥, rather\nthan a smaller, overconservative radius. Therefore, whenf\nis linear, there always exists a perturbationδ just beyond the\ncertiﬁed radius which changesg’s prediction.\nNoise level can scale with image resolution Since our\nexpression (3) for the certiﬁed radius does not depend ex-\nplicitly on the data dimensiond, one might worry that ran-\ndomized smoothing is less effective for images of higher\nresolution — certifying a ﬁxedℓ2 radius is “less impressive”\nfor, say, a 224 × 224 image than for a 56 × 56 image. How-\never, as illustrated by Figure 4, images in higher resolution\ncan tolerate higher levelsσ of isotropic Gaussian noise be-\nfore their class-distinguishing content gets destroyed. As\na consequence, in high resolution, smoothing can be per-\nformed with a largerσ, leading to larger certiﬁed radii. See\nAppendix G for a more rigorous version of this argument.\n3.2. Practical algorithms\nWe now present practical Monte Carlo algorithms for eval-\nuating g(x) and certifying the robustness of g around x.\nMore details can be found in Appendix C.\n3.2.1. P REDICTION\nEvaluating the smoothed classiﬁer’s prediction g(x) re-\nquires identifying the classcA with maximal weight in the\ncategorical distributionf(x +ε). The procedure described\nin pseudocode as PREDICT drawsn samples of f(x +ε)\nby runningn noise-corrupted copies ofx through the base\nclassiﬁer. Let ˆcA be the class which appeared the largest\nnumber of times. If ˆcA appeared much more often than any\nother class, then PREDICT returns ˆcA. Otherwise, it abstains\nfrom making a prediction. We use the hypothesis test from\nHung & Fithian (2019) to calibrate the abstention threshold\nso as to bound byα the probability of returning an incorrect\nanswer. PREDICT satisﬁes the following guarantee:\nProposition 1. With probability at least 1 −α over the\nrandomness in PREDICT , PREDICT will either abstain or\nreturng(x). (Equivalently: the probability that PREDICT\nreturns a class other thang(x) is at mostα.)\nThe function SAMPLE UNDER NOISE (f,x, num,σ) in the\npseudocode draws num samples of noise, ε1...ε num ∼\nN (0,σ 2I), runs eachx +εi through the base classiﬁerf,\nand returns a vector of class counts. BINOM PVALUE (nA,\nnA +nB,p) returns the p-value of the two-sided hypothesis\ntest thatnA ∼ Binomial(nA +nB,p ).\nEven if the true smoothed classiﬁerg is robust at radiusR,\nPREDICT will be vulnerable in a certain sense to adversarial\nperturbations withℓ2 norm slightly less than R. By engi-\nneering a perturbationδ for whichf(x +δ +ε) puts mass\njust over 1\n2 on classcA and mass just under 1\n2 on classcB,\nan adversary can force PREDICT to abstain at a high rate. If\nthis scenario is of concern, a variant of Theorem 1 could be\nproved to certify a radius in which P(f(x +δ +ε) =cA) is\nlarger by some margin than maxc̸=cA P(f(x +δ +ε) =c).\n3.2.2. C ERTIFICATION\nEvaluating and certifying the robustness of g around an\ninputx requires not only identifying the classcA with maxi-\nmal weight inf(x +ε), but also estimating a lower bound\npA on the probability that f(x +ε) = cA and an upper\nboundpB on the probability thatf(x +ε) equals any other\nclass. Doing all three of these at the same time in a sta-\ntistically correct manner requires some care. One simple\nFigure 4. Left to right: clean 56 x 56 image, clean 224 x 224 image,\nnoisy 56 x 56 image (σ = 0.5), noisy 224 x 224 image (σ = 0.5).\nCertiﬁed Adversarial Robustness via Randomized Smoothing\nPseudocode for certiﬁcation and prediction\n# evaluateg atx\nfunction PREDICT (f,σ,x,n,α)\ncounts ← SAMPLE UNDER NOISE (f,x,n,σ)\nˆcA, ˆcB ← top two indices in counts\nnA,nB ← counts[ˆcA], counts[ˆcB]\nif BINOM PVALUE (nA,nA +nB, 0.5) ≤α return ˆcA\nelse return ABSTAIN\n# certify the robustness ofg aroundx\nfunction CERTIFY (f,σ,x,n0,n,α)\ncounts0 ← SAMPLE UNDER NOISE (f,x,n 0,σ )\nˆcA ← top index in counts0\ncounts ← SAMPLE UNDER NOISE (f,x,n,σ 2)\npA ← LOWER CONF BOUND (counts[ˆcA],n, 1 −α)\nifpA > 1\n2 return prediction ˆcA and radiusσ Φ−1(pA)\nelse return ABSTAIN\nsolution is presented in pseudocode as CERTIFY : ﬁrst, use\na small number of samples from f(x +ε) to take a guess\natcA; then use a larger number of samples to estimatepA;\nthen simply takepB = 1 −pA.\nProposition 2. With probability at least 1 −α over the\nrandomness in CERTIFY , if CERTIFY returns a class ˆcA\nand a radiusR (i.e. does not abstain), then g predicts ˆcA\nwithin radiusR aroundx:g(x +δ) = ˆcA ∀ ∥δ∥2 <R .\nThe function LOWER CONF BOUND (k,n, 1 −α) in the pseu-\ndocode returns a one-sided (1 −α) lower conﬁdence in-\nterval for the Binomial parameter p given a sample k ∼\nBinomial(n,p ).\nCertifying large radii requires many samples Recall\nfrom Theorem 1 thatR approaches ∞ aspA approaches 1.\nUnfortunately, it turns out thatpA approaches 1 so slowly\nwithn thatR also approaches ∞ very slowly withn. Con-\nsider the most favorable situation:f(x) =cA everywhere.\nThis means thatg is robust at radius ∞. But after observing\nn samples off(x +ε) which all equalcA, the tightest (to\nour knowledge) lower bound would say that with probabil-\nity least 1 −α,pA ≥ α(1/n). Plugging pA = α(1/n) and\npB = 1 −pA into (3) yields an expression for the certiﬁed\nradius as a function of n: R = σ Φ−1(α1/n). Figure 5\n(right) plots this function for α = 0.001,σ = 1. Observe\nthat certifying a radius of 4σ with 99.9% conﬁdence would\nrequire ≈ 105 samples.\n3.3. Training the base classiﬁer\nTheorem 1 holds regardless of how the base classiﬁerf is\ntrained. However, in order forg to classify the labeled ex-\nample (x,c ) correctly and robustly,f needs to consistently\nclassify N (x,σ 2I) asc. In high dimension, the Gaussian\n0.5 0.6 0.7 0.8 0.9 1.0\npA\n0\n1\n2\n3radius\nours\n(Lecuyer et al, 2018)\n(Li et al, 2018)\n102\n104\n106\nnumber of samples\n0\n1\n2\n3\n4\n5radius\nFigure 5. Left: Certiﬁed radiusR as a function ofpA (withpB =\n1−pA andσ = 1) under all three randomized smoothing bounds.\nRight: A plot of R = σ Φ−1(α1/n) forα = 0.001 andσ = 1.\nThe radius we can certify with high probability grows slowly with\nthe number of samples, even in the best case wheref(x) = cA\neverywhere.\ndistribution N (x,σ 2I) places almost no mass near its mode\nx. As a consequence, when σ is moderately high, the distri-\nbution of natural images has virtually disjoint support from\nthe distribution of natural images corrupted by N (0,σ 2I);\nsee Figure 2 for a visual demonstration. Therefore, if the\nbase classiﬁerf is trained via standard supervised learning\non the data distribution, it will see no noisy images during\ntraining, and hence will not necessarily learn to classify\nN (x,σ 2I) withx’s true label. Indeed, we observed empiri-\ncally that when neural network base classiﬁers are trained\non noiseless data, they cannot recognize noisy images.\nTherefore, in this paper we follow Lecuyer et al. (2019) and\ntrain the base classiﬁer with Gaussian data augmentation at\nvarianceσ2. A justiﬁcation for this procedure is provided in\nAppendix F. However, we suspect that there may be room to\nimprove upon this training scheme, perhaps by training the\nbase classiﬁer so as to maximize the smoothed classiﬁer’s\ncertiﬁed accuracy at some tunable radiusr.\n4. Experiments\nIn adversarially robust classiﬁcation, one metric of interest\nis the certiﬁed test set accuracy at radiusr, deﬁned as the\nfraction of the test set whichg classiﬁes correctly with a pre-\ndiction that is certiﬁably robust within anℓ2 ball of radiusr.\nHowever, ifg is a randomized smoothing classiﬁer, comput-\ning this quantity exactly is not possible, so we instead report\nthe approximate certiﬁed test set accuracy, deﬁned as the\nfraction of the test set which CERTIFY classiﬁes correctly\n(without abstaining) and certiﬁes robust with a radiusR ≥r.\nAppendix D shows how to convert the approximate certiﬁed\naccuracy into a lower bound on the true certiﬁed accuracy\nthat holds with high probability over the randomness in\nCERTIFY . However Appendix H.2 demonstrates that when\nα is small, the difference between these two quantities is\nnegligible. Therefore, in our experiments we omit the step\nfor simplicity and report approximate certiﬁed accuracies.\nCertiﬁed Adversarial Robustness via Randomized Smoothing\n0.0 0.2 0.4 0.6 0.8 1.0 1.2 1.4\nradius\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0certified accuracy\nσ = 0.12\nσ = 0.25\nσ = 0.50\nσ = 1.00\nundefended\n0.0 0.5 1.0 1.5 2.0 2.5 3.0 3.5 4.0\nradius\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0certified accuracy\nσ = 0.25\nσ = 0.50\nσ = 1.00\nundefended\nFigure 6. Approximate certiﬁed accuracy attained by randomized\nsmoothing on CIFAR-10 (top) and ImageNet (bottom). The hyper-\nparameterσ controls a robustness/accuracy tradeoff. The dashed\nblack line is an upper bound on the empirical robust accuracy of\nan undefended classiﬁer with the base classiﬁer’s architecture.\nIn all experiments, unless otherwise stated, we ran CERTIFY\nwithα = 0.001, so there was at most a 0.1% chance that\nCERTIFY returned a radius in whichg was not truly robust.\nUnless otherwise stated, when running CERTIFY we used\nn0 = 100 Monte Carlo samples for selection and n =\n100,000 samples for estimation.\nIn the ﬁgures above that plot certiﬁed accuracy as a function\nof radiusr, the certiﬁed accuracy always decreases gradually\nwithr until reaching some point where it plummets to zero.\nThis drop occurs because for each noise levelσ and number\nof samplesn, there is a hard upper limit to the radius we can\ncertify with high probability, achieved when alln samples\nare classiﬁed byf as the same class.\nImageNet and CIFAR-10 results We applied random-\nized smoothing to CIFAR-10 (Krizhevsky, 2009) and Im-\nageNet (Deng et al., 2009). On each dataset we trained\nseveral smoothed classiﬁers, each with a different σ. On\nCIFAR-10 our base classiﬁer was a 110-layer residual\nnetwork; certifying each example took 15 seconds on an\nNVIDIA RTX 2080 Ti. On ImageNet our base classiﬁer\nwas a ResNet-50; certifying each example took 110 seconds.\nWe also trained a neural network with the base classiﬁer’s\narchitecture on clean data, and subjected it to a DeepFoolℓ2\nadversarial attack (Moosavi-Dezfooli et al., 2016), in order\nto obtain an empirical upper bound on its robust accuracy.\nWe certiﬁed the full CIFAR-10 test set and a subsample of\n500 examples from the ImageNet test set.\n0.0 0.5 1.0 1.5 2.0 2.5 3.0\nradius\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0certified accuracy\nsmoothing, large network \nsmoothing, small network\n(Wong et al, 2018) 1\n(Wong et al, 2018) 2\n(Wong et al, 2018) 3\nFigure 7. Comparison betwen randomized smoothing and Wong\net al. (2018). Each green line is a small resnet classiﬁer trained and\ncertiﬁed using the method of Wong et al. (2018) with a different\nsetting of its hyperparameter ϵ. The purple line is our method\nusing the same small resnet architecture as the base classiﬁer; the\nblue line is our method with a larger neural network as the base\nclassiﬁer. Wong et al. (2018) gives deterministic robustness guar-\nantees, whereas smoothing gives high-probabaility guaranatees;\ntherefore, we plot here the certiﬁed accuracy of Wong et al. (2018)\nagainst the “approximate” certiﬁed accuracy of smoothing.\nFigure 6 plots the certiﬁed accuracy attained by smoothing\nwith eachσ. The dashed black line is the empirical upper\nbound on the robust accuracy of the base classiﬁer architec-\nture; observe that smoothing improves substantially upon\nthe robustness of the undefended base classiﬁer architecture.\nWe see thatσ controls a robustness/accuracy tradeoff. When\nσ is low, small radii can be certiﬁed with high accuracy, but\nlarge radii cannot be certiﬁed. Whenσ is high, larger radii\ncan be certiﬁed, but smaller radii are certiﬁed at a lower ac-\ncuracy. This observation echoes the ﬁnding in Tsipras et al.\n(2019) that adversarially trained networks with higher ro-\nbust accuracy tend to have lower standard accuracy. Tables\nof these results are in Appendix E.\nFigure 8 (left) plots the certiﬁed accuracy obtained using our\nTheorem 1 guarantee alongside the certiﬁed accuracy ob-\ntained using the analogous bounds of Lecuyer et al. (2019)\nand Li et al. (2018). Since our expression for the certiﬁed\nradiusR is greater (and, in fact, tight), our bound delivers\nhigher certiﬁed accuracies. Figure 8 (middle) projects how\nthe certiﬁed accuracy would have changed had CERTIFY\nused more or fewer samplesn (under the assumption that the\nrelative class proportions in counts would have remained\nconstant). Finally, Figure 8 (right) plots the certiﬁed accu-\nracy as the conﬁdence parameterα is varied. Observe that\nthe certiﬁed accuracy is not very sensitive toα.\nComparison to baselines We compared randomized\nsmoothing to three baseline approaches for certiﬁedℓ2 ro-\nbustness: the duality approach from Wong et al. (2018),\nthe Lipschitz approach from Tsuzuku et al. (2018), and the\nCertiﬁed Adversarial Robustness via Randomized Smoothing\n0.0 0.2 0.4 0.6 0.8 1.0\nradius\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0certified accuracy\nours\n(Lecuyer et al, 2019)\n(Li et al, 2018)\n0.0 0.2 0.4 0.6 0.8 1.0 1.2 1.4\nradius\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0certified accuracy\nn = 1,000\nn = 10,000\nn = 100,000\nn = 1,000,000\nn = 10,000,000\n0.0 0.2 0.4 0.6 0.8 1.0\nradius\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0certified accuracy\n99.9999% confidence\n99.999% confidence\n99.99% confidence\n99.9% confidence\n99% confidence\nFigure 8. Experiments with randomized smoothing on ImageNet withσ = 0.25. Left: certiﬁed accuracies obtained using our Theorem 1\nversus those obtained using the robustness guarantees derived in prior work. Middle: projections for the certiﬁed accuracy if the number\nof samplesn used by CERTIFY had been larger or smaller. Right: certiﬁed accuracy as the failure probabilityα of CERTIFY is varied.\napproach from Weng et al. (2018a); Zhang et al. (2018).\nThe strongest baseline was Wong et al. (2018); we defer the\ncomparison to the other two baselines to Appendix H.\nIn Figure 7, we compare the largest publicly released model\nfrom Wong et al. (2018), a small resnet, to two randomized\nsmoothing classiﬁers: one which used the same small resnet\narchitecture for its base classiﬁer, and one which used a\nlarger 110-layer resnet for its base classiﬁer. First, observe\nthat smoothing with the large 110-layer resnet substantially\noutperforms the baseline (across all hyperparameter set-\ntings) at all radii. Second, observe that smoothing with the\nsmall resnet also outperformed the method of Wong et al.\n(2018) at all but the smallest radii. We attribute this latter re-\nsult to the fact that neural networks trained using the method\nof Wong et al. (2018) are “typically overregularized to the\npoint that many ﬁlters/weights become identically zero,” per\nthat paper. In contrast, the base classiﬁer in randomized\nsmoothing is a fully expressive neural network.\nPrediction It is computationally expensive to certify the\nrobustness of g around a point x, since the value of n in\nCERTIFY must be very large. However, it is far cheaper\nto evaluate g at x using PREDICT , since n can be small.\nFor example, when we ran PREDICT on ImageNet (σ =\n0.25) using n = 100, making each prediction only took\n0.15 seconds, and we attained a top-1 test accuracy of 65%\n(Appendix E).\nAs discussed earlier, an adversary can potentially forcePRE-\nDICT to abstain with high probability. However, it is rela-\ntively rare for PREDICT to abstain on the actual data dis-\ntribution. On ImageNet (σ = 0.25), PREDICT with failure\nprobabilityα = 0.001 abstained 12% of the time whenn =\n100, 4% whenn = 1000, and 1% whenn = 10,000.\nEmpirical tightness of bound When f is linear, the\nbound in Theorem 1 is tight, in that there always exists a\nclass-changing perturbation just beyond the certiﬁed radius.\nSince deep neural networks are not linear, we empirically as-\nsessed the tightness of our bound by subjecting an ImageNet\nrandomized smoothing classiﬁer (σ = 0.25) to a projected\ngradient descent-style adversarial attack. For each example,\nwe ran CERTIFY withα = 0.01, and, if the example was\ncorrectly classiﬁed and certiﬁed robust at radiusR, we tried\nﬁnding an adversarial example forg within radius 1.5R and\nwithin radius 2R. We succeeded 17% of the time at radius\n1.5R and 53% of the time at radius 2R. See Appendix J.3\nfor more details on the attack.\n5. Conclusion\nTheorem 2 establishes that smoothing with Gaussian noise\nnaturally confers adversarial robustness inℓ2 norm: if we\nhave no knowledge about the base classiﬁer beyond the dis-\ntribution off(x +ε), then the set of perturbations to which\nthe smoothed classiﬁer is provably robust is precisely anℓ2\nball. We suspect that smoothing with other noise distribu-\ntions may lead to similarly natural robustness guarantees for\nother perturbation sets such as generalℓp norm balls.\nOur strong empirical results suggest that randomized\nsmoothing is a promising direction for future research\ninto adversarially robust classiﬁcation. Most empirical ap-\nproaches (except PGD adversarial training) have been “bro-\nken,” and provable approaches based on certifying neural\nnetwork classiﬁers have not been shown to scale to networks\nof modern size. It seems to be computationally infeasible to\nreason in any sophisticated way about the decision bound-\naries of a large, expressive neural network. Randomized\nsmoothing circumvents this problem: the smoothed classi-\nﬁer is not itself a neural network, though it leverages the\ndiscriminative ability of a neural network base classiﬁer. To\nmake the smoothed classiﬁer robust, one need simply make\nthe base classiﬁer classify well under noise. In this way,\nrandomized smoothing reduces the unsolved problem of\nadversarially robust classiﬁcation to the comparably solved\ndomain of supervised learning.\nCertiﬁed Adversarial Robustness via Randomized Smoothing\n6. Acknowledgements\nWe thank Mateusz Kwa ´snicki for help with Lemma 4 in\nthe appendix, Aaditya Ramdas for pointing us toward the\nwork of Hung & Fithian (2019), and Siva Balakrishnan\nfor helpful discussions regarding the conﬁdence interval in\nAppendix D. We thank Tolani Olarinre, Adarsh Prasad, Ben\nCousins, and Ramon Van Handel for useful conversations.\nFinally, we are very grateful to Vaishnavh Nagarajan, Arun\nSai Suggala, Shaojie Bai, Mikhail Khodak, Han Zhao, and\nZachary Lipton for reviewing drafts of this work. Jeremy\nCohen is supported by a grant from the Bosch Center for\nAI.\nReferences\nAnil, C., Lucas, J., and Grosse, R. B. Sorting out lips-\nchitz function approximation. In Proceedings of the 36th\nInternational Conference on Machine Learning, 2019.\nAthalye, A. and Carlini, N. On the robustness of the cvpr\n2018 white-box adversarial example defenses. The Bright\nand Dark Sides of Computer Vision: Challenges and\nOpportunities for Privacy and Security, 2018.\nAthalye, A., Carlini, N., and Wagner, D. Obfuscated gra-\ndients give a false sense of security: Circumventing de-\nfenses to adversarial examples. InProceedings of the 35th\nInternational Conference on Machine Learning, 2018.\nBiggio, B., Corona, I., Maiorca, D., Nelson, B., ˇSrndi´c, N.,\nLaskov, P., Giacinto, G., and Roli, F. Evasion attacks\nagainst machine learning at test time. Joint European\nConference on Machine Learning and Knowledge Dis-\ncovery in Database, 2013.\nBlanchard, G. Lecture Notes, 2007. URL\nhttp://www.math.uni-potsdam.de/\n˜blanchard/lectures/lect_2.pdf.\nBunel, R. R., Turkaslan, I., Torr, P., Kohli, P., and\nMudigonda, P. K. A uniﬁed view of piecewise linear\nneural network veriﬁcation. In Advances in Neural Infor-\nmation Processing Systems 31. 2018.\nCao, X. and Gong, N. Z. Mitigating evasion attacks to deep\nneural networks via region-based classiﬁcation. 33rd An-\nnual Computer Security Applications Conference, 2017.\nCarlini, N. and Wagner, D. Adversarial examples are not\neasily detected: Bypassing ten detection methods. In\nProceedings of the 10th ACM Workshop on Artiﬁcial\nIntelligence and Security, 2017.\nCarlini, N., Katz, G., Barrett, C., and Dill, D. L. Provably\nminimally-distorted adversarial examples. arXiv preprint\narXiv: 1709.10207, 2017.\nCheng, C.-H., N ¨uhrenberg, G., and Ruess, H. Maximum\nresilience of artiﬁcial neural networks. International\nSymposium on Automated Technology for Veriﬁcation\nand Analysis, 2017.\nCisse, M., Bojanowski, P., Grave, E., Dauphin, Y ., and\nUsunier, N. Parseval networks: Improving robustness to\nadversarial examples. In Proceedings of the 34th Interna-\ntional Conference on Machine Learning, 2017.\nClopper, C. J. and Pearson, E. S. The use of conﬁdence\nor ﬁducial limits illustrated in the case of the binomial.\nBiometrika, 26(4):pp. 404–413, 1934. ISSN 00063444.\nCroce, F., Andriushchenko, M., and Hein, M. Provable\nrobustness of relu networks via maximization of linear\nregions. In Proceedings of the 22nd International Con-\nference on Artiﬁcial Intelligence and Statistics, 2019.\nDeng, J., Dong, W., Socher, R., Li, L.-J., Li, K., and Fei-\nFei, L. ImageNet: A Large-Scale Hierarchical Image\nDatabase. In IEEE Conference on Computer Vision and\nPattern Recognition (CVPR), 2009.\nDodge, S. and Karam, L. A study and comparison of hu-\nman and deep learning recognition performance under\nvisual distortions. 2017 26th International Conference on\nComputer Communication and Networks (ICCCN), 2017.\nDuchi, J. C., Bartlett, P. L., and Wainwright, M. J. Ran-\ndomized smoothing for stochastic optimization. SIAM\nJournal on Optimization, 2012. URL https://doi.\norg/10.1137/110831659.\nDutta, S., Jha, S., Sanakaranarayanan, S., and Tiwari, A.\nOutput range analysis for deep neural networks. arXiv\npreprint arXiv:1709.09130, 2017.\nDvijotham, K., Gowal, S., Stanforth, R., Arandjelovic, R.,\nO’Donoghue, B., Uesato, J., and Kohli, P. Training\nveriﬁed learners with learned veriﬁers. arXiv preprint\narXiv:1805.10265, 2018a.\nDvijotham, K., Stanforth, R., Gowal, S., Mann, T., and\nKohli, P. A dual approach to scalable veriﬁcation of\ndeep networks. Proceedings of the Thirty-Fourth Con-\nference Annual Conference on Uncertainty in Artiﬁcial\nIntelligence (UAI-18), 2018b.\nEhlers, R. Formal veriﬁcation of piece-wise linear feed-\nforward neural networks. In Automated Technology for\nVeriﬁcation and Analysis, 2017.\nFawzi, A., Moosavi-Dezfooli, S.-M., and Frossard, P. Ro-\nbustness of classiﬁers: from adversarial to random noise.\nIn Advances in Neural Information Processing Systems\n29. 2016.\nCertiﬁed Adversarial Robustness via Randomized Smoothing\nFischetti, M. and Jo, J. Deep neural networks and mixed\ninteger linear optimization. Constraints, 23(3):296–309,\nJuly 2018.\nFord, N., Gilmer, J., and Cubuk, E. D. Adversarial ex-\namples are a natural consequence of test error in noise.\nIn Proceedings of the 36th International Conference on\nMachine Learning, 2019.\nFranceschi, J.-Y ., Fawzi, A., and Fawzi, O. Robustness\nof classiﬁers to uniformℓ p and gaussian noise. In 21st\nInternational Conference on Artiﬁcial Intelligence and\nStatistics (AISTATS). 2018.\nGehr, T., Mirman, M., Drachsler-Cohen, D., Tsankov, P.,\nChaudhuri, S., and Vechev, M. T. AI2: safety and ro-\nbustness certiﬁcation of neural networks with abstract\ninterpretation. In 2018 IEEE Symposium on Security and\nPrivacy, SP 2018, Proceedings, 21-23 May 2018, San\nFrancisco, California, USA, pp. 3–18, 2018.\nGoodfellow, I. J., Shlens, J., and Szegedy, C. Explaining\nand harnessing adversarial examples. In International\nConference on Learning Representations, 2015.\nGouk, H., Frank, E., Pfahringer, B., and Cree, M. Regulari-\nsation of neural networks by enforcing lipschitz continu-\nity. arXiv preprint arXiv:1804.04368, 2018.\nGowal, S., Dvijotham, K., Stanforth, R., Bunel, R., Qin,\nC., Uesato, J., Arandjelovic, R., Mann, T., and Kohli, P.\nOn the effectiveness of interval bound propagation for\ntraining veriﬁably robust models, 2018.\nHein, M. and Andriushchenko, M. Formal guarantees on the\nrobustness of a classiﬁer against adversarial manipulation.\nIn Advances in Neural Information Processing Systems\n30. 2017.\nHuang, X., Kwiatkowska, M., Wang, S., and Wu, M. Safety\nveriﬁcation of deep neural networks. Computer Aided\nVeriﬁcation, 2017.\nHung, K. and Fithian, W. Rank veriﬁcation for exponential\nfamilies. The Annals of Statistics, (2):758–782, 04 2019.\nKannan, H., Kurakin, A., and Goodfellow, I. Adversarial\nlogit pairing. arXiv preprint arXiv:1803.06373, 2018.\nKatz, G., Barrett, C., Dill, D. L., Julian, K., and Kochender-\nfer, M. J. Reluplex: An efﬁcient smt solver for verifying\ndeep neural networks. Lecture Notes in Computer Sci-\nence, pp. 97–117, 2017. ISSN 1611-3349.\nKolter, J. Z. and Madry, A. Adversarial ro-\nbustness: Theory and practice. https:\n//adversarial-ml-tutorial.org/\nadversarial_examples/, 2018.\nKrizhevsky, A. Learning multiple layers of features from\ntiny images. Technical report, 2009.\nKurakin, A., Goodfellow, I. J., and Bengio, S. Adver-\nsarial machine learning at scale. 2017. URL https:\n//arxiv.org/abs/1611.01236.\nLecuyer, M., Atlidakis, V ., Geambasu, R., Hsu, D., and\nJana, S. Certiﬁed robustness to adversarial examples with\ndifferential privacy. In IEEE Symposium on Security and\nPrivacy (SP), 2019.\nLi, B., Chen, C., Wang, W., and Carin, L. Second-order ad-\nversarial attack and certiﬁable robustness. arXiv preprint\narXiv:1809.03113, 2018.\nLiu, X., Cheng, M., Zhang, H., and Hsieh, C.-J. Towards\nrobust neural networks via random self-ensemble. In\nThe European Conference on Computer Vision (ECCV),\nSeptember 2018.\nLomuscio, A. and Maganti, L. An approach to reachability\nanalysis for feed-forward relu neural networks, 2017.\nMadry, A., Makelov, A., Schmidt, L., Tsipras, D., and\nVladu, A. Towards deep learning models resistant to\nadversarial attacks. In International Conference on Learn-\ning Representations, 2018.\nMirman, M., Gehr, T., and Vechev, M. Differentiable ab-\nstract interpretation for provably robust neural networks.\nIn Proceedings of the 35th International Conference on\nMachine Learning, 2018.\nMoosavi-Dezfooli, S.-M., Fawzi, A., and Frossard, P. Deep-\nfool: A simple and accurate method to fool deep neural\nnetworks. 2016 IEEE Conference on Computer Vision\nand Pattern Recognition (CVPR), 2016.\nNeyman, J. and Pearson, E. S. On the problem of the most\nefﬁcient tests of statistical hypotheses. Philosophical\nTransactions of the Royal Society of London. Series A,\nContaining Papers of a Mathematical or Physical Char-\nacter, 231:289–337, 1933.\nRaghunathan, A., Steinhardt, J., and Liang, P. Certiﬁed\ndefenses against adversarial examples. In International\nConference on Learning Representations, 2018a.\nRaghunathan, A., Steinhardt, J., and Liang, P. Semideﬁ-\nnite relaxations for certifying robustness to adversarial\nexamples. In Advances in Neural Information Processing\nSystems 31, 2018b.\nSingh, G., Gehr, T., Mirman, M., P¨uschel, M., and Vechev,\nM. Fast and effective robustness certiﬁcation. In Ad-\nvances in Neural Information Processing Systems 31 .\n2018.\nCertiﬁed Adversarial Robustness via Randomized Smoothing\nSmilkov, D., Thorat, N., Kim, B., Vi´egas, F., and Watten-\nberg, M. Smoothgrad: removing noise by adding noise.\narXiv preprint arXiv:1706.03825, 2017.\nSzegedy, C., Zaremba, W., Sutskever, I., Bruna, J., Erhan,\nD., Goodfellow, I., and Fergus, R. Intriguing proper-\nties of neural networks. In International Conference on\nLearning Representations, 2014.\nTjeng, V ., Xiao, K. Y ., and Tedrake, R. Evaluating robust-\nness of neural networks with mixed integer programming.\nIn International Conference on Learning Representations,\n2019. URL https://openreview.net/forum?\nid=HyGIdiRqtm.\nTsipras, D., Santurkar, S., Engstrom, L., Turner, A., and\nMadry, A. Robustness may be at odds with accuracy. In\nInternational Conference on Learning Representations,\n2019. URL https://openreview.net/forum?\nid=SyxAb30cY7.\nTsuzuku, Y ., Sato, I., and Sugiyama, M. Lipschitz-margin\ntraining: Scalable certiﬁcation of perturbation invariance\nfor deep neural networks. In Advances in Neural Infor-\nmation Processing Systems 31. 2018.\nUesato, J., O’Donoghue, B., Kohli, P., and van den Oord,\nA. Adversarial risk and the dangers of evaluating against\nweak attacks. In Proceedings of the 35th International\nConference on Machine Learning, 2018.\nWang, S., Chen, Y ., Abdou, A., and Jana, S. Mixtrain: Scal-\nable training of formally robust neural networks. arXiv\npreprint arXiv:1811.02625, 2018a.\nWang, S., Pei, K., Whitehouse, J., Yang, J., and Jana, S.\nEfﬁcient formal safety analysis of neural networks. In\nAdvances in Neural Information Processing Systems 31.\n2018b.\nWebb, S., Rainforth, T., Teh, Y . W., and Kumar, M. P.\nStatistical veriﬁcation of neural networks. In In-\nternational Conference on Learning Representations ,\n2019. URL https://openreview.net/forum?\nid=S1xcx3C5FX.\nWeng, L., Zhang, H., Chen, H., Song, Z., Hsieh, C.-J.,\nDaniel, L., Boning, D., and Dhillon, I. Towards fast\ncomputation of certiﬁed robustness for ReLU networks.\nIn Proceedings of the 35th International Conference on\nMachine Learning, 2018a.\nWeng, T.-W., Zhang, H., Chen, P.-Y ., Yi, J., Su, D., Gao, Y .,\nHsieh, C.-J., and Daniel, L. Evaluating the robustness of\nneural networks: An extreme value theory approach. In\nInternational Conference on Learning Representations,\n2018b.\nWong, E. and Kolter, J. Z. Provable defenses against adver-\nsarial examples via the convex outer adversarial polytope.\nIn Proceedings of the 35th International Conference on\nMachine Learning, 2018.\nWong, E., Schmidt, F., Metzen, J. H., and Kolter, J. Z.\nScaling provable adversarial defenses. In Advances in\nNeural Information Processing Systems 31, 2018.\nZantedeschi, V ., Nicolae, M.-I., and Rawat, A. Efﬁcient de-\nfenses against adversarial attacks.Proceedings of the 10th\nACM Workshop on Artiﬁcial Intelligence and Security -\nAISec ’17, 2017.\nZhang, H., Weng, T.-W., Chen, P.-Y ., Hsieh, C.-J., and\nDaniel, L. Efﬁcient neural network robustness certiﬁca-\ntion with general activation functions. In Advances in\nNeural Information Processing Systems 31. 2018.",
  "values": {
    "Interpretable (to users)": "Yes",
    "Autonomy (power to decide)": "Yes",
    "Non-maleficence": "Yes",
    "Transparent (to users)": "Yes",
    "Respect for Law and public interest": "Yes",
    "Fairness": "Yes",
    "Explicability": "Yes",
    "User influence": "Yes",
    "Critiqability": "Yes",
    "Privacy": "Yes",
    "Not socially biased": "Yes",
    "Respect for Persons": "Yes",
    "Collective influence": "Yes",
    "Justice": "Yes",
    "Deferral to humans": "Yes",
    "Beneficence": "Yes"
  }
}