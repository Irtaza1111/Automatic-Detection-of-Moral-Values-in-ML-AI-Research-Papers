{
  "pdf": "du19c",
  "title": "Gradient Descent Finds Global Minima of Deep Neural Networks",
  "author": "Simon S. Du, Jason D. Lee, Haochuan Li, Liwei Wang, Xiyu Zhai",
  "paper_id": "du19c",
  "text": "Gradient Descent Finds Global Minima of Deep Neural Networks\nSimon S. Du * 1 Jason D. Lee * 2 Haochuan Li * 3 4 Liwei Wang * 5 4 Xiyu Zhai * 6\nAbstract\nGradient descent ﬁnds a global minimum in\ntraining deep neural networks despite the objec-\ntive function being non-convex. The current pa-\nper proves gradient descent achieves zero train-\ning loss in polynomial time for a deep over-\nparameterized neural network with residual con-\nnections (ResNet). Our analysis relies on the par-\nticular structure of the Gram matrix induced by\nthe neural network architecture. This structure al-\nlows us to show the Gram matrix is stable through-\nout the training process and this stability implies\nthe global optimality of the gradient descent al-\ngorithm. We further extend our analysis to deep\nresidual convolutional neural networks and obtain\na similar convergence result.\n1. Introduction\nOne of the mysteries in deep learning is randomly initial-\nized ﬁrst-order methods like gradient descent achieve zero\ntraining loss, even if the labels are arbitrary (Zhang et al.,\n2016). Over-parameterization is widely believed to be the\nmain reason for this phenomenon as only if the neural net-\nwork has a sufﬁciently large capacity, it is possible for this\nneural network to ﬁt all the training data. For example, Lu\net al. (2017) proved that except for a measure zero set, all\nfunctions cannot be approximated by ReLU networks with a\nwidth less than the input dimension. In practice, many neu-\nral network architectures are highly over-parameterized. For\nexample, Wide Residual Networks have 100x parameters\nthan the number of training data (Zagoruyko & Komodakis,\n2016).\n*Equal contribution 1Machine Learning Department, Carnegie\nMellon University 2Data Science and Operations Department, Uni-\nversity of Southern California 3School of Physics, Peking Univer-\nsity 4Center for Data Science, Peking University, Beijing Institute\nof Big Data Research 5Key Laboratory of Machine Perception,\nMOE, School of EECS, Peking University 6Department of EECS,\nMassachusetts Institute of Technology. Correspondence to: Simon\nS. Du <ssdu@cs.cmu.edu>.\nProceedings of the 36 th International Conference on Machine\nLearning, Long Beach, California, PMLR 97, 2019. Copyright\n2019 by the author(s).\nThe second mysterious phenomenon in training deep neural\nnetworks is “deeper networks are harder to train.” To solve\nthis problem, He et al. (2016) proposed the deep residual\nnetwork (ResNet) architecture which enables randomly ini-\ntialized ﬁrst order method to train neural networks with an\norder of magnitude more layers. Theoretically, Hardt & Ma\n(2016) showed that residual links in linear networks prevent\ngradient vanishing in a large neighborhood of zero, but for\nneural networks with non-linear activations, the advantages\nof using residual connections are not well understood.\nIn this paper, we demystify these two mysterious phenom-\nena. We consider the setting where there aren data points,\nand the neural network has H layers with width m. We\nfocus on the least-squares loss and assume the activation\nfunction is Lipschitz and smooth. This assumption holds\nfor many activation functions including the soft-plus and\nsigmoid. Our contributions are summarized below.\n• As a warm-up, we ﬁrst consider a fully-connected\nfeedforward network. We show if\nm =\nΩ\n(\npoly(n)2O(H))1, then randomly initialized gradi-\nent descent converges to zero training loss at a linear\nrate.\n• Next, we consider the ResNet architecture. We show\nas long asm = Ω (poly(n,H )), then randomly initial-\nized gradient descent converges to zero training loss at\na linear rate. Comparing with the ﬁrst result, the depen-\ndence on the number of layers improves exponentially\nfor ResNet. This theory demonstrates the advantage of\nusing residual architectures.\n• Lastly, we apply the same technique to analyze con-\nvolutional ResNet. We show if m = poly(n,p,H )\nwherep is the number of patches, then randomly ini-\ntialized gradient descent achieves zero training loss.\nOur proof builds on two ideas from previous work on gra-\ndient descent for two-layer neural networks. First, we use\nthe observation by (Li & Liang, 2018) that if the neural\nnetwork is over-parameterized, every weight matrix is close\nto its initialization. Second, following (Du et al., 2018b),\n1The precise polynomials and data-dependent parameters are\nstated in Section 5, 6, 7.\nGradient Descent Finds Global Minima of Deep Neural Networks\nwe analyze the dynamics of the predictions whose conver-\ngence is determined by the least eigenvalue of the Gram\nmatrix induced by the neural network architecture and to\nlower bound the least eigenvalue, it is sufﬁcient to bound\nthe distance of each weight matrix from its initialization.\nDifferent from these two works, in analyzing deep neural\nnetworks, we need to exploit more structural properties of\ndeep neural networks and develop new techniques for ana-\nlyzing both the initialization and gradient descent dynamics.\nIn Section 4 we give an overview of our proof technique.\n1.1. Organization\nThis paper is organized as follows. In Section 2, we discuss\nrelated works. In Section 3, we formally state the problem\nsetup. In Section 4, we present our main analysis techniques.\nIn Section 5, we give a warm-up result for the deep fully-\nconnected neural network. In Section 6, we give our main\nresult for the ResNet. In Section 7, we give our main result\nfor the convolutional ResNet. We conclude in Section 8 and\ndefer all proofs to the appendix.\n2. Related Works\nRecently, many works try to study the optimization problem\nin deep learning. Since optimizing a neural network is a\nnon-convex problem, one approach is ﬁrst to develop a gen-\neral theory for a class of non-convex problems which satisfy\ndesired geometric properties and then identify that the neu-\nral network optimization problem belongs to this class. One\npromising candidate class is the set of functions that satisfy:\na) all local minima are global and b) there exists a negative\ncurvature for every saddle point. For this function class,\nresearchers have shown (perturbed) gradient descent (Jin\net al., 2017; Ge et al., 2015; Lee et al., 2016; Du et al.,\n2017a) can ﬁnd a global minimum. Many previous works\nthus try to study the optimization landscape of neural net-\nworks with different activation functions (Soudry & Hoffer,\n2017; Safran & Shamir, 2018; 2016; Zhou & Liang, 2017;\nFreeman & Bruna, 2016; Hardt & Ma, 2016; Nguyen &\nHein, 2017; Kawaguchi, 2016; Venturi et al., 2018; Soudry\n& Carmon, 2016; Du & Lee, 2018; Soltanolkotabi et al.,\n2018; Haeffele & Vidal, 2015). However, even for a three-\nlayer linear network, there exists a saddle point that does\nnot have a negative curvature (Kawaguchi, 2016), so it is\nunclear whether this geometry-based approach can be used\nto obtain the global convergence guarantee of ﬁrst-order\nmethods.\nAnother way to attack this problem is to study the dynamics\nof a speciﬁc algorithm for a speciﬁc neural network architec-\nture. Our paper also belongs to this category. Many previous\nworks put assumptions on the input distribution and assume\nthe label is generated according to a planted neural net-\nwork. Based on these assumptions, one can obtain global\nconvergence of gradient descent for some shallow neural\nnetworks (Tian, 2017; Soltanolkotabi, 2017; Brutzkus &\nGloberson, 2017; Du et al., 2018a; Li & Yuan, 2017; Du\net al., 2017b). Some local convergence results have also\nbeen proved (Zhong et al., 2017a;b; Zhang et al., 2018). In\ncomparison, our paper does not try to recover the underly-\ning neural network. Instead, we focus on minimizing the\ntraining loss and rigorously prove that randomly initialized\ngradient descent can achieve zero training loss.\nThe most related papers are (Li & Liang, 2018; Du\net al., 2018b) who observed that when training an over-\nparametrized two-layer fully-connected neural network, the\nweights do not change a large amount, which we also use to\nshow the stability of the Gram matrix. They used this obser-\nvation to obtain the convergence rate of gradient descent on\na two-layer over-parameterized neural network for the cross-\nentropy and least-squares loss. More recently, Allen-Zhu\net al. (2018b) generalized ideas from (Li & Liang, 2018)\nto derive convergence rates of training recurrent neural net-\nworks.\nOur work extends these previous results in several ways: a)\nwe consider deep networks, b) we generalize to ResNet ar-\nchitectures, and c) we generalize to convolutional networks.\nTo improve the width dependencem on sample sizen, we\nutilize a smooth activation (e.g. smooth ReLU). For exam-\nple, our results specialized to depth H = 1 improve upon\n(Du et al., 2018b) in the required amount of overparametriza-\ntion fromm = Ω\n(\nn6)\ntom = Ω\n(\nn4)\n. See Theorem 5.1\nfor the precise statement.\nChizat & Bach (2018b) brought to our attention the paper\nof Jacot et al. (2018) which proved a similar weight stability\nphenomenon for deep networks, but only in the asymptotic\nsetting of inﬁnite-width networks and gradient ﬂow run\nfor a ﬁnite time. Jacot et al. (2018) do not establish the\nconvergence of gradient ﬂow to a global minimizer. In lieu\nof their results, our work can be viewed as a generalization\nof their result to: a) ﬁnite width, b) gradient descent as\nopposed to gradient ﬂow, and c) convergence to a global\nminimizer.\nMei et al. (2018); Chizat & Bach (2018a); Sirignano &\nSpiliopoulos (2018); Rotskoff & Vanden-Eijnden (2018);\nWei et al. (2018) used optimal transport theory to analyze\ngradient descent on over-parameterized models. However,\ntheir results are limited to two-layer neural networks and\nmay require an exponential amount of over-parametrization.\nDaniely (2017) developed the connection between deep\nneural networks with kernel methods and showed stochastic\ngradient descent can learn a function that is competitive with\nthe best function in the conjugate kernel space of the net-\nwork. Andoni et al. (2014) showed that gradient descent can\nGradient Descent Finds Global Minima of Deep Neural Networks\nlearn networks that are competitive with polynomial classi-\nﬁers. However, these results do not imply gradient descent\ncan ﬁnd a global minimum for the empirical loss minimiza-\ntion problem. Our analysis of the Gram matrices at random\ninitialization is closely related to prior work on the analysis\nof inﬁnite-width networks as Gaussian Processes (Raghu\net al., 2016; Matthews et al., 2018; Lee et al., 2017; Schoen-\nholz et al., 2016). Since we require the initialization analysis\nfor three distinct architectures (ResNet, feed-forward, and\nconvolutional ResNet), we re-derive many of these prior\nresults in a uniﬁed fashion in Appendix E.\nFinally, in concurrent work, Allen-Zhu et al. (2018c) also\nanalyze gradient descent on deep neural networks. The pri-\nmary difference between the two papers is that we analyze\ngeneral smooth activations, and Allen-Zhu et al. (2018c)\ndevelop speciﬁc analysis for ReLU activation. The two pa-\npers also differ signiﬁcantly on their data assumptions. We\nwish to emphasize a fair comparison is not possible due to\nthe difference in setting and data assumptions. We view the\ntwo papers as complementary since they address different\nneural net architectures.\nFor ResNet, the primary focus of this manuscript, the\nrequired width per layer for Allen-Zhu et al. (2018c) is\nm ≳ n30H 30 log2 1\nϵ and for this paper’s Theorem 6.1 is\nm ≳ n4H 2.2 Our paper requires a width m that does not\ndepend on the desired accuracyϵ. As a consequence, Theo-\nrem 6.1 guarantees the convergence of gradient descent to\na global minimizer. The iteration complexity of Allen-Zhu\net al. (2018c) is T ≳ n6H 2 log 1\nϵ and of Theorem 6.1 is\nT ≳n2 log 1\nϵ .\nFor fully-connected networks, Allen-Zhu et al. (2018c) re-\nquires width m ≳ n30H 30 log2 1\nϵ and iteration complex-\nity T ≳ n6H 2 log 1\nϵ . Theorem 5.1 requires width m ≳\nn42O(H) and iteration complexityT ≳n22O(H) log 1\nϵ . The\nprimary difference is for very deep fully-connected net-\nworks, Allen-Zhu et al. (2018c) has milder dependence on\nH, but worse dependence on n. Commonly used fully-\nconnected networks such as VGG are not extremely deep\n(H = 16), yet the dataset size such as ImageNet (n∼ 106)\nis very large.\nIn a second concurrent work, Zou et al. (2018) also analyzed\nthe convergence of gradient descent on fully-connected net-\nworks with ReLU activation. The emphasis is on different\nloss functions (e.g. hinge loss), so the results are not directly\ncomparable. Both Zou et al. (2018) and Allen-Zhu et al.\n(2018c) train a subset of the layers, instead of all the layers\nas in this work, but also analyze stochastic gradient.\n2In all comparisons, we ignore the polynomial dependency on\ndata-dependent parameters which only depends on the input data\nand the activation function. The two papers use different measures\nand are not directly comparable.\n3. Preliminaries\n3.1. Notations\nWe Let [n] = {1, 2,...,n }. We use N(0, I) to denote\nthe standard Gaussian distribution. For a matrix A, we\nuse Aij to denote its (i,j )-th entry. We will also use Ai,:\nto denote the i-th row vector of A and deﬁne Ai,j:k =\n(Ai,j, Ai,j+1,··· , Ai,k) as part of the vector. Similarly\nA:,i is the i-th column vector and Aj:k,i is a part of i-th\ncolumn vector. For a vector v, we use∥v∥2 to denote the\nEuclidean norm. For a matrix A we use∥A∥F to denote\nthe Frobenius norm and∥A∥2 to denote the operator norm.\nIf a matrix A is positive semi-deﬁnite, we useλmin(A) to\ndenote its smallest eigenvalue. We use⟨·,·⟩ to denote the\nstandard Euclidean inner product between two vectors or\nmatrices. We letO(·) and Ω (·) denote standard Big-O and\nBig-Omega notations, only hiding constants. In this paper\nwe will useC andc to denote constants. The speciﬁc value\ncan be different from line to line.\n3.2. Activation Function\nWe useσ (·) to denote the activation function. In this pa-\nper we impose some technical conditions on the activa-\ntion function. The guiding example is softplus: σ (z) =\nlog(1 + exp(z)).\nCondition 3.1 (Lipschitz and Smooth). There exists a con-\nstantc> 0 such that|σ (0)|≤ c and for anyz,z′∈ R,\n|σ (z)−σ (z′)|≤c|z−z′|,\nand|σ′(z)−σ′(z)|≤c|z−z′|.\nThese two conditions will be used to show the stability of the\ntraining process. Note for softplus both Lipschitz constant\nand smoothness constant are 1. In this paper, we view all\nactivation function related parameters as constants.\nCondition 3.2. σ (·) is analytic and is not a polynomial\nfunction.\nThis assumption is used to guarantee the positive-\ndeﬁniteness of certain Gram matrices which we will de-\nﬁne later. Softplus function satisﬁes this assumption by\ndeﬁnition.\n3.3. Problem Setup\nIn this paper, we focus on the empirical risk minimization\nproblem with the quadratic loss function\nmin\nθ\nL(θ) = 1\n2\nn∑\ni=1\n(f(θ, xi)−yi)2 (1)\nwhere{xi}n\ni=1 are the training inputs,{yi}n\ni=1\nare the labels,\nθ is the parameter we optimize over andf is the prediction\nGradient Descent Finds Global Minima of Deep Neural Networks\nfunction, which in our case is a neural network. We consider\nthe following architectures.\n• Multilayer fully-connected neural networks: Let\nx∈ Rd be the input, W(1)∈ Rm×d is the ﬁrst weight\nmatrix, W(h)∈ Rm×m is the weight at theh-th layer\nfor 2≤h≤H, a∈ Rm is the output layer andσ (·)\nis the activation function. 3 We deﬁne the prediction\nfunction recursively (for simplicity we let x(0) = x).\nx(h) =\n√cσ\nmσ\n(\nW(h)x(h−1)\n)\n, 1≤h≤H\nf(x,θ ) = a⊤x(H). (2)\nwherecσ =\n(\nEx∼N (0,1)\n[\nσ(x)2])−1\nis a scaling factor\nto normalize the input in the initialization phase.\n• ResNet4: We use the same notations as the multilayer\nfully connected neural networks. We deﬁne the predic-\ntion recursively.\nx(1) =\n√cσ\nmσ\n(\nW(1)x\n)\n,\nx(h) = x(h−1) + cres\nH√mσ\n(\nW(h)x(h−1)\n)\nfor 2≤h≤H,\nfres(x,θ ) = a⊤x(H) (3)\nwhere 0<c res < 1 is a small constant. Note here we\nuse a cres\nH√m scaling. This scaling plays an important\nrole in guaranteeing the width per layer only needs to\nscale polynomially withH. In practice, the small scal-\ning is enforced by a small initialization of the residual\nconnection (Hardt & Ma, 2016; Zhang et al., 2019),\nwhich obtains state-of-the-art performance for deep\nresidual networks. We choose to use an explicit scal-\ning, instead of altering the initialization scheme for\nnotational convenience.\n• Convolutional ResNet: Lastly, we consider the convo-\nlutional ResNet architecture. Again we deﬁne the pre-\ndiction function in a recursive way. Let x(0)∈ Rd0×p\nbe the input, whered0 is the number of input channels\nandp is the number of pixels. For h∈ [H], we let\nthe number of channels be dh = m and number of\n3We assume intermediate layers are square matrices for sim-\nplicity. It is not difﬁcult to generalize our analysis to rectangular\nweight matrices.\n4We will refer to this architecture as ResNet, although this dif-\nfers by the standard ResNet architecture since the skip-connections\nat every layer, instead of every two layers. This architecture was\npreviously studied in (Hardt & Ma, 2016). We study this archi-\ntecture for the ease of presentation and analysis. It is not hard to\ngeneralize our analysis to architectures with skip-connections are\nevery two or more layers.\npixels be p. Given x(h−1)∈ Rdh−1×p forh∈ [H],\nwe ﬁrst use an operatorφh(·) to divide x(h−1) intop\npatches. Each patch has sizeqdh−1 and this implies a\nmapφh(x(h−1))∈ Rqdh−1×p. For example, when the\nstride is 1 andq = 3\nφh(x(h−1))\n=\n\n\n(\nx(h−1)\n1,0:2\n)⊤\n, ... ,\n(\nx(h−1)\n1,p−1:p+1\n)⊤\n..., ..., ...(\nx(h−1)\ndh−1,0:2\n)⊤\n, ...,\n(\nx(h−1)\ndh−1,p−1:p+1\n)⊤\n\n\nwhere we let x(h−1)\n:,0 = x(h−1)\n:,p+1 = 0, i.e., zero-padding.\nNote this operator has the property\nx(h−1)\n\nF\n≤\nφh(x(h−1))\n\nF\n≤√q\nx(h−1)\n\nF\n.\nbecause each element from x(h−1) at least appears\nonce and at most appears q times. In practice, q is\noften small like 3× 3, so throughout the paper we view\nq as a constant in our theoretical analysis. To proceed,\nlet W(h)∈ Rdh×qdh−1, we have\nx(1) =\n√cσ\nmσ\n(\nW(1)φ1(x)\n)\n∈ Rm×p,\nx(h) =x(h−1) + cres\nH√mσ\n(\nW(h)φh(x(h−1))\n)\n∈ Rm×p\nfor 2≤h≤H,\nwhere 0 < cres < 1 is a small constant. Finally, for\na∈ Rm×p, the output is deﬁned as\nfcnn(x,θ ) =⟨a, x(H)⟩.\nNote here we use the similar scaling O( 1\nH√m) as\nResNet.\nTo learn the deep neural network, we consider the randomly\ninitialized gradient descent algorithm to ﬁnd the global min-\nimizer of the empirical loss (1). Speciﬁcally, we use the\nfollowing random initialization scheme. For every level\nh∈ [H], each entry is sampled from a standard Gaussian\ndistribution, W(h)\nij ∼N(0, 1) and each entry of the output\nlayer a is also sampled from N(0, 1). In this paper, we\ntrain all layers by gradient descent, for k = 1, 2,..., and\nh∈ [H]\nW(h)(k) = W(h)(k− 1)−η ∂L(θ(k− 1))\n∂W(h)(k− 1),\na(k) = a(k− 1)−η∂L(θ(k− 1))\n∂a(k− 1)\nwhereη >0 is the step size.\nGradient Descent Finds Global Minima of Deep Neural Networks\n4. Technique Overview\nIn this section, we describe our main idea of proving the\nglobal convergence of gradient descent. Our proof technique\nis inspired by Du et al. (2018b) who proposed to study the\ndynamics of differences between labels and predictions.\nHere the individual prediction at thek-th iteration is\nui(k) =f(θ(k), xi)\nand we denote u(k) = ( u1(k),...,u n(k))⊤ ∈ Rn. Du\net al. (2018b) showed that for two-layer fully-connected\nneural network, the sequence {y− u(k)}∞\nk=0 admits the\nfollowing dynamics\ny− u(k + 1) = (I−ηH(k)) (y− u(k))\nwhere H(k)∈ Rn×n is a Gram matrix with5\nHij(k) =\n⟨ ∂ui(k)\n∂W(1)(k), ∂uj(k)\n∂W(1)(k)\n⟩\n.\nThe key ﬁnding in (Du et al., 2018b) is that if m is sufﬁ-\nciently large, H(k)≈ H∞ for allk where H∞ is deﬁned\nas H∞\nij = Ew∼N (0,I)\n[\nσ′ (\nw⊤xi\n)\nσ′ (\nw⊤xj\n)\nx⊤\ni xj\n]\n. No-\ntably, H∞ is a ﬁxed matrix which only depends on the\ntraining input, but does not depend on neural network pa-\nrametersθ. As a direct result, in the large m regime, the\ndynamics of{y− u(k)}∞\nk=0 is approximately linear\ny− u(k + 1)≈ (I−ηH∞) (y− u(k)).\nFor this linear dynamics, using standard analysis technique\nfor power method, one can show{y− u(k)}∞\nk=0 converges\nto 0 where the rate is determined by the least eigenvalue of\nH∞ and the step sizeη.\nWe leverage this insight to our deep neural network setting.\nAgain we consider the sequence {y− u(k)}∞\nk=0, which\nadmits the dynamics\ny− u(k + 1) = (I−ηG(k)) (y− u(k))\nwhere\nGij(k)\n=\n⟨∂ui(k)\n∂θ(k),∂uj(k)\n∂θ(k)\n⟩\n=\nH∑\nh=1\n⟨ ∂ui(k)\n∂W(h)(k), ∂uj(k)\n∂W(h)(k)\n⟩\n+\n⟨∂ui(k)\n∂a(k),∂uj(k)\n∂a(k)\n⟩\n≜\nH+1∑\nh=1\nG(h)\nij (k).\n5This formula is for the setting that only the ﬁrst layer is trained.\nHere we deﬁne G(h) ∈ Rn×n with G(h)\nij (k) =⣨\n∂ui(k)\n∂W(h)(k), ∂uj (k)\n∂W(h)(k)\n⟩\nfor h = 1 ,...,H and\nG(H+1)\nij (k) =\n⣨\n∂ui(k)\n∂a(k), ∂uj (k)\n∂a(k)\n⟩\n. Note for allh∈ [H + 1],\neach entry of G(h)(k) is an inner product. Therefore,\nG(h)(k) is a positive semi-deﬁnite (PSD) matrix for\nh∈ [H + 1]. Furthermore, if there exists oneh∈ [H] that\nG(h)(k) is strictly positive deﬁnite, then if one chooses the\nstep sizeη to be sufﬁciently small, the loss decreases at the\nk-th iteration according the analysis of power method. In\nthis paper we focus on G(H)(k), the gram matrix induced\nby the weights fromH-th layer for simplicity at the cost of\na minor degradation in convergence rate.6\nWe use the similar observation in (Du et al., 2018b) that\nwe show if the width is large enough for all layers, for all\nk = 0, 1,... , G(H)(k) is close to a ﬁxed matrix K(H)∈\nRn×n which depends on the input data, neural network\narchitecture and the activation but does not depend on neural\nnetwork parameters θ. According to the analysis of the\npower method, once we establish this, as long as K(H) is\nstrictly positive deﬁnite, then the gradient descent enjoys a\nlinear convergence rate. We will show for K(H) is strictly\npositive deﬁnite as long as the training data is not degenerate\n(c.f. Proposition F.1 and F.2).\nWhile following the similar high-level analysis framework\nproposed by Du et al. (2018b), analyzing the convergence\nof gradient descent for deep neural network is signiﬁcantly\nmore involved and requires new technical tools. To show\nG(H)(k) is close to K(H), we have two steps. First, we\nshow in the initialization phase G(H)(0) is close to K(H).\nSecond, we show during training G(H)(k) is close to\nG(H)(0) for k = 1, 2,... . Below we give overviews of\nthese two steps.\nAnalysis of Random Initialization Unlike (Du et al.,\n2018b) in which they showed H(0) is close to H∞ via a\nsimple concentration inequality, showing G(H)(0) is close\nto K(H) requires more subtle calculations. First, as will\nbe clear in the following sections, K(H) is a recursively\ndeﬁned matrix. Therefore, we need to analyze how the per-\nturbation (due to randomness of initialization and ﬁnitem)\nfrom lower layers propagates to the H-th layer. Second,\nthis perturbation propagation involves non-linear operations\ndue to the activation function. To quantitatively characterize\nthis perturbation propagation dynamics, we use induction\nand leverage techniques from Malliavin calculus (Malliavin,\n1995). We derive a general framework that allows us to\nanalyze the initialization behavior for the fully-connected\nneural network, ResNet, convolutional ResNet and other\npotential neural network architectures in a uniﬁed way.\n6Using the contribution of all the gram matrices to the mini-\nmum eigenvalue can potentially improve the convergence rate.\nGradient Descent Finds Global Minima of Deep Neural Networks\nOne important ﬁnding in our analysis is that ResNet archi-\ntecture makes the “perturbation propagation” more stable.\nThe high level intuition is the following. For fully con-\nnected neural network, suppose we have some perturbationG(1)(0)− K(1)\n\n2≤E 1\nin the ﬁrst layer. This perturba-\ntion propagates to theH-th layer admits the form\n\nG(H)(0)− K(H)\n\n2\n≜EH ≲ 2O(H)E1. (4)\nTherefore, we need to haveE1≤ 1\n2O(H) and this makesm\nhave exponential dependency onH.7\nOn the other hand, for ResNet the perturbation propagation\nadmits the form\nEH ≲\n(\n1 +O\n( 1\nH\n))H\nϵ1 =O (ϵ1) (5)\nTherefore we do not have the exponential explosion problem\nfor ResNet. We refer readers to Section E for details.\nAnalysis of Perturbation of During Training The next\nstep is to show G(H)(k) is close to G(H)(0) for k =\n0, 1,... . Note G(H) depends on weight matrices from all\nlayers, so to establish that G(H)(k) is close to G(H)(0), we\nneed to show W(h)(k)− W(h)(0) is small for allh∈ [H]\nand a(k)− a(0) is small.\nIn the two-layer neural network setting (Du et al., 2018b),\nthey are able to show every weight vector of the ﬁrst layer\nis close to its initialization, i.e.,\nW(1)(k)− W(1)(0)\n\n\n2,∞\nis small fork = 0, 1,... . While establishing this condition\nfor two-layer neural network is not hard, this condition may\nnot hold for multi-layer neural networks. In this paper, we\nshow instead, the averaged Frobenius norm\n1√m\n\n\nW(h)(k)− W(h)(0)\n\n\n\nF\n(6)\nis small for allk = 0, 1,... .\nSimilar to the analysis in the initialization, showing Equa-\ntion (6) is small is highly involved because again, we need\nto analyze how the perturbation propagates. We develop\na uniﬁed proof strategy for the fully-connected neural net-\nwork, ResNet and convolutional ResNet. Our analysis in\nthis step again sheds light on the beneﬁt of using ResNet\narchitecture for training. The high-level intuition is similar\nto Equation (5). See Section B, C, and D for details.\n7We not mean to imply that fully-connected networks neces-\nsarily depend exponentially on H, but simply to illustrate in our\nanalysis why the exponential dependence arises. For speciﬁc ac-\ntivations such as ReLU and careful initialization schemes, this\nexponential dependence may be avoided.\n5. Warm Up: Convergence Result of GD for\nDeep Fully-connected Neural Networks\nIn this section, as a warm up, we show gradient descent\nwith a constant positive step size converges to the global\nminimum at a linear rate. As we discussed in Section 4, the\nconvergence rate depends on least eigenvalue of the Gram\nmatrix K(H).\nDeﬁnition 5.1. The Gram matrix K(H) is recursively de-\nﬁned as follows, for (i,j )∈ [n]×[n], andh = 1,...,H −1\nK(0)\nij =⟨xi, xj⟩,\nA(h)\nij =\n(\nK(h−1)\nii K(h−1)\nij\nK(h−1)\nji K(h−1)\njj\n)\n, (7)\nK(h)\nij =cσE(u,v)⊤∼N\n(\n0,A(h)\nij\n)[σ (u)σ (v)],\nK(H)\nij =cσK(H−1)\nij E(u,v)⊤∼N\n(\n0,A(H−1)\nij\n)[σ′(u)σ′(v)].\nThe derivation of this Gram matrix is deferred to Sec-\ntion E. The convergence rate and the amount of over-\nparameterization depends on the least eigenvalue of this\nGram matrix. In Section F.1 we show as long as the input\ntraining data is not degenerate, thenλmin\n(\nK(H))\nis strictly\npositive. We remark that ifH = 1, then K(H) is the same\nthe Gram matrix deﬁned in (Du et al., 2018b).\nNow we are ready to state our main convergence result of\ngradient descent for deep fully-connected neural networks.\nTheorem 5.1 (Convergence Rate of Gradient Descent for\nDeep Fully-connected Neural Networks). Assume for all\ni∈ [n],∥xi∥2 = 1,|yi| =O(1) and the number of hidden\nnodes per layer\nm = Ω\n(\n2O(H) max\n{\nn4\nλ4\nmin\n(\nK(H)),n\nδ, n2 log(Hn\nδ )\nλ2\nmin\n(\nK(H))\n})\nwhere K(H) is deﬁned in Equation (7). If we set the step\nsize\nη =O\n(\nλmin\n(\nK(H))\nn22O(H)\n)\n,\nthen with probability at least 1−δ over the random initial-\nization the loss, fork = 1, 2,... , the loss at each iteration\nsatisﬁes\nL(θ(k))≤\n(\n1− ηλmin\n(\nK(H))\n2\n)k\nL(θ(0)).\nThis theorem states that if the width m is large enough\nand we set step size appropriately then gradient descent\nconverges to the global minimum with zero loss at linear\nGradient Descent Finds Global Minima of Deep Neural Networks\nrate. The main assumption of the theorem is that we need\na large enough width of each layer. The widthm depends\non n, H and 1/λmin\n(\nK(H))\n. The dependency on n is\nonly polynomial, which is the same as previous work on\nshallow neural networks (Du et al., 2018b; Li & Liang,\n2018). Similar to (Du et al., 2018b),\nm also polynomially\ndepends on 1/λmin\n(\nK(H))\n. However, the dependency on\nthe number of layers H is exponential. As we discussed\nin Section B.1, this exponential comes from the instability\nof the fully-connected architecture (c.f. Equation (4)). In\nthe next section, we show with ResNet architecture, we can\nreduce the dependency onH from 2(H) to poly(H).\nNote the requirement ofm has three terms. The ﬁrst term is\nused to show the Gram matrix is stable during training. The\nsecond term is used to guarantee the output in each layer is\napproximately normalized at the initialization phase. The\nthird term is used to show the perturbation of Gram matrix\nat the initialization phase is small. See Section B for proofs.\nThe convergence rate depends step sizeη andλmin\n(\nK(H))\n,\nsimilar to (Du et al., 2018b). Here we require η =\nO\n(\nλmin(K(H))\nn22O(H)\n)\n. When H = 1 , this requirement is the\nsame as the one used in (Du et al., 2018b). However, for\ndeep fully-connected neural network, we require\nη to be\nexponentially small in terms of number of layers. The rea-\nson is similar to that we requirem to be exponentially large.\nAgain, this will be improved in the next section.\n6. Convergence Result of GD for ResNet\nIn this section we consider the convergence of gradient de-\nscent for training a ResNet. We will focus on how much\nover-parameterization is needed to ensure the global con-\nvergence of gradient descent and compare it with fully-\nconnected neural networks. Again we ﬁrst deﬁne the key\nGram matrix whose least eigenvalue will determine the con-\nvergence rate.\nDeﬁnition 6.1. The Gram matrix K(H) is recursively de-\nﬁned as follows, for (i,j )∈ [n]×[n] andh = 2,...,H −1:\nK(0)\nij =⟨xi, xj⟩,\nK(1)\nij =E\n(u,v)⊤∼N\n\n0,\n\n\nK(0)\nii K(0)\nij\nK(0)\nji K(0)\njj\n\n\n\n\ncσσ (u)σ (v),\nb(1)\ni =√\ncσEu∼N (0,K(0)\nii ) [σ (u)],\nA(h)\nij =\n(\nK(h−1)\nii K(h−1)\nij\nK(h−1)\nji K(h−1)\njj\n)\n(8)\nK(h)\nij =K(h−1)\nij +\nE(u,v)⊤∼N\n(\n0,A(h)\nij\n)\n[\ncresb(h−1)\ni σ (u)\nH\n+\ncresb(h−1)\nj σ (v)\nH + c2\nresσ (u)σ (v)\nH 2\n]\n,\nb(h)\ni =b(h−1)\ni + cres\nH Eu∼N (0,K(h−1)\nii ) [σ (u)],\nK(H)\nij =c2\nres\nH 2 K(H−1)\nij E(u,v)⊤∼N\n(\n0,A(H−1)\nij\n)[σ′(u)σ′(v)].\nComparing K(H) of the ResNet and the one of the fully-\nconnect neural network, the deﬁnition ofK(H) also depends\non a series of{b(h)}H−1\nh=1 . This dependency is comes from\nthe skip connection block in the ResNet architecture. See\nSection E. In Section F.2, we show as long as the input\ntraining data is not degenerate, thenλmin\n(\nK(H))\nis strictly\npositive. Furthermore, λmin\n(\nK(H))\ndoes not depend in-\nversely exponentially inH.\nNow we are ready to state our main theorem for ResNet.\nTheorem 6.1 (Convergence Rate of Gradient Descent for\nResNet). Assume for all i∈ [n],∥xi∥2 = 1,|yi| = O(1)\nand the number of hidden nodes per layer\nm =Ω\n(\nmax\n{\nn4\nλ4\nmin\n(\nK(H))\nH 6, n2\nλ2\nmin(K(H))H 2, (9)\nn\nδ, n2 log\n(Hn\nδ\n)\nλ2\nmin\n(\nK(H))\n})\n.If we set the step size η = O\n(\nλmin(K(H))H 2\nn2\n)\n, then with\nprobability at least 1−δ over the random initialization we\nhave fork = 1, 2,...\nL(θ(k))≤\n(\n1− ηλmin\n(\nK(H))\n2\n)k\nL(θ(0)).\nIn sharp contrast to Theorem 5.1, this theorem is fully poly-\nnomial in the sense that both the number of neurons and\nthe convergence rate is polynomially inn andH. Note the\namount of over-parameterization depends onλmin\n(\nK(H))\nwhich is the smallest eigenvalue of theH-th layer’s Gram\nmatrix. The main reason that we do not have any exponen-\ntial factor here is that the skip connection block makes the\noverall architecture more stable in both the initialization\nphase and the training phase.\nNote the requirement onm has 4 terms. The ﬁrst two terms\nare used to show the Gram matrix stable during training.\nThe third term is used to guarantee the output in each layer\nis approximately normalized at the initialization phase. The\nfourth term is used to show bound the size of the pertur-\nbation of the Gram matrix at the initialization phase. See\nSection C for details.\nGradient Descent Finds Global Minima of Deep Neural Networks\n7. Convergence Result of GD for\nConvolutional ResNet\nIn this section we generalize the convergence result of gra-\ndient descent for ResNet to convolutional ResNet. Again,\nwe focus on how much over-parameterization is needed to\nensure the global convergence of gradient descent. Simi-\nlar to previous sections, we ﬁrst deﬁne the K(H) for this\narchitecture.\nDeﬁnition 7.1. The Gram matrix K(H) is recursively de-\nﬁned as follows, for (i,j )∈ [n]× [n], (l,r )∈ [p]× [p] and\nh = 2,...,H − 1,\nK(0)\nij =φ1 (xi)⊤φ1 (xj)∈ Rp×p,\nK(1)\nij =E\n(u,v)∼N\n\n0,\n\n\nK(0)\nii K(0)\nij\nK(0)\nji K(0)\njj\n\n\n\n\ncσσ (u)⊤σ (v),\nb(1)\ni =√\ncσEu∼N\n(\n0,K(0)\nii\n)[σ (u)],\nA(h)\nij =\n(\nK(h−1)\nii K(h−1)\nij\nK(h−1)\nji K(h−1)\njj\n)\nH(h)\nij =K(h−1)\nij +\nE(u,v)∼N\n(\n0,A(h−1)\nij\n)\n[\ncresb(h−1)⊤\ni σ (u)\nH (10)\n+\ncresb(h−1)⊤\nj σ (v)\nH + c2\nresσ (u)⊤σ (v)\nH 2\n]\n,\nK(h)\nij,lr =tr\n(\nH(h)\nij,D(h)\nl D(h)\nr\n)\n,\nb(h)\ni =b(h−1)\ni + cres\nH Eu∼N\n(\n0,K(h−1)\nii\n)[σ (u)]\nM(H)\nij,lr =K(H−1)\nij,lr E(u,v)∼N\n(\n0,A(H−1)\nij\n)[σ′(ul)σ′(vr)]\nK(H)\nij =tr(M(H)\nij )\nwhere u and v are both random row vectors andD(h)\nl ≜\n{s : x(h−1)\n:,s ∈ thelth patch}.\nNote here K(h)\nij has dimensionp×p forh = 0,...,H − 1\nand Kij,lr denotes the (l,r )-th entry.\nNow we state our main convergence theorem for the convo-\nlutional ResNet.\nTheorem 7.1 (Convergence Rate of Gradient Descent for\nConvolutional ResNet). Assume for alli∈ [n],∥xi∥F = 1,\n|yi| =O(1) and the number of hidden nodes per layer\nm =Ω\n(\nmax\n{ n4\nλ4\n0H 6, n4\nλ4\n0H 2,\nn\nδ,n2 log\n(Hn\nδ\n)\nλ2\n0\n}\npoly(p)\n)\n. (11)\nIf we set the step sizeη =O\n(\nλ0H 2\nn2poly(p)\n)\n, then with proba-\nbility at least 1−δ over the random initialization we have\nfork = 1, 2,...\nL(θ(k))≤\n(\n1− ηλmin\n(\nK(H))\n2\n)k\nL(θ(0)).\nThis theorem is similar to that of ResNet. The number\nof neurons required per layer is only polynomial in the\ndepth and the number of data points and step size is only\npolynomially small. The only extra term is\npoly(p) in the\nrequirement of m and η. The analysis is also similar to\nResNet and we refer readers to Section D for details.\n8. Conclusion\nIn this paper, we show that gradient descent on deep over-\nparametrized networks can obtain zero training loss. Our\nproof builds on a careful analysis of the random initialization\nscheme and a perturbation analysis which shows that the\nGram matrix is increasingly stable under overparametriza-\ntion. These techniques allow us to show that every step of\ngradient descent decreases the loss at a geometric rate.\nWe list some directions for future research:\n1. The current paper focuses on the training loss, but does\nnot address the test loss. It would be an important\nproblem to show that gradient descent can also ﬁnd\nsolutions of low test loss. In particular, existing work\nonly demonstrate that gradient descent works under the\nsame situations as kernel methods and random feature\nmethods (Daniely, 2017; Li & Liang, 2018; Allen-Zhu\net al., 2018a; Arora et al., 2019). To further investigate\nof generalization behavior, we believe some algorithm-\ndependent analyses may be useful (Hardt et al., 2016;\nMou et al., 2018; Chen et al., 2018).\n2. The width of the layers m is polynomial in all the\nparameters for the ResNet architecture, but still very\nlarge. Realistic networks have number of parameters,\nnot width, a large constant multiple ofn. We consider\nimproving the analysis to cover commonly utilized\nnetworks an important open problem.\n3. The current analysis is for gradient descent, instead of\nstochastic gradient descent. We believe the analysis can\nbe extended to stochastic gradient, while maintaining\nthe linear convergence rate.\n4. The convergence rate can be potentially improved if the\nminimum eigenvalue takes into account the contribu-\ntion of all Gram matrices, but this would considerably\ncomplicate the initialization and perturbation analysis.\nGradient Descent Finds Global Minima of Deep Neural Networks\nAcknowledgments\nWe thank Lijie Chen and Ruosong Wang for useful dis-\ncussions. SSD acknowledges support from AFRL grant\nFA8750-17-2-0212 and DARPA D17AP00001. JDL ac-\nknowledges support of the ARO under MURI Award\nW911NF-11-1-0303. This is part of the collaboration be-\ntween US DOD, UK MOD and UK Engineering and Physi-\ncal Research Council (EPSRC) under the Multidisciplinary\nUniversity Research Initiative. HL and LW acknowlege sup-\nport from National Basic Research Program of China (973\nProgram) (grant no. 2015CB352502), NSFC (61573026)\nand BJNSF (L172037). Part of the work is done while SSD\nwas visiting Simons Institute.\nReferences\nAllen-Zhu, Z., Li, Y ., and Liang, Y . Learning and generaliza-\ntion in overparameterized neural networks, going beyond\ntwo layers. arXiv preprint arXiv:1811.04918, 2018a.\nAllen-Zhu, Z., Li, Y ., and Song, Z. On the convergence\nrate of training recurrent neural networks. arXiv preprint\narXiv:1810.12065, 2018b.\nAllen-Zhu, Z., Li, Y ., and Song, Z. A convergence theory for\ndeep learning via over-parameterization. arXiv preprint\narXiv:1811.03962, 2018c.\nAndoni, A., Panigrahy, R., Valiant, G., and Zhang, L. Learn-\ning polynomials with neural networks. In International\nConference on Machine Learning, pp. 1908–1916, 2014.\nArora, S., Du, S. S., Hu, W., Li, Z., and Wang, R. Fine-\ngrained analysis of optimization and generalization for\noverparameterized two-layer neural networks. arXiv\npreprint arXiv:1901.08584, 2019.\nBrutzkus, A. and Globerson, A. Globally optimal gradient\ndescent for a ConvNet with gaussian inputs. In Interna-\ntional Conference on Machine Learning , pp. 605–614,\n2017.\nChen, Y ., Jin, C., and Yu, B. Stability and Convergence\nTrade-off of Iterative Optimization Algorithms. arXiv\ne-prints, art. arXiv:1804.01619, Apr 2018.\nChizat, L. and Bach, F. On the global convergence of gradi-\nent descent for over-parameterized models using optimal\ntransport. arXiv preprint arXiv:1805.09545, 2018a.\nChizat, L. and Bach, F. A note on lazy training in su-\npervised differentiable programming. arXiv preprint\narXiv:1812.07956, 2018b.\nDaniely, A. SGD learns the conjugate kernel class of the\nnetwork. In Advances in Neural Information Processing\nSystems, pp. 2422–2430, 2017.\nDu, S. S. and Lee, J. D. On the power of over-\nparametrization in neural networks with quadratic activa-\ntion. Proceedings of the 35th International Conference\non Machine Learning, pp. 1329–1338, 2018.\nDu, S. S., Jin, C., Lee, J. D., Jordan, M. I., Singh, A., and\nPoczos, B. Gradient descent can take exponential time to\nescape saddle points. In Advances in Neural Information\nProcessing Systems, pp. 1067–1077, 2017a.\nDu, S. S., Lee, J. D., and Tian, Y . When is a convolutional\nﬁlter easy to learn? arXiv preprint arXiv:1709.06129,\n2017b.\nDu, S. S., Lee, J. D., Tian, Y ., Poczos, B., and Singh, A.\nGradient descent learns one-hidden-layer CNN: Don’t\nbe afraid of spurious local minima. Proceedings of the\n35th International Conference on Machine Learning, pp.\n1339–1348, 2018a.\nDu, S. S., Zhai, X., Poczos, B., and Singh, A. Gradient\ndescent provably optimizes over-parameterized neural\nnetworks. arXiv preprint arXiv:1810.02054, 2018b.\nFreeman, C. D. and Bruna, J. Topology and geometry\nof half-rectiﬁed network optimization. arXiv preprint\narXiv:1611.01540, 2016.\nGe, R., Huang, F., Jin, C., and Yuan, Y . Escaping from\nsaddle points− online stochastic gradient for tensor de-\ncomposition. In Proceedings of The 28th Conference on\nLearning Theory, pp. 797–842, 2015.\nHaeffele, B. D. and Vidal, R. Global optimality in tensor\nfactorization, deep learning, and beyond. arXiv preprint\narXiv:1506.07540, 2015.\nHardt, M. and Ma, T. Identity matters in deep learning.\narXiv preprint arXiv:1611.04231, 2016.\nHardt, M., Recht, B., and Singer, Y . Train faster, generalize\nbetter: Stability of stochastic gradient descent. In Bal-\ncan, M. F. and Weinberger, K. Q. (eds.),Proceedings of\nThe 33rd International Conference on Machine Learn-\ning, volume 48 of Proceedings of Machine Learning Re-\nsearch, pp. 1225–1234, New York, New York, USA, 20–\n22 Jun 2016. PMLR. URL http://proceedings.\nmlr.press/v48/hardt16.html.\nHe, K., Zhang, X., Ren, S., and Sun, J. Deep residual learn-\ning for image recognition. In Proceedings of the IEEE\nconference on computer vision and pattern recognition,\npp. 770–778, 2016.\nJacot, A., Gabriel, F., and Hongler, C. Neural tangent kernel:\nConvergence and generalization in neural networks.arXiv\npreprint arXiv:1806.07572, 2018.\nGradient Descent Finds Global Minima of Deep Neural Networks\nJin, C., Ge, R., Netrapalli, P., Kakade, S. M., and Jordan,\nM. I. How to escape saddle points efﬁciently. In Proceed-\nings of the 34th International Conference on Machine\nLearning, pp. 1724–1732, 2017.\nKawaguchi, K. Deep learning without poor local minima.\nIn Advances In Neural Information Processing Systems,\npp. 586–594, 2016.\nLee, J., Bahri, Y ., Novak, R., Schoenholz, S. S., Penning-\nton, J., and Sohl-Dickstein, J. Deep neural networks as\ngaussian processes. arXiv preprint arXiv:1711.00165 ,\n2017.\nLee, J. D., Simchowitz, M., Jordan, M. I., and Recht, B.\nGradient descent only converges to minimizers. In Con-\nference on Learning Theory, pp. 1246–1257, 2016.\nLi, Y . and Liang, Y . Learning overparameterized neural\nnetworks via stochastic gradient descent on structured\ndata. arXiv preprint arXiv:1808.01204, 2018.\nLi, Y . and Yuan, Y . Convergence analysis of two-layer\nneural networks with ReLU activation. In Advances in\nNeural Information Processing Systems , pp. 597–607,\n2017.\nLu, Z., Pu, H., Wang, F., Hu, Z., and Wang, L. The expres-\nsive power of neural networks: A view from the width.\nIn Advances in Neural Information Processing Systems\n30, pp. 6231–6239. Curran Associates, Inc., 2017.\nMalliavin, P. Gaussian sobolev spaces and stochastic calcu-\nlus of variations. 1995.\nMatthews, A. G. d. G., Rowland, M., Hron, J., Turner, R. E.,\nand Ghahramani, Z. Gaussian process behaviour in wide\ndeep neural networks. arXiv preprint arXiv:1804.11271,\n2018.\nMei, S., Montanari, A., and Nguyen, P.-M. A mean ﬁeld\nview of the landscape of two-layers neural networks. Pro-\nceedings of the National Academy of Sciences, pp. E7665–\nE7671, 2018.\nMou, W., Wang, L., Zhai, X., and Zheng, K. General-\nization bounds of sgld for non-convex learning: Two\ntheoretical viewpoints. In Bubeck, S., Perchet, V ., and\nRigollet, P. (eds.), Proceedings of the 31st Conference\nOn Learning Theory, volume 75 of Proceedings of Ma-\nchine Learning Research, pp. 605–638. PMLR, 06–09 Jul\n2018. URL http://proceedings.mlr.press/\nv75/mou18a.html.\nNguyen, Q. and Hein, M. The loss surface of deep and wide\nneural networks. In International Conference on Machine\nLearning, pp. 2603–2612, 2017.\nRaghu, M., Poole, B., Kleinberg, J., Ganguli, S., and Sohl-\nDickstein, J. On the expressive power of deep neural\nnetworks. arXiv preprint arXiv:1606.05336, 2016.\nRotskoff, G. M. and Vanden-Eijnden, E. Neural networks as\ninteracting particle systems: Asymptotic convexity of the\nloss landscape and universal scaling of the approximation\nerror. arXiv preprint arXiv:1805.00915, 2018.\nSafran, I. and Shamir, O. On the quality of the initial basin\nin overspeciﬁed neural networks. In International Con-\nference on Machine Learning, pp. 774–782, 2016.\nSafran, I. and Shamir, O. Spurious local minima are com-\nmon in two-layer ReLU neural networks.In International\nConference on Machine Learning, pp. 4433–4441, 2018.\nSchoenholz, S. S., Gilmer, J., Ganguli, S., and Sohl-\nDickstein, J. Deep information propagation. arXiv\npreprint arXiv:1611.01232, 2016.\nSirignano, J. and Spiliopoulos, K. Mean ﬁeld analysis of\nneural networks. arXiv preprint arXiv:1805.01053, 2018.\nSoltanolkotabi, M. Learning ReLUs via gradient descent.\nIn Advances in Neural Information Processing Systems,\npp. 2007–2017, 2017.\nSoltanolkotabi, M., Javanmard, A., and Lee, J. D. Theo-\nretical insights into the optimization landscape of over-\nparameterized shallow neural networks. IEEE Transac-\ntions on Information Theory, 2018.\nSoudry, D. and Carmon, Y . No bad local minima: Data in-\ndependent training error guarantees for multilayer neural\nnetworks. arXiv preprint arXiv:1605.08361, 2016.\nSoudry, D. and Hoffer, E. Exponentially vanishing sub-\noptimal local minima in multilayer neural networks.\narXiv preprint arXiv:1702.05777, 2017.\nTian, Y . An analytical formula of population gradient for\ntwo-layered ReLU network and its applications in con-\nvergence and critical point analysis. In International\nConference on Machine Learning, pp. 3404–3413, 2017.\nVenturi, L., Bandeira, A., and Bruna, J. Neural networks\nwith ﬁnite intrinsic dimension have no spurious valleys.\narXiv preprint arXiv:1802.06384, 2018.\nVershynin, R. Introduction to the non-asymptotic analysis\nof random matrices. arXiv preprint arXiv:1011.3027 ,\n2010.\nWei, C., Lee, J. D., Liu, Q., and Ma, T. On the margin\ntheory of feedforward neural networks. arXiv preprint\narXiv:1810.05369, 2018.\nGradient Descent Finds Global Minima of Deep Neural Networks\nZagoruyko, S. and Komodakis, N. Wide residual networks.\nNIN, 8:35–67, 2016.\nZhang, C., Bengio, S., Hardt, M., Recht, B., and Vinyals, O.\nUnderstanding deep learning requires rethinking general-\nization. arXiv preprint arXiv:1611.03530, 2016.\nZhang, H., Dauphin, Y . N., and Ma, T. Residual learn-\ning without normalization via better initialization. In\nInternational Conference on Learning Representations,\n2019. URL https://openreview.net/forum?\nid=H1gsz30cKX.\nZhang, X., Yu, Y ., Wang, L., and Gu, Q. Learning one-\nhidden-layer relu networks via gradient descent. arXiv\npreprint arXiv:1806.07808, 2018.\nZhong, K., Song, Z., and Dhillon, I. S. Learning non-\noverlapping convolutional neural networks with multiple\nkernels. arXiv preprint arXiv:1711.03440, 2017a.\nZhong, K., Song, Z., Jain, P., Bartlett, P. L., and Dhillon,\nI. S. Recovery guarantees for one-hidden-layer neural\nnetworks. arXiv preprint arXiv:1706.03175, 2017b.\nZhou, Y . and Liang, Y . Critical points of neural networks:\nAnalytical forms and landscape properties. arXiv preprint\narXiv:1710.11205, 2017.\nZou, D., Cao, Y ., Zhou, D., and Gu, Q. Stochastic gra-\ndient descent optimizes over-parameterized deep ReLU\nnetworks. arXiv preprint arXiv:1811.08888, 2018.",
  "values": {
    "Explicability": "No",
    "Critiqability": "No",
    "Non-maleficence": "No",
    "Collective influence": "No",
    "Fairness": "No",
    "User influence": "No",
    "Justice": "No",
    "Privacy": "No",
    "Beneficence": "No",
    "Respect for Persons": "No",
    "Deferral to humans": "No",
    "Not socially biased": "No",
    "Autonomy (power to decide)": "No",
    "Interpretable (to users)": "No",
    "Transparent (to users)": "No",
    "Respect for Law and public interest": "No"
  }
}