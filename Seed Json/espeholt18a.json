{
  "pdf": "espeholt18a",
  "title": "IMPALA: Scalable Distributed Deep-RL with Importance Weighted Actor-Learner Architectures",
  "author": "Lasse Espeholt, Hubert Soyer, Remi Munos, Karen Simonyan, Volodymyr Mnih, Tom Ward, Yotam Doron, Vlad Firoiu, Tim Harley, Iain Dunning, Shane Legg, Koray Kavukcuoglu",
  "paper_id": "espeholt18a",
  "text": "IMPALA: Scalable Distributed Deep-RL with Importance Weighted\nActor-Learner Architectures\nLasse Espeholt * 1 Hubert Soyer * 1 Remi Munos * 1 Karen Simonyan 1 Volodymyr Mnih1 Tom Ward1\nYotam Doron1 Vlad Firoiu 1 Tim Harley 1 Iain Dunning 1 Shane Legg 1 Koray Kavukcuoglu 1\nAbstract\nIn this work we aim to solve a large collection of\ntasks using a single reinforcement learning agent\nwith a single set of parameters. A key challenge\nis to handle the increased amount of data and ex-\ntended training time. We have developed a new\ndistributed agent IMPALA (Importance Weighted\nActor-Learner Architecture) that not only uses\nresources more efﬁciently in single-machine train-\ning but also scales to thousands of machines with-\nout sacriﬁcing data efﬁciency or resource utilisa-\ntion. We achieve stable learning at high through-\nput by combining decoupled acting and learning\nwith a novel off-policy correction method called\nV-trace. We demonstrate the effectiveness of IM-\nPALA for multi-task reinforcement learning on\nDMLab-30 (a set of 30 tasks from the DeepMind\nLab environment (Beattie et al., 2016)) and Atari-\n57 (all available Atari games in Arcade Learning\nEnvironment (Bellemare et al., 2013a)). Our re-\nsults show that IMPALA is able to achieve better\nperformance than previous agents with less data,\nand crucially exhibits positive transfer between\ntasks as a result of its multi-task approach.\n1. Introduction\nDeep reinforcement learning methods have recently mas-\ntered a wide variety of domains through trial and error\nlearning (Mnih et al., 2015; Silver et al., 2017; 2016; Zoph\net al., 2017; Lillicrap et al., 2015; Barth-Maron et al., 2018).\nWhile the improvements on tasks like the game of Go (Sil-\nver et al., 2017) and Atari games (Horgan et al., 2018) have\nbeen dramatic, the progress has been primarily in single\ntask performance, where an agent is trained on each task\n*Equal contribution 1DeepMind Technologies, London,\nUnited Kingdom. Correspondence to: Lasse Espeholt <lespe-\nholt@google.com>.\nProceedings of the 35 th International Conference on Machine\nLearning, Stockholm, Sweden, PMLR 80, 2018. Copyright 2018\nby the author(s).\nseparately. We are interested in developing new methods\ncapable of mastering a diverse set of tasks simultaneously as\nwell as environments suitable for evaluating such methods.\nOne of the main challenges in training a single agent on\nmany tasks at once is scalability. Since the current state-of-\nthe-art methods like A3C (Mnih et al., 2016) or UNREAL\n(Jaderberg et al., 2017b) can require as much as a billion\nframes and multiple days to master a single domain, training\nthem on tens of domains at once is too slow to be practical.\nWe propose the Importance Weighted Actor-Learner\nArchitecture (IMPALA) shown in Figure 1. IMPALA is\ncapable of scaling to thousands of machines without sacri-\nﬁcing training stability or data efﬁciency. Unlike the popular\nA3C-based agents, in which workers communicate gradi-\nents with respect to the parameters of the policy to a central\nparameter server, IMPALA actors communicate trajectories\nof experience (sequences of states, actions, and rewards) to a\ncentralised learner. Since the learner in IMPALA has access\nto full trajectories of experience we use a GPU to perform\nupdates on mini-batches of trajectories while aggressively\nparallelising all time independent operations. This type of\ndecoupled architecture can achieve very high throughput.\nHowever, because the policy used to generate a trajectory\ncan lag behind the policy on the learner by several updates at\nthe time of gradient calculation, learning becomes off-policy.\nTherefore, we introduce the V-trace off-policy actor-critic\nalgorithm to correct for this harmful discrepancy.\nWith the scalable architecture and V-trace combined, IM-\nPALA achieves exceptionally high data throughput rates of\n250,000 frames per second, making it over 30 times faster\nthan single-machine A3C. Crucially, IMPALA is also more\ndata efﬁcient than A3C based agents and more robust to\nhyperparameter values and network architectures, allow-\ning it to make better use of deeper neural networks. We\ndemonstrate the effectiveness of IMPALA by training a sin-\ngle agent on multi-task problems using DMLab-30, a new\nchallenge set which consists of 30 diverse cognitive tasks\nin the 3D DeepMind Lab (Beattie et al., 2016) environment\nand by training a single agent on all games in the Atari-57\nset of tasks.\nIMPALA: Importance Weighted Actor-Learner Architectures\nActor Actor\nActor\nActorActor\nActor Learner\nObservations\nParameters\nActor\nActor\nObservations\nObservations\nParameters Gradients\nLearner\nW orker\nMaster\nLearner\nActor\nActorActor\nFigure 1. Left: Single Learner. Each actor generates trajectories\nand sends them via a queue to the learner. Before starting the next\ntrajectory, actor retrieves the latest policy parameters from learner.\nRight: Multiple Synchronous Learners. Policy parameters are\ndistributed across multiple learners that work synchronously.\n2. Related Work\nThe earliest attempts to scale up deep reinforcement learn-\ning relied on distributed asynchronous SGD (Dean et al.,\n2012) with multiple workers. Examples include distributed\nA3C (Mnih et al., 2016) and Gorila (Nair et al., 2015), a\ndistributed version of Deep Q-Networks (Mnih et al., 2015).\nRecent alternatives to asynchronous SGD for RL include\nusing evolutionary processes (Salimans et al., 2017), dis-\ntributed BA3C (Adamski et al., 2018) and Ape-X (Horgan\net al., 2018) which has a distributed replay but a synchronous\nlearner.\nThere have also been multiple efforts that scale up reinforce-\nment learning by utilising GPUs. One of the simplest of\nsuch methods is batched A2C (Clemente et al., 2017). At\nevery step, batched A2C produces a batch of actions and\napplies them to a batch of environments. Therefore, the\nslowest environment in each batch determines the time it\ntakes to perform the entire batch step (see Figure 2a and\n2b). In other words, high variance in environment speed\ncan severely limit performance. Batched A2C works partic-\nularly well on Atari environments, because rendering and\ngame logic are computationally very cheap in comparison to\nthe expensive tensor operations performed by reinforcement\nlearning agents. However, more visually or physically com-\nplex environments can be slower to simulate and can have\nhigh variance in the time required for each step. Environ-\nments may also have variable length (sub)episodes causing\na slowdown when initialising an episode.\nThe most similar architecture to IMPALA is GA3C\n(Babaeizadeh et al., 2016), which also uses asynchronous\ndata collection to more effectively utilise GPUs. It de-\ncouples the acting/forward pass from the gradient calcu-\nlation/backward pass by using dynamic batching. The ac-\ntor/learner asynchrony in GA3C leads to instabilities during\nlearning, which (Babaeizadeh et al., 2016) only partially\nmitigates by adding a small constant to action probabilities\nEnvironment steps Forward pass Backward pass\nActor 2\nActor 3\nActor 1\nActor 0\n4 time steps\n(a) Batched A2C (sync step.)\nActor 2\nActor 3\nActor 1\nActor 0\n4 time steps\n(b) Batched A2C (sync traj.)\n…\n…\nActor 2\nActor 3\nActor 1\nActor 0\nActor 4\nActor 5\nActor 6\nActor 7\n... next unroll\n(c) IMPALA\nFigure 2. Timeline for one unroll with 4 steps using different ar-\nchitectures. Strategies shown in (a) and (b) can lead to low GPU\nutilisation due to rendering time variance within a batch. In (a),\nthe actors are synchronised after every step. In (b) after every n\nsteps. IMPALA (c) decouples acting from learning.\nduring the estimation of the policy gradient. In contrast,\nIMPALA uses the more principled V-trace algorithm.\nRelated previous work on off-policy RL include (Precup\net al., 2000; 2001; Wawrzynski, 2009; Geist & Scherrer,\n2014; O’Donoghue et al., 2017) and (Harutyunyan et al.,\n2016). The closest work to ours is the Retrace algorithm\n(Munos et al., 2016) which introduced an off-policy correc-\ntion for multi-step RL, and has been used in several agent\narchitectures (Wang et al., 2017; Gruslys et al., 2018). Re-\ntrace requires learning state-action-value functions Q in\norder to make the off-policy correction. However, many\nactor-critic methods such as A3C learn a state-value func-\ntionV instead of a state-action-value functionQ. V-trace is\nbased on the state-value function.\n3. IMPALA\nIMPALA (Figure 1) uses an actor-critic setup to learn a\npolicyπ and a baseline functionVπ. The process of gener-\nating experiences is decoupled from learning the parameters\nofπ andVπ. The architecture consists of a set of actors,\nrepeatedly generating trajectories of experience, and one or\nmore learners that use the experiences sent from actors to\nlearnπ off-policy.\nAt the beginning of each trajectory, an actor updates its\nown local policy µ to the latest learner policy π and runs\nit for n steps in its environment. After n steps, the ac-\ntor sends the trajectory of states, actions and rewards\nx1,a 1,r 1,...,x n,an,rn together with the corresponding\npolicy distributionsµ(at|xt) and initial LSTM state to the\nlearner through a queue. The learner then continuously\nupdates its policy π on batches of trajectories, each col-\nlected from many actors. This simple architecture enables\nthe learner(s) to be accelerated using GPUs and actors to\nbe easily distributed across many machines. However, the\nlearner policyπ is potentially several updates ahead of the\nactor’s policyµ at the time of update, therefore there is a\npolicy-lag between the actors and learner(s). V-trace cor-\nIMPALA: Importance Weighted Actor-Learner Architectures\nrects for this lag to achieve extremely high data throughput\nwhile maintaining data efﬁciency. Using an actor-learner ar-\nchitecture, provides fault tolerance like distributed A3C but\noften has lower communication overhead since the actors\nsend observations rather than parameters/gradients.\nWith the introduction of very deep model architectures, the\nspeed of a single GPU is often the limiting factor during\ntraining. IMPALA can be used with distributed set of learn-\ners to train large neural networks efﬁciently as shown in\nFigure 1. Parameters are distributed across the learners and\nactors retrieve the parameters from all the learners in par-\nallel while only sending observations to a single learner.\nIMPALA use synchronised parameter update which is vital\nto maintain data efﬁciency when scaling to many machines\n(Chen et al., 2016).\n3.1. Efﬁciency Optimisations\nGPUs and many-core CPUs beneﬁt greatly from running\nfew large, parallelisable operations instead of many small\noperations. Since the learner in IMPALA performs updates\non entire batches of trajectories, it is able to parallelise more\nof its computations than an online agent like A3C. As an\nexample, a typical deep RL agent features a convolutional\nnetwork followed by a Long Short-Term Memory (LSTM)\n(Hochreiter & Schmidhuber, 1997) and a fully connected\noutput layer after the LSTM. An IMPALA learner applies\nthe convolutional network to all inputs in parallel by folding\nthe time dimension into the batch dimension. Similarly, it\nalso applies the output layer to all time steps in parallel\nonce all LSTM states are computed. This optimisation\nincreases the effective batch size to thousands. LSTM-based\nagents also obtain signiﬁcant speedups on the learner by\nexploiting the network structure dependencies and operation\nfusion (Appleyard et al., 2016).\nFinally, we also make use of several off the shelf optimisa-\ntions available in TensorFlow (Abadi et al., 2017) such as\npreparing the next batch of data for the learner while still per-\nforming computation, compiling parts of the computational\ngraph with XLA (a TensorFlow Just-In-Time compiler) and\noptimising the data format to get the maximum performance\nfrom the cuDNN framework (Chetlur et al., 2014).\n4. V-trace\nOff-policy learning is important in the decoupled distributed\nactor-learner architecture because of the lag between when\nactions are generated by the actors and when the learner\nestimates the gradient. To this end, we introduce a novel off-\npolicy actor-critic algorithm for the learner, called V-trace.\nFirst, let us introduce some notations. We consider the\nproblem of discounted inﬁnite-horizon RL in Markov De-\ncision Processes (MDP), see (Puterman, 1994; Sutton &\nBarto, 1998) where the goal is to ﬁnd a policy π that\nmaximises the expected sum of future discounted rewards:\nVπ(x)\ndef\n= Eπ\n[∑\nt≥0γtrt\n]\n, where γ ∈ [0, 1) is the dis-\ncount factor,rt =r(xt,at) is the reward at timet,xt is the\nstate at timet (initialised inx0 =x) andat∼π(·|xt) is the\naction generated by following some policyπ.\nThe goal of an off-policy RL algorithm is to use trajectories\ngenerated by some policyµ, called the behaviour policy, to\nlearn the value functionVπ of another policyπ (possibly\ndifferent fromµ), called the target policy.\n4.1. V-trace target\nConsider a trajectory (xt,at,rt)t=s+n\nt=s generated by the ac-\ntor following some policyµ. We deﬁne then-steps V-trace\ntarget forV (xs), our value approximation at statexs, as:\nvs\ndef\n= V (xs) +∑s+n−1\nt=s γt−s\n(∏t−1\ni=sci\n)\nδtV, (1)\nwhereδtV\ndef\n= ρt\n(\nrt +γV (xt+1)−V (xt)\n)\nis a temporal\ndifference for V , and ρt\ndef\n= min\n(\n¯ρ, π(at|xt)\nµ(at|xt)\n)\nand ci\ndef\n=\nmin\n(\n¯c, π(ai|xi)\nµ(ai|xi)\n)\nare truncated importance sampling (IS)\nweights (we make use of the notation ∏t−1\ni=sci = 1 for\ns =t). In addition we assume that the truncation levels are\nsuch that ¯ρ≥ ¯c.\nNotice that in the on-policy case (when π = µ), and as-\nsuming that ¯c≥ 1, then all ci = 1 andρt = 1 , thus (1)\nrewrites\nvs =V (xs) +∑s+n−1\nt=s γt−s(\nrt +γV (xt+1)−V (xt)\n)\n=∑s+n−1\nt=s γt−srt +γnV (xs+n), (2)\nwhich is the on-policy n-steps Bellman target. Thus in\nthe on-policy case, V-trace reduces to the on-policyn-steps\nBellman update. This property (which Retrace (Munos et al.,\n2016) does not have) allows one to use the same algorithm\nfor off- and on-policy data.\nNotice that the (truncated) IS weights ci andρt play dif-\nferent roles. The weightρt appears in the deﬁnition of the\ntemporal differenceδtV and deﬁnes the ﬁxed point of this\nupdate rule. In a tabular case, where functions can be per-\nfectly represented, the ﬁxed point of this update (i.e., when\nV (xs) =vs for all states), characterised byδtV being equal\nto zero in expectation (underµ), is the value functionVπ¯ρ\nof some policyπ¯ρ, deﬁned by\nπ¯ρ(a|x)\ndef\n= min\n(\n¯ρµ(a|x),π (a|x)\n)\n∑\nb∈A min\n(\n¯ρµ(b|x),π (b|x)\n), (3)\n(see the analysis in Appendix A). So when ¯ρ is inﬁnite\n(i.e. no truncation ofρt), then this is the value functionVπ\nof the target policy. However if we choose a truncation\nIMPALA: Importance Weighted Actor-Learner Architectures\nlevel ¯ρ <∞, our ﬁxed point is the value functionVπ¯ρ of\na policyπ¯ρ which is somewhere betweenµ andπ. At the\nlimit when ¯ρ is close to zero, we obtain the value function\nof the behaviour policyVµ. In Appendix A we prove the\ncontraction of a related V-trace operator and the convergence\nof the corresponding online V-trace algorithm.\nThe weightsci are similar to the “trace cutting” coefﬁcients\nin Retrace. Their product cs...c t−1 measures how much\na temporal difference δtV observed at time t impacts the\nupdate of the value function at a previous times. The more\ndissimilar π and µ are (the more off-policy we are), the\nlarger the variance of this product. We use the truncation\nlevel ¯c as a variance reduction technique. However notice\nthat this truncation does not impact the solution to which\nwe converge (which is characterised by ¯ρ only).\nThus we see that the truncation levels ¯c and ¯ρ represent\ndifferent features of the algorithm: ¯ρ impacts the nature of\nthe value function we converge to, whereas ¯c impacts the\nspeed at which we converge to this function.\nRemark 1. V-trace targets can be computed recursively:\nvs =V (xs) +δsV +γcs\n(\nvs+1−V (xs+1)\n)\n.\nRemark 2. Like in Retrace(λ), we can also consider an\nadditional discounting parameterλ∈ [0, 1] in the deﬁnition\nof V-trace by setting ci = λ min\n(\n¯c, π(ai|xi)\nµ(ai|xi)\n)\n. In the on-\npolicy case, whenn =∞, V-trace then reduces to TD(λ).\n4.2. Actor-Critic algorithm\nPOLICY GRADIENT\nIn the on-policy case, the gradient of the value function\nVµ(x0) with respect to some parameter of the policyµ is\n∇Vµ(x0) = Eµ\n[∑\ns≥0γs∇ logµ(as|xs)Qµ(xs,as)\n]\n,\nwhere Qµ(xs,as)\ndef\n= Eµ\n[∑\nt≥sγt−srt|xs,as\n]\nis the\nstate-action value of policy µ at (xs,as). This is\nusually implemented by a stochastic gradient ascent\nthat updates the policy parameters in the direction of\nEas∼µ(·|xs)\n[\n∇ logµ(as|xs)qs\n⏐⏐xs\n]\n, whereqs is an estimate\nofQµ(xs,as), and averaged over the set of states xs that\nare visited under some behaviour policyµ.\nNow in the off-policy setting that we consider, we can use\nan IS weight between the policy being evaluatedπ¯ρ and the\nbehaviour policyµ, to update our policy parameter in the\ndirection of\nEas∼µ(·|xs)\n[π¯ρ(as|xs)\nµ(as|xs)∇ logπ¯ρ(as|xs)qs\n⏐⏐xs\n]\n(4)\nwhere qs\ndef\n= rs + γvs+1 is an estimate of Qπ¯ρ(xs,as)\nbuilt from the V-trace estimatevs+1 at the next statexs+1.\nThe reason why we use qs instead of vs as the target for\nour Q-value Qπ¯ρ(xs,as) is that, assuming our value esti-\nmate is correct at all states, i.e. V = Vπ¯ρ, then we have\nE[qs|xs,as] = Qπ¯ρ(xs,as) (whereas we do not have this\nproperty if we chooseqt =vt). See Appendix A for analy-\nsis and Appendix E.3 for a comparison of different ways to\nestimateqs.\nIn order to reduce the variance of the policy gradient es-\ntimate (4), we usually subtract from qs a state-dependent\nbaseline, such as the current value approximationV (xs).\nFinally notice that (4) estimates the policy gradient forπ¯ρ\nwhich is the policy evaluated by the V-trace algorithm when\nusing a truncation level ¯ρ. However assuming the bias\nVπ¯ρ−Vπ is small (e.g. if ¯ρ is large enough) then we can\nexpectqs to provide us with a good estimate ofQπ(xs,as).\nTaking into account these remarks, we derive the following\ncanonical V-trace actor-critic algorithm.\nV-TRACE ACTOR -CRITIC ALGORITHM\nConsider a parametric representationVθ of the value func-\ntion and the current policyπω. Trajectories have been gen-\nerated by actors following some behaviour policy µ. The\nV-trace targetsvs are deﬁned by(1). At training times, the\nvalue parametersθ are updated by gradient descent on the\nl2 loss to the targetvs, i.e., in the direction of\n(\nvs−Vθ(xs)\n)\n∇θVθ(xs),\nand the policy parameters ω in the direction of the policy\ngradient:\nρs∇ω logπω(as|xs)\n(\nrs +γvs+1−Vθ(xs)\n)\n.\nIn order to prevent premature convergence we may add an\nentropy bonus, like in A3C, along the direction\n−∇ω\n∑\na\nπω(a|xs) logπω(a|xs).\nThe overall update is obtained by summing these three gra-\ndients rescaled by appropriate coefﬁcients, which are hyper-\nparameters of the algorithm.\n5. Experiments\nWe investigate the performance of IMPALA under multiple\nsettings. For data efﬁciency, computational performance\nand effectiveness of the off-policy correction we look at the\nlearning behaviour of IMPALA agents trained on individual\ntasks. For multi-task learning we train agents—each with\none set of weights for all tasks—on a newly introduced\ncollection of 30 DeepMind Lab tasks and on all 57 games of\nthe Atari Learning Environment (Bellemare et al., 2013a).\nFor all the experiments we have used two different model\narchitectures: a shallow model similar to (Mnih et al., 2016)\nIMPALA: Importance Weighted Actor-Learner Architectures\n/255\nConv .8⇥ 8,stride 4\nReLU\nConv .4⇥ 4,stride 2\nReLU\nFC 256\nReLU\nV t\nrt\u0000 1 at\u0000 1\n32\n16\n3\n96 ⇥ 72\nLSTM 256ht\u0000 1\n⇡ (at)\nLSTM 64\nEmbedding 20\nblue ladder\n+\nConv .3⇥ 3,stride 1\nReLU\nConv .3⇥ 3,stride 1\n/255\nConv .3⇥ 3,stride 1\nFC 256\nReLU\nMax 3 ⇥ 3,stride 2\nResidual Block\nResidual Block\n⇥3\nReLU\n[16 ,32 ,32] ch .\nLSTM 256\nV t\nrt\u0000 1 at\u0000 1\nLSTM 256ht\u0000 1\n⇡ (at)\n96 ⇥ 72\n3\nReLU\nLSTM 64\nEmbedding 20\nblue ladder\nFigure 3. Model Architectures. Left: Small architecture, 2 convo-\nlutional layers and 1.2 million parameters. Right: Large architec-\nture, 15 convolutional layers and 1.6 million parameters.\nArchitecture CPUs GPUs 1 FPS2\nSingle-Machine Task 1 Task 2\nA3C 32 workers 64 0 6.5K 9K\nBatched A2C (sync step) 48 0 9K 5K\nBatched A2C (sync step) 48 1 13K 5.5K\nBatched A2C (sync traj.) 48 0 16K 17.5K\nBatched A2C (dyn. batch) 48 1 16K 13K\nIMPALA 48 actors 48 0 17K 20.5K\nIMPALA (dyn. batch) 48 actors3 48 1 21K 24K\nDistributed\nA3C 200 0 46K 50K\nIMPALA 150 1 80K\nIMPALA (optimised) 375 1 200K\nIMPALA (optimised) batch 128 500 1 250K\n1 Nvidia P100 2 In frames/sec (4 times the agent steps due to action repeat). 3 Limited by\namount of rendering possible on a single machine.\nTable 1. Throughput on seekavoid arena 01 (task 1) and\nrooms keys doors puzzle (task 2) with the shallow model\nin Figure 3. The latter has variable length episodes and slow\nrestarts. Batched A2C and IMPALA use batch size 32 if not other-\nwise mentioned.\nwith an LSTM before the policy and value (shown in Fig-\nure 3 (left)) and a deeper residual model (He et al., 2016)\n(shown in Figure 3 (right)). For tasks with a language chan-\nnel we used an LSTM with text embeddings as input.\n5.1. Computational Performance\nHigh throughput, computational efﬁciency and scalability\nare among the main design goals of IMPALA. To demon-\nstrate that IMPALA outperforms current algorithms in these\nmetrics we compare A3C (Mnih et al., 2016), batched A2C\nvariations and IMPALA variants with various optimisations.\nFor single-machine experiments using GPUs, we use dy-\nnamic batching in the forward pass to avoid several batch\nsize 1 forward passes. Our dynamic batching module is\nimplemented by specialised TensorFlow operations but is\nconceptual similar to the queues used in GA3C. Table 1\ndetails the results for single-machine and multi-machine\nversions with the shallow model from Figure 3. In the single-\nmachine case, IMPALA achieves the highest performance\non both tasks, ahead of all batched A2C variants and ahead\nof A3C. However, the distributed, multi-machine setup is\nwhere IMPALA can really demonstrate its scalability. With\nthe optimisations from Section 3.1 to speed up the GPU-\nbased learner, the IMPALA agent achieves a throughput rate\nof 250,000 frames/sec or 21 billion frames/day. Note, to\nreduce the number of actors needed per learner, one can\nuse auxiliary losses, data from experience replay or other\nexpensive learner-only computation.\n5.2. Single-Task Training\nTo investigate IMPALA’s learning dynamics, we employ the\nsingle-task scenario where we train agents individually on\n5 different DeepMind Lab tasks. The task set consists of a\nplanning task, two maze navigation tasks, a laser tag task\nwith scripted bots and a simple fruit collection task.\nWe perform hyperparameter sweeps over the weighting of\nentropy regularisation, the learning rate and the RMSProp\nepsilon. For each experiment we use an identical set of 24\npre-sampled hyperparameter combinations from the ranges\nin Appendix D.1. The other hyperparameters were ﬁxed to\nvalues speciﬁed in Appendix D.3.\n5.2.1. C ONVERGENCE AND STABILITY\nFigure 4 shows a comparison between IMPALA, A3C\nand batched A2C with the shallow model in Figure 3.\nIn all of the 5 tasks, either batched A2C or IMPALA\nreach the best ﬁnal average return and in all tasks but\nseekavoid arena 01 they are ahead of A3C through-\nout the entire course of training. IMPALA outperforms\nthe synchronous batched A2C on 2 out of 5 tasks while\nachieving much higher throughput (see Table 1). We hy-\npothesise that this behaviour could stem from the V-trace\noff-policy correction acting similarly to generalised advan-\ntage estimation (Schulman et al., 2016) and asynchronous\ndata collection yielding more diverse batches of experience.\nIn addition to reaching better ﬁnal performance, IMPALA is\nalso more robust to the choice of hyperparameters than A3C.\nFigure 4 compares the ﬁnal performance of the aforemen-\ntioned methods across different hyperparameter combina-\ntions, sorted by average ﬁnal return from high to low. Note\nthat IMPALA achieves higher scores over a larger number\nof combinations than A3C.\n5.2.2. V- TRACE ANALYSIS\nTo analyse V-trace we investigate four different algorithms:\n1. No-correction - No off-policy correction.\nIMPALA: Importance Weighted Actor-Learner Architectures\nIMPALA - 1 GPU - 200 actors Batched A2C - Single Machine - 32 workers A3C - Single Machine - 32 workers A3C - Distributed - 200 workers\n0.0 0.2 0.4 0.6 0.8 1.0\nEnvironment Frames 1e9\n10\n15\n20\n25\n30\n35\n40\n45\n50\n55Return\nrooms_watermaze\n0.0 0.2 0.4 0.6 0.8 1.0\nEnvironment Frames 1e9\n0\n5\n10\n15\n20\n25\n30\nrooms_keys_doors_puzzle\n0.0 0.2 0.4 0.6 0.8 1.0\nEnvironment Frames 1e9\n−5\n0\n5\n10\n15\n20\n25\n30\n35\nlasertag_three_opponents_small\n0.0 0.2 0.4 0.6 0.8 1.0\nEnvironment Frames 1e9\n0\n50\n100\n150\n200\n250\nexplore_goal_locations_small\n0.0 0.2 0.4 0.6 0.8 1.0\nEnvironment Frames 1e9\n5\n10\n15\n20\n25\n30\n35\n40\n45\nseekavoid_arena_01\n1 5 9 13 17 21 24\nHyperparameter Combination\n0\n10\n20\n30\n40\n50\n60Final Return\nrooms_watermaze\n1 5 9 13 17 21 24\nHyperparameter Combination\n0\n5\n10\n15\n20\n25\n30\n35\n40\nrooms_keys_doors_puzzle\n1 5 9 13 17 21 24\nHyperparameter Combination\n−5\n0\n5\n10\n15\n20\n25\n30\n35\n40\nlasertag_three_opponents_small\n1 5 9 13 17 21 24\nHyperparameter Combination\n0\n50\n100\n150\n200\n250\n300\nexplore_goal_locations_small\n1 5 9 13 17 21 24\nHyperparameter Combination\n0\n10\n20\n30\n40\n50\nseekavoid_arena_01\nFigure 4. Top Row: Single task training on 5 DeepMind Lab tasks. Each curve is the mean of the best 3 runs based on ﬁnal return.\nIMPALA achieves better performance than A3C. Bottom Row: Stability across hyperparameter combinations sorted by the ﬁnal\nperformance across different hyperparameter combinations. IMPALA is consistently more stable than A3C.\nTask 1 Task 2 Task 3 Task 4 Task 5\nWithout Replay\nV-trace 46.8 32.9 31.3 229.2 43.8\n1-Step 51.8 35.9 25.4 215.8 43.7\nε-correction 44.2 27.3 4.3 107.7 41.5\nNo-correction 40.3 29.1 5.0 94.9 16.1\nWith Replay\nV-trace 47.1 35.8 34.5 250.8 46.9\n1-Step 54.7 34.4 26.4 204.8 41.6\nε-correction 30.4 30.2 3.9 101.5 37.6\nNo-correction 35.0 21.1 2.8 85.0 11.2\nTasks: rooms watermaze, rooms keys doors puzzle,\nlasertag three opponents small,\nexplore goal locations small, seekavoid arena 01\nTable 2. Average ﬁnal return over 3 best hyperparameters for differ-\nent off-policy correction methods on 5 DeepMind Lab tasks. When\nthe lag in policy is negligible both V-trace and 1-step importance\nsampling perform similarly well and better than ε-correction/No-\ncorrection. However, when the lag increases due to use of expe-\nrience replay, V-trace performs better than all other methods in4\nout 5 tasks.\n2.ε-correction - Add a small value ( ε = 1e-6) during\ngradient calculation to prevent logπ(a) from becoming\nvery small and leading to numerical instabilities, similar\nto (Babaeizadeh et al., 2016).\n3. 1-step importance sampling - No off-policy correction\nwhen optimising V (x). For the policy gradient, multiply\nthe advantage at each time step by the corresponding im-\nportance weight. This variant is similar to V-trace without\n“traces” and is included to investigate the importance of\n“traces” in V-trace.\n4. V-trace as described in Section 4.\nFor V-trace and 1-step importance sampling we clip each\nimportance weight ρt andct at 1 (i.e. ¯c = ¯ρ = 1). This\nreduces the variance of the gradient estimate but introduces\na bias. Out of ¯ρ∈ [1, 10, 100] we found that ¯ρ = 1 worked\nbest.\nWe evaluate all algorithms on the set of 5 DeepMind Lab\ntasks from the previous section. We also add an experience\nreplay buffer on the learner to increase the off-policy gap\nbetweenπ andµ. In the experience replay experiments we\ndraw 50% of the items in each batch uniformly at random\nfrom the replay buffer. Table 2 shows the ﬁnal performance\nfor each algorithm with and without replay respectively. In\nthe no replay setting, V-trace performs best on 3 out of 5\ntasks, followed by 1-step importance sampling,ε-correction\nand No-correction. Although 1-step importance sampling\nperforms similarly to V-trace in the no-replay setting, the\ngap widens on 4 out 5 tasks when using experience replay.\nThis suggests that the cruder 1-step importance sampling ap-\nproximation becomes insufﬁcient as the target and behaviour\npolicies deviate from each other more strongly. Also note\nthat V-trace is the only variant that consistently beneﬁts\nfrom adding experience replay.ε-correction improves sig-\nniﬁcantly over No-correction on two tasks but lies far behind\nthe importance-sampling based methods, particularly in the\nmore off-policy setting with experience replay. Figure E.1\nshows results of a more detailed analysis. Figure E.2 shows\nthat the importance-sampling based methods also perform\nbetter across all hyperparameters and are typically more\nrobust.\n5.3. Multi-Task Training\nIMPALA’s high data throughput and data efﬁciency allow us\nto train not only on one task but on multiple tasks in parallel\nwith only a minimal change to the training setup. Instead\nof running the same task on all actors, we allocate a ﬁxed\nnumber of actors to each task in the multi-task suite. Note,\nthe model does not know which task it is being trained or\nevaluated on.\nIMPALA: Importance Weighted Actor-Learner Architectures\nModel Test score\nA3C, deep 23.8%\nIMPALA, shallow 37.1%\nIMPALA-Experts, deep 44.5%\nIMPALA, deep 46.5%\nIMPALA, deep, PBT 49.4%\nIMPALA, deep, PBT, 8 learners 49.1%\nTable 3. Mean capped human normalised scores on DMLab-30.\nAll models were evaluated on the test tasks with 500 episodes per\ntask. The table shows the best score for each architecture.\n5.3.1. DML AB-30\nTo test IMPALA’s performance in a multi-task setting we use\nDMLab-30, a set of 30 diverse tasks built on DeepMind Lab.\nAmong the many task types in the suite are visually complex\nenvironments with natural-looking terrain, instruction-based\ntasks with grounded language (Hermann et al., 2017), navi-\ngation tasks, cognitive (Leibo et al., 2018) and ﬁrst-person\ntagging tasks featuring scripted bots as opponents. A de-\ntailed description of DMLab-30 and the tasks are available\nat github.com/deepmind/lab and deepmind.com/dm-lab-30.\nWe compare multiple variants of IMPALA with a distributed\nA3C implementation. Except for agents using population-\nbased training (PBT) (Jaderberg et al., 2017a), all agents are\ntrained with hyperparameter sweeps across the same range\ngiven in Appendix D.1. We report mean capped human\nnormalised score where the score for each task is capped\nat 100% (see Appendix B). Using mean capped human\nnormalised score emphasises the need to solve multiple\ntasks instead of focusing on becoming super human on\na single task. For PBT we use the mean capped human\nnormalised score as ﬁtness function and tune entropy cost,\nlearning rate and RMSProp ε. See Appendix F for the\nspeciﬁcs of the PBT setup.\nIn particular, we compare the following agent variants. A3C,\ndeep, a distributed implementation with 210 workers (7\nper task) featuring the deep residual network architecture\n(Figure 3 (Right)). IMPALA, shallow with 210 actors and\nIMPALA, deep with 150 actors both with a single learner.\nIMPALA, deep, PBT, the same as IMPALA, deep, but ad-\nditionally using the PBT (Jaderberg et al., 2017a) for hy-\nperparameter optimisation. Finally IMPALA, deep, PBT, 8\nlearners, which utilises 8 learner GPUs to maximise learn-\ning speed. We also train IMPALA agents in an expert setting,\nIMPALA-Experts, deep, where a separate agent is trained\nper task. In this case we did not optimise hyperparameters\nfor each task separately but instead across all tasks on which\nthe 30 expert agents were trained.\nTable 3 and Figure 5 show all variants of IMPALA perform-\ning much better than the deep distributed A3C. Moreover,\nthe deep variant of IMPALA performs better than the shal-\nlow network version not only in terms of ﬁnal performance\nbut throughout the entire training. Note in Table 3 that\nIMPALA, deep, PBT, 8 learners, although providing much\nhigher throughput, reaches the same ﬁnal performance as\nthe 1 GPU IMPALA, deep, PBT in the same number of steps.\nOf particular importance is the gap between the IMPALA-\nExperts which were trained on each task individually and\nIMPALA, deep, PBT which was trained on all tasks at once.\nAs Figure 5 shows, the multi-task version is outperforms\nIMPALA-Experts throughout training and the breakdown\ninto individual scores in Appendix B shows positive transfer\non tasks such as language tasks and laser tag tasks.\nComparing A3C to IMPALA with respect to wall clock time\n(Figure 6) further highlights the scalability gap between\nthe two approaches. IMPALA with 1 learner takes only\naround 10 hours to reach the same performance that A3C\napproaches after 7.5 days. Using 8 learner GPUs instead of\n1 further speeds up training of the deep model by a factor of\n7 to 210K frames/sec, up from 30K frames/sec.\n5.3.2. A TARI\nThe Atari Learning Environment (ALE) (Bellemare et al.,\n2013b) has been the testing ground of most recent deep\nreinforcement agents. Its 57 tasks pose challenging rein-\nforcement learning problems including exploration, plan-\nning, reactive play and complex visual input. Most games\nfeature very different visuals and game mechanics which\nmakes this domain particularly challenging for multi-task\nlearning.\nWe train IMPALA and A3C agents on each game individu-\nally and compare their performance using the deep network\n(without the LSTM) introduced in Section 5. We also pro-\nvide results using a shallow network that is equivalent to\nthe feed forward network used in (Mnih et al., 2016) which\nfeatures a three convolutional layers. The network is pro-\nvided with a short term history by stacking the 4 most recent\nobservations at each step. For details on pre-processing and\nhyperparameter setup please refer to Appendix G.\nIn addition to individual per-game experts, trained for 200\nmillion frames with a ﬁxed set of hyperparameters, we train\nan IMPALA Atari-57 agent—one agent, one set of weights—\non all 57 Atari games at once for 200 million frames per\ngame or a total of 11.4 billion frames. For the Atari-57 agent,\nwe use population based training with a population size of\n24 to adapt entropy regularisation, learning rate, RMSPropε\nand the global gradient norm clipping threshold throughout\ntraining.\nWe compare all algorithms in terms of median human nor-\nmalised score across all 57 Atari games. Evaluation follows\na standard protocol, each game-score is the mean over 200\nevaluation episodes, each episode was started with a random\nIMPALA: Importance Weighted Actor-Learner Architectures\nFigure 5. Performance of best agent in each sweep/population dur-\ning training on the DMLab-30 task-set wrt. data consumed across\nall environments. IMPALA with multi-task training is not only\nfaster, it also converges at higher accuracy with better data efﬁ-\nciency across all 30 tasks. The x-axis is data consumed by one\nagent out of a hyperparameter sweep/PBT population of 24 agents,\ntotal data consumed across the whole population/sweep can be\nobtained by multiplying with the population/sweep size.\n0.0 0.2 0.4 0.6 0.8 1.0\nEnvironment Frames 1e10\n0\n10\n20\n30\n40\n50\n60Mean Capped Normalized Score\nIMPALA, deep, PBT - 8 GPUs\nIMPALA, deep, PBT\nIMPALA, deep\nIMPALA, shallow\nIMPALA-Experts, deep\nA3C, deep\n0 20 40 60 80 100 120 140 160 180\nWall Clock Time (hours)\n0\n10\n20\n30\n40\n50\n60Mean Capped Normalized Score\nIMPALA, deep, PBT - 8 GPUs\nIMPALA, deep, PBT\nIMPALA, deep\nIMPALA, shallow\nIMPALA-Experts, deep\nA3C, deep\nFigure 6. Performance on DMLab-30 wrt. wall-clock time. All\nmodels used the deep architecture (Figure 3). The high throughput\nof IMPALA results in orders of magnitude faster learning.\nnumber of no-op actions (uniformly chosen from [1, 30]) to\ncombat the determinism of the ALE environment.\nAs table 4 shows, IMPALA experts provide both better ﬁnal\nperformance and data efﬁciency than their A3C counterparts\nin the deep and the shallow conﬁguration. As in our Deep-\nMind Lab experiments, the deep residual network leads\nto higher scores than the shallow network, irrespective of\nthe reinforcement learning algorithm used. Note that the\nshallow IMPALA experiment completes training over 200\nmillion frames in less than one hour.\nWe want to particularly emphasise thatIMPALA, deep, multi-\ntask, a single agent trained on all 57 ALE games at once,\nreaches 59.7% median human normalised score. Despite\nHuman Normalised Return Median Mean\nA3C, shallow, experts 54.9% 285.9%\nA3C, deep, experts 117.9% 503.6%\nThe Reactor 187% N/A\nIMPALA, shallow, experts 93.2% 466.4%\nIMPALA, deep, experts 191.8% 957.6%\nIMPALA, deep, multi-task 59.7% 176.9%\nTable 4. Human normalised scores on Atari-57. Up to 30 no-ops\nat the beginning of each episode. For a level-by-level comparison\nto ACKTR (Wu et al., 2017) and Reactor see Appendix C.1.\nthe high diversity in visual appearance and game mechanics\nwithin the ALE suite, IMPALA multi-task still manages\nto stay competitive to A3C, shallow, experts , commonly\nused as a baseline in related work. ALE is typically con-\nsidered a hard multi-task environment, often accompanied\nby negative transfer between tasks (Rusu et al., 2016). To\nour knowledge, IMPALA is the ﬁrst agent to be trained in a\nmulti-task setting on all 57 games of ALE that is competitive\nwith a standard expert baseline.\n6. Conclusion\nWe have introduced a new highly scalable distributed agent,\nIMPALA, and a new off-policy learning algorithm, V-trace.\nWith its simple but scalable distributed architecture, IM-\nPALA can make efﬁcient use of available compute at small\nand large scale. This directly translates to very quick\nturnaround for investigating new ideas and opens up un-\nexplored opportunities.\nV-trace is a general off-policy learning algorithm that is\nmore stable and robust compared to other off-policy correc-\ntion methods for actor critic agents. We have demonstrated\nthat IMPALA achieves better performance compared to\nA3C variants in terms of data efﬁciency, stability and ﬁnal\nperformance. We have further evaluated IMPALA on the\nnew DMLab-30 set and the Atari-57 set. To the best of\nour knowledge, IMPALA is the ﬁrst Deep-RL agent that\nhas been successfully tested in such large-scale multi-task\nsettings and it has shown superior performance compared\nto A3C based agents (49.4% vs. 23.8% human normalised\nscore on DMLab-30). Most importantly, our experiments\non DMLab-30 show that, in the multi-task setting, positive\ntransfer between individual tasks lead IMPALA to achieve\nbetter performance compared to the expert training setting.\nWe believe that IMPALA provides a simple yet scalable and\nrobust framework for building better Deep-RL agents and\nhas the potential to enable research on new challenges.\nIMPALA: Importance Weighted Actor-Learner Architectures\nAcknowledgements\nWe would like to thank Denis Teplyashin, Ricardo Barreira,\nManuel Sanchez for their work improving the performance\non DMLab-30 environments and Matteo Hessel, Jony Hud-\nson, Igor Babuschkin, Max Jaderberg, Ivo Danihelka, Jacob\nMenick and David Silver for their comments and insightful\ndiscussions.\nReferences\nAbadi, M., Isard, M., and Murray, D. G. A computational\nmodel for tensorﬂow: An introduction. In Proceedings\nof the 1st ACM SIGPLAN International Workshop on\nMachine Learning and Programming Languages, MAPL\n2017, 2017. ISBN 978-1-4503-5071-6.\nAdamski, I., Adamski, R., Grel, T., Jedrych, A., Kaczmarek,\nK., and Michalewski, H. Distributed deep reinforcement\nlearning: Learn how to play atari games in 21 minutes.\nCoRR, abs/1801.02852, 2018.\nAppleyard, J., Kocisk ´y, T., and Blunsom, P. Optimizing\nperformance of recurrent neural networks on gpus. CoRR,\nabs/1604.01946, 2016.\nBabaeizadeh, M., Frosio, I., Tyree, S., Clemons, J., and\nKautz, J. GA3C: GPU-based A3C for deep reinforcement\nlearning. NIPS Workshop, 2016.\nBarth-Maron, G., Hoffman, M. W., Budden, D., Dabney,\nW., Horgan, D., Tirumala, D., Muldal, A., Heess, N., and\nLillicrap, T. Distributional policy gradients. ICLR, 2018.\nBeattie, C., Leibo, J. Z., Teplyashin, D., Ward, T., Wain-\nwright, M., Kuttler, H., Lefrancq, A., Green, S., Valdes,\nV ., Sadik, A., Schrittwieser, J., Anderson, K., York, S.,\nCant, M., Cain, A., Bolton, A., Gaffney, S., King, H.,\nHassabis, D., Legg, S., and Petersen, S. Deepmind lab.\nCoRR, abs/1612.03801, 2016.\nBellemare, M. G., Naddaf, Y ., Veness, J., and Bowling, M.\nThe Arcade Learning Environment: An evaluation plat-\nform for general agents. Journal of Artiﬁcial Intelligence\nResearch, 47:253–279, June 2013a.\nBellemare, M. G., Naddaf, Y ., Veness, J., and Bowling, M.\nThe arcade learning environment: An evaluation platform\nfor general agents. J. Artif. Intell. Res.(JAIR), 47:253–279,\n2013b.\nChen, J., Monga, R., Bengio, S., and J ´ozefowicz,\nR. Revisiting distributed synchronous SGD. CoRR,\nabs/1604.00981, 2016.\nChetlur, S., Woolley, C., Vandermersch, P., Cohen, J., Tran,\nJ., Catanzaro, B., and Shelhamer, E. cudnn: Efﬁcient\nprimitives for deep learning. CoRR, abs/1410.0759, 2014.\nClemente, A. V ., Mart´ınez, H. N. C., and Chandra, A. Ef-\nﬁcient parallel methods for deep reinforcement learning.\nCoRR, abs/1705.04862, 2017.\nDean, J., Corrado, G., Monga, R., Chen, K., Devin, M.,\nMao, M., Ranzato, M., Senior, A., Tucker, P., Yang, K.,\nLe, Q. V ., and Ng, A. Y . Large scale distributed deep\nnetworks. In Advances in Neural Information Processing\nSystems 25, pp. 1223–1231, 2012.\nGeist, M. and Scherrer, B. Off-policy learning with eligibil-\nity traces: A survey. The Journal of Machine Learning\nResearch, 15(1):289–333, 2014.\nGruslys, A., Dabney, W., Azar, M. G., Piot, B., Belle-\nmare, M. G., and Munos, R. The Reactor: A fast and\nsample-efﬁcient actor-critic agent for reinforcement learn-\ning. ICLR, 2018.\nHarutyunyan, A., Bellemare, M. G., Stepleton, T., and\nMunos, R. Q(λ) with Off-Policy Corrections, pp. 305–\n320. Springer International Publishing, Cham, 2016.\nHe, K., Zhang, X., Ren, S., and Sun, J. Identity mappings\nin deep residual networks. In European Conference on\nComputer Vision, pp. 630–645. Springer, 2016.\nHermann, K. M., Hill, F., Green, S., Wang, F., Faulkner, R.,\nSoyer, H., Szepesvari, D., Czarnecki, W., Jaderberg, M.,\nTeplyashin, D., et al. Grounded language learning in a\nsimulated 3d world. arXiv preprint arXiv:1706.06551,\n2017.\nHochreiter, S. and Schmidhuber, J. Long short-term memory.\nNeural computation, 9(8):1735–1780, 1997.\nHorgan, D., Quan, J., Budden, D., Barth-Maron, G., Hessel,\nM., van Hasselt, H., and Silver, D. Distributed prioritized\nexperience replay. ICLR, 2018.\nJaderberg, M., Dalibard, V ., Osindero, S., Czarnecki, W. M.,\nDonahue, J., Razavi, A., Vinyals, O., Green, T., Dunning,\nI., Simonyan, K., Fernando, C., and Kavukcuoglu, K.\nPopulation based training of neural networks. CoRR,\nabs/1711.09846, 2017a.\nJaderberg, M., Mnih, V ., Czarnecki, W. M., Schaul, T.,\nLeibo, J. Z., Silver, D., and Kavukcuoglu, K. Reinforce-\nment learning with unsupervised auxiliary tasks. ICLR,\n2017b.\nLeibo, J. Z., d’Autume, C. d. M., Zoran, D., Amos, D.,\nBeattie, C., Anderson, K., Casta˜neda, A. G., Sanchez, M.,\nGreen, S., Gruslys, A., et al. Psychlab: A psychology\nlaboratory for deep reinforcement learning agents. arXiv\npreprint arXiv:1801.08116, 2018.\nIMPALA: Importance Weighted Actor-Learner Architectures\nLillicrap, T. P., Hunt, J. J., Pritzel, A., Heess, N., Erez,\nT., Tassa, Y ., Silver, D., and Wierstra, D. Continuous\ncontrol with deep reinforcement learning. arXiv preprint\narXiv:1509.02971, 2015.\nMnih, V ., Kavukcuoglu, K., Silver, D., Rusu, A. A., Veness,\nJ., Bellemare, M. G., Graves, A., Riedmiller, M., Fidje-\nland, A. K., Ostrovski, G., et al. Human-level control\nthrough deep reinforcement learning. Nature, 518(7540):\n529–533, 2015.\nMnih, V ., Badia, A. P., Mirza, M., Graves, A., Lilli-\ncrap, T. P., Harley, T., Silver, D., and Kavukcuoglu, K.\nAsynchronous methods for deep reinforcement learning.\nICML, 2016.\nMunos, R., Stepleton, T., Harutyunyan, A., and Bellemare,\nM. Safe and efﬁcient off-policy reinforcement learning.\nIn Advances in Neural Information Processing Systems,\npp. 1046–1054, 2016.\nNair, A., Srinivasan, P., Blackwell, S., Alcicek, C., Fearon,\nR., Maria, A. D., Panneershelvam, V ., Suleyman, M.,\nBeattie, C., Petersen, S., Legg, S., Mnih, V ., Kavukcuoglu,\nK., and Silver, D. Massively parallel methods for deep\nreinforcement learning. CoRR, abs/1507.04296, 2015.\nO’Donoghue, B., Munos, R., Kavukcuoglu, K., and Mnih,\nV . Combining policy gradient and Q-learning. InICLR,\n2017.\nPrecup, D., Sutton, R. S., and Singh, S. Eligibility traces for\noff-policy policy evaluation. In Proceedings of the Sev-\nenteenth International Conference on Machine Learning,\n2000.\nPrecup, D., Sutton, R. S., and Dasgupta, S. Off-policy\ntemporal-difference learning with function approxima-\ntion. In Proceedings of the 18th International Conference\non Machine Laerning, pp. 417–424, 2001.\nPuterman, M. L. Markov Decision Processes: Discrete\nStochastic Dynamic Programming. John Wiley & Sons,\nInc., New York, NY , USA, 1st edition, 1994. ISBN\n0471619779.\nRusu, A. A., Rabinowitz, N. C., Desjardins, G., Soyer, H.,\nKirkpatrick, J., Kavukcuoglu, K., Pascanu, R., and Had-\nsell, R. Progressive neural networks. arXiv preprint\narXiv:1606.04671, 2016.\nSalimans, T., Ho, J., Chen, X., and Sutskever, I. Evolu-\ntion strategies as a scalable alternative to reinforcement\nlearning. arXiv preprint arXiv:1703.03864, 2017.\nSchulman, J., Moritz, P., Levine, S., Jordan, M., and Abbeel,\nP. High-dimensional continuous control using generalized\nadvantage estimation. In ICLR, 2016.\nSilver, D., Huang, A., Maddison, C. J., Guez, A., Sifre, L.,\nvan den Driessche, G., Schrittwieser, J., Antonoglou, I.,\nPanneershelvam, V ., Lanctot, M., Dieleman, S., Grewe,\nD., Nham, J., Kalchbrenner, N., Sutskever, I., Lillicrap, T.,\nLeach, M., Kavukcuoglu, K., Graepel, T., and Hassabis,\nD. Mastering the game of go with deep neural networks\nand tree search. Nature, 529:484–503, 2016.\nSilver, D., Schrittwieser, J., Simonyan, K., Antonoglou,\nI., Huang, A., Guez, A., Hubert, T., Baker, L., Lai, M.,\nBolton, A., Chen, Y ., Lillicrap, T., Hui, F., Sifre, L.,\nDriessche, G. v. d., Graepel, T., and Hassabis, D. Master-\ning the game of go without human knowledge. Nature,\n550(7676):354–359, 10 2017. ISSN 0028-0836. doi:\n10.1038/nature24270.\nSutton, R. and Barto, A. Reinforcement learning: An intro-\nduction, volume 116. Cambridge Univ Press, 1998.\nWang, Z., Bapst, V ., Heess, N., Mnih, V ., Munos, R.,\nKavukcuoglu, K., and de Freitas, N. Sample efﬁcient\nactor-critic with experience replay. In ICLR, 2017.\nWawrzynski, P. Real-time reinforcement learning by sequen-\ntial actor-critics and experience replay. Neural Networks,\n22(10):1484–1497, 2009.\nWu, Y ., Mansimov, E., Liao, S., Grosse, R. B., and Ba,\nJ. Scalable trust-region method for deep reinforcement\nlearning using kronecker-factored approximation. CoRR,\nabs/1708.05144, 2017.\nZoph, B., Vasudevan, V ., Shlens, J., and Le, Q. V . Learning\ntransferable architectures for scalable image recognition.\narXiv preprint arXiv:1707.07012, 2017.",
  "values": {
    "Explicability": "No",
    "Critiqability": "No",
    "Beneficence": "No",
    "Not socially biased": "No",
    "Privacy": "No",
    "Justice": "No",
    "Respect for Persons": "No",
    "Non-maleficence": "No",
    "Deferral to humans": "No",
    "Interpretable (to users)": "No",
    "Autonomy (power to decide)": "No",
    "Collective influence": "No",
    "User influence": "No",
    "Transparent (to users)": "No",
    "Fairness": "No",
    "Respect for Law and public interest": "No"
  }
}