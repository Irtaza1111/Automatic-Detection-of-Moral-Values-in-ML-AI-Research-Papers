{
  "pdf": "fujimoto18a",
  "title": "Addressing Function Approximation Error in Actor-Critic Methods",
  "author": "Scott Fujimoto, Herke van Hoof, David Meger",
  "paper_id": "fujimoto18a",
  "text": "Addressing Function Approximation Error in Actor-Critic Methods\nScott Fujimoto 1 Herke van Hoof 2 David Meger 1\nAbstract\nIn value-based reinforcement learning methods\nsuch as deep Q-learning, function approximation\nerrors are known to lead to overestimated value\nestimates and suboptimal policies. We show that\nthis problem persists in an actor-critic setting and\npropose novel mechanisms to minimize its effects\non both the actor and the critic. Our algorithm\nbuilds on Double Q-learning, by taking the mini-\nmum value between a pair of critics to limit over-\nestimation. We draw the connection between tar-\nget networks and overestimation bias, and suggest\ndelaying policy updates to reduce per-update error\nand further improve performance. We evaluate\nour method on the suite of OpenAI gym tasks,\noutperforming the state of the art in every envi-\nronment tested.\n1. Introduction\nIn reinforcement learning problems with discrete action\nspaces, the issue of value overestimation as a result of func-\ntion approximation errors is well-studied. However, similar\nissues with actor-critic methods in continuous control do-\nmains have been largely left untouched. In this paper, we\nshow overestimation bias and the accumulation of error in\ntemporal difference methods are present in an actor-critic\nsetting. Our proposed method addresses these issues, and\ngreatly outperforms the current state of the art.\nOverestimation bias is a property of Q-learning in which the\nmaximization of a noisy value estimate induces a consistent\noverestimation (Thrun & Schwartz, 1993). In a function\napproximation setting, this noise is unavoidable given the\nimprecision of the estimator. This inaccuracy is further\nexaggerated by the nature of temporal difference learning\n(Sutton, 1988), in which an estimate of the value function\nis updated using the estimate of a subsequent state. This\n1McGill University, Montreal, Canada 2University of Amster-\ndam, Amsterdam, Netherlands. Correspondence to: Scott Fujimoto\n<scott.fujimoto@mail.mcgill.ca>.\nProceedings of the 35 th International Conference on Machine\nLearning, Stockholm, Sweden, PMLR 80, 2018. Copyright 2018\nby the author(s).\nmeans using an imprecise estimate within each update will\nlead to an accumulation of error. Due to overestimation bias,\nthis accumulated error can cause arbitrarily bad states to\nbe estimated as high value, resulting in suboptimal policy\nupdates and divergent behavior.\nThis paper begins by establishing this overestimation prop-\nerty is also present for deterministic policy gradients (Silver\net al., 2014), in the continuous control setting. Furthermore,\nwe ﬁnd the ubiquitous solution in the discrete action setting,\nDouble DQN (Van Hasselt et al., 2016), to be ineffective\nin an actor-critic setting. During training, Double DQN\nestimates the value of the current policy with a separate tar-\nget value function, allowing actions to be evaluated without\nmaximization bias. Unfortunately, due to the slow-changing\npolicy in an actor-critic setting, the current and target value\nestimates remain too similar to avoid maximization bias.\nThis can be dealt with by adapting an older variant, Double\nQ-learning (Van Hasselt, 2010), to an actor-critic format\nby using a pair of independently trained critics. While this\nallows for a less biased value estimation, even an unbiased\nestimate with high variance can still lead to future overes-\ntimations in local regions of state space, which in turn can\nnegatively affect the global policy. To address this concern,\nwe propose a clipped Double Q-learning variant which lever-\nages the notion that a value estimate suffering from overes-\ntimation bias can be used as an approximate upper-bound to\nthe true value estimate. This favors underestimations, which\ndo not tend to be propagated during learning, as actions with\nlow value estimates are avoided by the policy.\nGiven the connection of noise to overestimation bias, this\npaper contains a number of components that address vari-\nance reduction. First, we show that target networks, a com-\nmon approach in deep Q-learning methods, are critical for\nvariance reduction by reducing the accumulation of errors.\nSecond, to address the coupling of value and policy, we\npropose delaying policy updates until the value estimate\nhas converged. Finally, we introduce a novel regularization\nstrategy, where a SARSA-style update bootstraps similar\naction estimates to further reduce variance.\nOur modiﬁcations are applied to the state of the art actor-\ncritic method for continuous control, Deep Deterministic\nPolicy Gradient algorithm (DDPG) (Lillicrap et al., 2015), to\nform the Twin Delayed Deep Deterministic policy gradient\nAddressing Function Approximation Error in Actor-Critic Methods\nalgorithm (TD3), an actor-critic algorithm which consid-\ners the interplay between function approximation error in\nboth policy and value updates. We evaluate our algorithm\non seven continuous control domains from OpenAI gym\n(Brockman et al., 2016), where we outperform the state of\nthe art by a wide margin.\nGiven the recent concerns in reproducibility (Henderson\net al., 2017), we run our experiments across a large num-\nber of seeds with fair evaluation metrics, perform abla-\ntion studies across each contribution, and open source both\nour code and learning curves (https://github.com/\nsfujim/TD3).\n2. Related Work\nFunction approximation error and its effect on bias and\nvariance in reinforcement learning algorithms have been\nstudied in prior works (Pendrith et al., 1997; Mannor et al.,\n2007). Our work focuses on two outcomes that occur as the\nresult of estimation error, namely overestimation bias and a\nhigh variance build-up.\nSeveral approaches exist to reduce the effects of overestima-\ntion bias due to function approximation and policy optimiza-\ntion in Q-learning. Double Q-learning uses two independent\nestimators to make unbiased value estimates (Van Hasselt,\n2010; Van Hasselt et al., 2016). Other approaches have\nfocused directly on reducing the variance (Anschel et al.,\n2017), minimizing over-ﬁtting to early high variance esti-\nmates (Fox et al., 2016), or through corrective terms (Lee\net al., 2013). Further, the variance of the value estimate\nhas been considered directly for risk-aversion (Mannor &\nTsitsiklis, 2011) and exploration (O’Donoghue et al., 2017),\nbut without connection to overestimation bias.\nThe concern of variance due to the accumulation of error in\ntemporal difference learning has been largely dealt with by\neither minimizing the size of errors at each time step or mix-\ning off-policy and Monte-Carlo returns. Our work shows\nthe importance of a standard technique, target networks, for\nthe reduction of per-update error, and develops a regulariza-\ntion technique for the variance reduction by averaging over\nvalue estimates. Concurrently, Nachum et al. (2018) showed\nsmoothed value functions could be used to train stochastic\npolicies with reduced variance and improved performance.\nMethods with multi-step returns offer a trade-off between\naccumulated estimation bias and variance induced by the\npolicy and the environment. These methods have been\nshown to be an effective approach, through importance sam-\npling (Precup et al., 2001; Munos et al., 2016), distributed\nmethods (Mnih et al., 2016; Espeholt et al., 2018), and ap-\nproximate bounds (He et al., 2016). However, rather than\nprovide a direct solution to the accumulation of error, these\nmethods circumvent the problem by considering a longer\nhorizon. Another approach is a reduction in the discount\nfactor (Petrik & Scherrer, 2009), reducing the contribution\nof each error.\nOur method builds on the Deterministic Policy Gradient\nalgorithm (DPG) (Silver et al., 2014), an actor-critic method\nwhich uses a learned value estimate to train a deterministic\npolicy. An extension of DPG to deep reinforcement learn-\ning, DDPG (Lillicrap et al., 2015), has shown to produce\nstate of the art results with an efﬁcient number of iterations.\nOrthogonal to our approach, recent improvements to DDPG\ninclude distributed methods (Popov et al., 2017), along with\nmulti-step returns and prioritized experience replay (Schaul\net al., 2016; Horgan et al., 2018), and distributional methods\n(Bellemare et al., 2017; Barth-Maron et al., 2018).\n3. Background\nReinforcement learning considers the paradigm of an agent\ninteracting with its environment with the aim of learning\nreward-maximizing behavior. At each discrete time step\nt, with a given state s ∈ S , the agent selects actions\na ∈ A with respect to its policy π : S → A , receiv-\ning a reward r and the new state of the environment s′.\nThe return is deﬁned as the discounted sum of rewards\nRt =∑T\ni=tγi−tr(si,ai), whereγ is a discount factor de-\ntermining the priority of short-term rewards.\nIn reinforcement learning, the objective is to ﬁnd the op-\ntimal policyπφ, with parametersφ, which maximizes the\nexpected returnJ(φ) = Esi∼pπ,ai∼π [R0]. For continuous\ncontrol, parametrized policiesπφ can be updated by taking\nthe gradient of the expected return ∇φJ(φ). In actor-critic\nmethods, the policy, known as the actor, can be updated\nthrough the deterministic policy gradient algorithm (Silver\net al., 2014):\n∇φJ(φ) = Es∼pπ\n[\n∇aQπ(s,a )|a=π(s)∇φπφ(s)\n]\n. (1)\nQπ(s,a ) = Esi∼pπ,ai∼π [Rt|s,a ], the expected return\nwhen performing action a in state s and following π af-\nter, is known as the critic or the value function.\nIn Q-learning, the value function can be learned using tem-\nporal difference learning (Sutton, 1988; Watkins, 1989), an\nupdate rule based on the Bellman equation (Bellman, 1957).\nThe Bellman equation is a fundamental relationship between\nthe value of a state-action pair (s,a ) and the value of the\nsubsequent state-action pair (s′,a′):\nQπ(s,a ) = r +γEs′,a′ [Qπ(s′,a′)], a ′ ∼π(s′). (2)\nFor a large state space, the value can be estimated with a\ndifferentiable function approximatorQθ(s,a ), with param-\netersθ. In deep Q-learning (Mnih et al., 2015), the network\nis updated by using temporal difference learning with a sec-\nondary frozen target networkQθ′(s,a ) to maintain a ﬁxed\nAddressing Function Approximation Error in Actor-Critic Methods\nobjectivey over multiple updates:\ny =r +γQθ′(s′,a′), a ′ ∼πφ′(s′), (3)\nwhere the actions are selected from a target actor network\nπφ′. The weights of a target network are either updated\nperiodically to exactly match the weights of the current\nnetwork, or by some proportion τ at each time step θ′ ←\nτθ + (1 −τ )θ′. This update can be applied in an off-policy\nfashion, sampling random mini-batches of transitions from\nan experience replay buffer (Lin, 1992).\n4. Overestimation Bias\nIn Q-learning with discrete actions, the value estimate is\nupdated with a greedy target y = r +γ maxa′Q(s′,a′),\nhowever, if the target is susceptible to errorϵ, then the max-\nimum over the value along with its error will generally be\ngreater than the true maximum, Eϵ[maxa′(Q(s′,a′) +ϵ)] ≥\nmaxa′Q(s′,a′) (Thrun & Schwartz, 1993). As a result,\neven initially zero-mean error can cause value updates to\nresult in a consistent overestimation bias, which is then prop-\nagated through the Bellman equation. This is problematic as\nerrors induced by function approximation are unavoidable.\nWhile in the discrete action setting overestimation bias is\nan obvious artifact from the analytical maximization, the\npresence and effects of overestimation bias is less clear in an\nactor-critic setting where the policy is updated via gradient\ndescent. We begin by proving that the value estimate in de-\nterministic policy gradients will be an overestimation under\nsome basic assumptions in Section 4.1 and then propose\na clipped variant of Double Q-learning in an actor-critic\nsetting to reduce overestimation bias in Section 4.2.\n4.1. Overestimation Bias in Actor-Critic\nIn actor-critic methods the policy is updated with respect\nto the value estimates of an approximate critic. In this\nsection we assume the policy is updated using the deter-\nministic policy gradient, and show that the update induces\noverestimation in the value estimate. Given current policy\nparametersφ, letφapprox deﬁne the parameters from the ac-\ntor update induced by the maximization of the approximate\ncriticQθ(s,a ) andφtrue the parameters from the hypothet-\nical actor update with respect to the true underlying value\nfunctionQπ(s,a ) (which is not known during learning):\nφapprox =φ + α\nZ1\nEs∼pπ\n[\n∇φπφ(s)∇aQθ(s,a )|a=πφ(s)\n]\nφtrue =φ + α\nZ2\nEs∼pπ\n[\n∇φπφ(s)∇aQπ(s,a )|a=πφ(s)\n]\n,\n(4)\nwhere we assume Z1 andZ2 are chosen to normalize the\ngradient, i.e., such that Z−1||E[·]|| = 1 . Without normal-\nized gradients, overestimation bias is still guaranteed to\n0.0 0.2 0.4 0.6 0.8 1.0\nTime steps (1e6)\n0\n100\n200\n300\n400Average Value\n CDQ\nDDPG\nTrue CDQ\nTrue DDPG\n0.0 0.2 0.4 0.6 0.8 1.0\nTime steps (1e6)\n0\n100\n200\n300\n400\n500\n(a) Hopper-v1 (b) Walker2d-v1\nFigure 1. Measuring overestimation bias in the value estimates\nof DDPG and our proposed method, Clipped Double Q-learning\n(CDQ), on MuJoCo environments over 1 million time steps.\noccur with slightly stricter conditions. We examine this case\nfurther in the supplementary material. We denote πapprox\nandπtrue as the policy with parametersφapprox andφtrue re-\nspectively.\nAs the gradient direction is a local maximizer, there existsϵ1\nsufﬁciently small such that ifα ≤ϵ1 then the approximate\nvalue ofπapprox will be bounded below by the approximate\nvalue ofπtrue:\nE [Qθ(s,π approx(s))] ≥ E [Qθ(s,π true(s))]. (5)\nConversely, there exists ϵ2 sufﬁciently small such that if\nα ≤ϵ2 then the true value ofπapprox will be bounded above\nby the true value ofπtrue:\nE [Qπ(s,π true(s))] ≥ E [Qπ(s,π approx(s))]. (6)\nIf in expectation the value estimate is at least as large as\nthe true value with respect toφtrue, E [Qθ (s,π true(s))] ≥\nE [Qπ (s,π true(s))], then Equations (5) and (6) imply that if\nα <min(ϵ1,ϵ 2), then the value estimate will be overesti-\nmated:\nE [Qθ(s,π approx(s))] ≥ E [Qπ(s,π approx(s))]. (7)\nAlthough this overestimation may be minimal with each\nupdate, the presence of error raises two concerns. Firstly, the\noverestimation may develop into a more signiﬁcant bias over\nmany updates if left unchecked. Secondly, an inaccurate\nvalue estimate may lead to poor policy updates. This is\nparticularly problematic because a feedback loop is created,\nin which suboptimal actions might be highly rated by the\nsuboptimal critic, reinforcing the suboptimal action in the\nnext policy update.\nDoes this theoretical overestimation occur in practice\nfor state-of-the-art methods? We answer this question by\nplotting the value estimate of DDPG (Lillicrap et al., 2015)\nover time while it learns on the OpenAI gym environments\nHopper-v1 and Walker2d-v1 (Brockman et al., 2016). In\nFigure 1, we graph the average value estimate over 10000\nstates and compare it to an estimate of the true value. The\nAddressing Function Approximation Error in Actor-Critic Methods\n0.0 0.2 0.4 0.6 0.8 1.0\nTime steps (1e6)\n0\n100\n200\n300\n400Average Value\n DQ-AC\nDDQN-AC\nTrue DQ-AC\nTrue DDQN-AC\n0.0 0.2 0.4 0.6 0.8 1.0\nTime steps (1e6)\n0\n100\n200\n300\n400\n(a) Hopper-v1 (b) Walker2d-v1\nFigure 2. Measuring overestimation bias in the value estimates of\nactor critic variants of Double DQN (DDQN-AC) and Double Q-\nlearning (DQ-AC) on MuJoCo environments over 1 million time\nsteps.\ntrue value is estimated using the average discounted return\nover 1000 episodes following the current policy, starting\nfrom states sampled from the replay buffer. A very clear\noverestimation bias occurs from the learning procedure,\nwhich contrasts with the novel method that we describe in\nthe following section, Clipped Double Q-learning, which\ngreatly reduces overestimation by the critic.\n4.2. Clipped Double Q-Learning for Actor-Critic\nWhile several approaches to reducing overestimation bias\nhave been proposed, we ﬁnd them ineffective in an actor-\ncritic setting. This section introduces a novel clipped variant\nof Double Q-learning (Van Hasselt, 2010), which can re-\nplace the critic in any actor-critic method.\nIn Double Q-learning, the greedy update is disentangled\nfrom the value function by maintaining two separate value\nestimates, each of which is used to update the other. If the\nvalue estimates are independent, they can be used to make\nunbiased estimates of the actions selected using the opposite\nvalue estimate. In Double DQN (Van Hasselt et al., 2016),\nthe authors propose using the target network as one of the\nvalue estimates, and obtain a policy by greedy maximization\nof the current value network rather than the target network.\nIn an actor-critic setting, an analogous update uses the cur-\nrent policy rather than the target policy in the learning target:\ny =r +γQθ′(s′,πφ(s′)). (8)\nIn practice however, we found that with the slow-changing\npolicy in actor-critic, the current and target networks were\ntoo similar to make an independent estimation, and offered\nlittle improvement. Instead, the original Double Q-learning\nformulation can be used, with a pair of actors ( πφ1,πφ2)\nand critics (Qθ1,Qθ2), whereπφ1 is optimized with respect\ntoQθ1 andπφ2 with respect toQθ2:\ny1 =r +γQθ′\n2 (s′,πφ1 (s′))\ny2 =r +γQθ′\n1 (s′,πφ2 (s′)). (9)\nWe measure the overestimation bias in Figure 2, which\ndemonstrates that the actor-critic Double DQN suffers from\na similar overestimation as DDPG (as shown in Figure 1).\nWhile Double Q-learning is more effective, it does not en-\ntirely eliminate the overestimation. We further show this\nreduction is not sufﬁcient experimentally in Section 6.1.\nAsπφ1 optimizes with respect to Qθ1, using an indepen-\ndent estimate in the target update of Qθ1 would avoid the\nbias introduced by the policy update. However the critics\nare not entirely independent, due to the use of the oppo-\nsite critic in the learning targets, as well as the same re-\nplay buffer. As a result, for some states s we will have\nQθ2 (s,πφ1 (s)) > Qθ1 (s,πφ1 (s)). This is problematic be-\ncauseQθ1 (s,πφ1 (s)) will generally overestimate the true\nvalue, and in certain areas of the state space the overestima-\ntion will be further exaggerated. To address this problem,\nwe propose to simply upper-bound the less biased value\nestimateQθ2 by the biased estimate Qθ1. This results in\ntaking the minimum between the two estimates, to give the\ntarget update of our Clipped Double Q-learning algorithm:\ny1 =r +γ min\ni=1,2\nQθ′\ni\n(s′,πφ1 (s′)). (10)\nWith Clipped Double Q-learning, the value target cannot\nintroduce any additional overestimation over using the stan-\ndard Q-learning target. While this update rule may induce\nan underestimation bias, this is far preferable to overesti-\nmation bias, as unlike overestimated actions, the value of\nunderestimated actions will not be explicitly propagated\nthrough the policy update.\nIn implementation, computational costs can be reduced by\nusing a single actor optimized with respect toQθ1. We then\nuse the same target y2 = y1 forQθ2. If Qθ2 > Qθ1 then\nthe update is identical to the standard update and induces no\nadditional bias. IfQθ2 <Q θ1, this suggests overestimation\nhas occurred and the value is reduced similar to Double Q-\nlearning. A proof of convergence in the ﬁnite MDP setting\nfollows from this intuition. We provide formal details and\njustiﬁcation in the supplementary material.\nA secondary beneﬁt is that by treating the function approxi-\nmation error as a random variable we can see that the min-\nimum operator should provide higher value to states with\nlower variance estimation error, as the expected minimum\nof a set of random variables decreases as the variance of\nthe random variables increases. This effect means that the\nminimization in Equation (10) will lead to a preference for\nstates with low-variance value estimates, leading to safer\npolicy updates with stable learning targets.\n5. Addressing Variance\nWhile Section 4 deals with the contribution of variance to\noverestimation bias, we also argue that variance itself should\nbe directly addressed. Besides the impact on overestimation\nAddressing Function Approximation Error in Actor-Critic Methods\nbias, high variance estimates provide a noisy gradient for the\npolicy update. This is known to reduce learning speed (Sut-\nton & Barto, 1998) as well as hurt performance in practice.\nIn this section we emphasize the importance of minimizing\nerror at each update, build the connection between target\nnetworks and estimation error and propose modiﬁcations to\nthe learning procedure of actor-critic for variance reduction.\n5.1. Accumulating Error\nDue to the temporal difference update, where an estimate of\nthe value function is built from an estimate of a subsequent\nstate, there is a build up of error. While it is reasonable to\nexpect small error for an individual update, these estimation\nerrors can accumulate, resulting in the potential for large\noverestimation bias and suboptimal policy updates. This is\nexacerbated in a function approximation setting where the\nBellman equation is never exactly satisﬁed, and each update\nleaves some amount of residual TD-errorδ(s,a ):\nQθ(s,a ) = r +γE[Qθ(s′,a′)] −δ(s,a ). (11)\nIt can then be shown that rather than learning an estimate\nof the expected return, the value estimate approximates the\nexpected return minus the expected discounted sum of future\nTD-errors:\nQθ(st,at) = rt +γE[Qθ(st+1,at+1)] −δt\n=rt +γE [rt+1 +γE [Qθ(st+2,at+2) −δt+1]] −δt\n= Esi∼pπ,ai∼π\n[ T∑\ni=t\nγi−t(ri −δi)\n]\n. (12)\nIf the value estimate is a function of future reward and es-\ntimation error, it follows that the variance of the estimate\nwill be proportional to the variance of future reward and es-\ntimation error. Given a large discount factorγ, the variance\ncan grow rapidly with each update if the error from each\nupdate is not tamed. Furthermore each gradient update only\nreduces error with respect to a small mini-batch which gives\nno guarantees about the size of errors in value estimates\noutside the mini-batch.\n5.2. Target Networks and Delayed Policy Updates\nIn this section we examine the relationship between target\nnetworks and function approximation error, and show the\nuse of a stable target reduces the growth of error. This\ninsight allows us to consider the interplay between high\nvariance estimates and policy performance, when designing\nreinforcement learning algorithms.\nTarget networks are a well-known tool to achieve stabil-\nity in deep reinforcement learning. As deep function ap-\nproximators require multiple gradient updates to converge,\ntarget networks provide a stable objective in the learning\n0.0 0.2 0.4 0.6 0.8 1.0\nTime steps (1e5)\n150\n200\n250\n300\n350Average Value τ = 1\nτ = 0.1\nτ = 0.01\nTrue Value\n0.0 0.2 0.4 0.6 0.8 1.0\nTime steps (1e5)\n102\n103\n104\n(a) Fixed Policy (b) Learned Policy\nFigure 3. Average estimated value of a randomly selected state\non Hopper-v1 without target networks, (τ = 1 ), and with slow-\nupdating target networks, ( τ = 0.1, 0.01), with a ﬁxed and a\nlearned policy.\nprocedure, and allow a greater coverage of the training data.\nWithout a ﬁxed target, each update may leave residual error\nwhich will begin to accumulate. While the accumulation of\nerror can be detrimental in itself, when paired with a policy\nmaximizing over the value estimate, it can result in wildly\ndivergent values.\nTo provide some intuition, we examine the learning behavior\nwith and without target networks on both the critic and actor\nin Figure 3, where we graph the value, in a similar manner to\nFigure 1, in the Hopper-v1 environment. In (a) we compare\nthe behavior with a ﬁxed policy and in (b) we examine the\nvalue estimates with a policy that continues to learn, trained\nwith the current value estimate. The target networks use a\nslow-moving update rate, parametrized byτ.\nWhile updating the value estimate without target networks\n(τ = 1) increases the volatility, all update rates result in sim-\nilar convergent behaviors when considering a ﬁxed policy.\nHowever, when the policy is trained with the current value\nestimate, the use of fast-updating target networks results in\nhighly divergent behavior.\nWhen do actor-critic methods fail to learn? These results\nsuggest that the divergence that occurs without target net-\nworks is the result of policy updates with a high variance\nvalue estimate. Figure 3, as well as Section 4, suggest failure\ncan occur due to the interplay between the actor and critic\nupdates. Value estimates diverge through overestimation\nwhen the policy is poor, and the policy will become poor if\nthe value estimate itself is inaccurate.\nIf target networks can be used to reduce the error over mul-\ntiple updates, and policy updates on high-error states cause\ndivergent behavior, then the policy network should be up-\ndated at a lower frequency than the value network, to ﬁrst\nminimize error before introducing a policy update. We pro-\npose delaying policy updates until the value error is as small\nas possible. The modiﬁcation is to only update the policy\nand target networks after a ﬁxed number of updatesd to the\ncritic. To ensure the TD-error remains small, we update the\nAddressing Function Approximation Error in Actor-Critic Methods\ntarget networks slowlyθ′ ←τθ + (1 −τ )θ′.\nBy sufﬁciently delaying the policy updates we limit the like-\nlihood of repeating updates with respect to an unchanged\ncritic. The less frequent policy updates that do occur will\nuse a value estimate with lower variance, and in principle,\nshould result in higher quality policy updates. This creates a\ntwo-timescale algorithm, as often required for convergence\nin the linear setting (Konda & Tsitsiklis, 2003). The effec-\ntiveness of this strategy is captured by our empirical results\npresented in Section 6.1, which show an improvement in\nperformance while using fewer policy updates.\n5.3. Target Policy Smoothing Regularization\nA concern with deterministic policies is they can overﬁt\nto narrow peaks in the value estimate. When updating the\ncritic, a learning target using a deterministic policy is highly\nsusceptible to inaccuracies induced by function approxima-\ntion error, increasing the variance of the target. This induced\nvariance can be reduced through regularization. We intro-\nduce a regularization strategy for deep value learning, target\npolicy smoothing, which mimics the learning update from\nSARSA (Sutton & Barto, 1998). Our approach enforces\nthe notion that similar actions should have similar value.\nWhile the function approximation does this implicitly, the\nrelationship between similar actions can be forced explicitly\nby modifying the training procedure. We propose that ﬁtting\nthe value of a small area around the target action\ny =r + Eϵ [Qθ′(s′,πφ′(s′) +ϵ)], (13)\nwould have the beneﬁt of smoothing the value estimate by\nbootstrapping off of similar state-action value estimates. In\npractice, we can approximate this expectation over actions\nby adding a small amount of random noise to the target\npolicy and averaging over mini-batches. This makes our\nmodiﬁed target update:\ny =r +γQθ′(s′,πφ′(s′) +ϵ),\nϵ ∼ clip(N (0,σ ), −c,c ), (14)\nwhere the added noise is clipped to keep the target in a\nsmall range. The outcome is an algorithm reminiscent of\nExpected SARSA (Van Seijen et al., 2009), where the value\nestimate is instead learned off-policy and the noise added to\nthe target policy is chosen independently of the exploration\npolicy. The value estimate learned is with respect to a noisy\npolicy deﬁned by the parameterσ.\nIntuitively, it is known that policies derived from SARSA\nvalue estimates tend to be safer, as they provide higher value\nto actions resistant to perturbations. Thus, this style of\nupdate can additionally lead to improvement in stochastic\ndomains with failure cases. A similar idea was introduced\nconcurrently by Nachum et al. (2018), smoothing overQθ,\nrather thanQθ′.\nAlgorithm 1 TD3\nInitialize critic networksQθ1,Qθ2, and actor networkπφ\nwith random parametersθ1,θ2,φ\nInitialize target networksθ′\n1 ←θ1,θ′\n2 ←θ2,φ′ ←φ\nInitialize replay buffer B\nfort = 1 toT do\nSelect action with exploration noisea ∼π(s) +ϵ,\nϵ ∼ N (0,σ ) and observe rewardr and new states′\nStore transition tuple (s,a,r,s ′) in B\nSample mini-batch ofN transitions (s,a,r,s ′) from B\n˜a ←πφ′(s) +ϵ, ϵ ∼ clip(N (0, ˜σ), −c,c )\ny ←r +γ mini=1,2Qθ′\ni\n(s′, ˜a)\nUpdate criticsθi ← minθiN−1∑(y −Qθi (s,a ))2\nift modd then\nUpdateφ by the deterministic policy gradient:\n∇φJ(φ) = N−1∑∇aQθ1 (s,a )|a=πφ(s)∇φπφ(s)\nUpdate target networks:\nθ′\ni ←τθi + (1 −τ )θ′\ni\nφ′ ←τφ + (1 −τ )φ′\nend if\nend for\n(a)\n (b)\n (c)\n (d)\nFigure 4. Example MuJoCo environments (a) HalfCheetah-v1, (b)\nHopper-v1, (c) Walker2d-v1, (d) Ant-v1.\n6. Experiments\nWe present the Twin Delayed Deep Deterministic policy\ngradient algorithm (TD3), which builds on the Deep Deter-\nministic Policy Gradient algorithm (DDPG) (Lillicrap et al.,\n2015) by applying the modiﬁcations described in Sections\n4.2, 5.2 and 5.3 to increase the stability and performance\nwith consideration of function approximation error. TD3\nmaintains a pair of critics along with a single actor. For each\ntime step, we update the pair of critics towards the minimum\ntarget value of actions selected by the target policy:\ny =r +γ min\ni=1,2\nQθ′\ni\n(s′,πφ′(s′) +ϵ),\nϵ ∼ clip(N (0,σ ), −c,c ).\n(15)\nEveryd iterations, the policy is updated with respect toQθ1\nfollowing the deterministic policy gradient algorithm (Silver\net al., 2014). TD3 is summarized in Algorithm 1.\nAddressing Function Approximation Error in Actor-Critic Methods\n0.0 0.2 0.4 0.6 0.8 1.0\nTime steps (1e6)\n0\n2000\n4000\n6000\n8000\n10000Average Return\nTD3 DDPG our DDPG PPO TRPO ACKTR SAC\n0.0 0.2 0.4 0.6 0.8 1.0\nTime steps (1e6)\n0\n500\n1000\n1500\n2000\n2500\n3000\n3500\n0.0 0.2 0.4 0.6 0.8 1.0\nTime steps (1e6)\n0\n1000\n2000\n3000\n4000\n5000\n0.0 0.2 0.4 0.6 0.8 1.0\nTime steps (1e6)\n−1000\n0\n1000\n2000\n3000\n4000\n(a) HalfCheetah-v1 (b) Hopper-v1 (c) Walker2d-v1 (d) Ant-v1\n0.0 0.2 0.4 0.6 0.8 1.0\nTime steps (1e6)\n−12\n−10\n−8\n−6\n−4\nAverage Return\n0.0 0.2 0.4 0.6 0.8 1.0\nTime steps (1e6)\n400\n500\n600\n700\n800\n900\n1000\n0.0 0.2 0.4 0.6 0.8 1.0\nTime steps (1e6)\n0\n2000\n4000\n6000\n8000\n10000\n(e) Reacher-v1 (f) InvertedPendulum-v1 (g) InvertedDoublePendulum-v1\nFigure 5. Learning curves for the OpenAI gym continuous control tasks. The shaded region represents half a standard deviation of the\naverage evaluation over 10 trials. Some graphs are cropped to display the interesting regions.\nTable 1. Max Average Return over 10 trials of 1 million time steps. Maximum value for each task is bolded. ± corresponds to a single\nstandard deviation over trials.\nEnvironment TD3 DDPG Our DDPG PPO TRPO ACKTR SAC\nHalfCheetah 9636.95 ± 859.065 3305.60 8577.29 1795.43 -15.57 1450.46 2347.19\nHopper 3564.07 ± 114.74 2020.46 1860.02 2164.70 2471.30 2428.39 2996.66\nWalker2d 4682.82 ± 539.64 1843.85 3098.11 3317.69 2321.47 1216.70 1283.67\nAnt 4372.44 ± 1000.33 1005.30 888.77 1083.20 -75.85 1821.94 655.35\nReacher -3.60 ± 0.56 -6.51 -4.01 -6.18 -111.43 -4.26 -4.44\nInvPendulum 1000.00 ± 0.00 1000.00 1000.00 1000.00 985.40 1000.00 1000.00\nInvDoublePendulum 9337.47 ± 14.96 9355.52 8369.95 8977.94 205.85 9081.92 8487.15\n6.1. Evaluation\nTo evaluate our algorithm, we measure its performance on\nthe suite of MuJoCo continuous control tasks (Todorov et al.,\n2012), interfaced through OpenAI Gym (Brockman et al.,\n2016) (Figure 4). To allow for reproducible comparison, we\nuse the original set of tasks from Brockman et al. (2016)\nwith no modiﬁcations to the environment or reward.\nFor our implementation of DDPG (Lillicrap et al., 2015), we\nuse a two layer feedforward neural network of 400 and 300\nhidden nodes respectively, with rectiﬁed linear units (ReLU)\nbetween each layer for both the actor and critic, and a ﬁnal\ntanh unit following the output of the actor. Unlike the orig-\ninal DDPG, the critic receives both the state and action as\ninput to the ﬁrst layer. Both network parameters are updated\nusing Adam (Kingma & Ba, 2014) with a learning rate of\n10−3. After each time step, the networks are trained with a\nmini-batch of a 100 transitions, sampled uniformly from a\nreplay buffer containing the entire history of the agent.\nThe target policy smoothing is implemented by addingϵ ∼\nN (0, 0.2) to the actions chosen by the target actor network,\nclipped to (−0.5, 0.5), delayed policy updates consists of\nonly updating the actor and target critic network every d\niterations, withd = 2 . While a larger d would result in a\nlarger beneﬁt with respect to accumulating errors, for fair\ncomparison, the critics are only trained once per time step,\nand training the actor for too few iterations would cripple\nlearning. Both target networks are updated withτ = 0.005.\nTo remove the dependency on the initial parameters of the\npolicy we use a purely exploratory policy for the ﬁrst 10000\ntime steps of stable length environments (HalfCheetah-v1\nand Ant-v1) and the ﬁrst 1000 time steps for the remaining\nenvironments. Afterwards, we use an off-policy exploration\nstrategy, adding Gaussian noise N (0, 0.1) to each action.\nUnlike the original implementation of DDPG, we used un-\ncorrelated noise for exploration as we found noise drawn\nfrom the Ornstein-Uhlenbeck (Uhlenbeck & Ornstein, 1930)\nprocess offered no performance beneﬁts.\nEach task is run for 1 million time steps with evaluations\nevery 5000 time steps, where each evaluation reports the\nAddressing Function Approximation Error in Actor-Critic Methods\naverage reward over 10 episodes with no exploration noise.\nOur results are reported over 10 random seeds of the Gym\nsimulator and the network initialization.\nWe compare our algorithm against DDPG (Lillicrap et al.,\n2015) as well as the state of art policy gradient algorithms:\nPPO (Schulman et al., 2017), ACKTR (Wu et al., 2017)\nand TRPO (Schulman et al., 2015), as implemented by\nOpenAI’s baselines repository (Dhariwal et al., 2017), and\nSAC (Haarnoja et al., 2018), as implemented by the author’s\nGitHub1. Additionally, we compare our method with our\nre-tuned version of DDPG, which includes all architecture\nand hyper-parameter modiﬁcations to DDPG without any\nof our proposed adjustments. A full comparison between\nour re-tuned version and the baselines DDPG is provided in\nthe supplementary material.\nOur results are presented in Table 1 and learning curves in\nFigure 5. TD3 matches or outperforms all other algorithms\nin both ﬁnal performance and learning speed across all tasks.\n6.2. Ablation Studies\nWe perform ablation studies to understand the contribution\nof each individual component: Clipped Double Q-learning\n(Section 4.2), delayed policy updates (Section 5.2) and target\npolicy smoothing (Section 5.3). We present our results in\nTable 2 in which we compare the performance of removing\neach component from TD3 along with our modiﬁcations to\nthe architecture and hyper-parameters. Additional learning\ncurves can be found in the supplementary material.\nThe signiﬁcance of each component varies task to task.\nWhile the addition of only a single component causes in-\nsigniﬁcant improvement in most cases, the addition of com-\nbinations performs at a much higher level. The full algo-\nrithm outperforms every other combination in most tasks.\nAlthough the actor is trained for only half the number of\niterations, the inclusion of delayed policy update generally\nimproves performance, while reducing training time.\nWe additionally compare the effectiveness of the actor-critic\nvariants of Double Q-learning (Van Hasselt, 2010) and Dou-\nble DQN (Van Hasselt et al., 2016), denoted DQ-AC and\nDDQN-AC respectively, in Table 2. For fairness in com-\nparison, these methods also beneﬁted from delayed policy\nupdates, target policy smoothing and use our architecture\nand hyper-parameters. Both methods were shown to reduce\noverestimation bias less than Clipped Double Q-learning in\nSection 4. This is reﬂected empirically, as both methods\nresult in insigniﬁcant improvements over TD3 - CDQ, with\nan exception in the Ant-v1 environment, which appears to\nbeneﬁt greatly from any overestimation reduction. As the\ninclusion of Clipped Double Q-learning into our full method\n1See the supplementary material for hyper-parameters and a\ndiscussion on the discrepancy in the reported results of SAC.\nTable 2. Average return over the last 10 evaluations over 10 trials\nof 1 million time steps, comparing ablation over delayed policy\nupdates (DP), target policy smoothing (TPS), Clipped Double\nQ-learning (CDQ) and our architecture, hyper-parameters and\nexploration (AHE). Maximum value for each task is bolded.\nMethod HCheetah Hopper Walker2d Ant\nTD3 9532.99 3304.75 4565.24 4185.06\nDDPG 3162.50 1731.94 1520.90 816.35\nAHE 8401.02 1061.77 2362.13 564.07\nAHE + DP 7588.64 1465.11 2459.53 896.13\nAHE + TPS 9023.40 907.56 2961.36 872.17\nAHE + CDQ 6470.20 1134.14 3979.21 3818.71\nTD3 - DP 9590.65 2407.42 4695.50 3754.26\nTD3 - TPS 8987.69 2392.59 4033.67 4155.24\nTD3 - CDQ 9792.80 1837.32 2579.39 849.75\nDQ-AC 9433.87 1773.71 3100.45 2445.97\nDDQN-AC 10306.90 2155.75 3116.81 1092.18\noutperforms both prior methods, this suggests that subdu-\ning the overestimations from the unbiased estimator is an\neffective measure to improve performance.\n7. Conclusion\nOverestimation has been identiﬁed as a key problem in\nvalue-based methods. In this paper, we establish overesti-\nmation bias is also problematic in actor-critic methods. We\nﬁnd the common solutions for reducing overestimation bias\nin deep Q-learning with discrete actions are ineffective in an\nactor-critic setting, and develop a novel variant of Double\nQ-learning which limits possible overestimation. Our re-\nsults demonstrate that mitigating overestimation can greatly\nimprove the performance of modern algorithms.\nDue to the connection between noise and overestimation,\nwe examine the accumulation of errors from temporal dif-\nference learning. Our work investigates the importance of\na standard technique in deep reinforcement learning, target\nnetworks, and examines their role in limiting errors from\nimprecise function approximation and stochastic optimiza-\ntion. Finally, we introduce a SARSA-style regularization\ntechnique which modiﬁes the temporal difference target to\nbootstrap off similar state-action pairs.\nTaken together, these improvements deﬁne our proposed\napproach, the Twin Delayed Deep Deterministic policy gra-\ndient algorithm (TD3), which greatly improves both the\nlearning speed and performance of DDPG in a number of\nchallenging tasks in the continuous control setting. Our\nalgorithm exceeds the performance of numerous state of\nthe art algorithms. As our modiﬁcations are simple to im-\nplement, they can be easily added to any other actor-critic\nalgorithm.\nAddressing Function Approximation Error in Actor-Critic Methods\nReferences\nAnschel, O., Baram, N., and Shimkin, N. Averaged-dqn:\nVariance reduction and stabilization for deep reinforce-\nment learning. In International Conference on Machine\nLearning, pp. 176–185, 2017.\nBarth-Maron, G., Hoffman, M. W., Budden, D., Dabney,\nW., Horgan, D., TB, D., Muldal, A., Heess, N., and Lil-\nlicrap, T. Distributional policy gradients. International\nConference on Learning Representations, 2018.\nBellemare, M. G., Dabney, W., and Munos, R. A distribu-\ntional perspective on reinforcement learning. In Interna-\ntional Conference on Machine Learning , pp. 449–458,\n2017.\nBellman, R. Dynamic Programming. Princeton University\nPress, 1957.\nBrockman, G., Cheung, V ., Pettersson, L., Schneider, J.,\nSchulman, J., Tang, J., and Zaremba, W. Openai gym,\n2016.\nDhariwal, P., Hesse, C., Plappert, M., Radford, A., Schul-\nman, J., Sidor, S., and Wu, Y . Openai baselines.https:\n//github.com/openai/baselines, 2017.\nEspeholt, L., Soyer, H., Munos, R., Simonyan, K., Mnih,\nV ., Ward, T., Doron, Y ., Firoiu, V ., Harley, T., Dunning,\nI., et al. Impala: Scalable distributed deep-rl with impor-\ntance weighted actor-learner architectures. arXiv preprint\narXiv:1802.01561, 2018.\nFox, R., Pakman, A., and Tishby, N. Taming the noise in\nreinforcement learning via soft updates. InProceedings of\nthe Thirty-Second Conference on Uncertainty in Artiﬁcial\nIntelligence, pp. 202–211. AUAI Press, 2016.\nHaarnoja, T., Zhou, A., Abbeel, P., and Levine, S. Soft\nactor-critic: Off-policy maximum entropy deep reinforce-\nment learning with a stochastic actor. arXiv preprint\narXiv:1801.01290, 2018.\nHe, F. S., Liu, Y ., Schwing, A. G., and Peng, J. Learning\nto play in a day: Faster deep reinforcement learning by\noptimality tightening. arXiv preprint arXiv:1611.01606,\n2016.\nHenderson, P., Islam, R., Bachman, P., Pineau, J., Precup,\nD., and Meger, D. Deep Reinforcement Learning that\nMatters. arXiv preprint arXiv:1709.06560, 2017.\nHorgan, D., Quan, J., Budden, D., Barth-Maron, G., Hessel,\nM., van Hasselt, H., and Silver, D. Distributed prioritized\nexperience replay. International Conference on Learning\nRepresentations, 2018.\nKingma, D. and Ba, J. Adam: A method for stochastic\noptimization. arXiv preprint arXiv:1412.6980, 2014.\nKonda, V . R. and Tsitsiklis, J. N. On actor-critic algorithms.\nSIAM journal on Control and Optimization, 42(4):1143–\n1166, 2003.\nLee, D., Defourny, B., and Powell, W. B. Bias-corrected\nq-learning to control max-operator bias in q-learning.\nIn Adaptive Dynamic Programming And Reinforcement\nLearning (ADPRL), 2013 IEEE Symposium on, pp. 93–99.\nIEEE, 2013.\nLillicrap, T. P., Hunt, J. J., Pritzel, A., Heess, N., Erez,\nT., Tassa, Y ., Silver, D., and Wierstra, D. Continuous\ncontrol with deep reinforcement learning. arXiv preprint\narXiv:1509.02971, 2015.\nLin, L.-J. Self-improving reactive agents based on reinforce-\nment learning, planning and teaching. Machine learning,\n8(3-4):293–321, 1992.\nMannor, S. and Tsitsiklis, J. N. Mean-variance optimization\nin markov decision processes. In International Confer-\nence on Machine Learning, pp. 177–184, 2011.\nMannor, S., Simester, D., Sun, P., and Tsitsiklis, J. N. Bias\nand variance approximation in value function estimates.\nManagement Science, 53(2):308–322, 2007.\nMnih, V ., Kavukcuoglu, K., Silver, D., Rusu, A. A., Veness,\nJ., Bellemare, M. G., Graves, A., Riedmiller, M., Fidje-\nland, A. K., Ostrovski, G., et al. Human-level control\nthrough deep reinforcement learning. Nature, 518(7540):\n529–533, 2015.\nMnih, V ., Badia, A. P., Mirza, M., Graves, A., Lillicrap,\nT., Harley, T., Silver, D., and Kavukcuoglu, K. Asyn-\nchronous methods for deep reinforcement learning. In\nInternational Conference on Machine Learning, pp. 1928–\n1937, 2016.\nMunos, R., Stepleton, T., Harutyunyan, A., and Bellemare,\nM. Safe and efﬁcient off-policy reinforcement learning.\nIn Advances in Neural Information Processing Systems,\npp. 1054–1062, 2016.\nNachum, O., Norouzi, M., Tucker, G., and Schuurmans, D.\nSmoothed action value functions for learning gaussian\npolicies. arXiv preprint arXiv:1803.02348, 2018.\nO’Donoghue, B., Osband, I., Munos, R., and Mnih, V . The\nuncertainty bellman equation and exploration. arXiv\npreprint arXiv:1709.05380, 2017.\nPendrith, M. D., Ryan, M. R., et al. Estimator variance in\nreinforcement learning: Theoretical problems and practi-\ncal solutions. University of New South Wales, School of\nComputer Science and Engineering, 1997.\nAddressing Function Approximation Error in Actor-Critic Methods\nPetrik, M. and Scherrer, B. Biasing approximate dynamic\nprogramming with a lower discount factor. InAdvances in\nNeural Information Processing Systems, pp. 1265–1272,\n2009.\nPopov, I., Heess, N., Lillicrap, T., Hafner, R., Barth-\nMaron, G., Vecerik, M., Lampe, T., Tassa, Y ., Erez,\nT., and Riedmiller, M. Data-efﬁcient deep reinforce-\nment learning for dexterous manipulation. arXiv preprint\narXiv:1704.03073, 2017.\nPrecup, D., Sutton, R. S., and Dasgupta, S. Off-policy\ntemporal-difference learning with function approxima-\ntion. In International Conference on Machine Learning,\npp. 417–424, 2001.\nSchaul, T., Quan, J., Antonoglou, I., and Silver, D. Priori-\ntized experience replay. In International Conference on\nLearning Representations, Puerto Rico, 2016.\nSchulman, J., Levine, S., Abbeel, P., Jordan, M., and Moritz,\nP. Trust region policy optimization. In International\nConference on Machine Learning, pp. 1889–1897, 2015.\nSchulman, J., Wolski, F., Dhariwal, P., Radford, A., and\nKlimov, O. Proximal policy optimization algorithms.\narXiv preprint arXiv:1707.06347, 2017.\nSilver, D., Lever, G., Heess, N., Degris, T., Wierstra, D., and\nRiedmiller, M. Deterministic policy gradient algorithms.\nIn International Conference on Machine Learning , pp.\n387–395, 2014.\nSutton, R. S. Learning to predict by the methods of temporal\ndifferences. Machine learning, 3(1):9–44, 1988.\nSutton, R. S. and Barto, A. G. Reinforcement learning: An\nintroduction, volume 1. MIT press Cambridge, 1998.\nThrun, S. and Schwartz, A. Issues in using function approx-\nimation for reinforcement learning. In Proceedings of the\n1993 Connectionist Models Summer School Hillsdale, NJ.\nLawrence Erlbaum, 1993.\nTodorov, E., Erez, T., and Tassa, Y . Mujoco: A physics\nengine for model-based control. In Intelligent Robots\nand Systems (IROS), 2012 IEEE/RSJ International Con-\nference on, pp. 5026–5033. IEEE, 2012.\nUhlenbeck, G. E. and Ornstein, L. S. On the theory of the\nbrownian motion. Physical review, 36(5):823, 1930.\nVan Hasselt, H. Double q-learning. In Advances in Neural\nInformation Processing Systems, pp. 2613–2621, 2010.\nVan Hasselt, H., Guez, A., and Silver, D. Deep reinforce-\nment learning with double q-learning. In AAAI, pp. 2094–\n2100, 2016.\nVan Seijen, H., Van Hasselt, H., Whiteson, S., and Wiering,\nM. A theoretical and empirical analysis of expected sarsa.\nIn Adaptive Dynamic Programming and Reinforcement\nLearning, 2009. ADPRL’09. IEEE Symposium on , pp.\n177–184. IEEE, 2009.\nWatkins, C. J. C. H. Learning from delayed rewards. PhD\nthesis, King’s College, Cambridge, 1989.\nWu, Y ., Mansimov, E., Grosse, R. B., Liao, S., and Ba,\nJ. Scalable trust-region method for deep reinforcement\nlearning using kronecker-factored approximation. In Ad-\nvances in Neural Information Processing Systems , pp.\n5285–5294, 2017.",
  "values": {
    "Critiqability": "No",
    "Interpretable (to users)": "No",
    "Explicability": "No",
    "Transparent (to users)": "No",
    "Autonomy (power to decide)": "No",
    "Not socially biased": "No",
    "User influence": "No",
    "Respect for Persons": "No",
    "Fairness": "No",
    "Privacy": "No",
    "Non-maleficence": "No",
    "Respect for Law and public interest": "No",
    "Justice": "No",
    "Collective influence": "No",
    "Beneficence": "No",
    "Deferral to humans": "No"
  }
}