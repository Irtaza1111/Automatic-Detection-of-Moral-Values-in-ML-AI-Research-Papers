{
  "pdf": "gilmer19a",
  "title": "Adversarial Examples Are a Natural Consequence of Test Error in Noise",
  "author": "Nicolas Ford, Justin Gilmer, Nicholas Carlini, Ekin D. Cubuk",
  "paper_id": "gilmer19a",
  "text": "Adversarial Examples Are a Natural Consequence of Test Error in Noise\nNicolas Ford * 1 2 Justin Gilmer * 1 Nicholas Carlini 1 Ekin D. Cubuk 1\nAbstract\nOver the last few years, the phenomenon of ad-\nversarial examples — maliciously constructed in-\nputs that fool trained machine learning models —\nhas captured the attention of the research commu-\nnity, especially when the adversary is restricted\nto small modiﬁcations of a correctly handled in-\nput. Less surprisingly, image classiﬁers also lack\nhuman-level performance on randomly corrupted\nimages, such as images with additive Gaussian\nnoise. In this paper we provide both empirical and\ntheoretical evidence that these are two manifes-\ntations of the same underlying phenomenon. We\nestablish close connections between the adversar-\nial robustness and corruption robustness research\nprograms, with the strongest connection in the\ncase of additive Gaussian noise. This suggests\nthat improving adversarial robustness should go\nhand in hand with improving performance in the\npresence of more general and realistic image cor-\nruptions. Based on our results we recommend that\nfuture adversarial defenses consider evaluating\nthe robustness of their methods to distributional\nshift with benchmarks such as ImageNet-C.\n1. Introduction\nState-of-the-art computer vision models can achieve impres-\nsive performance on many image classiﬁcation tasks. De-\nspite this, these same models still lack the robustness of the\nhuman visual system to various forms of image corruptions.\nFor example, they are distinctly subhuman when classifying\nimages distorted with additive Gaussian noise (Dodge &\nKaram, 2017), they lack robustness to different types of\nblur, pixelation, and changes in brightness (Hendrycks &\nDietterich, 2019), lack robustness to random translations\nof the input (Azulay & Weiss, 2018), and even make er-\nrors when foreign objects are inserted into the ﬁeld of view\n*Equal contribution 1Google Brain 2This work was completed\nas part of the Google AI Residency. Correspondence to: Nicolas\nFord <nicf@google.com>, Justin Gilmer <gilmer@google.com>.\nProceedings of the 36th International Conference on Machine\nLearning, Long Beach, California, PMLR 97, 2019. Copyright\n2019 by the author(s).\n(Rosenfeld et al., 2018). At the same time, they are also\nsensitive to small, worst-case perturbations of the input, so-\ncalled “adversarial examples” (Szegedy et al., 2014). This\nlatter phenomenon has struck many in the machine learning\ncommunity as surprising and has attracted a great deal of re-\nsearch interest, while the former has received less attention.\nThe machine learning community has researchers working\non each of these two types of errors: adversarial exam-\nple researchers seek to measure and improve robustness to\nsmall-worst case perturbations of the input while corruption\nrobustness researchers seek to measure and improve model\nrobustness to distributional shift. In this work we analyze\nthe connection between these two research directions, and\nwe see that adversarial robustness is closely related to ro-\nbustness to certain kinds of distributional shift. In other\nwords, the existence of adversarial examples follows natu-\nrally from the fact that our models have nonzero test error\nin certain corrupted image distributions.\nWe make this connection in several ways. First, in Section 4,\nwe provide a novel analysis of the error set of an image\nclassiﬁer. We see that, given the error rates we observe in\nGaussian noise, the small adversarial perturbations we ob-\nserve in practice appear at roughly the distances we would\nexpect from a linear model, and that therefore there is no\nneed to invoke any strange properties of the decision bound-\nary to explain them. This relationship was also explored in\nFawzi et al. (2018b; 2016).\nIn Section 5, we show that improving an alternate notion of\nadversarial robustness requires that error rates under large\nadditive noise be reduced to essentially zero.\nFinally, this suggests that methods which increase the dis-\ntance to the decision boundary should also improve robust-\nness to Gaussian noise, and vice versa. In Section 6 we\nconﬁrm that this is true by examining both adversarially\ntrained models and models trained with additive Gaussian\nnoise. We also show that measuring corruption robustness\ncan effectively distinguish successful adversarial defense\nmethods from ones that merely cause vanishing gradients.\nWe hope that this work will encourage both the adversar-\nial and corruption robustness communities to work more\nclosely together, since their goals seem to be so closely re-\nlated. In particular, it is not common for adversarial defense\nAdversarial Examples Are a Natural Consequence of Test Error in Noise\nmethods to measure corruption robustness. Given that suc-\ncessful adversarial defense methods should also improve\nsome types of corruption robustness we recommend that\nfuture researchers consider evaluating corruption robustness\nin addition to adversarial robustness.\n2. Related Work\nAdversarial machine learning studies general ways in which\nan adversary may interact with an ML system, and dates\nback to 2004 (Dalvi et al., 2004; Biggio & Roli, 2018). Since\nSzegedy et al. (2014), a subﬁeld has focused speciﬁcally on\nsmall adversarial perturbations of the input, or “adversarial\nexamples.” Many algorithms have been developed to ﬁnd\nthe smallest perturbation in input space which fool a classi-\nﬁer (Carlini & Wagner, 2017; Madry et al., 2017). Defenses\nhave been proposed for increasing the robustness of classi-\nﬁers to small adversarial perturbations, however many have\nlater been shown ineffective (Carlini & Wagner, 2017). To\nour knowledge the only method which has been conﬁrmed\nby a third party to increaselp-robustness (for certain values\nofϵ) is adversarial training (Madry et al., 2017). However,\nthis method remains sensitive to slightly larger perturbations\n(Sharma & Chen, 2017).\nSeveral recent papers (Gilmer et al., 2018b; Mahloujifar\net al., 2018; Dohmatob, 2018; Fawzi et al., 2018a) use con-\ncentation of measure to prove rigorous upper bounds on ad-\nversarial robustness for certain distributions in terms of test\nerror, suggesting non-zero test error may imply the existence\nof adversarial perturbations. This may seem in contradiction\nwith empirical observations that increasing small perturba-\ntion robustness tends to reduce model accuracy (Tsipras\net al., 2019). We note that these two conclusions do not\nnecessarily contradict each other. It could be the case that\nhard bounds on adversarial robustness in terms of test error\nexist, but current classiﬁers have yet to approach these hard\nbounds.\nBecause we establish a connection between adversarial ro-\nbustness and model accuracy in corrupted image distribu-\ntions, our results do not contradict reports that adversarial\ntraining reduces accuracy in the clean distribution (Tsipras\net al., 2019). In fact, we ﬁnd that improving adversarial\nrobustness also improves corruption robustness.\n3. Adversarial and Corruption Robustness\nBoth adversarial robustness and corruption robustness can\nbe thought of as functions of the error set of a statistical\nclassiﬁer. This set, which we will denote E, is the set of\npoints in the input space on which the classiﬁer makes an\nincorrect prediction. In this paper we will only consider\nperturbed versions of training or test points, and we will\nalways assume the input is corrupted such that the “correct”\nlabel for the corrupted point is the same as for the clean\npoint. This assumption is commonly made in works which\nstudy model robustness to random corruptions of the input\n(Hendrycks & Dietterich, 2019; Dodge & Karam, 2017).\nBecause we are interested in how our models perform on\nboth clean images and corrupted ones, we introduce some\nnotation for both distributions. We will write p for the\nnatural image distribution, that is, the distribution from\nwhich the training data was sampled. We will useq to denote\nwhichever corrupted image distribution we are working\nwith. A sample fromq will always look like a sample from\np with a random corruption applied to it, like some amount\nof Gaussian noise. Some examples of noisy images can be\nfound in Figure 10 in the appendix.\nWe will be interested in two quantities. The ﬁrst, corrup-\ntion robustness under a given corrupted image distribution\nq, is Px∼q[x /∈ E], the probability that a random sample\nfrom theq is not an error. The second is called adversarial\nrobustness. For a clean input x and a metric on the input\nspaced, letd(x,E ) denote the distance fromx to the nearest\npoint inE. The adversarial robustness of the model is then\nPx∼p[d(x,E ) > ϵ], the probability that a random sample\nfromp is not within distanceϵ of some point in the error set.\nWhen we refer to “adversarial examples” in this paper, we\nwill always mean these nearby errors.\nIn this work we will investigate several different mod-\nels trained on the CIFAR-10 and ImageNet datasets. For\nCIFAR-10 we look at the naturally trained and adversarially\ntrained models which have been open-sourced by Madry\net al. (2017). We also trained the same model on CIFAR-10\nwith Gaussian data augmentation. For ImageNet, we inves-\ntigate an Inception v3 (Szegedy et al., 2016) trained with\nGaussian data augmentation. In all cases, Gaussian data aug-\nmentation was performed by ﬁrst sampling aσ uniformly\nbetween 0 and some speciﬁed upper bound and then adding\nrandom Gaussian noise at that scale. Additional training de-\ntails can be found in Appendix A. We were unable to study\nthe effects of adversarial training on ImageNet because no\nrobust open sourced model exists. (The models released in\nTramèr et al. (2017) only minimally improve robustness to\nthe white box PGD adversaries we consider here.)\n4. Errors in Gaussian Noise Suggest\nAdversarial Examples\nWe will start by examining the relationship between adver-\nsarial and corruption robustness in the case whereq consists\nof images with additive Gaussian noise.\nThe Linear Case. For linear models, the error rate in Gaus-\nsian noise exactly determines the distance to the decision\nboundary. This observation was also made in Fawzi et al.\n(2016; 2018b).\nAdversarial Examples Are a Natural Consequence of Test Error in Noise\nx0\nE\nA\nσ√n\nB\nd(x0,E )\nFigure 1. When the input dimension, n, is large and the model\nis linear, even a small error rate in additive noise implies the\nexistence of small adversarial perturbations. For a point x0 in\nimage space, most samples from N (x0;σ2I) (pointB) lie close\nto a sphere of radiusσ√n aroundx0, drawn here as a circle. For\na linear model the error set E is a half-space, and the error rate\nµ is approximately equal to the fraction of the sphere lying in\nthis half-space. The distanced(x0,E ) fromx0 to its nearest error\n(pointA) is also drawn. Note the relationship betweenσ,µ, and\nd(x0,E ) does not depend on the dimension. However, because\nthe typical distance to a sample from the Gaussian is σ√n the\nratio between the distance fromx0 toA and the distance fromx0\ntoB shrinks as the dimension increases.\nIt will be useful to keep the following intuitive picture in\nmind. In high dimensions, most samples from the Gaussian\ndistributionN (x0;σ2I) lie close to the surface of a sphere\nof radius σ centered at x0. The decision boundary of a\nlinear model is a plane, and since we are assuming that the\n“correct” label for each noisy point is the same as the label\nforx0, our error set is simply the half-space on the far side\nof this plane.\nThe relationship between adversarial and corruption robust-\nness corresponds to a simple geometric picture. If we slice a\nsphere with a plane, as in Figure 1, the distance to the near-\nest error is equal to the distance from the plane to the center\nof the sphere, and the corruption robustness is the fraction\nof the surface area cut off by the plane. This relationship\nchanges drastically as the dimension increases: most of the\nsurface area of a high-dimensional sphere lies very close\nto the equator, which means that cutting off even, say, 1%\nof the surface area requires a plane which is very close to\nthe center. Thus, for a linear model, even a relatively small\nerror rate on Gaussian noise implies the existence of errors\nvery close to the clean image (i.e., an adversarial example).\nTo formalize this relationship, pick some clean image x0\nand consider the Gaussian distributionN (x0;σ2I). For a\nﬁxedµ, letσ(x0,µ ) be theσ for which the error rate isµ,\nthat is, for which\nPx∼N (x0;σ2I)[x∈E] =µ.\nThen, lettingd denotel2 distance, we have\nd(x0,E ) =−σ(x0,µ )Φ−1(µ), (1)\nwhere\nΦ(t) = 1√\n2π\n∫ t\n−∞\nexp(−x2/2)dx\nis the cdf of the univariate standard normal distribution.\n(Note that Φ−1(µ) is negative whenµ< 1\n2.)\nThis expression depends only on the error rate µ and the\nstandard deviationσ of a single component, and not directly\non the dimension, but the dimension appears if we consider\nthe distance fromx0 to a typical sample fromN (x0;σ2I),\nwhich isσ√n. When the dimension is large the distance to\nthe decision boundary will be signiﬁcantly smaller than the\ndistance to a noisy image.\nFor example, this formula says that a linear model with\nan error rate of 0.01 in noise with σ = 0.1 will have an\nerror at distance about 0.23. In three dimensions, a typical\nsample from this noise distribution will be at a distance of\naround 0.1\n√\n3≈ 0.17. However when n = 150528 , the\ndimension of the ImageNet image space, these samples lie\nat a distance of about 38.8. So, in the latter case, a 1% error\nrate on random perturbations of size 38.8 implies an error at\ndistance 0.23, more than 150 times closer. Detailed curves\nshowing this relationship can be found in Appendix F.\nComparing Neural Networks to the Linear Case. The\ndecision boundary of a neural network is, of course, not\nlinear. However, by comparing the ratio betweend(x0,E )\nandσ(x0,µ ) for neural networks to what it would be for a\nlinear model, we can investigate the relationship between ad-\nversarial and corruption robustness. We ran experiments on\nseveral neural network image classiﬁers and found results\nthat closely resemble Equation 1. Adversarial examples\ntherefore are not “surprisingly” close to x0 given the perfor-\nmance of each model in Gaussian noise.\nConcretely, we examine this relationship when µ = 0.01.\nFor each test point, we compareσ(x0, 0.01) to an estimate\nofd(x0,E ). Because it is not feasible to computed(x0,E )\nexactly, we instead search for an error using PGD (Madry\net al., 2017) and report the nearest error we can ﬁnd.\nFigure 2 shows the results for several CIFAR-10 and Ima-\ngeNet models, including ordinarily trained models, models\ntrained with Gaussian data augmentation withσ = 0.4, and\nan adversarially trained CIFAR-10 model. We also included\na line representing how these quantities would be related for\na linear model, as in Equation 1. Because most test points\nlie close to the predicted relationship for a linear model, we\nsee that the half-space model shown in Figure 1 accurately\npredicts the existence of small perturbation adversarial ex-\namples.\nAdversarial Examples Are a Natural Consequence of Test Error in Noise\nFigure 2. (Top) Comparing thel2 distance to the decision boundary with theσ for which the error rate in Gaussian noise is 1%. Each\npoint represents 50 images from the test set, and the median values for each coordinate are shown. The error bars cover the 25th to\n75th percentiles. The PGD attack was run withϵ = 1, so the distances to the decision boundary reported here are cut off at 1. (Bottom)\nHistograms of thex coordinates from the above plots. A misclassiﬁed point is assignedσ = 0.\nIt is interesting to observe how each training procedure\naffected the two quantities we measured. First, adversarial\ntraining and Gaussian data augmentation increased both\nσ(x0, 0.01) and d(x0,E ) on average. The adversarially\ntrained model deviates from the linear case the most, but it\ndoes so in the direction of greater distances to the decision\nboundary. While both augmentation methods do improve\nboth quantities, Gaussian data augmentation had a greater\neffect on σ (as seen in the histograms) while adversarial\ntraining had a greater effect ond. We explore this further in\nSection 6.\nVisual Conﬁrmation of the Half-space ModelIn Figure 3\nwe draw two-dimensional slices in image space through\nthree points. (Similar visualizations have appeared in Fawzi\net al. (2018b), and are called “church window plots.”)\nThis visualized decision boundary closely matches the half-\nspace model in Figure 1. We see that an error found in\nGaussian noise lies in the same connected component of the\nerror set as an error found using PGD, and that at this scale\nthat component visually resembles a half-space. This ﬁgure\nalso illustrates the connection between adversarial example\nresearch and corruption robustness research. To measure\nadversarial robustness is to ask whether or not there are any\nerrors in the l∞ ball — the small diamond-shaped region\nin the center of the image — and to measure corruption\nrobustness is to measure the volume of the error set in the\ndeﬁned noise distribution. At least in this slice, nothing\ndistinguishes the PGD error from any other point in the\nerror set apart from its proximity to the clean image.\nWe give many more church window plots in Appendix G.\n5. Concentration of Measure for Noisy Images\nThere is an existing research program (Gilmer et al., 2018b;\nMahloujifar et al., 2018; Dohmatob, 2018) which proves\nhard upper bounds on adversarial robustness in terms of\nthe error rate of a model. This phenomenon is sometimes\ncalled concentration of measure. Because proving a the-\norem like this requires understanding the distribution in\nquestion precisely, these results typically deal with simple\n“toy” distributions rather than those corresponding to real\ndata. In this section we take a ﬁrst step toward bridging this\ngap. By comparing our models to a classical concentration\nof measure bound for the Gaussian distribution, we gain\nanother perspective on our motivating question.\nThe Gaussian Isoperimetric Inequality. As in Section 4,\nletx0 be a correctly classiﬁed image and consider the dis-\ntribution q =N (x0;σ2I). Note q is the distribution of\nrandom Gaussian perturbations ofx0. The previous section\ndiscussed the distance fromx0 to its nearest error. In this\nAdversarial Examples Are a Natural Consequence of Test Error in Noise\nFigure 3. Two-dimensional slices of image space together with the classes assigned by trained models. Each slice goes through three\npoints, a clean image from the test set (black), an error found by randomly perturbing the center image with Gaussian noise (blue), and an\nerror found using a targeted PGD attack (red). The black circles have radiusσ√n, indicating the typical size of the Gaussian perturbation\nused. The diamond-shaped region in the center of the right image shows thel∞ ball of radius 8/255. In both slices, the decision boundary\nresembles a half-space as predicted in Figure 1, demonstrating how non-zero error rate in noise predicts the existence of small adversarial\nperturbations. The CIFAR-10 model on the left was evaluated withσ = 0.04 (black circle has radius 2.22), where 0.21% of Gaussian\nperturbations are classiﬁed as “frog” (cyan region). The adversarial error was found at distance 0.159 while the half-space model predicts\nerrors at distance 0.081. The ImageNet model on the right was evaluated at σ = 0.08 (black circle has radius 31.4) where 0.1% of\nGaussian perturbations were misclassiﬁed as “miniture poodle” (cyan). The adversarial error has distance 0.189 while the half-space\nmodel predicts errors at distance 0.246. For the panda picture on the right we also found closer errors than what is shown by using an\nuntargeted attack (an image was assigned class “indri” at distance 0.024). Slices showing more complicated behavior can be found in\nAppendix G.\nsection we will instead discuss the distance from a typical\nsample fromq (e.g. pointB in Figure 1) to its nearest error.\nFor random samples fromq, there is a precise sense in which\nsmall adversarial perturbations exist only because test error\nis nonzero. That is, given the error rates we actually observe\non noisy images, most noisy images must be close to the\nerror set. This result holds completely independently of any\nassumptions about the model and follows from a fundamen-\ntal geometric property of the Gaussian distribution, which\nwe will now make precise.\nLetϵ∗\nq(E) be the median distance from one of these noisy\nimages to the nearest error. (In other words, it is the ϵ for\nwhich Px∼q[d(x,E )≤ ϵ] = 1\n2.) As before, let Px∼q[x∈\nE] be the probability that a random Gaussian perturbation of\nx0 lies inE. It is possible to deduce a bound relating these\ntwo quantities from the Gaussian isoperimetric inequality\n(Borell, 1975). The form we will use is:\nTheorem (Gaussian Isoperimetric Inequality) . Let q =\nN (0;σ2I) be the Gaussian distribution on Rn with vari-\nanceσ2I, and, for some setE⊆ Rn, letµ = Px∼q[x∈E].\nAs before, write Φ for the cdf of the univariate standard\nnormal distribution. Ifµ≥ 1\n2, thenϵ∗\nq(E) = 0. Otherwise,\nϵ∗\nq(E)≤−σΦ−1(µ)\n, with equality whenE is a half space.\nIn particular, for any machine learning model for which\nthe error rate in the distributionq is at leastµ, the median\ndistance to the nearest error is at most−σΦ−1(µ). Because\neach coordinate of a multivariate normal is a univariate\nnormal,−σΦ−1(µ) is the distance to a half space for which\nthe error rate isµ. In other words, the right hand side of the\ninequality is the same expression that appears in Equation 1.\nSo, among models with some ﬁxed error ratePx∼q[x∈E],\nthe most robust are the ones whose error set is a half space\n(as shown in Figure 1). In Appendix E we will give a more\ncommon statement of the Gaussian isoperimetric inequality\nalong with a proof of the version presented here.\nComparing Neural Networks to the Isoperimetric\nBound. We evaluated these quantities for several models\non the CIFAR-10 and ImageNet test sets.\nAs in Section 4, we report an estimate of ϵ∗\nq. For each\ntest image, we took 1,000 samples from the corresponding\nGaussian and estimated ϵ∗\nq\nusing PGD with 200 steps on\neach sample and reported the median.\nWe ﬁnd that for the ﬁve models we considered, the rela-\ntionship between our estimate of ϵ∗\nq(E)\nand Px∼q[x∈E]\nis already close to optimal. This is visualized in Figure 4.\nFor CIFAR-10, adversarial training improves robustness to\nsmall perturbations, but the gains are primarily because er-\nror rates in Gaussian noise were improved. In particular, it is\nclear from the graph on the bottom left that adversarial train-\ning increases theσ at which the error rate is 1% on average.\nThis shows that improved adversarial robustness results in\nAdversarial Examples Are a Natural Consequence of Test Error in Noise\nFigure 4. These plots give two ways to visualize the relationship between the error rate in noise and the distance from noisy points to the\ndecision boundary (found using PGD). Each point on each plot represents one image from the test set. On the top row, we compare the\nerror rate of the model with Gaussian perturbations atσ = 0.1 to the distance from the median noisy point to its nearest error. On the\nbottom row, we compare theσ at which the error rate is 0.01 to this same median distance. (These are therefore similar to the plots in\nFigure 2.) The thick black line at the top of each plot is the upper bound provided by the Gaussian isoperimetric inequality. We include\ndata from a model trained on clean images, an adversarially trained model, and a model trained on Gaussian noise (σ = 0.4.)\nimproved robustness to large random perturbations, as the\nisoperimetric inequality says it must.\n6. Evaluating Corruption Robustness\nThe previous two sections show a relationship between ad-\nversarial robustness and one type of corruption robustness.\nThis suggests that methods designed to improve adversarial\nrobustness ought to also improve corruption robustness, and\nvice versa. In this section we investigate this relationship.\nWe analyzed the performance of our models on the cor-\nruption robustness benchmark described in Hendrycks &\nDietterich (2019). There are 15 different corruptions in this\nbenchmark, each of which is tested at ﬁve different levels\nof severity. The results are summarized in Figure 5, where\nwe have aggregated the corruption types based on whether\nthe ordinarily trained model did better or worse than the\naugmented models. We found a signiﬁcant difference in per-\nformance on this benchmark when the model is evaluated\non the compressed images provided with the benchmark\nrather than applying the corruptions in memory. (In this\nsection we report performance on corruptions applied in-\nmemory.) Figure 8 in the appendix shows an example for\nthe Gaussian-5 corruption, where performance degraded\nfrom 57% accuracy (in memory) to 10% accuracy (com-\npressed images). Detailed results on both versions of this\nbenchmark are presented in Appendix B.\nGaussian data augmentation and adversarial training both\nimprove the overall benchmark1, which requires averaging\nthe performance across all corruptions, and the results were\nquite close. Adversarial training helped more with blur-\nring corruptions and Gaussian data augmentation helped\nmore with noise corruptions. Interestingly, both methods\nperformed much worse than the clean model on the fog and\ncontrast corruptions. For example, the adversarially trained\nmodel was 55% accurate on the most severe contrast cor-\nruption compared to 85% for the clean model. Note that\nHendrycks & Dietterich (2019) also observed that adversar-\nial training improves robustness on this benchmark on Tiny\nImageNet.\nThe fact that adversarial training is so successful against the\nnoise corruptions further supports the connection we have\nbeen describing. For other corruptions, the relationship is\nmore complicated, and it would be interesting to explore\nthis in future work.\n1In reporting overall performance on this benchmark, we omit\nthe Gaussian noise corruption.\nAdversarial Examples Are a Natural Consequence of Test Error in Noise\nFigure 5. The performance of the models we considered on the corruption robustness benchmark, together with our measurements of\nthose models’ robustness to smalllp perturbations. For all the robustness tests we used PGD with 100 steps and a step size ofϵ/25. The\nadversarially trained CIFAR-10 model is the open sourced model from Madry et al. (2017).\nWe also evaluated these two augmentation methods on stan-\ndard measures of lp robustness. We see a similar story\nthere: while adversarial training performs better, Gaussian\ndata augmentation improves adversarial robustness as well.\nGaussian data augmenation has been proposed as an adver-\nsarial defense in prior work (Zantedeschi et al., 2017). Here\nwe evaluate this method not to propose it as a novel defense\nbut to provide further evidence of the connection between\nadversarial and corruption robustness.\nWe also considered the MNIST adversarially trained model\nfrom Madry et al. (2017), and found it to be a special case\nwhere robustness to small perturbations was increased while\ngeneralization in noise was not improved (see Appendix D).\nThis is because this model violates the linearity assumption\ndiscussed in Section 4.\nCorruption Robustness as a Sanity Check for Defenses.\nWe also analyzed the performance several previously pub-\nlished adversarial defense strategies in Gaussian noise.\nThese methods have already been shown to result in van-\nishing gradients, which causes standard optimization proce-\ndures to fail to ﬁnd errors, rather than actually improving\nadversarial robustness (Athalye et al., 2018). We ﬁnd that\nthese methods also show no improvement in Gaussian noise.\nThe results are shown in Figure 6. Had these prior defenses\nperformed an analysis like this, they would have been able\nto determine that their methods relied on vanishing gradients\nand fail to improve robustness.\nObtaining Zero Test Error in Noise is Nontrivial. It is\nimportant to note that applying Gaussian data augmenta-\ntion does not reduce error rates in Gaussian noise to zero.\nFor example, we performed Gaussian data augmentation on\nCIFAR-10 atσ =.15 and obtained 99.9% training accuracy\nbut 77.5% test accuracy in the same noise distribution. (For\ncomparison, the naturally trained obtains 95% clean test\naccuracy.) Previous work (Dodge & Karam, 2017) has also\nobserved that obtaining perfect generalization in large Gaus-\nsian noise is nontrivial. This mirrors Schmidt et al. (2018),\nwhich found that adversarial robustness did not generalize\nto the test set, providing yet another similarity between ad-\nversarial and corruption robustness. This is perhaps not\nsurprising given that error rates on the clean test set are also\nnon-zero. Although the model is in some sense “superhu-\nman” with respect to clean test accuracy, it still makes many\nmistakes on the clean test set that a human would never\nAdversarial Examples Are a Natural Consequence of Test Error in Noise\nFigure 6. (Left) The performance in Gaussian noise of the CIFAR models described in this paper. (Right) The performance in Gaussian\nnoise of several previously published defenses for ImageNet, along with an ImageNet model trained on Gaussian noise atσ = 0.4 for\ncomparison. For each point we ran ten trials; the error bars show one standard deviation. All of these defenses are now known not\nto improve adversarial robustness (Athalye et al., 2018). The defense strategies include bitdepth reduction (Guo et al., 2017), JPEG\ncompression (Guo et al., 2017; Dziugaite et al., 2016; Liu et al., 2018; Aydemir et al., 2018; Das et al., 2018; 2017), Pixel Deﬂection\n(Prakash et al., 2018), total variance minimization (Guo et al., 2017), respresentation-guided denoising (Liao et al., 2018), and random\nresizing and random padding of the input image (Xie et al., 2017).\nmake. We collected some examples in Appendix I. More\ndetailed results on training and testing in noise can be found\nin Appendices C and H.\n7. Conclusion\nThis paper investigates whether we should be surprised to\nﬁnd adversarial examples as close as we do, given the error\nrates we observe in corrupted image distributions. After\nrunning several experiments, we argue that the answer to\nthis question is no. Speciﬁcally:\n1. The nearby errors we can ﬁnd show up at the same\ndistance scales we would expect from a linear model\nwith the same corruption robustness.\n2. Concentration of measure shows that a non-zero error\nrate in Gaussian noise logically implies the existence\nof small adversarial perturbations of noisy images.\n3. Finally, training procedures designed to improve adver-\nsarial robustness also improve many types of corrup-\ntion robustness, and training on Gaussian noise moder-\nately improves adversarial robustness.\nIn light of this, we believe it would be beneﬁcial for the\nadversarial defense literature to start reporting generaliza-\ntion to distributional shift, such as the common corruption\nbenchmark introduced in Hendrycks & Dietterich (2019),\nin addition to empirical estimates of adversarial robustness.\nThere are several reasons for this recommendation.\nFirst, a varied suite of corruptions can expose failure modes\nof a model that we might otherwise miss. For example, we\nfound that adversarial training signiﬁcantly degraded perfor-\nmance on the fog and contrast corruptions despite improving\nsmall perturbation robustness. In particular, performance\non constrast-5 dropped to 55.3% accuracy vs 85.7% for the\nvanilla model (see Appendix B for more details).\nSecond, measuring corruption robustness is signiﬁcantly\neasier than measuring adversarial robustness — computing\nadversarial robustness perfectly requires solving an NP-\nhard problem for every point in the test set (Katz et al.,\n2017). Since Szegedy et al. (2014), hundreds of adversarial\ndefense papers have been published. To our knowledge, only\none (Madry et al., 2017) has reported robustness numbers\nwhich were conﬁrmed by a third party. We believe the\ndifﬁculty of measuring robustness under the usual deﬁnition\nhas contributed to this unproductive situation.\nThird, all of the failed defense strategies we examined also\nfailed to improve performance in Gaussian noise. For this\nreason, we should be highly skeptical of defense strategies\nthat only claim improved lp robustness but are unable to\ndemonstrate robustness to distributional shift.\nFinally, if the goal is improving the security of our models\nin adversarial settings, errors on corrupted images already\nimply that our models are not secure. Until our models are\nperfectly robust in the presence of average-case corruptions,\nthey will not be robust in worst-case settings.\nThe communities of researchers studying adversarial and\ncorruption robustness seem to be attacking essentially the\nsame problem in two different ways. We believe that the\ncorruption robustness problem is also interesting indepen-\ndently of its connection to adversarial examples, and we\nhope that the results presented here will encourage more\ncollaboration between these two communities.\nAdversarial Examples Are a Natural Consequence of Test Error in Noise\nReferences\nAnish Athalye, Nicholas Carlini, and David Wagner. Ob-\nfuscated gradients give a false sense of security: Circum-\nventing defenses to adversarial examples. arXiv preprint\narXiv:1802.00420, 2018.\nAyse Elvan Aydemir, Alptekin Temizel, and Tugba Taskaya\nTemizel. The effects of jpeg and jpeg2000 compression\non attacks using adversarial examples. arXiv preprint\narXiv:1803.10418, 2018.\nAharon Azulay and Yair Weiss. Why do deep convolutional\nnetworks generalize so poorly to small image transforma-\ntions? arXiv preprint arXiv:1805.12177, 2018.\nBattista Biggio and Fabio Roli. Wild patterns: Ten years\nafter the rise of adversarial machine learning. Pattern\nRecognition, 84:317–331, 2018.\nChrister Borell. The Brunn-Minkowski inequality in Gauss\nspace. Inventiones mathematicae, 30(2):207–216, 1975.\nNicholas Carlini and David Wagner. Adversarial examples\nare not easily detected: Bypassing ten detection methods.\narXiv preprint arXiv:1705.07263, 2017.\nEkin D Cubuk, Barret Zoph, Dandelion Mane, Vijay Vasude-\nvan, and Quoc V Le. Autoaugment: Learning augmenta-\ntion policies from data. arXiv preprint arXiv:1805.09501,\n2018.\nNilesh Dalvi, Pedro Domingos, Sumit Sanghai, Deepak\nVerma, et al. Adversarial classiﬁcation. InProceedings\nof the tenth ACM SIGKDD international conference on\nKnowledge discovery and data mining, pp. 99–108. ACM,\n2004.\nNilaksh Das, Madhuri Shanbhogue, Shang-Tse Chen, Fred\nHohman, Li Chen, Michael E Kounavis, and Duen Horng\nChau. Keeping the bad guys out: Protecting and vaccinat-\ning deep learning with jpeg compression. arXiv preprint\narXiv:1705.02900, 2017.\nNilaksh Das, Madhuri Shanbhogue, Shang-Tse Chen, Fred\nHohman, Siwei Li, Li Chen, Michael E Kounavis, and\nDuen Horng Chau. Shield: Fast, practical defense and\nvaccination for deep learning using jpeg compression.\narXiv preprint arXiv:1802.06816, 2018.\nSamuel Dodge and Lina Karam. A study and comparison\nof human and deep learning recognition performance un-\nder visual distortions. In Computer Communication and\nNetworks (ICCCN), 2017 26th International Conference\non, pp. 1–7. IEEE, 2017.\nElvis Dohmatob. Limitations of adversarial robust-\nness: strong no free lunch theorem. arXiv preprint\narXiv:1810.04065, 2018.\nGintare Karolina Dziugaite, Zoubin Ghahramani, and\nDaniel M Roy. A study of the effect of jpg compression\non adversarial images. arXiv preprint arXiv:1608.00853,\n2016.\nAlhussein Fawzi, Seyed-Mohsen Moosavi-Dezfooli, and\nPascal Frossard. Robustness of classiﬁers: from adversar-\nial to random noise. In Advances in Neural Information\nProcessing Systems, pp. 1632–1640, 2016.\nAlhussein Fawzi, Hamza Fawzi, and Omar Fawzi. Ad-\nversarial vulnerability for any classiﬁer. arXiv preprint\narXiv:1802.08686, 2018a.\nAlhussein Fawzi, Seyed-Mohsen Moosavi-Dezfooli, Pascal\nFrossard, and Stefano Soatto. Empirical study of the\ntopology and geometry of deep networks. In IEEE CVPR,\nnumber CONF, 2018b.\nRobert Geirhos, Patricia Rubisch, Claudio Michaelis,\nMatthias Bethge, Felix A Wichmann, and Wieland Bren-\ndel. Imagenet-trained cnns are biased towards texture;\nincreasing shape bias improves accuracy and robustness.\narXiv preprint arXiv:1811.12231, 2018.\nJustin Gilmer, Ryan P Adams, Ian Goodfellow, David An-\ndersen, and George E Dahl. Motivating the rules of the\ngame for adversarial example research. arXiv preprint\narXiv:1807.06732, 2018a.\nJustin Gilmer, Luke Metz, Fartash Faghri, Samuel S\nSchoenholz, Maithra Raghu, Martin Wattenberg, and\nIan Goodfellow. Adversarial spheres. arXiv preprint\narXiv:1801.02774, 2018b.\nChuan Guo, Mayank Rana, Moustapha Cisse, and Laurens\nvan der Maaten. Countering adversarial images using\ninput transformations. arXiv preprint arXiv:1711.00117,\n2017.\nDan Hendrycks and Thomas G Dietterich. Benchmarking\nneural network robustness to common corruptions and\nsurface variations. International Conference on Learning\nRepresentations, 2019.\nGuy Katz, Clark Barrett, David L Dill, Kyle Julian, and\nMykel J Kochenderfer. Reluplex: An efﬁcient smt solver\nfor verifying deep neural networks. In International\nConference on Computer Aided Veriﬁcation, pp. 97–117.\nSpringer, 2017.\nFangzhou Liao, Ming Liang, Yinpeng Dong, Tianyu Pang,\nJun Zhu, and Xiaolin Hu. Defense against adversarial\nattacks using high-level representation guided denoiser.\nIn Proceedings of the IEEE Conference on Computer\nVision and Pattern Recognition, pp. 1778–1787, 2018.\nAdversarial Examples Are a Natural Consequence of Test Error in Noise\nZihao Liu, Qi Liu, Tao Liu, Yanzhi Wang, and Wujie\nWen. Feature distillation: Dnn-oriented jpeg com-\npression against adversarial examples. arXiv preprint\narXiv:1803.05787, 2018.\nAleksander Madry, Aleksander Makelov, Ludwig Schmidt,\nDimitris Tsipras, and Adrian Vladu. Towards deep learn-\ning models resistant to adversarial examples. arXiv\npreprint arXiv:1706.06083, 2017.\nSaeed Mahloujifar, Dimitrios I Diochnos, and Mohammad\nMahmoody. The curse of concentration in robust learn-\ning: Evasion and poisoning attacks from concentration of\nmeasure. arXiv preprint arXiv:1809.03063, 2018.\nAaditya Prakash, Nick Moran, Solomon Garber, Antonella\nDiLillo, and James Storer. Deﬂecting adversarial attacks\nwith pixel deﬂection. In Proceedings of the IEEE Con-\nference on Computer Vision and Pattern Recognition, pp.\n8571–8580, 2018.\nAmir Rosenfeld, Richard Zemel, and John K Tsotsos. The\nelephant in the room. arXiv preprint arXiv:1808.03305,\n2018.\nLudwig Schmidt, Shibani Santurkar, Dimitris Tsipras, Ku-\nnal Talwar, and Aleksander M ˛ adry. Adversarially ro-\nbust generalization requires more data. arXiv preprint\narXiv:1804.11285, 2018.\nYash Sharma and Pin-Yu Chen. Breaking the madry defense\nmodel with l1-based adversarial examples. arXiv preprint\narXiv:1710.10733, 2017.\nChristian Szegedy, Wojciech Zaremba, Ilya Sutskever, Joan\nBruna, Dumitru Erhan, Ian Goodfellow, and Rob Fergus.\nIntriguing properties of neural networks. In International\nConference on Learning Representations , 2014. URL\nhttp://arxiv.org/abs/1312.6199.\nChristian Szegedy, Vincent Vanhoucke, Sergey Ioffe, Jon\nShlens, and Zbigniew Wojna. Rethinking the inception\narchitecture for computer vision. In Proceedings of the\nIEEE conference on computer vision and pattern recogni-\ntion, pp. 2818–2826, 2016.\nFlorian Tramèr, Alexey Kurakin, Nicolas Papernot, Dan\nBoneh, and Patrick McDaniel. Ensemble adversar-\nial training: Attacks and defenses. arXiv preprint\narXiv:1705.07204, 2017.\nDimitris Tsipras, Shibani Santurkar, Logan Engstrom,\nAlexander Turner, and Aleksander Madry. Robustness\nmay be at odds with accuracy. International Confer-\nence on Learning Representations, 2019. URL https:\n//openreview.net/forum?id=SyxAb30cY7.\nZ. Wang and A. C. Bovik. Mean squared error: Love it or\nleave it? a new look at signal ﬁdelity measures. IEEE\nSignal Processing Magazine, 26(1):98–117, 2009.\nChaowei Xiao, Jun-Yan Zhu, Bo Li, Warren He, Mingyan\nLiu, and Dawn Song. Spatially transformed adversarial\nexamples. arXiv preprint arXiv:1801.02612, 2018.\nCihang Xie, Jianyu Wang, Zhishuai Zhang, Zhou Ren, and\nAlan Yuille. Mitigating adversarial effects through ran-\ndomization. arXiv preprint arXiv:1711.01991, 2017.\nSergey Zagoruyko and Nikos Komodakis. Wide residual\nnetworks. arXiv preprint arXiv:1605.07146, 2016.\nValentina Zantedeschi, Maria-Irina Nicolae, and Ambrish\nRawat. Efﬁcient defenses against adversarial attacks.\nIn Proceedings of the 10th ACM Workshop on Artiﬁcial\nIntelligence and Security, pp. 39–49. ACM, 2017.",
  "values": {
    "Transparent (to users)": "Yes",
    "Interpretable (to users)": "Yes",
    "Respect for Law and public interest": "Yes",
    "Autonomy (power to decide)": "Yes",
    "Respect for Persons": "Yes",
    "Justice": "Yes",
    "Critiqability": "Yes",
    "Privacy": "Yes",
    "Not socially biased": "Yes",
    "User influence": "Yes",
    "Explicability": "Yes",
    "Fairness": "Yes",
    "Beneficence": "Yes",
    "Deferral to humans": "Yes",
    "Non-maleficence": "Yes",
    "Collective influence": "Yes"
  }
}