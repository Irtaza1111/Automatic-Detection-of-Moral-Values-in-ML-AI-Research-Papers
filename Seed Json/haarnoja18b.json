{
  "pdf": "haarnoja18b",
  "title": "haarnoja18b",
  "author": "Unknown",
  "paper_id": "haarnoja18b",
  "text": "Soft Actor-Critic:\nOff-Policy Maximum Entropy Deep Reinforcement\nLearning with a Stochastic Actor\nTuomas Haarnoja 1 Aurick Zhou 1 Pieter Abbeel 1 Sergey Levine 1\nAbstract\nModel-free deep reinforcement learning (RL) al-\ngorithms have been demonstrated on a range of\nchallenging decision making and control tasks.\nHowever, these methods typically suffer from two\nmajor challenges: very high sample complexity\nand brittle convergence properties, which necessi-\ntate meticulous hyperparameter tuning. Both of\nthese challenges severely limit the applicability\nof such methods to complex, real-world domains.\nIn this paper, we propose soft actor-critic, an off-\npolicy actor-critic deep RL algorithm based on the\nmaximum entropy reinforcement learning frame-\nwork. In this framework, the actor aims to maxi-\nmize expected reward while also maximizing en-\ntropy. That is, to succeed at the task while acting\nas randomly as possible. Prior deep RL methods\nbased on this framework have been formulated\nas Q-learning methods. By combining off-policy\nupdates with a stable stochastic actor-critic formu-\nlation, our method achieves state-of-the-art per-\nformance on a range of continuous control bench-\nmark tasks, outperforming prior on-policy and\noff-policy methods. Furthermore, we demonstrate\nthat, in contrast to other off-policy algorithms, our\napproach is very stable, achieving very similar\nperformance across different random seeds.\n1. Introduction\nModel-free deep reinforcement learning (RL) algorithms\nhave been applied in a range of challenging domains, from\ngames (Mnih et al., 2013; Silver et al., 2016) to robotic\ncontrol (Schulman et al., 2015). The combination of RL\nand high-capacity function approximators such as neural\n1Berkeley Artiﬁcial Intelligence Research, University of Cal-\nifornia, Berkeley, USA. Correspondence to: Tuomas Haarnoja\n<haarnoja@berkeley.edu>.\nProceedings of the 35 th International Conference on Machine\nLearning, Stockholm, Sweden, PMLR 80, 2018. Copyright 2018\nby the author(s).\nnetworks holds the promise of automating a wide range of\ndecision making and control tasks, but widespread adoption\nof these methods in real-world domains has been hampered\nby two major challenges. First, model-free deep RL meth-\nods are notoriously expensive in terms of their sample com-\nplexity. Even relatively simple tasks can require millions of\nsteps of data collection, and complex behaviors with high-\ndimensional observations might need substantially more.\nSecond, these methods are often brittle with respect to their\nhyperparameters: learning rates, exploration constants, and\nother settings must be set carefully for different problem\nsettings to achieve good results. Both of these challenges\nseverely limit the applicability of model-free deep RL to\nreal-world tasks.\nOne cause for the poor sample efﬁciency of deep RL meth-\nods is on-policy learning: some of the most commonly used\ndeep RL algorithms, such as TRPO (Schulman et al., 2015),\nPPO (Schulman et al., 2017b) or A3C (Mnih et al., 2016),\nrequire new samples to be collected for each gradient step.\nThis quickly becomes extravagantly expensive, as the num-\nber of gradient steps and samples per step needed to learn\nan effective policy increases with task complexity. Off-\npolicy algorithms aim to reuse past experience. This is not\ndirectly feasible with conventional policy gradient formula-\ntions, but is relatively straightforward for Q-learning based\nmethods (Mnih et al., 2015). Unfortunately, the combina-\ntion of off-policy learning and high-dimensional, nonlinear\nfunction approximation with neural networks presents a ma-\njor challenge for stability and convergence (Bhatnagar et al.,\n2009). This challenge is further exacerbated in continuous\nstate and action spaces, where a separate actor network is\noften used to perform the maximization in Q-learning. A\ncommonly used algorithm in such settings, deep determinis-\ntic policy gradient (DDPG) (Lillicrap et al., 2015), provides\nfor sample-efﬁcient learning but is notoriously challenging\nto use due to its extreme brittleness and hyperparameter\nsensitivity (Duan et al., 2016; Henderson et al., 2017).\nWe explore how to design an efﬁcient and stable model-\nfree deep RL algorithm for continuous state and action\nspaces. To that end, we draw on the maximum entropy\nframework, which augments the standard maximum reward\nSoft Actor-Critic\nreinforcement learning objective with an entropy maximiza-\ntion term (Ziebart et al., 2008; Toussaint, 2009; Rawlik et al.,\n2012; Fox et al., 2016; Haarnoja et al., 2017). Maximum en-\ntropy reinforcement learning alters the RL objective, though\nthe original objective can be recovered using a tempera-\nture parameter (Haarnoja et al., 2017). More importantly,\nthe maximum entropy formulation provides a substantial\nimprovement in exploration and robustness: as discussed\nby Ziebart (2010), maximum entropy policies are robust\nin the face of model and estimation errors, and as demon-\nstrated by (Haarnoja et al., 2017), they improve exploration\nby acquiring diverse behaviors. Prior work has proposed\nmodel-free deep RL algorithms that perform on-policy learn-\ning with entropy maximization (O’Donoghue et al., 2016),\nas well as off-policy methods based on soft Q-learning and\nits variants (Schulman et al., 2017a; Nachum et al., 2017a;\nHaarnoja et al., 2017). However, the on-policy variants suf-\nfer from poor sample complexity for the reasons discussed\nabove, while the off-policy variants require complex approx-\nimate inference procedures in continuous action spaces.\nIn this paper, we demonstrate that we can devise an off-\npolicy maximum entropy actor-critic algorithm, which we\ncall soft actor-critic (SAC), which provides for both sample-\nefﬁcient learning and stability. This algorithm extends read-\nily to very complex, high-dimensional tasks, such as the\nHumanoid benchmark (Duan et al., 2016) with 21 action\ndimensions, where off-policy methods such as DDPG typi-\ncally struggle to obtain good results (Gu et al., 2016). SAC\nalso avoids the complexity and potential instability associ-\nated with approximate inference in prior off-policy maxi-\nmum entropy algorithms based on soft Q-learning (Haarnoja\net al., 2017). We present a convergence proof for policy\niteration in the maximum entropy framework, and then in-\ntroduce a new algorithm based on an approximation to this\nprocedure that can be practically implemented with deep\nneural networks, which we call soft actor-critic. We present\nempirical results that show that soft actor-critic attains a\nsubstantial improvement in both performance and sample\nefﬁciency over both off-policy and on-policy prior methods.\nWe also compare to twin delayed deep deterministic (TD3)\npolicy gradient algorithm (Fujimoto et al., 2018), which is\na concurrent work that proposes a deterministic algorithm\nthat substantially improves on DDPG.\n2. Related Work\nOur soft actor-critic algorithm incorporates three key in-\ngredients: an actor-critic architecture with separate policy\nand value function networks, an off-policy formulation that\nenables reuse of previously collected data for efﬁciency, and\nentropy maximization to enable stability and exploration.\nWe review prior works that draw on some of these ideas in\nthis section. Actor-critic algorithms are typically derived\nstarting from policy iteration, which alternates between pol-\nicy evaluation—computing the value function for a policy—\nand policy improvement—using the value function to obtain\na better policy (Barto et al., 1983; Sutton & Barto, 1998). In\nlarge-scale reinforcement learning problems, it is typically\nimpractical to run either of these steps to convergence, and\ninstead the value function and policy are optimized jointly.\nIn this case, the policy is referred to as the actor, and the\nvalue function as the critic. Many actor-critic algorithms\nbuild on the standard, on-policy policy gradient formulation\nto update the actor (Peters & Schaal, 2008), and many of\nthem also consider the entropy of the policy, but instead of\nmaximizing the entropy, they use it as an regularizer (Schul-\nman et al., 2017b; 2015; Mnih et al., 2016; Gruslys et al.,\n2017). On-policy training tends to improve stability but\nresults in poor sample complexity.\nThere have been efforts to increase the sample efﬁciency\nwhile retaining robustness by incorporating off-policy sam-\nples and by using higher order variance reduction tech-\nniques (O’Donoghue et al., 2016; Gu et al., 2016). How-\never, fully off-policy algorithms still attain better efﬁ-\nciency. A particularly popular off-policy actor-critic method,\nDDPG (Lillicrap et al., 2015), which is a deep variant of the\ndeterministic policy gradient (Silver et al., 2014) algorithm,\nuses a Q-function estimator to enable off-policy learning,\nand a deterministic actor that maximizes this Q-function.\nAs such, this method can be viewed both as a determinis-\ntic actor-critic algorithm and an approximate Q-learning\nalgorithm. Unfortunately, the interplay between the deter-\nministic actor network and the Q-function typically makes\nDDPG extremely difﬁcult to stabilize and brittle to hyperpa-\nrameter settings (Duan et al., 2016; Henderson et al., 2017).\nAs a consequence, it is difﬁcult to extend DDPG to complex,\nhigh-dimensional tasks, and on-policy policy gradient meth-\nods still tend to produce the best results in such settings (Gu\net al., 2016). Our method instead combines off-policy actor-\ncritic training with a stochastic actor, and further aims to\nmaximize the entropy of this actor with an entropy maxi-\nmization objective. We ﬁnd that this actually results in a\nconsiderably more stable and scalable algorithm that, in\npractice, exceeds both the efﬁciency and ﬁnal performance\nof DDPG. A similar method can be derived as a zero-step\nspecial case of stochastic value gradients (SVG(0)) (Heess\net al., 2015). However, SVG(0) differs from our method in\nthat it optimizes the standard maximum expected return ob-\njective, and it does not make use of a separate value network,\nwhich we found to make training more stable.\nMaximum entropy reinforcement learning optimizes poli-\ncies to maximize both the expected return and the ex-\npected entropy of the policy. This framework has been\nused in many contexts, from inverse reinforcement learn-\ning (Ziebart et al., 2008) to optimal control (Todorov, 2008;\nToussaint, 2009; Rawlik et al., 2012). In guided policy\nSoft Actor-Critic\nsearch (Levine & Koltun, 2013; Levine et al., 2016), the\nmaximum entropy distribution is used to guide policy learn-\ning towards high-reward regions. More recently, several\npapers have noted the connection between Q-learning and\npolicy gradient methods in the framework of maximum en-\ntropy learning (O’Donoghue et al., 2016; Haarnoja et al.,\n2017; Nachum et al., 2017a; Schulman et al., 2017a). While\nmost of the prior model-free works assume a discrete action\nspace, Nachum et al. (2017b) approximate the maximum en-\ntropy distribution with a Gaussian and Haarnoja et al. (2017)\nwith a sampling network trained to draw samples from the\noptimal policy. Although the soft Q-learning algorithm pro-\nposed by Haarnoja et al. (2017) has a value function and\nactor network, it is not a true actor-critic algorithm: the\nQ-function is estimating the optimal Q-function, and the\nactor does not directly affect the Q-function except through\nthe data distribution. Hence, Haarnoja et al. (2017) moti-\nvates the actor network as an approximate sampler, rather\nthan the actor in an actor-critic algorithm. Crucially, the\nconvergence of this method hinges on how well this sampler\napproximates the true posterior. In contrast, we prove that\nour method converges to the optimal policy from a given\npolicy class, regardless of the policy parameterization. Fur-\nthermore, these prior maximum entropy methods generally\ndo not exceed the performance of state-of-the-art off-policy\nalgorithms, such as DDPG, when learning from scratch,\nthough they may have other beneﬁts, such as improved ex-\nploration and ease of ﬁne-tuning. In our experiments, we\ndemonstrate that our soft actor-critic algorithm does in fact\nexceed the performance of prior state-of-the-art off-policy\ndeep RL methods by a wide margin.\n3. Preliminaries\nWe ﬁrst introduce notation and summarize the standard and\nmaximum entropy reinforcement learning frameworks.\n3.1. Notation\nWe address policy learning in continuous action spaces.\nWe consider an inﬁnite-horizon Markov decision process\n(MDP), deﬁned by the tuple (S, A, p, r), where the state\nspace S and the action space A are continuous, and the\nunknown state transition probability p : S × S × A →\n[0, ∞) represents the probability density of the next state\nst+1 ∈ S given the current state st ∈ S and action at ∈ A.\nThe environment emits a bounded reward r : S × A →\n[rmin, rmax] on each transition. We will use ρπ(st) and\nρπ(st, at) to denote the state and state-action marginals of\nthe trajectory distribution induced by a policy π(at|st).\n3.2. Maximum Entropy Reinforcement Learning\nStandard RL maximizes the expected sum of rewards∑\nt E(st,at)∼ρπ [r(st, at)]. We will consider a more gen-\neral maximum entropy objective (see e.g. Ziebart (2010)),\nwhich favors stochastic policies by augmenting the objective\nwith the expected entropy of the policy over ρπ(st):\nJ(π) =\nT∑\nt=0\nE(st,at)∼ρπ [r(st, at) + αH(π( · |st))] . (1)\nThe temperature parameter α determines the relative im-\nportance of the entropy term against the reward, and thus\ncontrols the stochasticity of the optimal policy. The maxi-\nmum entropy objective differs from the standard maximum\nexpected reward objective used in conventional reinforce-\nment learning, though the conventional objective can be\nrecovered in the limit as α → 0. For the rest of this paper,\nwe will omit writing the temperature explicitly, as it can\nalways be subsumed into the reward by scaling it by α−1.\nThis objective has a number of conceptual and practical\nadvantages. First, the policy is incentivized to explore more\nwidely, while giving up on clearly unpromising avenues.\nSecond, the policy can capture multiple modes of near-\noptimal behavior. In problem settings where multiple ac-\ntions seem equally attractive, the policy will commit equal\nprobability mass to those actions. Lastly, prior work has ob-\nserved improved exploration with this objective (Haarnoja\net al., 2017; Schulman et al., 2017a), and in our experi-\nments, we observe that it considerably improves learning\nspeed over state-of-art methods that optimize the conven-\ntional RL objective function. We can extend the objective to\ninﬁnite horizon problems by introducing a discount factorγ\nto ensure that the sum of expected rewards and entropies is\nﬁnite. Writing down the maximum entropy objective for the\ninﬁnite horizon discounted case is more involved (Thomas,\n2014) and is deferred to Appendix A.\nPrior methods have proposed directly solving for the op-\ntimal Q-function, from which the optimal policy can be\nrecovered (Ziebart et al., 2008; Fox et al., 2016; Haarnoja\net al., 2017). We will discuss how we can devise a soft\nactor-critic algorithm through a policy iteration formulation,\nwhere we instead evaluate the Q-function of the current\npolicy and update the policy through an off-policy gradient\nupdate. Though such algorithms have previously been pro-\nposed for conventional reinforcement learning, our method\nis, to our knowledge, the ﬁrst off-policy actor-critic method\nin the maximum entropy reinforcement learning framework.\n4. From Soft Policy Iteration to Soft\nActor-Critic\nOur off-policy soft actor-critic algorithm can be derived\nstarting from a maximum entropy variant of the policy it-\neration method. We will ﬁrst present this derivation, verify\nthat the corresponding algorithm converges to the optimal\npolicy from its density class, and then present a practical\nSoft Actor-Critic\ndeep reinforcement learning algorithm based on this theory.\n4.1. Derivation of Soft Policy Iteration\nWe will begin by deriving soft policy iteration, a general al-\ngorithm for learning optimal maximum entropy policies that\nalternates between policy evaluation and policy improve-\nment in the maximum entropy framework. Our derivation\nis based on a tabular setting, to enable theoretical analysis\nand convergence guarantees, and we extend this method\ninto the general continuous setting in the next section. We\nwill show that soft policy iteration converges to the optimal\npolicy within a set of policies which might correspond, for\ninstance, to a set of parameterized densities.\nIn the policy evaluation step of soft policy iteration, we\nwish to compute the value of a policy π according to the\nmaximum entropy objective in Equation 1. For a ﬁxed\npolicy, the soft Q-value can be computed iteratively, starting\nfrom any function Q : S × A → R and repeatedly applying\na modiﬁed Bellman backup operatorT π given by\nT πQ(st, at) ≜ r(st, at) + γ Est+1∼p [V (st+1)] , (2)\nwhere\nV (st) = Eat∼π [Q(st, at) − log π(at|st)] (3)\nis the soft state value function. We can obtain the soft value\nfunction for any policy π by repeatedly applying T π as\nformalized below.\nLemma 1 (Soft Policy Evaluation). Consider the soft Bell-\nman backup operator T π in Equation 2 and a mapping\nQ0 : S ×A → R with |A| < ∞, and deﬁne Qk+1 = T πQk.\nThen the sequence Qk will converge to the soft Q-value of\nπ as k → ∞.\nProof. See Appendix B.1.\nIn the policy improvement step, we update the policy to-\nwards the exponential of the new Q-function. This particular\nchoice of update can be guaranteed to result in an improved\npolicy in terms of its soft value. Since in practice we prefer\npolicies that are tractable, we will additionally restrict the\npolicy to some set of policies Π, which can correspond, for\nexample, to a parameterized family of distributions such as\nGaussians. To account for the constraint that π ∈ Π, we\nproject the improved policy into the desired set of policies.\nWhile in principle we could choose any projection, it will\nturn out to be convenient to use the information projection\ndeﬁned in terms of the Kullback-Leibler divergence. In the\nother words, in the policy improvement step, for each state,\nwe update the policy according to\nπnew = arg min\nπ′∈Π\nDKL\n(\nπ′( · |st)\n\nexp (Qπold(st, · ))\nZ πold(st)\n)\n.\n(4)\nThe partition function Z πold(st) normalizes the distribution,\nand while it is intractable in general, it does not contribute to\nthe gradient with respect to the new policy and can thus be\nignored, as noted in the next section. For this projection, we\ncan show that the new, projected policy has a higher value\nthan the old policy with respect to the objective in Equa-\ntion 1. We formalize this result in Lemma 2.\nLemma 2 (Soft Policy Improvement). Let πold ∈ Π and let\nπnew be the optimizer of the minimization problem deﬁned\nin Equation 4. Then Qπnew(st, at) ≥ Qπold(st, at) for all\n(st, at) ∈ S × A with |A| < ∞.\nProof. See Appendix B.2.\nThe full soft policy iteration algorithm alternates between\nthe soft policy evaluation and the soft policy improvement\nsteps, and it will provably converge to the optimal maxi-\nmum entropy policy among the policies in Π (Theorem 1).\nAlthough this algorithm will provably ﬁnd the optimal solu-\ntion, we can perform it in its exact form only in the tabular\ncase. Therefore, we will next approximate the algorithm for\ncontinuous domains, where we need to rely on a function\napproximator to represent the Q-values, and running the\ntwo steps until convergence would be computationally too\nexpensive. The approximation gives rise to a new practical\nalgorithm, called soft actor-critic.\nTheorem 1 (Soft Policy Iteration). Repeated application of\nsoft policy evaluation and soft policy improvement from any\nπ ∈ Π converges to a policy π∗ such that Qπ∗\n(st, at) ≥\nQπ(st, at) for all π ∈ Π and (st, at) ∈ S × A , assuming\n|A| < ∞.\nProof. See Appendix B.3.\n4.2. Soft Actor-Critic\nAs discussed above, large continuous domains require us to\nderive a practical approximation to soft policy iteration. To\nthat end, we will use function approximators for both the\nQ-function and the policy, and instead of running evaluation\nand improvement to convergence, alternate between opti-\nmizing both networks with stochastic gradient descent. We\nwill consider a parameterized state value function Vψ(st),\nsoft Q-function Qθ(st, at), and a tractable policy πφ(at|st).\nThe parameters of these networks are ψ, θ , and φ. For\nexample, the value functions can be modeled as expressive\nneural networks, and the policy as a Gaussian with mean\nand covariance given by neural networks. We will next\nderive update rules for these parameter vectors.\nThe state value function approximates the soft value. There\nis no need in principle to include a separate function approx-\nimator for the state value, since it is related to the Q-function\nand policy according to Equation 3. This quantity can be\nSoft Actor-Critic\nestimated from a single action sample from the current pol-\nicy without introducing a bias, but in practice, including a\nseparate function approximator for the soft value can stabi-\nlize training and is convenient to train simultaneously with\nthe other networks. The soft value function is trained to\nminimize the squared residual error\nJV(ψ) =Est∼D\n[\n1\n2\n(Vψ(st) −Eat∼πφ [Qθ(st,at) −logπφ(at|st)])2]\n(5)\nwhere D is the distribution of previously sampled states and\nactions, or a replay buffer. The gradient of Equation 5 can\nbe estimated with an unbiased estimator\nˆ∇ψJV (ψ) = ∇ψVψ(st) (Vψ(st) − Qθ(st, at) + logπφ(at|st)),\n(6)\nwhere the actions are sampled according to the current pol-\nicy, instead of the replay buffer. The soft Q-function param-\neters can be trained to minimize the soft Bellman residual\nJQ(θ) = E(st,at)∼D\n[ 1\n2\n(\nQθ(st, at) − ˆQ(st, at)\n)2]\n,\n(7)\nwith\nˆQ(st, at) = r(st, at) + γ Est+1∼p\n[\nV ¯ψ(st+1)\n]\n, (8)\nwhich again can be optimized with stochastic gradients\nˆ∇θJQ(θ) = ∇θQθ(at, st)(Qθ(st, at) − r(st, at) − γV¯ψ(st+1)).\n(9)\nThe update makes use of a target value network V ¯ψ, where\n¯ψ can be an exponentially moving average of the value\nnetwork weights, which has been shown to stabilize train-\ning (Mnih et al., 2015). Alternatively, we can update the\ntarget weights to match the current value function weights\nperiodically (see Appendix E). Finally, the policy param-\neters can be learned by directly minimizing the expected\nKL-divergence in Equation 4:\nJπ(φ) = Est∼D\n[\nDKL\n(\nπφ( · |st)\n\nexp (Qθ(st, · ))\nZθ(st)\n)]\n.\n(10)\nThere are several options for minimizing Jπ. A typical\nsolution for policy gradient methods is to use the likelihood\nratio gradient estimator (Williams, 1992), which does not\nrequire backpropagating the gradient through the policy and\nthe target density networks. However, in our case, the target\ndensity is the Q-function, which is represented by a neural\nnetwork an can be differentiated, and it is thus convenient\nto apply the reparameterization trick instead, resulting in a\nlower variance estimator. To that end, we reparameterize\nthe policy using a neural network transformation\nat = fφ(ϵt; st), (11)\nAlgorithm 1 Soft Actor-Critic\nInitialize parameter vectors ψ, ¯ψ, θ, φ.\nfor each iteration do\nfor each environment step do\nat ∼ πφ(at|st)\nst+1 ∼ p(st+1|st, at)\nD ← D ∪ { (st, at, r(st, at), st+1)}\nend for\nfor each gradient step do\nψ ← ψ − λV ˆ∇ψJV (ψ)\nθi ← θi − λQ ˆ∇θi JQ(θi) for i ∈ {1, 2}\nφ ← φ − λπ ˆ∇φJπ(φ)\n¯ψ ← τ ψ + (1 − τ) ¯ψ\nend for\nend for\nwhere ϵt is an input noise vector, sampled from some ﬁxed\ndistribution, such as a spherical Gaussian. We can now\nrewrite the objective in Equation 10 as\nJπ(φ) =Est∼D,ϵt∼N [logπφ(fφ(ϵt; st)|st) − Qθ(st, fφ(ϵt; st))],\n(12)\nwhere πφ is deﬁned implicitly in terms offφ, and we have\nnoted that the partition function is independent of φ and can\nthus be omitted. We can approximate the gradient of Equa-\ntion 12 with\nˆ∇φJπ(φ) = ∇φ log πφ(at|st)\n+ (∇at log πφ(at|st) − ∇at Q(st, at))∇φfφ(ϵt; st),\n(13)\nwhere at is evaluated at fφ(ϵt; st). This unbiased gradient\nestimator extends the DDPG style policy gradients (Lillicrap\net al., 2015) to any tractable stochastic policy.\nOur algorithm also makes use of two Q-functions to mitigate\npositive bias in the policy improvement step that is known to\ndegrade performance of value based methods (Hasselt, 2010;\nFujimoto et al., 2018). In particular, we parameterize two Q-\nfunctions, with parameters θi, and train them independently\nto optimize JQ(θi). We then use the minimum of the the\nQ-functions for the value gradient in Equation 6 and pol-\nicy gradient in Equation 13, as proposed by Fujimoto et al.\n(2018). Although our algorithm can learn challenging tasks,\nincluding a 21-dimensional Humanoid, using just a single\nQ-function, we found two Q-functions signiﬁcantly speed\nup training, especially on harder tasks. The complete algo-\nrithm is described in Algorithm 1. The method alternates\nbetween collecting experience from the environment with\nthe current policy and updating the function approximators\nusing the stochastic gradients from batches sampled from a\nreplay buffer. In practice, we take a single environment step\nfollowed by one or several gradient steps (see Appendix D\nSoft Actor-Critic\n0.0 0.2 0.4 0.6 0.8 1.0\nmillion steps\n0\n1000\n2000\n3000\n4000average return\nHopper-v1\n(a) Hopper-v1\n0.0 0.2 0.4 0.6 0.8 1.0\nmillion steps\n0\n1000\n2000\n3000\n4000\n5000\n6000average return\nWalker2d-v1 (b) Walker2d-v1\n0.0 0.5 1.0 1.5 2.0 2.5 3.0\nmillion steps\n0\n5000\n10000\n15000average return\nHalfCheetah-v1 (c) HalfCheetah-v1\n0.0 0.5 1.0 1.5 2.0 2.5 3.0\nmillion steps\n0\n2000\n4000\n6000average return\nAnt-v1\n(d) Ant-v1\n0 2 4 6 8 10\nmillion steps\n0\n2000\n4000\n6000\n8000average return\nHumanoid-v1 (e) Humanoid-v1\n0 2 4 6 8 10\nmillion steps\n0\n2000\n4000\n6000average return\nHumanoid (rllab)\nSAC\nDDPG\nPPO\nSQL\nTD3 (concurrent) (f) Humanoid (rllab)\nFigure 1. Training curves on continuous control benchmarks. Soft actor-critic (yellow) performs consistently across all tasks and\noutperforming both on-policy and off-policy methods in the most challenging tasks.\nfor all hyperparameter). Using off-policy data from a replay\nbuffer is feasible because both value estimators and the pol-\nicy can be trained entirely on off-policy data. The algorithm\nis agnostic to the parameterization of the policy, as long as\nit can be evaluated for any arbitrary state-action tuple.\n5. Experiments\nThe goal of our experimental evaluation is to understand\nhow the sample complexity and stability of our method\ncompares with prior off-policy and on-policy deep rein-\nforcement learning algorithms. We compare our method\nto prior techniques on a range of challenging continuous\ncontrol tasks from the OpenAI gym benchmark suite (Brock-\nman et al., 2016) and also on the rllab implementation of\nthe Humanoid task (Duan et al., 2016). Although the easier\ntasks can be solved by a wide range of different algorithms,\nthe more complex benchmarks, such as the 21-dimensional\nHumanoid (rllab), are exceptionally difﬁcult to solve with\noff-policy algorithms (Duan et al., 2016). The stability of\nthe algorithm also plays a large role in performance: eas-\nier tasks make it more practical to tune hyperparameters\nto achieve good results, while the already narrow basins of\neffective hyperparameters become prohibitively small for\nthe more sensitive algorithms on the hardest benchmarks,\nleading to poor performance (Gu et al., 2016).\nWe compare our method to deep deterministic policy gra-\ndient (DDPG) (Lillicrap et al., 2015), an algorithm that\nis regarded as one of the more efﬁcient off-policy deep\nRL methods (Duan et al., 2016); proximal policy optimiza-\ntion (PPO) (Schulman et al., 2017b), a stable and effective\non-policy policy gradient algorithm; and soft Q-learning\n(SQL) (Haarnoja et al., 2017), a recent off-policy algorithm\nfor learning maximum entropy policies. Our SQL imple-\nmentation also includes two Q-functions, which we found\nto improve its performance in most environments. We addi-\ntionally compare to twin delayed deep deterministic policy\ngradient algorithm (TD3) (Fujimoto et al., 2018), using\nthe author-provided implementation. This is an extension\nto DDPG, proposed concurrently to our method, that ﬁrst\napplied the double Q-learning trick to continuous control\nalong with other improvements. We have included trust re-\ngion path consistency learning (Trust-PCL) (Nachum et al.,\n2017b) and two other variants of SAC in Appendix E. We\nturned off the exploration noise for evaluation for DDPG\nand PPO. For maximum entropy algorithms, which do not\nexplicitly inject exploration noise, we either evaluated with\nthe exploration noise (SQL) or use the mean action (SAC).\nThe source code of our SAC implementation1 and videos2\nare available online.\n1github.com/haarnoja/sac\n2sites.google.com/view/soft-actor-critic\nSoft Actor-Critic\n5.1. Comparative Evaluation\nFigure 1 shows the total average return of evaluation rollouts\nduring training for DDPG, PPO, and TD3. We train ﬁve\ndifferent instances of each algorithm with different random\nseeds, with each performing one evaluation rollout every\n1000 environment steps. The solid curves corresponds to the\nmean and the shaded region to the minimum and maximum\nreturns over the ﬁve trials.\nThe results show that, overall, SAC performs comparably\nto the baseline methods on the easier tasks and outperforms\nthem on the harder tasks with a large margin, both in terms\nof learning speed and the ﬁnal performance. For example,\nDDPG fails to make any progress on Ant-v1, Humanoid-\nv1, and Humanoid (rllab), a result that is corroborated by\nprior work (Gu et al., 2016; Duan et al., 2016). SAC also\nlearns considerably faster than PPO as a consequence of\nthe large batch sizes PPO needs to learn stably on more\nhigh-dimensional and complex tasks. Another maximum\nentropy RL algorithm, SQL, can also learn all tasks, but it\nis slower than SAC and has worse asymptotic performance.\nThe quantitative results attained by SAC in our experiments\nalso compare very favorably to results reported by other\nmethods in prior work (Duan et al., 2016; Gu et al., 2016;\nHenderson et al., 2017), indicating that both the sample\nefﬁciency and ﬁnal performance of SAC on these benchmark\ntasks exceeds the state of the art. All hyperparameters used\nin this experiment for SAC are listed in Appendix D.\n5.2. Ablation Study\nThe results in the previous section suggest that algorithms\nbased on the maximum entropy principle can outperform\nconventional RL methods on challenging tasks such as the\nhumanoid tasks. In this section, we further examine which\nparticular components of SAC are important for good perfor-\nmance. We also examine how sensitive SAC is to some of\nthe most important hyperparameters, namely reward scaling\nand target value update smoothing constant.\nStochastic vs. deterministic policy. Soft actor-critic\nlearns stochastic policies via a maximum entropy objec-\ntive. The entropy appears in both the policy and value\nfunction. In the policy, it prevents premature convergence of\nthe policy variance (Equation 10). In the value function, it\nencourages exploration by increasing the value of regions of\nstate space that lead to high-entropy behavior (Equation 5).\nTo compare how the stochasticity of the policy and entropy\nmaximization affects the performance, we compare to a\ndeterministic variant of SAC that does not maximize the en-\ntropy and that closely resembles DDPG, with the exception\nof having two Q-functions, using hard target updates, not\nhaving a separate target actor, and using ﬁxed rather than\nlearned exploration noise. Figure 2 compares ﬁve individual\nruns with both variants, initialized with different random\n0 2 4 6 8 10\nmillion steps\n0\n2000\n4000\n6000average return\nHumanoid (rllab)\nstochastic policy\ndeterministic policy\nFigure 2. Comparison of SAC (blue) and a deterministic variant of\nSAC (red) in terms of the stability of individual random seeds on\nthe Humanoid (rllab) benchmark. The comparison indicates that\nstochasticity can stabilize training as the variability between the\nseeds becomes much higher with a deterministic policy.\nseeds. Soft actor-critic performs much more consistently,\nwhile the deterministic variant exhibits very high variability\nacross seeds, indicating substantially worse stability. As\nevident from the ﬁgure, learning a stochastic policy with\nentropy maximization can drastically stabilize training. This\nbecomes especially important with harder tasks, where tun-\ning hyperparameters is challenging. In this comparison, we\nupdated the target value network weights with hard updates,\nby periodically overwriting the target network parameters\nto match the current value network (see Appendix E for\na comparison of average performance on all benchmark\ntasks).\nPolicy evaluation. Since SAC converges to stochastic\npolicies, it is often beneﬁcial to make the ﬁnal policy deter-\nministic at the end for best performance. For evaluation, we\napproximate the maximum a posteriori action by choosing\nthe mean of the policy distribution. Figure 3(a) compares\ntraining returns to evaluation returns obtained with this strat-\negy indicating that deterministic evaluation can yield better\nperformance. It should be noted that all of the training\ncurves depict the sum of rewards, which is different from\nthe objective optimized by SAC and other maximum en-\ntropy RL algorithms, including SQL and Trust-PCL, which\nmaximize also the entropy of the policy.\nReward scale. Soft actor-critic is particularly sensitive to\nthe scaling of the reward signal, because it serves the role\nof the temperature of the energy-based optimal policy and\nthus controls its stochasticity. Larger reward magnitudes\ncorrespond to lower entries. Figure 3(b) shows how learn-\ning performance changes when the reward scale is varied:\nFor small reward magnitudes, the policy becomes nearly\nuniform, and consequently fails to exploit the reward signal,\nresulting in substantial degradation of performance. For\nlarge reward magnitudes, the model learns quickly at ﬁrst,\nSoft Actor-Critic\n0.0 0.5 1.0 1.5 2.0 2.5 3.0\nmillion steps\n0\n2000\n4000\n6000average return\nAnt-v1\ndeterministic evaluation\nstochastic evaluation\n(a) Evaluation\n0.0 0.5 1.0 1.5 2.0 2.5 3.0\nmillion steps\n0\n2000\n4000\n6000average return\nAnt-v1\n1\n3\n10\n30\n100 (b) Reward Scale\n0.0 0.5 1.0 1.5 2.0 2.5 3.0\nmillion steps\n−2000\n0\n2000\n4000\n6000average return\nAnt-v1\n0.0001\n0.001\n0.01\n0.1 (c) Target Smoothing Coefﬁcient (τ)\nFigure 3. Sensitivity of soft actor-critic to selected hyperparameters on Ant-v1 task. (a) Evaluating the policy using the mean action\ngenerally results in a higher return. Note that the policy is trained to maximize also the entropy, and the mean action does not, in general,\ncorrespond the optimal action for the maximum return objective. (b) Soft actor-critic is sensitive to reward scaling since it is related to the\ntemperature of the optimal policy. The optimal reward scale varies between environments, and should be tuned for each task separately.\n(c) Target value smoothing coefﬁcientτ is used to stabilize training. Fast moving target (largeτ) can result in instabilities (red), whereas\nslow moving target (smallτ) makes training slower (blue).\nbut the policy then becomes nearly deterministic, leading\nto poor local minima due to lack of adequate exploration.\nWith the right reward scaling, the model balances explo-\nration and exploitation, leading to faster learning and better\nasymptotic performance. In practice, we found reward scale\nto be the only hyperparameter that requires tuning, and its\nnatural interpretation as the inverse of the temperature in\nthe maximum entropy framework provides good intuition\nfor how to adjust this parameter.\nTarget network update. It is common to use a separate\ntarget value network that slowly tracks the actual value func-\ntion to improve stability. We use an exponentially moving\naverage, with a smoothing constant τ, to update the target\nvalue network weights as common in the prior work (Lill-\nicrap et al., 2015; Mnih et al., 2015). A value of one cor-\nresponds to a hard update where the weights are copied\ndirectly at every iteration and zero to not updating the target\nat all. In Figure 3(c), we compare the performance of SAC\nwhen τ varies. Large τ can lead to instabilities while small\nτ can make training slower. However, we found the range\nof suitable values of τ to be relatively wide and we used\nthe same value (0.005) across all of the tasks. In Figure 4\n(Appendix E) we also compare to another variant of SAC,\nwhere instead of using exponentially moving average, we\ncopy over the current network weights directly into the tar-\nget network every 1000 gradient steps. We found this variant\nto beneﬁt from taking more than one gradient step between\nthe environment steps, which can improve performance but\nalso increases the computational cost.\n6. Conclusion\nWe present soft actor-critic (SAC), an off-policy maximum\nentropy deep reinforcement learning algorithm that provides\nsample-efﬁcient learning while retaining the beneﬁts of en-\ntropy maximization and stability. Our theoretical results\nderive soft policy iteration, which we show to converge to\nthe optimal policy. From this result, we can formulate a\nsoft actor-critic algorithm, and we empirically show that it\noutperforms state-of-the-art model-free deep RL methods,\nincluding the off-policy DDPG algorithm and the on-policy\nPPO algorithm. In fact, the sample efﬁciency of this ap-\nproach actually exceeds that of DDPG by a substantial mar-\ngin. Our results suggest that stochastic, entropy maximizing\nreinforcement learning algorithms can provide a promising\navenue for improved robustness and stability, and further\nexploration of maximum entropy methods, including meth-\nods that incorporate second order information (e.g., trust\nregions (Schulman et al., 2015)) or more expressive policy\nclasses is an exciting avenue for future work.\nAcknowledgments\nWe would like to thank Vitchyr Pong for insightful discus-\nsions and help in implementing our algorithm as well as\nproviding the DDPG baseline code; Oﬁr Nachum for offer-\ning support in running Trust-PCL experiments; and George\nTucker for his valuable feedback on an early version of this\npaper. This work was supported by Siemens and Berkeley\nDeepDrive.\nSoft Actor-Critic\nReferences\nBarto, A. G., Sutton, R. S., and Anderson, C. W. Neuronlike\nadaptive elements that can solve difﬁcult learning con-\ntrol problems. IEEE transactions on systems, man, and\ncybernetics, pp. 834–846, 1983.\nBhatnagar, S., Precup, D., Silver, D., Sutton, R. S., Maei,\nH. R., and Szepesv´ari, C. Convergent temporal-difference\nlearning with arbitrary smooth function approximation.\nIn Advances in Neural Information Processing Systems\n(NIPS), pp. 1204–1212, 2009.\nBrockman, G., Cheung, V ., Pettersson, L., Schneider, J.,\nSchulman, J., Tang, J., and Zaremba, W. OpenAI gym.\narXiv preprint arXiv:1606.01540, 2016.\nDuan, Y ., Chen, X. Houthooft, R., Schulman, J., and Abbeel,\nP. Benchmarking deep reinforcement learning for contin-\nuous control. In International Conference on Machine\nLearning (ICML), 2016.\nFox, R., Pakman, A., and Tishby, N. Taming the noise in\nreinforcement learning via soft updates. In Conference\non Uncertainty in Artiﬁcial Intelligence (UAI), 2016.\nFujimoto, S., van Hoof, H., and Meger, D. Addressing func-\ntion approximation error in actor-critic methods. arXiv\npreprint arXiv:1802.09477, 2018.\nGruslys, A., Azar, M. G., Bellemare, M. G., and Munos, R.\nThe reactor: A sample-efﬁcient actor-critic architecture.\narXiv preprint arXiv:1704.04651, 2017.\nGu, S., Lillicrap, T., Ghahramani, Z., Turner, R. E., and\nLevine, S. Q-prop: Sample-efﬁcient policy gradient with\nan off-policy critic. arXiv preprint arXiv:1611.02247,\n2016.\nHaarnoja, T., Tang, H., Abbeel, P., and Levine, S. Rein-\nforcement learning with deep energy-based policies. In\nInternational Conference on Machine Learning (ICML),\npp. 1352–1361, 2017.\nHasselt, H. V . Double Q-learning. In Advances in Neural\nInformation Processing Systems (NIPS), pp. 2613–2621,\n2010.\nHeess, N., Wayne, G., Silver, D., Lillicrap, T., Erez, T., and\nTassa, Y . Learning continuous control policies by stochas-\ntic value gradients. In Advances in Neural Information\nProcessing Systems (NIPS), pp. 2944–2952, 2015.\nHenderson, P., Islam, R., Bachman, P., Pineau, J., Precup,\nD., and Meger, D. Deep reinforcement learning that\nmatters. arXiv preprint arXiv:1709.06560, 2017.\nKingma, D. and Ba, J. Adam: A method for stochastic\noptimization. In International Conference for Learning\nPresentations (ICLR), 2015.\nLevine, S. and Koltun, V . Guided policy search. InInterna-\ntional Conference on Machine Learning (ICML), pp. 1–9,\n2013.\nLevine, S., Finn, C., Darrell, T., and Abbeel, P. End-to-end\ntraining of deep visuomotor policies. Journal of Machine\nLearning Research, 17(39):1–40, 2016.\nLillicrap, T. P., Hunt, J. J., Pritzel, A., Heess, N., Erez,\nT., Tassa, Y ., Silver, D., and Wierstra, D. Continuous\ncontrol with deep reinforcement learning. arXiv preprint\narXiv:1509.02971, 2015.\nMnih, V ., Kavukcuoglu, K., Silver, D., Graves, A.,\nAntonoglou, I., Wierstra, D., and Riedmiller, M. Playing\natari with deep reinforcement learning. arXiv preprint\narXiv:1312.5602, 2013.\nMnih, V ., Kavukcuoglu, K., Silver, D., Rusu, A. A., Veness,\nJ., Bellemare, M. G., Graves, A., Riedmiller, M., Fidje-\nland, A. K., Ostrovski, G., et al. Human-level control\nthrough deep reinforcement learning. Nature, 518(7540):\n529–533, 2015.\nMnih, V ., Badia, A. P., Mirza, M., Graves, A., Lillicrap,\nT. P., Harley, T., Silver, D., and Kavukcuoglu, K. Asyn-\nchronous methods for deep reinforcement learning. In\nInternational Conference on Machine Learning (ICML),\n2016.\nNachum, O., Norouzi, M., Xu, K., and Schuurmans, D.\nBridging the gap between value and policy based rein-\nforcement learning. In Advances in Neural Information\nProcessing Systems (NIPS), pp. 2772–2782, 2017a.\nNachum, O., Norouzi, M., Xu, K., and Schuurmans, D.\nTrust-PCL: An off-policy trust region method for contin-\nuous control. arXiv preprint arXiv:1707.01891, 2017b.\nO’Donoghue, B., Munos, R., Kavukcuoglu, K., and Mnih, V .\nPGQ: Combining policy gradient and Q-learning. arXiv\npreprint arXiv:1611.01626, 2016.\nPeters, J. and Schaal, S. Reinforcement learning of motor\nskills with policy gradients. Neural networks, 21(4):682–\n697, 2008.\nRawlik, K., Toussaint, M., and Vijayakumar, S. On stochas-\ntic optimal control and reinforcement learning by approx-\nimate inference. Robotics: Science and Systems (RSS) ,\n2012.\nSchulman, J., Levine, S., Abbeel, P., Jordan, M. I., and\nMoritz, P. Trust region policy optimization. In Inter-\nnational Conference on Machine Learning (ICML), pp.\n1889–1897, 2015.\nSoft Actor-Critic\nSchulman, J., Abbeel, P., and Chen, X. Equivalence be-\ntween policy gradients and soft Q-learning.arXiv preprint\narXiv:1704.06440, 2017a.\nSchulman, J., Wolski, F., Dhariwal, P., Radford, A., and\nKlimov, O. Proximal policy optimization algorithms.\narXiv preprint arXiv:1707.06347, 2017b.\nSilver, D., Lever, G., Heess, N., Degris, T., Wierstra, D.,\nand Riedmiller, M. Deterministic policy gradient algo-\nrithms. In International Conference on Machine Learning\n(ICML), 2014.\nSilver, D., Huang, A., Maddison, C. J., Guez, A., Sifre, L.,\nvan den Driessche, G., Schrittwieser, J., Antonoglou, I.,\nPanneershelvam, V ., Lanctot, M., Dieleman, S., Grewe,\nD., Nham, J., Kalchbrenner, N., Sutskever, I., Lillicrap, T.,\nLeach, M., Kavukcuoglu, K., Graepel, T., and Hassabis,\nD. Mastering the game of go with deep neural networks\nand tree search. Nature, 529(7587):484–489, Jan 2016.\nISSN 0028-0836. Article.\nSutton, R. S. and Barto, A. G. Reinforcement learning: An\nintroduction, volume 1. MIT press Cambridge, 1998.\nThomas, P. Bias in natural actor-critic algorithms. In Inter-\nnational Conference on Machine Learning (ICML), pp.\n441–448, 2014.\nTodorov, E. General duality between optimal control and\nestimation. In IEEE Conference on Decision and Control\n(CDC), pp. 4286–4292. IEEE, 2008.\nToussaint, M. Robot trajectory optimization using approxi-\nmate inference. In International Conference on Machine\nLearning (ICML), pp. 1049–1056. ACM, 2009.\nWilliams, R. J. Simple statistical gradient-following algo-\nrithms for connectionist reinforcement learning. Machine\nlearning, 8(3-4):229–256, 1992.\nZiebart, B. D. Modeling purposeful adaptive behavior with\nthe principle of maximum causal entropy. Carnegie Mel-\nlon University, 2010.\nZiebart, B. D., Maas, A. L., Bagnell, J. A., and Dey, A. K.\nMaximum entropy inverse reinforcement learning. In\nAAAI Conference on Artiﬁcial Intelligence (AAAI) , pp.\n1433–1438, 2008.",
  "values": {
    "User influence": "No",
    "Non-maleficence": "No",
    "Collective influence": "No",
    "Privacy": "No",
    "Respect for Persons": "No",
    "Interpretable (to users)": "No",
    "Transparent (to users)": "No",
    "Deferral to humans": "No",
    "Fairness": "No",
    "Beneficence": "No",
    "Autonomy (power to decide)": "No",
    "Justice": "No",
    "Not socially biased": "No",
    "Respect for Law and public interest": "No",
    "Explicability": "No",
    "Critiqability": "No"
  }
}