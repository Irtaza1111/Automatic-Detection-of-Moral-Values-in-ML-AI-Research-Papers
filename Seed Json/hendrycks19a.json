{
  "pdf": "hendrycks19a",
  "title": "Using Pre-Training Can Improve Model Robustness and Uncertainty",
  "author": "Dan Hendrycks and Kimin Lee and Mantas Mazeika",
  "paper_id": "hendrycks19a",
  "text": "Using Pre-Training Can Improve Model Robustness and Uncertainty\nDan Hendrycks 1 Kimin Lee 2 Mantas Mazeika 3\nAbstract\nHe et al. (2018) have called into question the\nutility of pre-training by showing that train-\ning from scratch can often yield similar per-\nformance to pre-training. We show that al-\nthough pre-training may not improve perfor-\nmance on traditional classiﬁcation metrics, it im-\nproves model robustness and uncertainty esti-\nmates. Through extensive experiments on la-\nbel corruption, class imbalance, adversarial ex-\namples, out-of-distribution detection, and con-\nﬁdence calibration, we demonstrate large gains\nfrom pre-training and complementary effects\nwith task-speciﬁc methods. We show approx-\nimately a 10% absolute improvement over the\nprevious state-of-the-art in adversarial robust-\nness. In some cases, using pre-training without\ntask-speciﬁc methods also surpasses the state-\nof-the-art, highlighting the need for pre-training\nwhen evaluating future methods on robustness\nand uncertainty tasks.\n1. Introduction\nPre-training is a central technique in the research and appli-\ncations of deep convolutional neural networks (Krizhevsky\net al., 2012). In research settings, pre-training is ubiqui-\ntously applied in state-of-the-art object detection and seg-\nmentation (He et al., 2017). Moreover, some researchers\naim to use pre-training to create “universal representations”\nthat transfer to multiple domains (Rebufﬁ et al., 2017). In\napplications, the “pre-train then tune” paradigm is com-\nmonplace, especially when data for a target task is acutely\nscarce (Zeiler & Fergus, 2014). This broadly applicable\ntechnique enables state-of-the-art model convergence.\nHowever, He et al. (2018) argue that model convergence\nis merely faster with pre-training, so that the beneﬁt on\nmodern research datasets is only improved wall-clock time.\n1UC Berkeley 2KAIST 3University of Chicago. Correspon-\ndence to: Dan Hendrycks<hendrycks@berkeley.edu>.\nProceedings of the 36 th International Conference on Machine\nLearning, Long Beach, California, PMLR 97, 2019. Copyright\n2019 by the author(s).\nSurprisingly, pre-training provides no performance bene-\nﬁt on various tasks and architectures over training from\nscratch, provided the model trains for long enough. Even\nmodels trained from scratch on only 10% of the COCO\ndataset (Lin et al., 2014) attain the same performance as\npre-trained models. This casts doubt on our understanding\nof pre-training and raises the important question of whether\nthere are any uses for pre-training beyond tuning for ex-\ntremely small datasets. They conclude that, with modern\nresearch datasets, ImageNet pre-training is not necessary.\nIn this work, we demonstrate that pre-training is not\nneedless. While He et al. (2018) are correct that models\nfor traditional tasks such as classiﬁcation perform well\nwithout pre-training, pre-training substantially improves\nthe quality of various complementary model components.\nFor example, we show that while accuracy may not no-\nticeably change with pre-training, what does tremendously\nimprove with pre-training is the model’s adversarial\nrobustness. Furthermore, even though training for longer\non clean datasets allows models without pre-training to\ncatch up, training for longer on a corrupted dataset leads\nto model deterioration. And the claim that “pre-training\ndoes not necessarily help reduce overﬁtting” (He et al.,\n2018) is valid when measuring only model accuracy,\nbut it becomes apparent that pre-training does reduce\noverﬁtting when also measuring model calibration. We\nbring clarity to the doubts raised about pre-training by\nshowing that pre-training can improve model robustness to\nlabel corruption (Sukhbaatar et al., 2014), class imbalance\n(Japkowicz, 2000), and adversarial attacks (Szegedy et al.,\n2014); it additionally improves uncertainty estimates\nfor out-of-distribution detection (Hendrycks & Gimpel,\n2017b) and calibration (Nguyen & O’Connor, 2015),\nthough not necessarily traditional accuracy metrics.\nPre-training yields improvements so signiﬁcant that on\nmany robustness and uncertainty tasks we surpass state-\nof-the-art performance. We even ﬁnd that pre-training\nalone improves over techniques devised for a speciﬁc task.\nNote that experiments on these tasks typically overlook\npre-training, even though pre-training is ubiquitous else-\nwhere. This is problematic since we ﬁnd there are tech-\nniques which do not comport well with pre-training; thus\nsome evaluations of robustness are less representative of\nreal-world performance than previously thought. Thus re-\nUsing Pre-Training Can Improve Model Robustness and Uncertainty\n0 50 100 150 200\nEpoch\n40\n50\n60\n70\n80\n90\n100Error (%)\nCIFAR-100, Uniform Label Corruption\nTrain Error\nTest Error\nFigure 1. Training for longer is not a suitable strategy for label\ncorruption. By training for longer, the network eventually be-\ngins to model and memorize label noise, which harms its overall\nperformance. Labels are corrupted uniformly to incorrect classes\nwith 60% probability, and the Wide Residual Network classiﬁer\nhas learning rate drops at epochs 80, 120, and 160.\nsearchers would do well to adopt the “pre-train then tune”\nparadigm for increased performance and greater realism.\n2. Related Work\nPre-Training. It is well-known that pre-training im-\nproves generalization when the dataset for the target task\nis extremely small. Prior work on transfer learning has\nanalyzed the properties of this effect, such as when ﬁne-\ntuning should stop (Agrawal et al., 2014) and which layers\nshould be ﬁne-tuned (Yosinski et al., 2014). In a series of\nablation studies, Huh et al. (2016) show that the beneﬁts of\npre-training are robust to signiﬁcant variation in the dataset\nused for pre-training, including the removal of classes re-\nlated to the target task. In our work, we observe similar\nrobustness to change in the dataset used for pre-training.\nPre-training has also been used when the dataset for the tar-\nget task is large, such as Microsoft COCO (Lin et al., 2014)\nfor object detection and segmentation. However, in a recent\nwork He et al. (2018) show that pre-training merely speeds\nconvergence on these tasks, and real gains in performance\nvanish if one trains from scratch for long enough, even with\nonly 10% of the data for the target task. They conclude\nthat pre-training is not necessary for these tasks. Moreover,\nSun et al. (2017) show that the accuracy gains from more\ndata are exponentially diminishing, severely limiting the\nutility of pre-training for improving performance metrics\nfor traditional tasks. In contrast, we show that pre-training\ndoes markedly improve model robustness and uncertainty.\nRobustness. Learning in the presence of corrupted labels\nhas been well-studied. In the context of deep learning,\nSukhbaatar et al. (2014) investigate using a stochastic ma-\ntrix encoding the label noise, though they note that this ma-\ntrix is difﬁcult to estimate. Patrini et al. (2017) propose a\ntwo-step training procedure to estimate this stochastic ma-\ntrix and train a corrected classiﬁer. These approaches are\nextended by Hendrycks et al. (2018), who consider hav-\ning access to a small dataset of cleanly labeled examples,\nleverage these trusted data to improve performance.\nZhang & Sabuncu (2018) show that networks overﬁt to\nthe incorrect labels when trained for too long (Figure 1).\nThis observation suggests pre-training as a potential ﬁx,\nsince one need only ﬁne-tune for a short period to attain\ngood performance. We show that pre-training not only im-\nproves performance with no label noise correction, but also\ncomplements methods proposed in prior work. Also note\nthat most prior works (Goldberger & Ben-Reuven, 2017;\nMa et al., 2018; Han et al., 2018) only experiment with\nsmall-scale images since label corruption demonstrations\ncan require training hundreds of models (Hendrycks et al.,\n2018). Since pre-training is typically reserved for large-\nscale datasets, such works do not explore the impact of pre-\ntraining.\nNetworks tend not to effectively model underrepresented\nclasses, which can affect a classiﬁer’s fairness of underrep-\nresented groups. To handle class imbalance, many training\nstrategies have been investigated in the literature. One\ndirection is rebalancing an imbalanced training dataset. To\nthis end, He & Garcia (2008) propose to remove samples\nfrom the majority classes, while Huang et al. (2016)\nreplicate samples from the minority classes. Generating\nsynthetic samples through linear interpolation between\ndata samples belonging in the same minority class has\nbeen studied in Chawla et al. (2002). An alternative\napproach is to modify the supervised loss function. Cost\nsensitive learning (Japkowicz, 2000) balances the loss\nfunction by re-weighting each sample by the inverse\nfrequency of its class. Huang et al. (2016) and Dong et al.\n(2018) demonstrate that enlarging the margin of a classiﬁer\nhelps mitigate the class imbalance problem. However,\nadopting such training methods often incurs various time\nand memory costs.\nThe susceptibility of neural networks to small, adversari-\nally chosen input perturbations has received much atten-\ntion. Over the years, many methods have been proposed as\ndefenses against adversarial examples (Metzen et al., 2017;\nHendrycks & Gimpel, 2017a), but these are often circum-\nvented in short order (Carlini & Wagner, 2017). In fact,\nthe only defense widely regarded as having stood the test\nof time is the adversarial training procedure of Madry et al.\n(2018). In this algorithm, white-box adversarial examples\nare created at each step of training and substituted in place\nof normal examples. This does provide some amount of\nUsing Pre-Training Can Improve Model Robustness and Uncertainty\n0.0 0.2 0.4 0.6 0.8 1.0\nCorruption Strength\n0\n20\n40\n60\n80\n100Test Error (%)\nCIFAR-100, No Correction\nNormal\nPre-Training\n0.0 0.2 0.4 0.6 0.8 1.0\nCorruption Strength\n0\n20\n40\n60\n80\n100Test Error (%)\nCIFAR-100, Forward Correction\n0.0 0.2 0.4 0.6 0.8 1.0\nCorruption Strength\n0\n20\n40\n60\n80\n100Test Error (%)\nCIFAR-100, GLC 5% Trusted\n0.0 0.2 0.4 0.6 0.8 1.0\nCorruption Strength\n0\n20\n40\n60\n80\n100Test Error (%)\nCIFAR-10, No Correction\n0.0 0.2 0.4 0.6 0.8 1.0\nCorruption Strength\n0\n20\n40\n60\n80\n100Test Error (%)\nCIFAR-10, Forward Correction\n0.0 0.2 0.4 0.6 0.8 1.0\nCorruption Strength\n0\n20\n40\n60\n80\n100Test Error (%)\nCIFAR-10, GLC 5% Trusted\nFigure 2. Error curves for label noise correction methods using training from scratch and pre-training across a full range of label cor-\nruption strengths. For the No Correction baseline, using pre-training results in a visibly improved slope of degradation with a more\npronounced elbow at higher corruption strengths. This also occurs in the complementary combinations of pre-training with previously\nproposed correction methods.\nadversarial robustness, but it requires substantially longer\ntraining times. In a later work, Schmidt et al. (2018) argue\nfurther progress on this problem may require signiﬁcantly\nmore task-speciﬁc data. However, given that data from a\ndifferent distribution can be beneﬁcial for a given task (Huh\net al., 2016), it is conceivable that the need for task-speciﬁc\ndata could be obviated with pre-training.\nUncertainty. Even though deep networks have achieved\nhigh accuracy on many classiﬁcation tasks, measuring\nthe uncertainty in their predictions remains a challenging\nproblem. Obtaining well-calibrated predictive uncertainty\ncould be useful in many machine learning applications such\nas medicine or autonomous vehicles. Uncertainty estimates\nneed to be useful for detecting out-of-distribution samples.\nHendrycks & Gimpel (2017b) propose out-of-distribution\ndetection tasks and use the maximum value of a classiﬁer’s\nsoftmax distribution as a baseline method. Lee et al.\n(2018b) propose Mahalanobis distance-based scores which\ncharacterize out-of-distribution samples using hidden fea-\ntures. Lee et al. (2018a) propose using a GAN (Goodfellow\net al., 2014) to generate out-of-distribution samples; the\nnetwork is taught to assign low conﬁdence to these GAN-\ngenerated samples. Hendrycks et al. (2019) demonstrate\nthat using non-speciﬁc, real, and diverse outlier images\nor text in place of GAN-generated samples can allow\nclassiﬁers and density estimators to improve their out-of-\ndistribution detection performance and calibration. Guo\net al. (2017) show that contemporary networks can easily\nbecome miscalibrated without additional regularization,\nand we show pre-training can provide useful regularization.\n3. Robustness\nDatasets. For the following robustness experiments, we\nevaluate on CIFAR-10 and CIFAR-100 (Krizhevsky &\nHinton, 2009). These datasets contain 32× 32 color\nimages, both with 60,000 images split into 50,000 for\ntraining and 10,000 for testing. CIFAR-10 and CIFAR-\n100 have 10 and 100 classes, respectively. For pre-\ntraining, we use Downsampled ImageNet (Chrabaszcz\net al., 2017), which is the 1,000-class ImageNet dataset\n(Deng et al., 2009) resized to 32× 32 resolution. For\nablation experiments, we remove 153 CIFAR-10-related\nclasses from the Downsampled ImageNet dataset. In this\npaper we tune the entire network. Code is available at\ngithub.com/hendrycks/pre-training.\n3.1. Robustness to Label Corruption\nSetup. In the task of classiﬁcation under label corrup-\ntion, the goal is to learn as good a classiﬁer as possible\non a dataset with corrupted labels. In accordance with prior\nwork (Sukhbaatar et al., 2014) we focus on multi-class clas-\nUsing Pre-Training Can Improve Model Robustness and Uncertainty\nTable 1. Label corruption robustness results with and without pre-training. Each value is an area under the error curve summarizing\nperformance at 11 corruption strengths. Lower is better. All values are percentages. Pre-training greatly improves performance, in some\ncases halving the error, and it can even surpass the task-speciﬁc Forward Correction.\nCIFAR-10 CIFAR-100\nNormal Training Pre-Training Normal Training Pre-Training\nNo Correction 28.7 15.9 55.4 39.1\nForward Correction 25.5 15.7 52.6 42.8\nGLC (5% Trusted) 14.0 7.2 46.8 33.7\nGLC (10% Trusted) 11.5 6.4 38.9 28.4\nsiﬁcation. Let x,y, and ˜y be an input, clean label, and po-\ntentially corrupted label respectively. The labels take val-\nues from 1 toK. Given a dataset D of (x, ˜y) pairs with x\ndrawn fromp(x) and ˜y drawn fromp(˜y| y,x ), the task is\nto predict arg maxyp(y|x).\nTo experiment with a variety of corruption severities, we\ncorrupt the true label with a given probability to a randomly\nchosen incorrect class. Formally, we generate corrupted la-\nbels with a ground truth matrix of corruption probabilities\nC, where Cij = p(˜y = j | y = i) is the probability of\ncorrupting an example with label i to labelj. Given a cor-\nruption strengths, we constructC with (1−s)I +s11T/K,\nI theK×K identity matrix. To measure performance, we\nuse the area under the curve plotting test error against cor-\nruption strength. This is generated via linear interpolation\nbetween test errors at corruption strengths from 0 to 1 in\nincrements of 0.1, summarizing a total of 11 experiments.\nMethods. We ﬁrst consider the baseline of training from\nscratch. This is denoted as Normal Training in Table 1.\nWe also consider state-of-the-art methods for classiﬁcation\nunder label noise. The F orwardmethod of Patrini et al.\n(2017) uses a two-stage training procedure. The ﬁrst stage\nestimates the matrixC describing the expected label noise,\nand the second stage trains a corrected classiﬁer to pre-\ndict the clean label distribution. We also consider the Gold\nLoss Correction (GLC) method of Hendrycks et al. (2018),\nwhich assumes access to a small, trusted dataset of cleanly\nlabeled (gold standard) examples, which is also known as\na semi-veriﬁed setting (Charikar et al., 2017). This method\nalso attempts to estimate C. For this method, we specify\nthe “trusted fraction,” which is the fraction of the available\ntraining data that is trusted or known to be cleanly labeled.\nIn all experiments, we use 40-2 Wide Residual Networks,\nSGD with Nesterov momentum, and a cosine learning rate\nschedule (Loshchilov & Hutter, 2016). The “Normal” ex-\nperiments train for 100 epochs with a learning rate of 0.1\nand use dropout at a drop rate of 0.3, as in Zagoruyko\n& Komodakis (2016). The experiments with pre-training\ntrain for 10 epochs without dropout, and use a learning rate\nof 0.001 in the “No Correction” experiment and 0.01 in\nthe experiments with label noise corrections. We found the\nlatter experiments required a larger learning rate because\nof variance introduced by the stochastic matrix corrections.\nMost parameter and architecture choices recur in later sec-\ntions of this paper. Results are in Table 1.\nAnalysis. In all experiments, pre-training gives large per-\nformance gains over the models trained from scratch. With\nno correction, we see a 45% relative reduction in the area\nunder the error curve on CIFAR-10 and a 29% reduction on\nCIFAR-100. These improvements exceed those of the task-\nspeciﬁc Forward method. Therefore in the setting with-\nout trusted data, pre-training attains new state-of-the-art\nAUCs of 15.9% and 39.1% on CIFAR-10 and CIFAR-100\nrespectively. These results are stable, since pre-training\non Downsampled ImageNet with CIFAR-10-related classes\nremoved yields a similar AUC on CIFAR-10 of 14.5%.\nMoreover, we found that these gains could not be bought\nby simply training for longer. As shown in Figure 1, train-\ning for a long time with corrupted labels actually harms\nperformance as the network destructively memorizes the\nmisinformation in the incorrect labels.\nWe also observe complementary gains of combining pre-\ntraining with previously proposed label noise correction\nmethods. In particular, using pre-training together with the\nGLC on CIFAR-10 at a trusted fraction of 5% cuts the area\nunder the error curve in half. Moreover, using pre-training\nwith the same amount of trusted data provides larger per-\nformance boosts than doubling the amount of trusted data,\neffectively allowing one to reach a target performance level\nwith half as much trusted data. Qualitatively, Figure 2\nshows that pre-training softens the performance degrada-\ntion as the corruption strength increases.\nImportantly, although pre-training does have substantial\nadditive effects on performance with the Forward Correc-\ntion method, we ﬁnd that pre-training with no correction\nyields superior performance. This observation implies that\nfuture research on label corruption should evaluate with\npre-trained networks or else researchers may develop meth-\nods that are suboptimal.\nWe observe that pre-training also provides substantial im-\nUsing Pre-Training Can Improve Model Robustness and Uncertainty\nTable 2. Experimental results on the imbalanced CIFAR-10 and CIFAR-100 datasets.\nDataset Method\nImbalance Ratio 0.2 0.4 0.6 0.8 1.0 1.5 2.0\nTotal Test Error Rate / Minority Test Error Rate (%)\nCIFAR-10\nNormal Training 23.7 / 26.0 21.8 / 26.5 21.1 / 25.8 20.3 / 24.7 20.0 / 24.5 18.3 / 23.1 15.8 / 20.2\nCost Sensitive 22.6 / 24.9 21.8 / 26.2 21.1 / 25.7 20.2 / 24.3 20.2 / 24.6 18.1 / 22.9 16.0 / 20.1\nOversampling 21.0 / 23.1 19.4 / 23.6 19.0 / 23.2 18.2 / 22.2 18.3 / 22.4 17.3 / 22.2 15.3 / 19.8\nSMOTE 19.7 / 21.7 19.7 / 24.0 19.2 / 23.4 19.2 / 23.4 18.1 / 22.1 17.2 / 22.1 15.7 / 20.4\nPre-Training 8.0 / 8.8 7.9 / 9.5 7.6 / 9.2 8.0 / 9.7 7.4 / 9.1 7.4 / 9.5 7.2 / 9.4\nCIFAR-100\nNormal Training 69.7 / 72.0 66.6 / 70.5 63.2 / 69.2 58.7 / 65.1 57.2 / 64.4 50.2 / 59.7 47.0 / 57.1\nCost Sensitive 67.6 / 70.6 66.5 / 70.4 62.2 / 68.1 60.5 / 66.9 57.1 / 64.0 50.6 / 59.6 46.5 / 56.7\nOversampling 62.4 / 66.2 59.7 / 63.8 59.2 / 65.5 55.3 / 61.7 54.6 / 62.2 49.4 / 59.0 46.6 / 56.9\nSMOTE 57.4 / 61.0 56.2 / 60.3 54.4 / 60.2 52.8 / 59.7 51.3 / 58.4 48.5 / 57.9 45.8 / 56.3\nPre-Training 37.8 / 41.8 36.9 / 41.3 36.2 / 41.7 36.4 / 42.3 34.9 / 41.5 34.0 / 41.9 33.5 / 42.2\nprovements when swapping out the Wide ResNet for an\nAll Convolutional Network (Springenberg et al., 2014). In\nthe No Correction setting, area under the error curves on\nCIFAR-10 for Normal Training and Pre-Training are 23.7%\nand 14.8% respectively. On CIFAR-100, they are 46.5%\nand 41.0% respectively. Additionally, when ﬁne-tuning a\nWide ResNet on Places365 downsampled in the same fash-\nion as ImageNet in earlier experiments, we obtain area\nunder the error curves of 19.3% and 49.5% compared to\n28.7% and 55.4% with Normal Training. These experi-\nments demonstrate the generalizability of our results across\narchitectures and datasets used for pre-training.\n3.2. Robustness to Class Imbalance\nIn most real-world classiﬁcation problems, some classes\nare more abundant than others, which naturally results in\nclass imbalance (Van Horn et al., 2018). Unfortunately,\ndeep networks tend to model prevalent classes at the ex-\npense of minority classes. This need not be the case. Deep\nnetworks are capable of learning both the prevalent and\nminority classes, but to accomplish this, task-speciﬁc ap-\nproaches have been necessary. In this section, we show\nthat pre-training can also be useful for handling such im-\nbalanced scenarios better than approaches speciﬁcally cre-\nated for this task (Japkowicz, 2000; Chawla et al., 2002;\nHuang et al., 2016; Dong et al., 2018).\nSetup. Similar to Dong et al. (2018), we simulate class\nimbalance with a power law model. Speciﬁcally, we set\nthe number of training samples for a class c as follows,\nnc =⌊a/(b + (c− 1)−γ)⌉, where⌊·⌉ is the integer round-\ning function, γ represents an imbalance ratio, a and b\nare offset parameters to specify the largest and small-\nest class sizes. Our training data becomes a power law\nclass distribution as the imbalance ratio γ decreases. We\ntest 7 different degrees of imbalance; speciﬁcally, γ ∈\n{0.2, 0.4, 0.6, 0.8, 1.0, 1.5, 2.0} and (a,b ) are set to force\n(maxcnc, mincnc) to become (5000, 250) for CIFAR-10\nand (500, 25) for CIFAR-100. A class is deﬁned as a mi-\nnority class if its size is smaller than the average class size.\nFor evaluation, we measure the average test set error rates\nof all classes and error rates of minority classes.\n1 2 3 4 5 6 7 8 9 10\nClass Number\n0\n5\n10\n15\n20\n25\n30\n35\n40Test Error (%)\nClass-Wise Error Rates\nNormal\nCost Sensitive\nCRL\nOversampling\nSMOTE\nFigure 3. Class-wise test set error rates are lower across all classes\nwith pre-training. Here the imbalanced dataset is a CIFAR-10\nmodiﬁcation with imbalance ratioγ = 0.2.\nMethods. The class imbalance baseline methods are as\nfollows. Normal Training is the conventional approach of\ntraining from scratch with cross-entropy loss. Oversam-\npling (Japkowicz, 2000) is a re-sampling method to build\na balanced training set before learning through augmenting\nthe samples of minority classes with random replication.\nSMOTE (Chawla et al., 2002) is an oversampling method\nthat uses synthetic samples by interpolating linearly with\nneighbors. Cost Sensitive (Huang et al., 2016) introduces\nadditional weights in the loss function for each class pro-\nportional to inverse class frequency.\nUsing Pre-Training Can Improve Model Robustness and Uncertainty\nTable 3. Adversarial accuracies of models trained from scratch, with adversarial training, and with adversarial training with pre-training.\nAll values are percentages. The pre-trained models have comparable clean accuracy to adversarially trained models from scratch, as\nimplied by He et al. (2018), but pre-training can markedly improve adversarial accuracy.\nCIFAR-10 CIFAR-100\nClean Adversarial Clean Adversarial\nNormal Training 96.0 0.0 81.0 0.0\nAdversarial Training 87.3 45.8 59.1 24.3\nAdv. Pre-Training and Tuning 87.1 57.4 59.2 33.5\nHere we use 40-2 Wide Residual Networks, SGD with Nes-\nterov momentum, and a cosine learning rate schedule. The\nexperiments with pre-training train for 50 epochs without\ndropout and use a learning rate of 0.001, and the exper-\niments with other baselines train for 100 epochs with a\nlearning rate of 0.1 and use dropout at a drop rate of 0.3.\nAnalysis. Table 2 shows that the pre-training alone sig-\nniﬁcantly improves the test set error rates compared to\ntask-speciﬁc methods that can incur expensive back-and-\nforth costs, requiring additional training time and memory.\nHere, we remark that much of the gain from pre-training is\nfrom the low test error rates on minority classes (i.e., those\nwith greater class indices), as shown in Figure 3. Further-\nmore, if we tune a network on CIFAR-10 that is pre-trained\non Downsampled ImageNet with CIFAR-10-related classes\nremoved, the total error rate increases by only 2.1% com-\npared to pre-training on all classes. By contrast, the dif-\nference between pre-training and SMOTE is 12.6%. This\nimplies that pre-training is indeed useful for improving ro-\nbustness against class imbalance.\n3.3. Robustness to Adversarial Perturbations\nSetup. Deep networks are notably unstable and less ro-\nbust than the human visual system (Geirhos et al., 2018;\nHendrycks & Dietterich, 2019). For example, a network\nmay produce a correct prediction for a clean image, but\nshould the image be perturbed carefully, its verdict may\nchange entirely (Szegedy et al., 2014). This has led re-\nsearchers to defend networks against “adversarial” noise\nwith a small ℓp norm, so that networks correctly general-\nize to images with a worst-case perturbation applied.\nNearly all adversarial defenses have been broken (Carlini &\nWagner, 2017), and adversarial robustness for large-scale\nimage classiﬁers remains elusive (Engstrom et al., 2018).\nThe exception is that adversarial training in the style of\nMadry et al. (2018) has been partially successful for de-\nfending small-scale image classiﬁers against ℓ∞ perturba-\ntions. Following their work and using their state-of-the-art\nadversarial training procedure, we experiment with CIFAR\nimages and assume the adversary can corrupt images with\nperturbations of an ℓ∞ norm less than or equal to 8/255.\nThe initial learning rate is 0.1 and the learning rate anneals\nfollowing a cosine learning rate schedule. We adversarially\ntrain the model against a 10-step adversary for 100 epochs\nand test against 20-step untargeted adversaries. Additional\nresults with 100-step adversaries and random restarts are in\nthe Supplementary Materials. Unless otherwise speciﬁed,\nwe use 28-10 Wide Residual Networks, as adversarially\ntrained high-capacity networks exhibit greater adversarial\nrobustness (Kurakin et al., 2017; Madry et al., 2018).\nAnalysis. It could be reasonable to expect that pre-\ntraining would not improve adversarial robustness. First,\nnearly all adversarial defenses fail, and even some adver-\nsarial training methods can fail too (Engstrom et al., 2018).\nCurrent adversarial defenses result in networks with large\ngeneralization gaps, even when the train and test distribu-\ntions are similar. For instance, CIFAR-10 Wide ResNets\nare made so wide that their adversarial train accuracies are\n100% but their adversarial test accuracies are only 45.8%.\nSchmidt et al. (2018) speculate that a signiﬁcant increase in\ntask-speciﬁc data is necessary to close this gap. This gen-\neralization gap swells under slight changes to the problem\nsetup (Sharma & Chen, 2018). We attempt to reduce this\ngap and make pre-trained representations transfer across\ndata distributions, but doing so requires an unconventional\nchoice. Choosing to use targeted adversaries or no adver-\nsaries during pre-training does not provide substantial ro-\nbustness. Instead, we choose to adversarially pre-train a\nDownsampled ImageNet model against an untargeted ad-\nversary, contra Kurakin et al. (2017); Kannan et al. (2018);\nXie et al. (2018).\nWe ﬁnd that an adversarially pre-trained network can sur-\npass the long-standing state-of-the-art model by a signif-\nicant margin. By pre-training a Downsampled ImageNet\nclassiﬁer against an untargeted adversary, then adversari-\nally ﬁne-tuning on CIFAR-10 or CIFAR-100 for 5 epochs\nwith a learning rate of 0.001, we obtain networks which\nimprove adversarial robustness by 11.6% and 9.2% in ab-\nsolute accuracy respectively.\nAs in the other tasks we consider, a Downsampled Ima-\ngeNet model with CIFAR-10-related classes removed sees\nUsing Pre-Training Can Improve Model Robustness and Uncertainty\nsimilar robustness gains. As a quick check, we pre-trained\nand tuned two 40-2 Wide ResNets, one pre-trained typi-\ncally and one pre-trained with CIFAR-10-related classes\nexcluded from Downsampled ImageNet. We observed only\na 1.04% decrease in adversarial accuracy compared to the\ntypically pre-trained model, which demonstrates that the\npre-trained models do not rely on seeing CIFAR-10-related\nimages, and that simply training on more natural images\nincreases adversarial robustness. Notice that in Table 3 the\nclean accuracy is approximately the same while the adver-\nsarial accuracy is far larger. This indicates again that pre-\ntraining may have a limited effect on accuracy for tradi-\ntional tasks, but it has a strong effect on robustness.\nIt is even the case that the pre-trained representations can\ntransfer to a new task without adversarially tuning the en-\ntire network. In point of fact, if we only adversarially\ntune the last afﬁne classiﬁcation layer, and no other pa-\nrameters, for CIFAR-10 and CIFAR-100 we respectively\nobtain adversarial accuracies of 46.6% and 26.1%. Thus\nadversarially tuning only the last afﬁne layer also surpasses\nthe previous adversarial accuracy state-of-the-art. This fur-\nther demonstrates that that adversarial features can robustly\ntransfer across data distributions. In addition to robust-\nness gains, adversarial pre-training could save much wall-\nclock time since pre-training speeds up convergence; com-\npared to typical training routines, adversarial training pro-\nhibitively requires at least10× the usual amount of training\ntime. By surpassing the previous state-of-the-art, we have\nshown that pre-training enhances adversarial robustness.\n4. Uncertainty\nTo demonstrate that pre-training improves model uncer-\ntainty estimates, we use the CIFAR-10, CIFAR-100, and\nTiny ImageNet datasets (Johnson et al.). We did not use\nTiny ImageNet in the robustness section, because adversar-\nial training is not known to work on images of this size,\nand using Tiny ImageNet is computationally prohibitive\nfor the label corruption experiments. Tiny ImageNet con-\nsists of 200 ImageNet classes at 64× 64 resolution, so we\nuse a 64× 64 version of Downsampled ImageNet for pre-\ntraining. We also remove the 200 overlapping Tiny Ima-\ngeNet classes from Downsampled ImageNet for all experi-\nments on Tiny ImageNet.\nIn all experiments, we use 40-2 Wide ResNets trained using\nSGD with Nesterov momentum and a cosine learning rate.\nPre-trained networks train on Downsampled ImageNet for\n100 epochs, and are ﬁne-tuned for 10 epochs for CIFAR\nand 20 for Tiny ImageNet without dropout and with a learn-\ning rate of 0.001. Baseline networks train from scratch for\n100 epochs with a dropout rate of 0.3. When performing\ntemperature tuning in Section 4.2, we train without 10% of\nthe training data to estimate the optimum temperature.\nTable 4. Out-of-distribution detection performance with models\ntrained from scratch and with models pre-trained. Results are an\naverage of ﬁve runs. Values are percentages.\nAUROC AUPR\nNormal Pre-Train Normal Pre-Train\nCIFAR-10 91.5 94.5 63.4 73.5\nCIFAR-100 69.4 83.1 29.7 52.7\nTiny ImageNet 71.8 73.9 30.8 31.0\n4.1. Out-of-Distribution Detection\nSetup. In the problem of out-of-distribution detection\n(Hendrycks & Gimpel, 2017b; Hendrycks et al., 2019; Lee\net al., 2018a;b; Liu et al., 2018), models are tasked with\nassigning anomaly scores to indicate whether a sample is\nin- or out-of-distribution. Hendrycks & Gimpel (2017b)\nshow that the discriminative features learned by a classi-\nﬁer are well-suited for this task. They use the maximum\nsoftmax probability maxkp(y = k| x) for each sample\nx as a way to rank in- and out-of-distribution (OOD) sam-\nples. OOD samples tend to have lower maximum softmax\nprobabilities. Improving over this baseline is a difﬁcult\nchallenge without assuming knowledge of the test distri-\nbution of anomalies (Chen et al., 2018). Without assuming\nsuch knowledge, we use the maximum softmax probabil-\nities to score anomalies and show that models which are\npre-trained then tuned provide superior anomaly scores.\nTo measure the quality of out-of-distribution detection, we\nemploy two standard metrics. The ﬁrst is the AUROC,\nor the Area Under the Receiver Operating Characteristic\ncurve. This is the probability that an OOD example is as-\nsigned a higher anomaly score than an in-distribution ex-\nample. Thus a higher AUROC is better. A similar measure\nis the AUPR, or the Area Under the Precision-Recall Curve;\nas before, a higher AUPR is better. For in-distribution data\nwe use the test dataset. For out-of-distribution data we\nuse the various anomalous distributions from Hendrycks\net al. (2019), including Gaussian noise, textures, Places365\nscene images (Zhou et al., 2017), etc. All OOD datasets do\nnot have samples from Downsampled ImageNet. Further\nevaluation details are in the Supplementary Materials.\nAnalysis. By using pre-training, both the AUROC and\nAUPR consistently improve over the baseline, as shown\nin Table 4. Note that results are an average of the AUROC\nand AUPR values from detecting samples from various\nOOD datasets. Observe that with pre-training, CIFAR-100\nOOD detection signiﬁcantly improves. Consequently\npre-training can directly improve uncertainty estimates.\nUsing Pre-Training Can Improve Model Robustness and Uncertainty\nTable 5. Calibration errors for models trained from scratch and\nmodels with pre-training. All values are percentages.\nRMS Error MAD Error\nNormal Pre-Train Normal Pre-Train\nCIFAR-10 6.4 2.9 2.9 1.2\nCIFAR-100 13.3 3.6 10.3 2.5\nTiny ImageNet 8.5 4.2 7.0 2.9\n4.2. Calibration\nSetup. A central component of uncertainty estimation in\nclassiﬁcation problems is conﬁdence calibration. From a\nclassiﬁcation system that produces probabilistic conﬁdence\nestimates C of its predictions ˆY being correct, we would\nlike trustworthy estimates. That is, when a classiﬁer pre-\ndicts a class with eighty percent conﬁdence, we would\nlike it to be correct eighty percent of the time. Nguyen\n& O’Connor (2015); Hendrycks & Gimpel (2017b) found\nthat deep neural network classiﬁers display severe overcon-\nﬁdence in their predictions, and that the problem becomes\nworse with increased representational capacity (Guo et al.,\n2017). Integrating uncalibrated classiﬁers into decision-\nmaking processes could result in egregious assessments,\nmotivating the task of conﬁdence calibration.\nTo measure the calibration of a classiﬁer, we adopt\ntwo measures from the literature. The Root Mean\nSquare Calibration Error (RMS) is the square root of\nthe expected squared difference between the classiﬁer’s\nconﬁdence and its accuracy at said conﬁdence level,√\nEC[(P(Y = ˆY|C =c)−c)2] . The Mean Absolute\nValue Calibration Error (MAD) uses the expected abso-\nlute difference rather than squared difference between the\nsame quantities. The MAD Calibration Error has the same\nform as the Expected Calibration Error used by Guo et al.\n(2017), but it employs adaptive binning of conﬁdences for\nimproved estimation. In our experiments, we use a bin size\nof 100. We refer the reader to Hendrycks et al. (2019) for\nfurther details on these measures.\nAnalysis. In all experiments, we observe large improve-\nments in calibration from using pre-training. In Figure 4\nand Table 5, we can see that RMS Calibration Error is at\nleast halved on all datasets through the use of pre-training,\nwith CIFAR-100 seeing the largest improvement. The\nsame is true of the MAD error. In fact, the MAD error on\nCIFAR-100 is reduced by a factor of 4.1 with pre-training,\nwhich can be interpreted as the stated conﬁdence being\nfour times closer to the true frequency of occurrence.\nWe ﬁnd that these calibration gains are robust across pre-\ntraining datasets. With Places365 pre-training the RMS er-\nror is 3.1 on CIFAR-10, and with ImageNet pre-training\nSVHN CIFAR-10 CIFAR-100Tiny ImageNet\n0\n5\n10\n15\n20\n25RMS Calibration Error (%)\nCalibration Error with Outlier Exposure\nTemperature Tuning\n+OE\nFigure 4. Root Mean Square Calibration Error values for mod-\nels trained from scratch and models that are pre-trained. On all\ndatasets, pre-training reduces the RMS error by more than half.\nthe RMS error is 2.9; meanwhile, the baseline RMS error\nis 6.4. The gains are also complementary with the tem-\nperature tuning method of Guo et al. (2017), which fur-\nther reduces RMS Calibration Error from 4.15 to 3.55 for\nTiny ImageNet when combined with pre-training. How-\never, temperature tuning is computationally expensive and\nrequires additional data, whereas pre-training does not re-\nquire collecting extra data and can naturally and directly\nmake the model more calibrated.\n5. Conclusion\nAlthough He et al. (2018) assert that pre-training does\nnot improve performance on traditional tasks, for other\ntasks this is not so. On robustness and uncertainty tasks,\npre-training results in models that surpass the previous\nstate-of-the-art. For uncertainty tasks, we ﬁnd pre-trained\nrepresentations directly translate to improvements in pre-\ndictive uncertainty estimates. He et al. (2018) argue that\nboth pre-training and training from scratch result in models\nof similar accuracy, but we show this only holds for unper-\nturbed data. In fact, pre-training with an untargeted adver-\nsary surpasses the long-standing state-of-the-art in adver-\nsarial accuracy by a signiﬁcant margin. Robustness to label\ncorruption is similarly improved by wide margins, such\nthat pre-training alone outperforms certain task-speciﬁc\nmethods, sometimes even after combining these methods\nwith pre-training. This suggests future work on model\nrobustness should evaluate proposed methods with pre-\ntraining in order to correctly gauge their utility, and some\nwork could specialize pre-training for these downstream\ntasks. In sum, the beneﬁts of pre-training extend beyond\nmerely quick convergence, as previously thought, since\npre-training can improve model robustness and uncertainty.\nUsing Pre-Training Can Improve Model Robustness and Uncertainty\nACKNOWLEDGMENTS\nThis research was partially supported by the Engineering\nResearch Center Program through the National Research\nFoundation of Korea (NRF) funded by the Korean Govern-\nment MSIT (NRF-2018R1A5A1059921).\nReferences\nAgrawal, P., Girshick, R., and Malik, J. Analyzing the per-\nformance of multilayer neural networks for object recog-\nnition. ECCV, 2014.\nCarlini, N. and Wagner, D. Adversarial examples are not\neasily detected: Bypassing ten detection methods, 2017.\nCharikar, M., Steinhardt, J., and Valiant, G. Learning from\nuntrusted data. STOC, 2017.\nChawla, N. V ., Bowyer, K. W., Hall, L. O., and\nKegelmeyer, W. P. SMOTE: synthetic minority over-\nsampling technique. JAIR, 2002.\nChen, W., Shen, Y ., Wang, X., and Wang, W. Enhancing\nthe robustness of prior network in out-of-distribution de-\ntection. arXiv, 2018.\nChrabaszcz, P., Loshchilov, I., and Hutter, F. A downsam-\npled variant of ImageNet as an alternative to the CIFAR\ndatasets. arXiv, 2017.\nDeng, J., Dong, W., Socher, R., jia Li, L., Li, K., and\nFei-Fei, L. ImageNet: A large-scale hierarchical image\ndatabase. CVPR, 2009.\nDong, Q., Gong, S., and Zhu, X. Imbalanced deep learn-\ning by minority class incremental rectiﬁcation. IEEE\nTPAMI, 2018.\nEngstrom, L., Ilyas, A., and Athalye, A. Evaluating and\nunderstanding the robustness of adversarial logit pairing.\narXiv preprint, 2018.\nGeirhos, R., Temme, C. R. M., Rauber, J., Sch ¨utt, H. H.,\nBethge, M., and Wichmann, F. A. Generalisation in hu-\nmans and deep neural networks. NeurIPS, 2018.\nGoldberger, J. and Ben-Reuven, E. Training deep neural-\nnetworks using a noise adaptation layer. In ICLR, 2017.\nGoodfellow, I. J., Pouget-Abadie, J., Mirza, M., Xu, B.,\nWarde-Farley, D., Ozair, S., Courville, A., and Bengio,\nY . Generative adversarial networks. InNeurIPS, 2014.\nGuo, C., Pleiss, G., Sun, Y ., and Weinberger, K. Q. On cal-\nibration of modern neural networks. International Con-\nference on Machine Learning, 2017.\nHan, B., Yao, Q., Yu, X., Niu, G., Xu, M., Hu, W., Tsang,\nI., and Sugiyama, M. Co-teaching: robust training\ndeep neural networks with extremely noisy labels. In\nNeurIPS, 2018.\nHe, H. and Garcia, E. A. Learning from imbalanced data.\nTKDE, 2008.\nHe, K., Gkioxari, G., Dollar, P., and Girshick, R. Mask\nR-CNN. ICCV, 2017.\nHe, K., Girshick, R., and Dollar, P. Rethinking ImageNet\npre-training. arXiv, 2018.\nHendrycks, D. and Dietterich, T. Benchmarking neural net-\nwork robustness to common corruptions and perturba-\ntions. ICLR, 2019.\nHendrycks, D. and Gimpel, K. Early methods for detecting\nadversarial images. ICLR Workshop, 2017a.\nHendrycks, D. and Gimpel, K. A baseline for detecting\nmisclassiﬁed and out-of-distribution examples in neural\nnetworks. ICLR, 2017b.\nHendrycks, D., Mazeika, M., Wilson, D., and Gimpel, K.\nUsing trusted data to train deep networks on labels cor-\nrupted by severe noise. NeurIPS, 2018.\nHendrycks, D., Mazeika, M., and Dietterich, T. Deep\nanomaly detection with outlier exposure. ICLR, 2019.\nHuang, C., Li, Y ., Change Loy, C., and Tang, X. Learn-\ning deep representation for imbalanced classiﬁcation. In\nCVPR, 2016.\nHuh, M., Agrawal, P., and Efros, A. A. What makes Ima-\ngeNet good for transfer learning? arXiv, 2016.\nJapkowicz, N. The class imbalance problem: Signiﬁcance\nand strategies. In ICAI, 2000.\nJohnson et al. Tiny ImageNet visual recognition\nchallenge. URL https://tiny-imagenet.\nherokuapp.com.\nKannan, H., Kurakin, A., and Goodfellow, I. Adversarial\nlogit pairing. NeurIPS, 2018.\nKrizhevsky, A. and Hinton, G. Learning multiple layers of\nfeatures from tiny images. Technical report, University\nof Toronto, 2009.\nKrizhevsky, A., Sutskever, I., and Hinton, G. E. ImageNet\nclassiﬁcation with deep convolutional neural networks.\nNIPS, 2012.\nKurakin, A., Goodfellow, I., and Bengio, S. Adversarial\nmachine learning at scale. ICLR, 2017.\nUsing Pre-Training Can Improve Model Robustness and Uncertainty\nLee, K., Lee, H., Lee, K., and Shin, J. Training conﬁdence-\ncalibrated classiﬁers for detecting out-of-distribution\nsamples. ICLR, 2018a.\nLee, K., Lee, K., Lee, H., and Shin, J. A simple uniﬁed\nframework for detecting out-of-distribution samples and\nadversarial attacks. NeurIPS, 2018b.\nLin, T.-Y ., Maire, M., Belongie, S., Bourdev, L., Girshick,\nR., Hays, J., Perona, P., Ramanan, D., Zitnick, C. L., and\nDollar, P. Microsoft COCO: Common objects in context.\nECCV, 2014.\nLiu, S., Garrepalli, R., Dietterich, T., Fern, A., and\nHendrycks, D. Open category detection with PAC guar-\nantees. In ICML, 2018.\nLoshchilov, I. and Hutter, F. SGDR: stochastic gradient\ndescent with warm restarts. ICLR, 2016.\nMa, X., Wang, Y ., Houle, M. E., Zhou, S., Erfani,\nS. M., Xia, S.-T., Wijewickrema, S., and Bailey, J.\nDimensionality-driven learning with noisy labels. In\nICML, 2018.\nMadry, A., Makelov, A., Schmidt, L., Tsipras, D., and\nVladu, A. Towards deep learning models resistant to\nadversarial attacks. ICLR, 2018.\nMetzen, J. H., Genewein, T., Fischer, V ., and Bischoff, B.\nOn detecting adversarial perturbations, 2017.\nNguyen, K. and O’Connor, B. Posterior calibration and ex-\nploratory analysis for natural language processing mod-\nels. EMNLP, 2015.\nPatrini, G., Rozza, A., Menon, A., Nock, R., and Qu, L.\nMaking deep neural networks robust to label noise: a\nloss correction approach. CVPR, 2017.\nRebufﬁ, S.-A., Bilen, H., and Vedaldi, A. Learning mul-\ntiple visual domains with residual adapters. NeurIPS,\n2017.\nSchmidt, L., Santurkar, S., Tsipras, D., Talwar, K., and\nMadry, A. Adversarially robust generalization requires\nmore data. NeurIPS, 2018.\nSharma, Y . and Chen, P.-Y . Attacking the Madry defense\nmodel withl1-based adversarial examples. ICLR Work-\nshop, 2018.\nSpringenberg, J. T., Dosovitskiy, A., Brox, T., and Ried-\nmiller, M. A. Striving for simplicity: The all convolu-\ntional net. CoRR, abs/1412.6806, 2014.\nSukhbaatar, S., Bruna, J., Paluri, M., Bourdev, L., and Fer-\ngus, R. Training convolutional networks with noisy la-\nbels. ICLR Workshop, 2014.\nSun, C., Shrivastava, A., Singh, S., and Gupta, A. Revis-\niting unreasonable effectiveness of data in deep learning\nera. ICCV, 2017.\nSzegedy, C., Zaremba, W., Sutskever, I., Bruna, J., Erhan,\nD., Goodfellow, I., and Fergus, R. Intriguing properties\nof neural networks, 2014.\nVan Horn, G., Mac Aodha, O., Song, Y ., Cui, Y ., Sun, C.,\nShepard, A., Adam, H., Perona, P., and Belongie, S. The\ninaturalist species classiﬁcation and detection dataset. In\nCVPR, 2018.\nXie, C., Wu, Y ., van der Maaten, L., Yuille, A., and He, K.\nFeature denoising for improving adversarial robustness.\narXiv preprint, 2018.\nYosinski, J., Clune, J., Bengio, Y ., and Lipson, H.\nHow transferable are features in deep neural networks?\nNeurIPS, 2014.\nZagoruyko, S. and Komodakis, N. Wide residual networks.\nBMVC, 2016.\nZeiler, M. D. and Fergus, R. Visualizing and understanding\nconvolutional networks. ECCV, 2014.\nZhang, Z. and Sabuncu, M. Generalized cross entropy\nloss for training deep neural networks with noisy labels.\nNeurIPS, 2018.\nZhou, B., Lapedriza, A., Khosla, A., Oliva, A., and Tor-\nralba, A. Places: A 10 million image database for scene\nrecognition. PAMI, 2017.",
  "values": {
    "Non-maleficence": "No",
    "Not socially biased": "No",
    "Deferral to humans": "No",
    "Autonomy (power to decide)": "No",
    "Interpretable (to users)": "No",
    "Fairness": "No",
    "User influence": "No",
    "Respect for Law and public interest": "No",
    "Transparent (to users)": "No",
    "Privacy": "No",
    "Justice": "No",
    "Collective influence": "No",
    "Critiqability": "No",
    "Explicability": "No",
    "Respect for Persons": "No",
    "Beneficence": "No"
  }
}