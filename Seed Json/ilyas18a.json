{
  "pdf": "ilyas18a",
  "title": "Black-box Adversarial Attacks with Limited Queries and Information",
  "author": "Andrew Ilyas, Logan Engstrom, Anish Athalye, Jessy Lin",
  "paper_id": "ilyas18a",
  "text": "Black-box Adversarial Attacks with Limited Queries and Information\nAndrew Ilyas * 1 2 Logan Engstrom * 1 2 Anish Athalye * 1 2 Jessy Lin * 1 2\nAbstract\nCurrent neural network-based classiﬁers are sus-\nceptible to adversarial examples even in the\nblack-box setting, where the attacker only has\nquery access to the model. In practice, the threat\nmodel for real-world systems is often more re-\nstrictive than the typical black-box model where\nthe adversary can observe the full output of the\nnetwork on arbitrarily many chosen inputs. We\ndeﬁne three realistic threat models that more\naccurately characterize many real-world clas-\nsiﬁers: the query-limited setting, the partial-\ninformation setting, and the label-only setting.\nWe develop new attacks that fool classiﬁers un-\nder these more restrictive threat models, where\nprevious methods would be impractical or inef-\nfective. We demonstrate that our methods are ef-\nfective against an ImageNet classiﬁer under our\nproposed threat models. We also demonstrate a\ntargeted black-box attack against a commercial\nclassiﬁer, overcoming the challenges of limited\nquery access, partial information, and other prac-\ntical issues to break the Google Cloud Vision\nAPI.\n1. Introduction\nNeural network-based image classiﬁers are susceptible to\nadversarial examples, minutely perturbed inputs that fool\nclassiﬁers (Szegedy et al., 2013; Biggio et al., 2013). These\nadversarial examples can potentially be exploited in the real\nworld (Kurakin et al., 2016; Athalye et al., 2017; Sharif\net al., 2017; Evtimov et al., 2017). For many commercial\nor proprietary systems, adversarial examples must be con-\nsidered under a limited threat model. This has motivated\nblack-box attacks that do not require access to the gradient\nof the classiﬁer.\nOne approach to attacking a classiﬁer in this setting trains\n*Equal contribution 1Massachusetts Institute of Technology\n2LabSix. Correspondence to: LabSix <team@labsix.org>.\nProceedings of the 35 th International Conference on Machine\nLearning, Stockholm, Sweden, PMLR 80, 2018. Copyright 2018\nby the author(s).\na substitute network to emulate the original network and\nthen attacks the substitute with ﬁrst-order white-box meth-\nods (Papernot et al., 2016a; 2017). Recent works note\nthat adversarial examples for substitute networks do not\nalways transfer to the target model, especially when con-\nducting targeted attacks (Chen et al., 2017; Narodytska &\nKasiviswanathan, 2017). These works instead construct ad-\nversarial examples by estimating the gradient through the\nclassiﬁer with coordinate-wise ﬁnite difference methods.\nWe consider additional access and resource restrictions on\nthe black-box model that characterize restrictions in real-\nworld systems. These restrictions render targeted attacks\nwith prior methods impractical or infeasible. We present\nnew algorithms for generating adversarial examples that\nrender attacks in the proposed settings tractable.\n1.1. Deﬁnitions\nAt a high level, an adversarial example for a classiﬁer is an\ninput that is slightly perturbed to cause misclassiﬁcation.\nPrior work considers various threat models (Papernot et al.,\n2016b; Carlini & Wagner, 2017). In this work, we consider\nℓ∞-bounded perturbation that causes targeted misclassiﬁ-\ncation (i.e. misclassiﬁcation as a given target class). Thus,\nthe task of the adversary is: given an input x, target class\nyadv, and perturbation boundϵ, ﬁnd an inputxadv such that\n||xadv −x||∞ <ϵ andxadv is classiﬁed asyadv.\nAll of the threat models considered in this work are addi-\ntional restrictions on the black-box setting:\nBlack-box setting. In this paper, we use the deﬁnition of\nblack-box access as query access (Chen et al., 2017; Liu\net al., 2017; Hayes & Danezis, 2017). In this model, the\nadversary can supply any inputx and receive the predicted\nclass probabilities, P (y|x) for all classes y. This setting\ndoes not allow the adversary to analytically compute the\ngradient ∇P (y|x) as is doable in the white-box case.\nWe introduce the following threat models as more limited\nvariants of the black-box setting that reﬂect access and re-\nsource restrictions in real-world systems:\n1. Query-limited setting. In the query-limited setting,\nthe attacker has a limited number of queries to the\nBlack-box Adversarial Attacks with Limited Queries and Information\nclassiﬁer. In this setting, we are interested in query-\nefﬁcient algorithms for generating adversarial exam-\nples. A limit on the number of queries can be a result\nof limits on other resources, such as a time limit if in-\nference time is a bottleneck or a monetary limit if the\nattacker incurs a cost for each query.\nExample. The Clarifai NSFW (Not Safe for Work)\ndetection API 1 is a binary classiﬁer that outputs\nP (NSFW |x) for any image x and can be queried\nthrough an API. However, after the ﬁrst 2500 pre-\ndictions, the Clarifai API costs upwards of $2.40 per\n1000 queries. This makes a 1-million query attack, for\nexample, cost $2400.\n2. Partial-information setting. In the partial-\ninformation setting, the attacker only has access to the\nprobabilities P (y|x) for y in the top k (e.g. k = 5 )\nclasses {y1,...,y k}. Instead of a probability, the clas-\nsiﬁer may even output a score that does not sum to 1\nacross the classes to indicate relative conﬁdence in the\npredictions.\nNote that in the special case of this setting where\nk = 1 , the attacker only has access to the top la-\nbel and its probability—a partial-information attack\nshould succeed in this case as well.\nExample. The Google Cloud Vision API2 (GCV) only\noutputs scores for a number of the top classes (the\nnumber varies between queries). The score is not a\nprobability but a “conﬁdence score” (that does not\nsum to one).\n3. Label-only setting. In the label-only setting, the\nadversary does not have access to class probabilities\nor scores. Instead, the adversary only has access to\na list of k inferred labels ordered by their predicted\nprobabilities. Note that this is a generalization of the\ndecision-only setting deﬁned in Brendel et al. (2018),\nwherek = 1, and the attacker only has access to the\ntop label. We aim to devise an attack that works in\nthis special case but can exploit extra information in\nthe case wherek> 1.\nExample. Photo tagging apps such as Google Pho-\ntos3 add labels to user-uploaded images. However, no\n“scores” are assigned to the labels, and so an attacker\ncan only see whether or not the classiﬁer has inferred\na given label for the image (and where that label ap-\npears in the ordered list).\n1https://clarifai.com/models/\nnsfw-image-recognition-model-\ne9576d86d2004ed1a38ba0cf39ecb4b1\n2https://cloud.google.com/vision/\n3https://photos.google.com/\n1.2. Contributions\nQuery-efﬁcient adversarial examples. Previous meth-\nods using substitute networks or coordinate-wise gradient\nestimation for targeted black-box attacks require on the or-\nder of millions of queries to attack an ImageNet classi-\nﬁer. Low throughput, high latency, and rate limits on com-\nmercially deployed black-box classiﬁers heavily impact the\nfeasibility of current approaches to black-box attacks on\nreal-world systems.\nWe propose the variant of NES described in Salimans et al.\n(2017) (inspired by Wierstra et al. (2014)) as a method for\ngenerating adversarial examples in the query-limited set-\nting. We use NES as a black-box gradient estimation tech-\nnique and employ PGD (as used in white-box attacks) with\nthe estimated gradient to construct adversarial examples.\nWe relate NES in this special case with the ﬁnite differ-\nence method over Gaussian bases, providing a theoretical\ncomparison with previous attempts at black-box adversar-\nial examples. The method does not require a substitute net-\nwork and is 2-3 orders of magnitude more query-efﬁcient\nthan previous methods based on gradient estimation such\nas Chen et al. (2017). We show that our approach reli-\nably produces targeted adversarial examples in the black-\nbox setting.\nAdversarial examples with partial information. We\npresent a new algorithm for attacking neural networks in\nthe partial-information setting. The algorithm starts with an\nimage of the target class and alternates between blending in\nthe original image and maximizing the likelihood of the tar-\nget class. We show that our method reliably produces tar-\ngeted adversarial examples in the partial-information set-\nting, even when the attacker only sees the top probability.\nTo our knowledge, this is the ﬁrst attack algorithm pro-\nposed for this threat model.\nWe use our method to perform the ﬁrst targeted attack on\nthe Google Cloud Vision API, demonstrating the applica-\nbility of the attack on large, commercial systems: the GCV\nAPI is an opaque (no published enumeration of labels),\npartial-information (queries return only up to 10 classes\nwith uninterpretable “scores”), several-thousand-way com-\nmercial classiﬁer.\nAdversarial examples with scoreless feedback. Often,\nin deployed machine learning systems, even the score is\nhidden from the attacker. We introduce an approach for\nproducing adversarial examples even when no scores of any\nkind are available. We assume the adversary only receives\nthe top k sorted labels when performing a query. We in-\ntegrate noise robustness as a proxy for classiﬁcation score\ninto our partial-information attack to mount a targeted at-\ntack in the label-only setting. We show that even in the\nBlack-box Adversarial Attacks with Limited Queries and Information\ndecision-only setting, where k = 1 , we can mount a suc-\ncessful attack.\n2. Approach\nWe outline the key components of our approach for con-\nducting an attack in each of the proposed threat models.\nWe begin with a description of our application of Natu-\nral Evolutionary Strategies (Wierstra et al., 2014) to enable\nquery-efﬁcient generation of black-box adversarial exam-\nples. We then show the need for a new technique for attack\nin the partial-information setting, and we discuss our algo-\nrithm for such an attack. Finally, we describe our method\nfor attacking a classiﬁer with access only to a sorted list of\nthe topk labels (k ≥ 1). We have released full source code\nfor the attacks we describe 4.\nWe deﬁne some notation before introducing the approach.\nThe projection operatorΠ[x−ϵ,x+ϵ](x′) is theℓ∞ projection\nofx′ onto anϵ-ball aroundx. Whenx is clear from context,\nwe abbreviate this as Πϵ(x′), and in pseudocode we denote\nthis projection with the function CLIP(x′,x −ϵ,x +ϵ). We\ndeﬁne the function rank(y|x) to be the smallestk such that\ny is in the top-k classes in the classiﬁcation ofx. We use\nN and U to represent the normal and uniform distributions\nrespectively.\n2.1. Query-Limited Setting\nIn the query-limited setting, the attacker has a query bud-\nget L and aims to cause targeted misclassiﬁcation in L\nqueries or less. To attack this setting, we can use “stan-\ndard” ﬁrst-order techniques for generating adversarial ex-\namples Goodfellow et al. (2015); Papernot et al. (2016b);\nMadry et al. (2017); Carlini & Wagner (2017), substitut-\ning the gradient of the loss function with an estimate of the\ngradient, which is approximated by querying the classiﬁer\nrather than computed by autodifferentiation. This idea is\nused in Chen et al. (2017), where the gradient is estimated\nvia pixel-by-pixel ﬁnite differences, and then the CW at-\ntack (Carlini & Wagner, 2017) is applied. In this section,\nwe detail our algorithm for efﬁciently estimating the gradi-\nent from queries, based on the Natural Evolutionary Strate-\ngies approach of Wierstra et al. (2014), and then state how\nthe estimated gradient is used to generate adversarial ex-\namples.\n2.1.1. N ATURAL EVOLUTIONARY STRATEGIES\nTo estimate the gradient, we use NES (Wierstra et al.,\n2014), a method for derivative-free optimization based on\nthe idea of a search distribution π(θ|x). Rather than max-\nimizing an objective function F (x) directly, NES maxi-\n4https://github.com/labsix/limited-\nblackbox-attacks\nmizes the expected value of the loss function under the\nsearch distribution. This allows for gradient estimation\nin far fewer queries than typical ﬁnite-difference methods.\nFor a loss function F (·) and a current set of parameters x,\nwe have from Wierstra et al. (2014):\nEπ(θ|x) [F (θ)] =\n∫\nF (θ)π(θ|x) dθ\n∇xEπ(θ|x) [F (θ)] = ∇x\n∫\nF (θ)π(θ|x) dθ\n=\n∫\nF (θ)∇xπ(θ|x) dθ\n=\n∫\nF (θ)π(θ|x)\nπ(θ|x) ∇xπ(θ|x) dθ\n=\n∫\nπ(θ|x)F (θ)∇x log (π(θ|x)) dθ\n= Eπ(θ|x) [F (θ)∇x log (π(θ|x))]\nIn a manner similar to that in Wierstra et al. (2014), we\nchoose a search distribution of random Gaussian noise\naround the current image x; that is, we have θ = x +σδ,\nwhere δ ∼ N (0,I ). Like Salimans et al. (2017), we\nemploy antithetic sampling to generate a population of\nδi values: instead of generating n values δi ∼ N (0,I ),\nwe sample Gaussian noise for i ∈ { 1,..., n\n2 } and set\nδj = −δn−j+1 forj ∈ { (n\n2 + 1),...,n }. This optimiza-\ntion has been empirically shown to improve performance of\nNES. Evaluating the gradient with a population ofn points\nsampled under this scheme yields the following variance-\nreduced gradient estimate:\n∇E[F (θ)] ≈ 1\nσn\nn∑\ni=1\nδiF (θ +σδi)\nFinally, we perform a projected gradient descent update\n(Madry et al., 2017) with momentum based on the NES\ngradient estimate.\nThe special case of NES that we have described here can be\nseen as a ﬁnite-differences estimate on a random Gaussian\nbasis.\nGorban et al. (2016) shows that for ann-dimensional space\nand N randomly sampled Gaussian vectors v1...v N , we\ncan lower bound the probability that N random Gaussians\narec-orthogonal:\nN ≤ −e\nc2 n\n4 ln (p)\n1\n2 =⇒ P\n{ vi ·vj\n||vi||||vj|| ≤c ∀ (i,j )\n}\n≥p\nConsidering a matrix Θ with columns δi, NES gives the\nprojection Θ(∇F ), so we can use standard results from\nconcentration theory to analyze our estimate. A more com-\nplex treatment is given in Dasgupta et al. (2006), but using\nBlack-box Adversarial Attacks with Limited Queries and Information\nAlgorithm 1 NES Gradient Estimate\nInput: ClassiﬁerP (y|x) for classy, imagex\nOutput: Estimate of ∇P (y|x)\nParameters: Search variance σ, number of samples n,\nimage dimensionalityN\ng ← 0n\nfori = 1 ton do\nui ← N (0N, IN·N)\ng ←g +P (y|x +σ ·ui) ·ui\ng ←g −P (y|x −σ ·ui) ·ui\nend for\nreturn 1\n2nσg\na straightforward application of the Johnson-Lindenstrauss\nTheorem, we can upper and lower bound the norm of our\nestimated gradient ˆ∇ in terms of the true gradient ∇. As\nσ → 0, we have that:\nP\n{\n(1−δ)||∇||2 ≤ ||ˆ∇||2 ≤ (1+δ)||∇||2\n}\n≥ 1 − 2p\nwhere 0<δ < 1 andN =O(−δ−2 log(p))\nMore rigorous analyses of these “Gaussian-projected ﬁ-\nnite difference” gradient estimates and bounds (Nesterov\n& Spokoiny, 2017) detail the algorithm’s interaction with\ndimensionality, scaling, and various other factors.\n2.1.2. Q UERY-L IMITED ATTACK\nIn the query-limited setting, we use NES as an unbiased,\nefﬁcient gradient estimator, the details of which are given\nin Algorithm 1. Projected gradient descent (PGD) is per-\nformed using the sign of the estimated gradient:\nx(t) = Π[x0−ϵ,x0+ϵ](x(t−1) −η · sign(gt))\nThe algorithm takes hyperparameters η, the step size, and\nN, the number of samples to estimate each gradient. In\nthe query-limited setting with a query limit ofL, we useN\nqueries to estimate each gradient and perform L\nN steps of\nPGD.\n2.2. Partial-Information Setting\nIn the partial-information setting, rather than beginning\nwith the image x, we instead begin with an instance x0\nof the target classyadv, so thatyadv will initially appear in\nthe top-k classes.\nAt each stept, we then alternate between:\n(1) projecting onto ℓ∞ boxes of decreasing sizes ϵt cen-\ntered at the original image x0, maintaining that the adver-\nsarial class remains within the top-k at all times:\nϵt = minϵ′ s.t. rank\n(\nyadv|Πϵ′(x(t−1))\n)\n<k\nAlgorithm 2 Partial Information Attack\nInput: Initial image x, Target class yadv, Classiﬁer\nP (y|x) : Rn × Y → [0, 1]k (access to probabilities fory\nin topk), imagex\nOutput: Adversarial imagexadv with ||xadv −x||∞ ≤ϵ\nParameters: Perturbation bound ϵadv, starting pertur-\nbation ϵ0, NES Parameters (σ,N,n ), epsilon decay δϵ,\nmaximum learning rate ηmax, minimum learning rate\nηmin\nϵ ←ϵ0\nxadv ← image of target classyadv\nxadv ← CLIP(xadv,x −ϵ,x +ϵ)\nwhileϵ>ϵ adv or maxyP (y|x) ̸=yadv do\ng ← NESE STGRAD(P (yadv|xadv))\nη ←ηmax\nˆxadv ←xadv −ηg\nwhile notyadv ∈ TOP-K(P (·|ˆxadv)) do\nifη <ηmin then\nϵ ←ϵ +δϵ\nδϵ ←δϵ/2\nˆxadv ←xadv\nbreak\nend if\nη ← η\n2\nˆxadv ← CLIP(xadv −ηg,x −ϵ,x +ϵ)\nend while\nxadv ← ˆxadv\nϵ ←ϵ −δϵ\nend while\nreturnxadv\n(2) perturbing the image to maximize the probability of the\nadversarial target class,\nx(t) = arg max\nx′\nP (yadv|Πϵt−1(x′))\nWe implement this iterated optimization using backtrack-\ning line search to ﬁndϵt that maintains the adversarial class\nwithin the top-k, and several iterations of projected gradi-\nent descent (PGD) to ﬁnd x(t). Pseudocode is shown in\nAlgorithm 2. Details regarding further optimizations (e.g.\nlearning rate adjustment) can be found in our source code.\n2.3. Label-Only Setting\nNow, we consider the setting where we only assume access\nto the top-k sorted labels. As previously mentioned, we\nexplicitly include the setting wherek = 1 but aim to design\nan algorithm that can incorporate extra information when\nk> 1.\nThe key idea behind our attack is that in the absence of\noutput scores, we ﬁnd an alternate way to characterize the\nsuccess of an adversarial example. First, we deﬁne thedis-\nBlack-box Adversarial Attacks with Limited Queries and Information\ncretized scoreR(x(t)) of an adversarial example to quan-\ntify how adversarial the image is at each stept simply based\non the ranking of the adversarial labelyadv:\nR(x(t)) =k − rank(yadv|x(t))\nAs a proxy for the softmax probability, we consider the ro-\nbustness of the adversarial image to random perturbations\n(uniformly chosen from a ℓ∞ ball of radius µ), using the\ndiscretized score to quantify adversariality:\nS(x(t)) = Eδ∼U [−µ,µ][R(x(t) +δ)]\nWe estimate this proxy score with a Monte Carlo approxi-\nmation:\nˆS(x(t)) = 1\nn\nn∑\ni=1\nR(x(t) +µδi)\nA visual representation of this process is given in Figure 1.\nFigure 1. An illustration of the derivation of the proxy score ˆS in\nthe label-only setting.\nWe proceed to treatˆS(x) as a proxy for the output probabil-\nities P (yadv|x) and use the partial-information technique\nwe introduce in Section 2.2 to ﬁnd an adversarial example\nusing an estimate of the gradient ∇xˆS(x).\n3. Evaluation\nWe evaluate the methods proposed in Section 2 on their ef-\nfectiveness in producing targeted adversarial examples in\nthe three threat models we consider: query-limited, partial-\ninformation, and label-only. First, we present our evalua-\ntion methodology. Then, we present evaluation results for\nour three attacks. Finally, we demonstrate an attack against\na commercial system: the Google Cloud Vision (GCV)\nclassiﬁer.\n3.1. Methodology\nWe evaluate the effectiveness of our attacks against an\nImageNet classiﬁer. We use a pre-trained InceptionV3 net-\nwork (Szegedy et al., 2015) that has 78% top-1 accuracy,\nThreat model Success rate Median queries\nQL 99.2% 11,550\nPI 93.6% 49,624\nLO 90% 2.7 × 106\nTable 1. Quantitative analysis of targetedϵ = 0.05 adversarial at-\ntacks in three different threat models: query-limited (QL), partial-\ninformation (PI), and label-only (LO). We perform attacks over\n1000 randomly chosen test images (100 for label-only) with ran-\ndomly chosen target classes. For each attack, we use the same\nhyperparameters across all images. Here, we report the overall\nsuccess rate (percentage of times the adversarial example was\nclassiﬁed as the target class) and the median number of queries\nrequired.\nand for each attack, we restrict our access to the classiﬁer\naccording to the threat model we are considering.\nFor each evaluation, we randomly choose 1000 images\nfrom the ImageNet test set, and we randomly choose a\ntarget class for each image. We limit ℓ∞ perturbation to\nϵ = 0.05. We use a ﬁxed set of hyperparameters across all\nimages for each attack algorithm, and we run the attack un-\ntil we produce an adversarial example or until we time out\nat a chosen query limit (e.g.L = 106 for the query-limited\nthreat model).\nWe measure the success rate of the attack, where an attack\nis considered successful if the adversarial example is clas-\nsiﬁed as the target class and considered unsuccessful oth-\nerwise (whether it’s classiﬁed as the true class or any other\nincorrect class). This is a strictly harder task than produc-\ning untargeted adversarial examples. We also measure the\nnumber of queries required for each attack.\n3.2. Evaluation on ImageNet\nIn our evaluation, we do not enforce a particular limit on\nthe number of queries as there might be in a real-world\nattack. Instead, we cap the number of queries at a large\nnumber, measure the number of queries required for each\nattack, and present the distribution of the number of queries\nrequired. For both the the partial-information attack and\nthe label-only attack, we consider the special case where\nk = 1, i.e. the attack only has access to the top label. Note\nthat in the partial-information attack the adversary also has\naccess to the probability score of the top label.\nTable 1 summarizes evaluation results our attacks for the\nthree different threat models we consider, and Figure 2\nshows the distribution of the number of queries. Figure 3\nshows a sample of the adversarial examples we produced.\nTable 2 gives our hyperparameters; for each attack, we use\nthe same set of hyperparameters across all images.\nBlack-box Adversarial Attacks with Limited Queries and Information\n0\n50000 100000 150000 200000\nQueries Required\n0\n50\n100\n150\n200\n250\n300\n350\n0\n100000200000300000400000500000600000700000\nQueries Required\n0\n50\n100\n150\n200\n250\n300\nFigure 2. The distribution of the number of queries required for\nthe query-limited (top) and partial-information with k = 1 (bot-\ntom) attacks.\nFigure 3. ϵ = 0 .05 targeted adversarial examples for the\nInceptionV3 network. The top row contains unperturbed images,\nand the bottom row contains corresponding adversarial examples\n(with randomly chosen target classes).\nGeneral\nσ for NES 0.001\nn, size of each NES population 50\nϵ,l∞ distance to the original image 0.05\nη, learning rate 0.01\nPartial-Information Attack\nϵ0, initial distance from source image 0.5\nδϵ, rate at which to decayϵ 0.001\nLabel-Only Attack\nm, number of samples for proxy score 50\nµ,ℓ∞ radius of sampling ball 0.001\nTable 2. Hyperparameters used for evaluation\n3.3. Real-world attack on Google Cloud Vision\nTo demonstrate the relevance and applicability of our ap-\nproach to real-world systems, we attack the Google Cloud\nVision (GCV) API, a publicly available computer vision\nsuite offered by Google. We attack the most general object\nlabeling classiﬁer, which performs n-way classiﬁcation on\nimages. Attacking GCV is considerably more challenging\nthan attacking a system in the typical black-box setting be-\ncause of the following properties:\n• The number of classes is large and unknown — a full\nenumeration of labels is unavailable.\n• The classiﬁer returns “conﬁdence scores” for each la-\nbel it assigns to an image, which seem to be neither\nprobabilities nor logits.\n• The classiﬁer does not return scores for all labels, but\ninstead returns an unspeciﬁed-length list of labels that\nvaries based on image.\nThis closely mirrors our partial-information threat model,\nwith the additional challenges that a full list of classes is\nunavailable and the length of the results is unspeciﬁed and\nvaries based on the input. Despite these challenges, we suc-\nceed in constructing targeted adversarial examples against\nthis classiﬁer.\nFigure 4 shows an unperturbed image being correctly la-\nbeled as several skiing-related classes, including “skiing”\nand “ski.” We run our partial-information attack to force\nthis image to be classiﬁed as “dog” (an arbitrarily chosen\ntarget class). Note that the label “dog” does not appear in\nthe output for the unperturbed image. Using our partial-\ninformation algorithm, we initialize our attack with a pho-\ntograph of a dog (classiﬁed by GCV as a dog) and success-\nfully synthesize an image that looks like the skiers but is\nBlack-box Adversarial Attacks with Limited Queries and Information\nFigure 4. The Google Cloud Vision Demo labeling on the unper-\nturbed image.\nFigure 5. The Google Cloud Vision Demo labeling on the ad-\nversarial image generated with ℓ∞ bounded perturbation with\nϵ = 0.1: the image is labeled as the target class.\nclassiﬁed as “dog,” as shown in Figure 5 5.\n4. Related work\nBiggio et al. (2012) and Szegedy et al. (2013) discovered\nthat machine learning classiﬁers are vulnerable to adversar-\nial examples. Since then, a number of techniques have been\ndeveloped to generate adversarial examples in the white-\nbox case (Goodfellow et al., 2015; Carlini & Wagner, 2017;\nMoosavi-Dezfooli et al., 2016; Moosavi-Dezfooli et al.,\n2017; Hayes & Danezis, 2017), where an attacker has full\naccess to the model parameters and architecture.\nIn this section, we focus on prior work that speciﬁcally ad-\ndress the black-box case and practical attack settings more\ngenerally and compare them to our contributions. Through-\nout this section, it is useful to keep in the mind the axes for\ncomparison: (1) white-box vs. black-box; (2) access to\ntrain-time information + query access vs. only query ac-\ncess; (3) the scale of the targeted model and the dataset it\nwas trained on (MNIST vs. CIFAR-10 vs. ImageNet); (4)\nuntargeted vs. targeted.\n5https://www.youtube.com/watch?v=\n1h9bU7WBTUg demonstrates our algorithm transforming\nthe image of a dog into an image of the skier while retaining the\noriginal classiﬁcation\n4.1. Black-box adversarial attacks\nSeveral papers have investigated practical black-box at-\ntacks on real-world systems such as speech recognition sys-\ntems (Carlini et al., 2016), malware detectors (Hu & Tan,\n2017; Xu et al., 2016), and face recognition systems (Sharif\net al., 2017). Current black-box attacks use either substitute\nnetworks or gradient estimation techniques.\n4.1.1. B LACK -BOX ATTACKS WITH SUBSTITUTE\nNETWORKS\nOne approach to generating adversarial examples in the\nblack-box case is with a substitute model, where an ad-\nversary trains a new model with synthesized data labeled\nby using the target model as an oracle. Adversarial exam-\nples can then be generated for the substitute with white-\nbox methods, and they will often transfer to the target\nmodel, even if it has a different architecture or training\ndataset (Szegedy et al., 2013; Goodfellow et al., 2015).\nPapernot et al. (2016a; 2017) have successfully used this\nmethod to attack commercial classiﬁers like the Google\nCloud Prediction API, the Amazon Web Services Oracle,\nand the MetaMind API, even evading various defenses\nagainst adversarial attacks. A notable subtlety is that the\nGoogle Cloud Vision API 6 we attack in this work is not\nthe same as the Google Cloud Prediction API 7 (now the\nGoogle Cloud Machine Learning Engine) attacked in Pa-\npernot et al. (2016a; 2017). Both systems are black-box,\nbut the Prediction API is intended to be trained with the\nuser’s own data, while the Cloud Vision API has been\ntrained on large amounts of Google’s own data and works\n“out-of-the-box.” In the black-box threat model we con-\nsider in our work, the adversary does not have access to the\ninternals of the model architecture and has no knowledge\nof how the model was trained or what datasets were used.\nPapernot et al. (2016a; 2017) trained the Cloud Predic-\ntion API with small datasets like MNIST and successfully\ndemonstrated an untargeted attack. As Liu et al. (2017)\ndemonstrated, it is more difﬁcult to transfer targeted ad-\nversarial examples with or without their target labels, par-\nticularly when attacking models trained on large datasets\nlike ImageNet. Using ensemble-based methods, Liu et al.\n(2017) overcame these limitations to attack the Clarifai\nAPI. Their threat model speciﬁes that the adversary does\nnot have any knowledge of the targeted model, its training\nprocess, or training and testing data, matching our deﬁni-\ntion of black-box. While Liu et al.’s substitute network at-\ntack does not require any queries to the target model (the\nmodels in the ensemble are all trained on ImageNet), only\n18% of the targeted adversarial examples generated by the\nensemble model are transferable in the Clarifai attack. In\n6https://cloud.google.com/vision/\n7https://cloud.google.com/prediction/docs/\nBlack-box Adversarial Attacks with Limited Queries and Information\ncontrast, our method needs to query the model many times\nto perform a similar attack but has better guarantees that an\nadversarial example will be generated successfully (94%\neven in the partial-information case, and over 99% in the\nstandard black-box setting).\n4.1.2. B LACK -BOX ATTACKS WITH GRADIENT\nESTIMATION\nChen et al. (2017) explore black-box gradient estimation\nmethods as an alternative to substitute networks, where\nwe have noted that transferability is not always reliable.\nThey work under the same threat model, restricting an\nadversary solely to querying the target model as an or-\nacle. As they note, applying zeroth order optimization\nnaively in this case is not a tractable solution, requiring\n2 × 299 × 299 × 3 = 536406 queries to estimate the gradi-\nents with respect to all pixels. To resolve this problem, they\ndevise an iterative coordinate descent procedure to decrease\nthe number of evaluations needed and successfully perform\nuntargeted and targeted attacks on MNIST and CIFAR-10\nand untargeted attacks on ImageNet. Although we do not\nprovide a direct comparison due to the incompability of the\nℓ2 andℓ∞ metric as well as the ﬁxed-budget nature of the\noptimization algorithm in Chen et al. (2017), our method\ntakes far fewer queries to generate imperceptible adversar-\nial examples.\nNarodytska & Kasiviswanathan (2017) propose a black-\nbox gradient estimation attack using a local-search based\ntechnique, showing that perturbing only a small fraction of\npixels in an image is often sufﬁcient for it to be misclassi-\nﬁed. They successfully perform targeted black-box attacks\non an ImageNet classiﬁer with only query access and ad-\nditionally with a more constrained threat model where an\nadversary only has access to a “proxy” model. For the most\nsuccessful misclassiﬁcation attack on CIFAR-10 (70% suc-\ncess) the method takes 17,000 queries on average. Targeted\nadversarial attacks on ImageNet are not considered.\n4.2. Adversarial attacks with limited information\nOur work is concurrent with Brendel et al. (2018), which\nalso explores the label-only case using their “Boundary At-\ntack,” which is similar to our two-step partial information\nalgorithm. Starting with an image of the target adversarial\nclass, they alternate between taking steps on the decision\nboundary to maintain the adversarial classiﬁcation of the\nimage and taking steps towards the original image.\n4.3. Other adversarial attacks\nSeveral notable works in adversarial examples use simi-\nlar techniques but with different adversarial goals or threat\nmodels. Xu et al. (2016) explore black-box adversarial ex-\namples to fool PDF malware classiﬁers. To generate an\nadversarial PDF, they start with an instance of a malicious\nPDF and use genetic algorithms to evolve it into a PDF that\nis classiﬁed as benign but preserves its malicious behavior.\nThis attack is similar in spirit to our partial-information al-\ngorithm, although our technique (NES) is more similar to\ntraditional gradient-based techniques than evolutionary al-\ngorithms, and we consider multiway image classiﬁers un-\nder a wider set of threat models rather than binary classi-\nﬁers for PDFs. Nguyen et al. (2014) is another work that\nuses genetic algorithms and gradient ascent to produce im-\nages that fool a classiﬁer, but their adversarial goal is dif-\nferent: instead of aiming to make a interpretable image of\nsome class (e.g. skiiers) be misclassiﬁed as another class\n(e.g. a dog), they generate entirely unrecognizable images\nof noise or abstract patterns that are classiﬁed as a paricu-\nlar class. Another work generates adversarial examples by\ninverting the image instead of taking local steps; their goal\nis to show that CNNs do not generalize to inverted images,\nrather than to demonstrate a novel attack or to consider a\nnew threat model (Hosseini et al., 2017).\n5. Conclusion\nOur work deﬁnes three new black-box threat models that\ncharacterize many real world systems: the query-limited\nsetting, partial-information setting, and the label-only set-\nting. We introduce new algorithms for attacking classiﬁers\nunder each of these threat models and show the effective-\nness of these algorithms by attacking an ImageNet classi-\nﬁer. Finally, we demonstrate targeted adversarial examples\nfor the Google Cloud Vision API, showing that our meth-\nods enable black-box attacks on real-world systems in chal-\nlenging settings. Our results suggest that machine learning\nsystems remain vulnerable even with limited queries and\ninformation.\nAcknowledgements\nWe wish to thank Nat Friedman and Daniel Gross for pro-\nviding compute resources for this work.\nReferences\nAthalye, A., Engstrom, L., Ilyas, A., and Kwok, K. Synthesizing\nrobust adversarial examples. 2017. URL https://arxiv.\norg/abs/1707.07397.\nBiggio, B., Nelson, B., and Laskov, P. Poisoning attacks against\nsupport vector machines. In Proceedings of the 29th Inter-\nnational Coference on International Conference on Machine\nLearning, ICML’12, pp. 1467–1474, 2012. ISBN 978-1-4503-\n1285-1. URL http://dl.acm.org/citation.cfm?\nid=3042573.3042761.\nBiggio, B., Corona, I., Maiorca, D., Nelson, B., ˇSrndi´c, N.,\nLaskov, P., Giacinto, G., and Roli, F. Evasion attacks against\nmachine learning at test time. In Joint European Conference\nBlack-box Adversarial Attacks with Limited Queries and Information\non Machine Learning and Knowledge Discovery in Databases,\npp. 387–402. Springer, 2013.\nBrendel, W., Rauber, J., and Bethge, M. Decision-based adversar-\nial attacks: Reliable attacks against black-box machine learn-\ning models. In Proceedings of the International Conference\non Learning Representations (ICLR) , 2018. URL https:\n//arxiv.org/abs/1712.04248.\nCarlini, N. and Wagner, D. Towards evaluating the robustness of\nneural networks. In IEEE Symposium on Security & Privacy ,\n2017.\nCarlini, N., Mishra, P., Vaidya, T., Zhang, Y ., Sherr, M., Shields,\nC., Wagner, D., and Zhou, W. Hidden voice commands. In25th\nUSENIX Security Symposium (USENIX Security 16), Austin,\nTX, 2016.\nChen, P.-Y ., Zhang, H., Sharma, Y ., Yi, J., and Hsieh, C.-\nJ. Zoo: Zeroth order optimization based black-box attacks\nto deep neural networks without training substitute models.\nIn Proceedings of the 10th ACM Workshop on Artiﬁcial In-\ntelligence and Security , AISec ’17, pp. 15–26, New York,\nNY , USA, 2017. ACM. ISBN 978-1-4503-5202-4. doi:\n10.1145/3128572.3140448. URL http://doi.acm.org/\n10.1145/3128572.3140448.\nDasgupta, S., Hsu, D., and Verma, N. A concentration theorem\nfor projections. In Conference on Uncertainty in Artiﬁcial In-\ntelligence, 2006.\nEvtimov, I., Eykholt, K., Fernandes, E., Kohno, T., Li, B.,\nPrakash, A., Rahmati, A., and Song, D. Robust physical-world\nattacks on machine learning models. CoRR, abs/1707.08945,\n2017.\nGoodfellow, I. J., Shlens, J., and Szegedy, C. Explaining and har-\nnessing adversarial examples. In Proceedings of the Interna-\ntional Conference on Learning Representations (ICLR), 2015.\nGorban, A. N., Tyukin, I. Y ., Prokhorov, D. V ., and Sofeikov,\nK. I. Approximation with random bases. Inf. Sci., 364(C):\n129–145, October 2016. ISSN 0020-0255. doi: 10.1016/j.ins.\n2015.09.021. URL http://dx.doi.org/10.1016/j.\nins.2015.09.021.\nHayes, J. and Danezis, G. Machine learning as an adversar-\nial service: Learning black-box adversarial examples. CoRR,\nabs/1708.05207, 2017.\nHosseini, H., Xiao, B., Jaiswal, M., and Poovendran, R. On the\nlimitation of convolutional neural networks in recognizing neg-\native images. 2017 16th IEEE International Conference on\nMachine Learning and Applications (ICMLA) , pp. 352–358,\n2017.\nHu, W. and Tan, Y . Black-box attacks against RNN based mal-\nware detection algorithms. CoRR, abs/1705.08131, 2017.\nKurakin, A., Goodfellow, I., and Bengio, S. Adversarial examples\nin the physical world. 2016. URL https://arxiv.org/\nabs/1607.02533.\nLiu, Y ., Chen, X., Liu, C., and Song, D. Delving into transferable\nadversarial examples and black-box attacks. In Proceedings\nof the International Conference on Learning Representations\n(ICLR), 2017.\nMadry, A., Makelov, A., Schmidt, L., Tsipras, D., and Vladu, A.\nTowards deep learning models resistant to adversarial attacks.\n2017. URL https://arxiv.org/abs/1706.06083.\nMoosavi-Dezfooli, S., Fawzi, A., Fawzi, O., and Frossard, P. Uni-\nversal adversarial perturbations. In CVPR, pp. 86–94. IEEE\nComputer Society, 2017.\nMoosavi-Dezfooli, S.-M., Fawzi, A., and Frossard, P. Deepfool:\na simple and accurate method to fool deep neural networks. In\nIEEE Conference on Computer Vision and Pattern Recognition\n(CVPR), 2016.\nNarodytska, N. and Kasiviswanathan, S. P. Simple black-box ad-\nversarial perturbations for deep networks. In IEEE Conference\non Computer Vision and Pattern Recognition (CVPR), 2017.\nNesterov, Y . and Spokoiny, V . Random gradient-free minimiza-\ntion of convex functions. Found. Comput. Math., 17(2):527–\n566, April 2017. ISSN 1615-3375. doi: 10.1007/s10208-015-\n9296-2. URL https://doi.org/10.1007/s10208-\n015-9296-2.\nNguyen, A. M., Yosinski, J., and Clune, J. Deep neural networks\nare easily fooled: High conﬁdence predictions for unrecogniz-\nable images. CoRR, abs/1412.1897, 2014.\nPapernot, N., McDaniel, P., and Goodfellow, I. Transferability in\nmachine learning: from phenomena to black-box attacks using\nadversarial samples. 2016a.\nPapernot, N., McDaniel, P., Jha, S., Fredrikson, M., Celik, Z. B.,\nand Swami, A. The limitations of deep learning in adversarial\nsettings. In IEEE European Symposium on Security & Privacy,\n2016b.\nPapernot, N., McDaniel, P., Goodfellow, I., Jha, S., Celik, Z. B.,\nand Swami, A. Practical black-box attacks against machine\nlearning. In Proceedings of the 2017 ACM on Asia Conference\non Computer and Communications Security , ASIA CCS ’17,\npp. 506–519, New York, NY , USA, 2017. ACM. ISBN 978-1-\n4503-4944-4. doi: 10.1145/3052973.3053009. URL http:\n//doi.acm.org/10.1145/3052973.3053009.\nSalimans, T., Ho, J., Chen, X., and Sutskever, I. Evolution strate-\ngies as a scalable alternative to reinforcement learning. CoRR,\nabs/1703.03864, 2017. URL http://arxiv.org/abs/\n1703.03864.\nSharif, M., Bhagavatula, S., Bauer, L., and Reiter, M. K. Adver-\nsarial generative nets: Neural network attacks on state-of-the-\nart face recognition. 2017.\nSzegedy, C., Zaremba, W., Sutskever, I., Bruna, J., Erhan, D.,\nGoodfellow, I., and Fergus, R. Intriguing properties of neu-\nral networks. 2013. URL https://arxiv.org/abs/\n1312.6199.\nSzegedy, C., Vanhoucke, V ., Ioffe, S., Shlens, J., and Wojna,\nZ. Rethinking the inception architecture for computer vision.\n2015. URL https://arxiv.org/abs/1512.00567.\nWierstra, D., Schaul, T., Glasmachers, T., Sun, Y ., Peters, J.,\nand Schmidhuber, J. Natural evolution strategies. J. Mach.\nLearn. Res. , 15(1):949–980, January 2014. ISSN 1532-\n4435. URL http://dl.acm.org/citation.cfm?\nid=2627435.2638566.\nBlack-box Adversarial Attacks with Limited Queries and Information\nXu, W., Yi, Y ., and Evans, D. Automatically evading classiﬁers: A\ncase study on pdf malware classiﬁers.Network and Distributed\nSystem Security Symposium (NDSS), 2016.",
  "values": {
    "Justice": "Yes",
    "Privacy": "Yes",
    "Fairness": "Yes",
    "Transparent (to users)": "Yes",
    "User influence": "Yes",
    "Interpretable (to users)": "Yes",
    "Autonomy (power to decide)": "Yes",
    "Collective influence": "Yes",
    "Respect for Law and public interest": "Yes",
    "Deferral to humans": "Yes",
    "Respect for Persons": "Yes",
    "Not socially biased": "Yes",
    "Beneficence": "Yes",
    "Non-maleficence": "Yes",
    "Critiqability": "Yes",
    "Explicability": "Yes"
  }
}