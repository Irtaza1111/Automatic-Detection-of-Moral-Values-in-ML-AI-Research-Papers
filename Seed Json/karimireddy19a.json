{
  "pdf": "karimireddy19a",
  "title": "Error Feedback Fixes SignSGD and other Gradient Compression Schemes",
  "author": "Sai Praneeth Karimireddy, Quentin Rebjock, Sebastian U. Stich, Martin Jaggi",
  "paper_id": "karimireddy19a",
  "text": "Error Feedback Fixes SignSGD and other Gradient Compression Schemes\nSai Praneeth Karimireddy 1 Quentin Rebjock 1 Sebastian U. Stich 1 Martin Jaggi 1\nAbstract\nSign-based algorithms (e.g. SIGN SGD) have been\nproposed as a biased gradient compression tech-\nnique to alleviate the communication bottleneck\nin training large neural networks across multi-\nple workers. We show simple convex counter-\nexamples where signSGD does not converge to\nthe optimum. Further, even when it does converge,\nsignSGD may generalize poorly when compared\nwith SGD. These issues arise because of the bi-\nased nature of the sign compression operator.\nWe then show that using error-feedback, i.e. in-\ncorporating the error made by the compression\noperator into the next step, overcomes these is-\nsues. We prove that our algorithm ( EF-SGD )\nwith arbitrary compression operator achieves the\nsame rate of convergence as SGD without any\nadditional assumptions. Thus EF-SGD achieves\ngradient compression for free. Our experiments\nthoroughly substantiate the theory.\n1. Introduction\nStochastic optimization algorithms (Bottou, 2010) which are\namenable to large-scale parallelization, taking advantage of\nmassive computational resources (Krizhevsky et al., 2012;\nDean et al., 2012) have been at the core of signiﬁcant recent\nprogress in deep learning (Schmidhuber, 2015; LeCun et al.,\n2015). One such example is the SIGN SGD algorithm and\nits variants, c.f. (Seide et al., 2014; Bernstein et al., 2018;\n2019).\nTo minimize a continuous (possibly) non-convex function\nf : Rd→ R, the classic stochastic gradient algorithm (SGD)\n(Robbins & Monro, 1951) performs iterations of the form\nxt+1 := xt−γ gt, (SGD)\nwhereγ∈ R is the step-size (or learning-rate) and gt is the\nstochastic gradient such that E[gt] =∇f(xt).\n1EPFL, Switzerland. Correspondence to: Sai Praneeth Karim-\nireddy <sai.karimireddy@epﬂ.ch>.\nProceedings of the 36 th International Conference on Machine\nLearning, Long Beach, California, PMLR 97, 2019. Copyright\n2019 by the author(s).\nAlgorithm 1 EF-SIGN SGD ( SIGN SGD with Error-Feedb.)\n1: Input: learning rateγ, initial iterate x0∈ Rd, e0 = 0\n2: fort = 0,...,T − 1 do\n3: gt := stochasticGradient(xt)\n4: pt :=γgt + et ⊿ error correction\n5: ∆t := (∥pt∥1/d) sign(pt) ⊿ compression\n6: xt+1 := xt− ∆t ⊿ update iterate\n7: et+1 := pt− ∆t ⊿ update residual error\n8: end for\nMethods performing updates only based on the sign of each\ncoordinate of the gradient have recently gaining popularity\nfor training deep learning models (Seide et al., 2014; Carl-\nson et al., 2015; Wen et al., 2017; Balles & Hennig, 2018;\nBernstein et al., 2018; Zaheer et al., 2018; Liu et al., 2018).\nFor example, the update step of SIGN SGD is given by:\nxt+1 := xt−γ sign(gt). (SIGN SGD)\nSuch sign-based algorithms are particularly interesting since\nthey can be viewed through two lenses: as i) approximations\nof adaptive gradient methods such as ADAM (Balles & Hen-\nnig, 2018), and also a ii) communication efﬁcient gradient\ncompression scheme (Seide et al., 2014). However, we show\nthat a severe handicap of sign-based algorithms is that they\ndo not converge in general. To substantiate this claim, we\npresent in this work simple convex counter-examples where\nSIGN SGD cannot converge. The main reasons being that\nthe sign operator loses information about, i.e. ‘forgets’, i)\nthe magnitude, as well as ii) the direction of gt. We present\nan elegant solution that provably ﬁxes these problems of\nSIGN SGD, namely algorithms with error-feedback.\nError-feedback. We demonstrate that the aforemen-\ntioned problems of SIGN SGD can be ﬁxed by i) scaling\nthe signed vector by the norm of the gradient to ensure the\nmagnitude of the gradient is not forgotten, and ii) locally\nstoring the difference between the actual and compressed\ngradient, and iii) adding it back into the next step so that\nthe correct direction is not forgotten. We call our ‘ﬁxed’\nmethod EF-SIGN SGD (Algorithm 1).\nIn Algorithm 1, et denotes the accumulated error from all\nquantization/compression steps. This residual error is added\nto the gradient stepγgt to obtain the corrected direction pt.\nError Feedback Fixes SignSGD\nWhen compressing pt, the signed vector is again scaled\nby∥pt∥1 and hence does not lose information about the\nmagnitude. Note that our algorithm does not introduce any\nadditional parameters and requires only the step-sizeγ.\nOur contributions. We show that naively using biased\ngradient compression schemes (such as e.g. SIGN SGD )\ncan lead to algorithms which may not generalize or even\nconverge. We show both theoretically and experimentally\nthat simply adding error-feedback solves such problems and\nrecovers the performance of full SGD, thereby saving on\ncommunication costs. We state our results for SIGN SGD to\nease our exposition but our positive results are valid for gen-\neral compression schemes, and our counterexamples extend\nto SIGN SGD with momentum, multiple worker settings, and\neven other biased compression schemes. More speciﬁcally\nour contributions are:\n1. We construct a simple convex non-smooth counterex-\nample where SIGN SGD cannot converge, even with\nthe full batch (sub)-gradient and tuning the step-size.\nAnother counterexample for a wide class of smooth\nconvex functions proves thatSIGN SGD with stochastic\ngradients cannot converge with batch-size one.\n2. We prove that by incorporating error-feedback,\nSIGN SGD —as well as any other gradient compres-\nsion schemes—always converge. Further, our analysis\nfor non-convex smooth functions recovers the same\nrate as SGD, i.e. we get gradient compression for free.\n3. We show that our algorithm EF-SIGN SGD which in-\ncorporates error-feedback approaches the linear span\nof the past gradients. Therefore, unlike SIGN SGD ,\nEF-SIGN SGD converges to the max-margin solution\nin over-parameterized least-squares. This provides a\ntheoretical justiﬁcation for why EF-SIGN SGD can be\nexpected to generalize much better than SIGN SGD.\n4. We show extensive experiments on CIFAR10 and CI-\nFAR100 using Resnet and VGG architectures demon-\nstrating that EF-SIGN SGD indeed signiﬁcantly outper-\nforms SIGN SGD , and matches SGD both on test as\nwell as train datasets while reducing communication\nby a factor of∼ 64×.\n2. Signiﬁcance and Related Work\nRelation to adaptive methods. Introduced in (Kingma\n& Ba, 2015), ADAM has gained immense popularity as\nthe algorithm of choice for adaptive stochastic optimization\nfor its perceived lack of need for parameter-tuning. How-\never since, the convergence (Reddi et al., 2018) as well the\ngeneralization performance (Wilson et al., 2017) of such\nadaptive algorithms has been called into question. Under-\nstanding when ADAM performs poorly and providing a\nprincipled ‘ﬁx’ for these cases is crucial given its impor-\ntance as the algorithm of choice for many researchers. It\nwas recently noted by Balles & Hennig (2018) that the be-\nhavior of ADAM is in fact identical to that of SIGN SGD\nwith momentum: More formally, the SIGN SGD M algorithm\n(referred to as ’signum’ by Bernstein et al. (2018; 2019))\nadds momentum to the SIGN SGD update as:\nmt+1 := gt +βmt\nxt+1 := xt−γ sign(mt+1). (SIGN SGD M)\nfor parameter β > 0. This connection between signed\nmethods and fast stochastic algorithms is not surprising\nsince sign-based gradient methods were ﬁrst studied as a\nway to speed up SGD (Riedmiller & Braun, 1993). Given\ntheir similarity, understanding the behavior of SIGN SGD\nand SIGN SGD M can help shed light on ADAM.\nRelation to gradient compression methods. As the size\nof the models keeps getting bigger, the training process can\noften take days or even weeks (Dean et al., 2012). This\nprocess can be signiﬁcantly accelerated by massive paral-\nlelization (Li et al., 2014; Goyal et al., 2017). However, at\nthese scales communication of the gradients between the\nmachines becomes a bottleneck hindering us from making\nfull use of the impressive computational resources available\nin today’s data centers (Chilimbi et al., 2014; Seide et al.,\n2014; Strom, 2015). A simple solution to alleviate this bot-\ntleneck is to compress the gradient and reduce the number of\nbits transmitted. While the analyses of such methods have\nlargely been restricted to unbiased compression schemes\n(Alistarh et al., 2017; Wen et al., 2017; Wang et al., 2018),\nbiased schemes which perform extreme compression practi-\ncally perform much better (Seide et al., 2014; Strom, 2015;\nLin et al., 2018)—often without any loss in convergence or\naccuracy. Of these, (Seide et al., 2014; Strom, 2015; Wen\net al., 2017) are all sign-based compression schemes. Inter-\nestingly, all the practical works (Seide et al., 2014; Strom,\n2015; Lin et al., 2018) use some form of error-feedback.\nError-feedback. The idea of error-feedback was, as far\nas we are aware, ﬁrst introduced in 1-bit SGD (Seide et al.,\n2014; Strom, 2015). The algorithm 1-bit SGD is very similar\nto our EF-SIGN SGD algorithm, but tailored for the speciﬁc\nrecurrent network studied there. Though not presented as\nsuch, the ‘momentum correction’ used in (Lin et al., 2018)\nis a variant of error-feedback. However the error-feedback\nis not on the vanilla SGD algorithm, but on SGD with mo-\nmentum. Recently, Stich et al. (2018) conducted the ﬁrst\ntheoretical analysis of error-feedback for compressed gradi-\nent methods (they call it ‘memory’) in the strongly convex\ncase. Our convergence results can be seen as an extension\nof theirs to the non-convex and weakly convex cases.\nError Feedback Fixes SignSGD\nGeneralization of deep learning methods. Deep net-\nworks are almost always over-parameterized and are known\nto be able to ﬁt arbitrary data and always achieve zero train-\ning error (Zhang et al., 2017). This ability of deep networks\nto generalize well on real data, while simultaneously being\nable to ﬁt arbitrary data has recently received a lot of atten-\ntion (e.g. Soudry et al. (2018); Dinh et al. (2017); Zhang\net al. (2018); Arpit et al. (2017); Kawaguchi et al. (2017)).\nSIGN SGD and ADAM are empirically known to general-\nize worse than SGD (Wilson et al., 2017; Balles & Hennig,\n2018). A number of recent papers try close this gap for\nADAM. Luo et al. (2019) show that by bounding the adap-\ntive step-sizes in ADAM leads to closing the generalization\ngap. They require new hyper-parameters on top of ADAM\nto adaptively tune these bounds on the step-sizes. Chen &\nGu (2019) interpolate between SGD and ADAM using a\nnew hyper-parameterp and show that tuning this can recover\nperformance of SGD. Zaheer et al. (2018) introduce a new\nadaptive algorithm which is closer to Adagrad (Duchi et al.,\n2011). Similarly, well-tuned ADAM (where all the hyper-\nparameters and not just the learning rate are tuned) is also\nknown to close the generalization gap (Gugger & Howard,\n2018). In all of these algorithms, new hyper-parameters\nare introduced which essentially control the effect of the\nadaptivity. Thus they require additional tuning while the\nimprovement upon traditional SGD is questionable. We are\nnot aware of other work bridging the generalization gap in\nsign-based methods.\n3. Counterexamples for SignSGD\nIn this section we study the limitations of SIGN SGD . Un-\nder benign conditions—for example if i) the functionf is\nsmooth, and ii) the stochastic noise is gaussian or an ex-\ntemely large batch-size is used (equal to the total number of\niterations)—the algorithm can be shown to converge (Bern-\nstein et al., 2018; 2019). However, we show that SIGN SGD\ndoes not converge under more standard assumptions. We\ndemonstrate this ﬁrst on a few pedagogic examples and later\nalso for realistic and general sum-structured loss functions.\nIf we use a ﬁxed step-size γ ≥ 0, SIGN SGD does not\nconverge even for simple one-dimensional linear functions.\nCounterexample 1. Forx∈ R consider the constrained\nproblem\nmin\nx∈[−1,1]\n[\nf(x) := 1\n4x\n]\n,\nwith minimum at x⋆ =−1. Assume stochastic gradients\nare given as (note thatf(x) = 1\n4(4x−x−x−x))\ng =\n{\n4, with prob. 1\n4\n−1, with prob. 3\n4\nwith E[g] =∇f(x).\nFor SGD with any step-sizeγ,\nEt[f(xt+1)] = 1\n4(xt−γ E[g]) =f(xt)− γ\n16.\nOn the other hand, for SIGN SGD with any ﬁxed γ,\nEt[f(xt+1)] = 1\n4(xt−γ E[sign(g)]) =f(xt) +γ\n8,\ni.e. the objective function increases in expectation forγ≥ 0.\nRemark 1. In the above example, we exploit that the sign\noperator loses track of the magnitude of the stochastic gra-\ndient. Also note that our noise is bimodal. The counter-\nexamples for the convergence of ADAM (Reddi et al., 2018;\nLuo et al., 2019) also use similar ideas. Such examples were\npreviouslyt noted for SIGN SGD by (Bernstein et al., 2019).\nIn the example above the step-size γ was ﬁxed. However\nincreasing batch-size or tuning the step-size may still allow\nconvergence. Next we show that even with adaptive step-\nsizes (e.g. decreasing, or adaptively chosen optimal step-\nsizes) SIGN SGD does not converge. This even holds if the\nfull (sub)-gradient is available (non-stochastic case).\nCounterexample 2. For x∈ R2 consider the following\nnon-smooth convex problem with x⋆ = (0, 0)⊤:\nmin\nx∈R2\n[\nf(x) :=ϵ|x1 +x2| +|x1−x2|\n]\n,\nfor parameter 0<ϵ< 1 and subgradient\ng(x) = sign(x1 +x2)·ϵ\n(1\n1\n)\n+ sign(x1−x2)\n( 1\n−1\n)\n.\n1\ng\n1\ng2\ns1\ns2\ne1\ne2\nFigure 1: The gradients g\n(in solid black), singed gra-\ndient direction s = sign(g)\n(in dashed black), and the er-\nror e (in red) are plotted for\nϵ = 0.5. SIGN SGD moves\nonly along s = ±(1,−1)\nwhile the error e is ignored.\nSee Fig. 1. The iter-\nates of SIGN SGD started at\nx0 = (1, 1)⊤ lie along the\nline x1 +x2 = 2 . Note\nthat for any x s.t. x1 +\nx2 > 0, sign(g(x)) =\n±(1,−1)⊤, and hencex1+\nx2 remains constant among\nthe iterations of SIGN SGD.\nConsequently, for any step-\nsize sequenceγt,f(xt)≥\nf(x0).\nRemark 2. In this exam-\nple, we exploit the fact\nthat the sign operator is\na biased approximation\nof the gradient—it consis-\ntently ignores the direction\ne = ϵ(1, 1)⊤ (see Fig 1). Tuning the step-size would not\nhelp either.\nOne might wonder if the smooth-case is easier. Unfortu-\nnately, the previous example can easily be extended to show\nthat SIGN SGD with stochastic gradients may not converge\neven for smooth functions.\nError Feedback Fixes SignSGD\nCounterexample 3. For x∈ R2 consider the following\nleast-squares problem with x⋆ = (0, 0)⊤:\nmin\nx∈R2\n[\nf(x) := (⟨a1, x⟩)2 + (⟨a2, x⟩)2]\n, where\na1,2 :=±(1,−1) +ϵ(1, 1),\nfor parameter 0 < ϵ <1 and stochastic gradient g(x) =\n∇x(⟨a1, x⟩)2 with prob. 1\n2 and g(x) =∇x(⟨a2, x⟩)2 with\nprob. 1\n2. The stochastic gradient is then either ea1 orea2\nfor some scalar e. Exactly as in the non-smooth case, for\nx0 = (1, 1)⊤, the sign of the gradient sign(g) =±(1,−1).\nHence SIGN SGD with any step-size sequence remains stuck\nalong the linex1 +x2 = 2 andf(xt)≥f(x0) a.s.\nWe can generalize the above counter-example to arbitrary\ndimensions and loss functions.\nTheorem I. Suppose that scalar loss functions{li : R→\nR}n\ni=1 and data-points{ai}n\ni=1 ∈ Rd for d≥ 2 satisfy:\ni)f(x) := ∑n\ni=1li(⟨ai, x⟩) has a unique optimum at x⋆,\nand ii) there exists s∈{− 1, 1}d such that sign(ai) =±s\nfor all i. Then SIGN SGD with batch-size 1 and stochas-\ntic gradientsg(x) =∇xli(⟨ai, x⟩) fori chosen uniformly\nat random does not converge to x⋆ a.s. for any adaptive\nsequence of step-sizes, even with random initialization.\n4. Convergence of Compressed Methods\nWe show the rather surprising result that incorporating error-\nfeedback is sufﬁcient to ensure that the algorithm converges\nat a rate which matches that of SGD. In this section we\nconsider a general gradient compression scheme.\nAlgorithm 2 EF-SGD (Compr. SGD with Error-Feedback)\n1: Input: learning rateγ, compressorC(·), x0∈ Rd\n2: Initialize: e0 = 0∈ Rd\n3: fort = 0,...,T − 1 do\n4: gt := stochasticGradient(xt)\n5: pt :=γgt + et ⊿ error correction\n6: ∆t :=C(pt) ⊿ compression\n7: xt+1 := xt− ∆t ⊿ update iterate\n8: et+1 := pt− ∆t ⊿ update residual error\n9: end for\nWe generalize the notion of a compressor from (Stich et al.,\n2018).\nAssumption A (Compressor). An operatorC : Rd→ Rd\nis aδ-approximate compressor overQ forδ∈ (0, 1] if\n∥C(x)− x∥2\n2≤ (1−δ)∥x∥2\n2,∀x∈Q .\nNote that δ = 1 implies that C(x) = x. Examples of\ncompressors include: i) the sign operator, ii) top-k which\nselectsk coordinates with the largest absolute value while\nzero-ing out the rest (Lin et al., 2018; Stich et al., 2018),\niii)k-PCA which approximates a matrix X with its top k\neigenvectors (Wang et al., 2018). Randomized compressors\nsatisfying the assumption in expectation are also allowed.\nWe now state a key lemma that shows that the residual errors\nmaintained in Algorithm 2 do not accumulate too much.\nLemma 3 (Error is bounded). Assume that E[∥gt∥2]≤σ2\nfor allt≥ 0. Then at any iterationt of EF-SGD , the norm\nof the error et in Algorithm 2 is bounded:\nE∥et∥2\n2≤ 4(1−δ)γ2σ2\nδ2 , ∀t≥ 0.\nIfδ = 1, then∥et∥ = 0 and the error is zero as expected.\nWe employ standard assumptions of smoothness of the loss\nfunction and the variance of the stochastic gradient.\nAssumption B (Smoothness). A function f : Rd→ R is\nL-smooth if for all x, y∈ Rd the following holds:\n|f(y)− (f(x) +⟨∇f(x), y− x⟩)|≤ L\n2∥y− x∥2\n2.\nAssumption C (Moment bound). For any x, our query for\na stochastic gradient returns g such that\nE[g] =∇f(x) and E∥g∥2\n2≤σ2.\nGiven these assumptions, we can formally state our theorem\nfollowed by a sketch of the proof.\nTheorem II (Non-convex convergence of EF-SGD ). Let\n{xt}t≥0 denote the iterates of Algorithm 2 for any step-size\nγ >0. Under Assumptions A, B, and C,\nmin\nt∈[T ]\nE[∥∇f(xt)∥2]≤ 2f0\nγ(T + 1)+γLσ 2\n2 +4γ2L2σ2(1−δ)\nδ2 ,\nwithf0 :=f(x0)−f ⋆.\nProof Sketch. Intuitively, the condition that C(·) is a δ-\napproximate compressor implies that at each iteration a\nδ-fraction of the gradient information is sent. The rest is\nadded to et to be transmitted later. Eventually, all the gradi-\nent information is transmitted—albeit with a delay which\ndepends onδ. Thus EF-SGD can intuitively be viewed as\na delayed gradient method. If the function is smooth, the\ngradient does not change quickly and so the delay does not\nsigniﬁcantly matter.\nMore formally, consider the error-corrected sequence ˜xt\nwhich represents xt with the ‘delayed’ information added:\n˜xt := xt− et.\nIt satisﬁes the recurrence\n˜xt+1 = xt− et+1−C (pt) = xt− pt = ˜xt−γgt.\nError Feedback Fixes SignSGD\nIf xt was exactly equal to ˜xt (i.e. there was zero ‘delay’),\nthen we could proceed with the standard proof of SGD. We\ninstead rely on Lemma 3 which shows ˜xt≈ xt and on the\nsmoothness off which shows∇f(xt)≈∇f(˜xt).\nRemark 4. If we substituteγ := 1√T +1 in Theorem II, we\nget\nmin\nt∈[T ]\nE[∥∇f(xt)∥2]≤ 4(f(x0)−f ⋆) +Lσ2\n2\n√\nT + 1 +4L2σ2(1−δ)\nδ2(T + 1)\nIn the above rate, the compression factor δ only appears\nin the higher orderO(1/T ) term. For comparison, SGD\nunder the exact same assumptions achieves\nmin\nt∈[T ]\nE[∥∇f(xt)∥2]≤ 2(f(x0)−f ⋆) +Lσ2\n2\n√\nT + 1 .\nThis means that after T≥O (1/δ2) iterations the second\nterm becomes negligible and the rate of convergence catches\nup with full SGD—this is usually true after just the ﬁrst few\nepochs. Thus we prove that compressing the gradient does\nnot change the asymptotic rate of SGD.1\nRemark 5. The use of error-feedback was motivated by our\ncounter-examples for biased compression schemes. How-\never our rates show that even if using an unbiased com-\npression (e.g. QSGD (Alistarh et al., 2017)), using error-\nfeedback gives signiﬁcantly better rates. Suppose we are\ngiven an unbiased compressorcU(·) such that E[U(x)] = x\nand E\n[\n∥U(x)∥2\n2\n]\n≤k∥x∥2. Then without feedback, using\nstandard analysis (e.g. (Alistarh et al., 2017)) the algorithm\nconvergesk times slower:\nmin\nt∈[T ]\nE[∥∇f(xt)∥2]≤ (f(x0)−f ⋆) +Lkσ2\n2\n√\nT + 1 .\nInstead, if we useC(x) = 1\nkU(x) with error-feedback, we\nwould achieve\nmin\nt∈[T ]\nE[∥∇f(xt)∥2]≤ 4(f(x0)−f ⋆) +Lσ2\n2\n√\nT + 1 +2L2σ2k2\nT + 1 ,\nthereby pushing the dependence onk into the higher order\nO(1/T ) term.\nOur counter-examples showed that biased compressors may\nnot converge for non-smooth functions. Below we prove that\nadding error-feedback ensures convergence under standard\nassumptions even for non-smooth functions.\nTheorem III (Non-smooth convergence of EF-SGD ). Let\n{xt}t≥0 denote the iterates of Algorithm 2 for any step-size\nγ >0 and deﬁne ¯xt = 1\nT\n∑T\nt=0 xt. Given thatf is convex\nand Assumptions A and C hold,\n1The astute reader would have observed that the asymptotic\nrate for EF-SGD is in fact 2 times slower than SGD. This is just\nfor simplicity of presentation and can easily be ﬁxed with a tighter\nanalysis.\nE[f(¯xt)]−f ⋆≤∥x0− x⋆∥2\n2γ(T + 1) +γσ 2\n(1\n2 + 2\n√\n1−δ\nδ\n)\n.\nRemark 6. By picking the optimalγ =O(1/\n√\nT ), we see\nthat\nE[f(¯xt)]−f ⋆≤∥x0− x⋆∥σ\n2\n√\nT + 1\n√\n1 + 4\n√\n1−δ\nδ .\nFor comparison, the rate of convergence under the same\nassumptions for SGD is\nE[f(¯xt)]−f ⋆≤∥x0− x⋆∥σ\n2\n√\nT + 1 .\nFor non-smooth functions, unlike in the smooth case, the\ncompression qualityδ appears directly in the leading term\nof the convergence rate. This is to be expected since we can\nno longer assume that∇f(˜xt)≈∇ f(xt), which formed\nthe crux of our argument for the smooth case.\nRemark 7. Consider the top-1 compressor which just picks\nthe coordinate with the largest absolute value, and ze-\nroes out everything else. It is obvious that top-1 is a 1\nd-\napproximate compressor (cf. (Stich et al., 2018, Lemma\nA.1)). Running EF-SGD withC as top-1 results in a greedy\ncoordinate algorithm. This is the ﬁrst result we are aware\nwhich shows the convergence of a greedy-coordinate type\nalgorithm on non-smooth functions.\nIf the function is both smooth and convex, we can fall back\nto the analysis of (Stich et al., 2018) and so we won’t exam-\nine it in more detail here.\nConvergence of\nEF-SIGN SGD\nFigure 2: The density φ(·)\nfor the stochastic gradients\ngt and the error-corrected\nstochastic gradients gt + et\nfor VGG19 on CIFAR10\nand batchsize 128. Mini-\nmum value is above 0.13.\nSee Sec. 6 for setup.\nWhat do our proven rates\nimply for EF-SIGN SGD\n(Algorithm 1), the method\nof our interest here?\nLemma 8 (Compressed\nsign). The operator\nC(v) := ∥v∥1\nd sign(v) is a\nφ(v) = ∥v∥2\n1\nd∥v∥2\n2\ncompressor.\nWe refer to the quantity φ(v) as the density of v. If the\nvector v had only one non-zero element, the value ofδ for\nEF-SIGN SGD could be as bad as 1/d. However, in deep\nlearning the gradients are usually dense and henceφ(v) is\nmuch larger (see Fig. 2). Note that for our convergence\nrates, it is not the density of the gradient gt which matters\nbut the density of the error-corrected gradient gt + et.\nError Feedback Fixes SignSGD\nFaster convergence than SGD? (Kingma & Ba, 2015)\nand (Bernstein et al., 2018) note that different coordinates\nof the stochastic gradient g may have different variances.\nIn standard SGD, the learning rate γ would be reduced\nto account for the maximum of these coordinate-wise vari-\nances since otherwise the path might be dominated by the\nnoise in these sparse coordinates. Instead, using coordinate-\nwise learning-rates like ADAM does, or using only the\ncoordinate-wise sign of g as SIGN SGD does, might mit-\nigate the effect of such ‘bad’ coordinates by effectively\nscaling down the noisy coordinates. This is purported to be\nthe reason why ADAM and SIGN SGD can be faster than\nSGD on train dataset.\nIn EF-SIGN SGD, the noise from the ‘bad’ coordinates gets\naccumulated in the error-term et and is not forgotten or\nscaled down. Thus, if there are ‘bad’ coordinates whose\nvariance slows down convergence of SGD, EF-SIGN SGD\nshould be similarly slow. Conﬁrming this, in a toy ex-\nperiment with sparse noise (Appendix A.1), SGD and\nEF-SIGN SGD converge at the same slower rate, whereas\nSIGN SGD is faster. However, our real world experiments\ncontradict this—even with the feedback, EF-SIGN SGD is\nconsistently faster than SGD, SIGN SGD , and SIGN SGD M\non training data. Thus the coordinate-wise variance adaption\nexplanation proposed by (Bernstein et al., 2018; Kingma\n& Ba, 2015) does not explain the faster convergence of EF-\nSIGN SGD , and is probably an incomplete explanation of\nwhy sign based and adaptive methods are faster than SGD!\n5. Generalization of SignSGD\nSo far our discussion has mostly focused on the conver-\ngence of the methods i.e. their performance on training data.\nHowever for deep-learning, we actually care about their\nperformance on test data i.e. their generalization. It has\nbeen observed that the optimization algorithm being used\nsigniﬁcantly impacts the properties of the optima reached\n(Im et al., 2016; Li et al., 2018). For instance, ADAM and\nSIGN SGD are known to generalize poorly compared with\nSGD (Wilson et al., 2017; Balles & Hennig, 2018).\nThe proposed explanation for this phenomenon is that in an\nover-parameterized setting, SGD reaches the ‘max-margin’\nsolution wheras ADAM and SIGN SGD do not (Zhang et al.,\n2017; Wilson et al., 2017), and (Balles & Hennig, 2018).\nAs with the issues in convergence, the issues of SIGN SGD\nwith generalization also turn out to be related to the biased\nnature of the sign operator. We explore how error-feedback\nmay also alleviate the issues with generalization for any\ncompression operator.\n5.1. Distance to gradient span\nLike (Zhang et al., 2017; Wilson et al., 2017), we consider\nan over-parameterized least-squares problem\nmin\nx∈Rd\n[\nf(x) :=∥Ax− y∥2\n2\n]\n,\nwhere A∈ Rn×d ford > nis the data matrix and y∈\n{−1, 1}n is the set of labels. The set of solutions X ⋆ :=\n{x: f(x) = 0} of this problem forms a subspace in Rd. Of\nparticular interest is the solution with smallest norm:\narg min\nx :∈X ⋆\n∥x∥2 = A†y = A⊤(\nAA⊤)−1\ny,\nas this corresponds to the maximum margin solution in the\ndual.\nMaximizing margin is known to have a regularizing effect\nand is said to improve generalization (Valiant, 1984; Cortes\n& Vapnik, 1995).\nThe key property that SGD (with or without momentum)\ntrivially satisﬁes is that the iterates always lie in the linear\nspan of the gradients.\nLemma 9. Given any over-parameterized least-squares\nproblem, suppose that the iterates of the algorithm always\nlie in the linear span of the gradients and it converges to a\n0 loss solution. This solution corresponds to the minimum\nnorm/maximum margin solution.\nIf we instead use a biased compressor (e.g. SIGN SGD), it is\nclear that the iterate may not lie in the span of the gradients.\nIn fact it is easy to construct examples where this happens\nfor SIGN SGD (Balles & Hennig, 2018), as well as top- k\nsparsiﬁcation (Gunasekar et al., 2018), perhaps explaining\nthe poor generalization of these schemes. Error-feedback is\nable to overcome this issue as well.\nTheorem IV. Suppose that we run Algorithm 2 for t iter-\nations starting from x0 = 0. Let Gt = [g⊤\n0,...,g ⊤\nt−1]∈\nRd×t denote the matrix of the stochastic gradients and let\nΠGt : Rn→ Im(Gt) denote the projection onto the range\nof Gt.\n∥xt− ΠGt(xt)∥2\n2≤∥ et∥2\n2.\nHere et is the error as deﬁned in Algorithm 2. The theo-\nrem follows directly from observing that (xt+1 + et+1) =\n(x0 +∑t\ni=0γgi), and hence lies in the linear span of the\ngradients.\nRemark 10. Theorem IV along with Lemma 3 implies that\nthe iterates of Algorithm 2 are always close to the linear\nspan of the gradients.\n∥xt− ΠGt(xt)∥2\n2≤ 4γ2(1−δ)\nδ2 max\ni∈[t]\n∥gi∥2.\nThis distance further reduces as the algorithm progresses\nsince the the step-sizeγ is typically reduced.\n5.2. Simulations\nWe compare the generalization of the four algorithms with\nfull batch gradient: i) SGD ii) SIGN SGD , iii) SIGN SGD M,\nand iv) EF-SIGN SGD . The data is generated as in (Wilson\nError Feedback Fixes SignSGD\nFigure 3: Left shows the distance of the iterate from the\nlinear span of the gradients∥xt− ΠGt(xt)∥. The middle\nand the right plots show the train and test loss. SIGN SGD\nand SIGN SGD M have a high distance to the span, and do\nnot generalize (test loss is higher than 0.8). Distance of\nEF-SIGN SGD to the linear span (and the test loss) goes to 0.\net al., 2017) and is randomly split into test and train. The\nstep-sizeγ and (where applicable) the momentum parame-\nterβ are tuned to obtain fastest convergence, but the results\nare representative across parameter choices.\nIn all four cases (Fig. 3), the train loss quickly goes to 0.\nThe distance to the linear span of gradients is quite large\nfor SIGN SGD and SIGN SGD M. For EF-SIGN SGD , ex-\nactly as predicted by our theory, it ﬁrst increases to a cer-\ntain limit and then goes to 0 as the algorithm converges.\nThe test error, almost exactly corresponding to the distance\n∥xt− ΠGt(xt)∥, goes down to 0. SIGN SGD M oscillates\nsigniﬁcantly because of the momentum term, however the\nconclusion remains unchanged—the best test error is higher\nthan 0.8. This indicates that using error-feedback might\nresult in generalization performance comparable with SGD.\n6. Experiments\nWe run experiments on deep networks to test the validity of\nour insights. The results conﬁrm that i) EF-SIGN SGD with\nerror feedback always outperforms the standard SIGN SGD\nand SIGN SGD M, ii) the generalization gap of SIGN SGD\nand SIGN SGD M vs. SGD gets larger for smaller batch sizes,\niii) the performance of EF-SIGN SGD on the other hand is\nmuch closer to SGD, and iv) SIGN SGD behaves erratically\nwhen using small batch-sizes. Our code is available at\ngithub.com/epfml/error-feedback-SGD.\n6.1. Experimental setup\nAll our experiments used the PyTorch framework (Paszke\net al., 2017) on the CIFAR-10/100 dataset (Krizhevsky &\nHinton, 2009). Each experiment is repeated three times\nand the results are reported in Fig 4. Additional details and\nexperiments can be found in Appendix A.\nAlgorithms: We experimentally compared the following\nfour algorithms: i) SGD M which is SGD with momentum,\nii) (scaled)SIGN SGD with step-size scaled by theL1-norm\nof the gradient, iii) SIGN SGD M which is SIGN SGD using\nmomentum, and iv) EF-SIGN SGD (Alg. 1). The scaled\nSIGN SGD performs the update\nxt+1 := xt−γ∥gt∥1\nd sign(gt).\nWe chose to include this in our experiments since we wanted\nto isolate the effects of error-feedback from that of scaling.\nFurther we drop the unscaledSIGN SGD from our discussion\nhere since it was observed to perform worse than the scaled\nversion. As is standard in compression schemes (Seide et al.,\n2014; Lin et al., 2018; Wang et al., 2018), we apply our\ncompression layer-wise. Thus the net communication for\nthe (scaled) SIGN SGD and EF-SIGN SGD is∑l\ni=1(di + 32)\nbits wheredi is the dimension of layeri, andl is the total\nnumber of layers. If the total number of parameters is much\nlarger than the number of layers, then the cost of the extra\n32l bits is negligible—usually the number of parameters is\nthree orders of magnitude more than the number of layers.\nAll algorithms are run for 200 epochs. The learning-rate\nis decimated at 100 epochs and then again at 150 epochs.\nThe initial learning rate is tuned manually (see Appendix A)\nfor all algorithms using batch-size 128. For the smaller\nbatch-sizes, the learning-rate is proportionally reduced as\nsuggested in (Goyal et al., 2017). The momentum parame-\nterβ (where applicable) was ﬁxed to 0.9 and weight decay\nwas left to the default value of 5× 10−4.\nModels: We use the VGG+BN+Dropout network on\nCIFAR-10 (VGG19) from (Simonyan & Zisserman, 2014)\nand Resnet+BN+Dropout network (Resnet18) from (He\net al., 2016a). We adopt the standard data augmentation\nscheme and preprocessing scheme (He et al., 2016a;b).\n6.2. Results\nThe results of the experiments for Resnet18 on Cifar100 are\nshown in Fig. 4 and Table 1. Results for VGG19 on Cifar10\nare also similar and can be found in the Appendix. We make\nfour main observations:\nEF-SIGN SGD is faster than SGD M on train. On the\ntrain dataset, both the accuracy and the losses (Fig. 6) is\nbetter for EF-SIGN SGD than for SGD for all batch-sizes\nand models (Fig. 7). In fact even SIGN SGD is faster than\nSGD M on the train dataset on VGG19 (Fig. 7) for batch-size\n128. As we note in Section 4, the result that the scaled sign\nBatch-size SGD M SIGN SGD SIGN SGD M EF-SIGN SGD\n128 75.35 -2.21 -3.15 -0.92\n32 76.22 -3.04 -3.57 -0.79\n8 74.91 -36.35 -6.6 -0.64\nTable 1: Generalization gap on CIFAR-100 using Resnet18\nfor different batch-sizes. For SGD M we report the best\nmean test accuracy percentage, and for the other algorithms\nwe report their difference to the SGD M accuracy (i.e. the\ngeneralization gap). EF-SIGN SGD has a much smaller gap.\nError Feedback Fixes SignSGD\nFigure 4: Experimental results showing the train and test accuracy percentages on CIFAR-100 using Resnet18 for different\nbatch-sizes. The solid curves represent the mean value and shaded region spans one standard deviation obtained over\nthree replications. Note that the scale of the y-axis varies across the plots. EF-SIGN SGD consistently and signiﬁcantly\noutperforms the other sign-based methods, closely matching the performance of SGD M.\nmethods are also faster than SGD (and in fact faster than\neven the without feedback algorithms) overturns previously\nunderstood intuition (cf. (Kingma & Ba, 2015; Bernstein\net al., 2018)) for why SIGN SGD and other adaptive methods\noutperform SGD—i.e. restricting the effect of a some ‘bad’\ncoordinates with high variance may not be the main reason\nwhy sign based methods are faster than SGD on train.\nEF-SIGN SGD almost matches SGD M on test. On the\ntest dataset (Table 1), the accuracy and the loss is much\ncloser to SGD than the other sign methods across batch-\nsizes and models (Tables 3, 4). The generalization gap\n(both in accuracy and loss) reduces with decreasing batch-\nsize. We believe this is because the learning-rate was scaled\nproportional to the batch-size and smaller learning-rates\nlead to smaller generalization gap, as noted in Remark 10.\nSIGN SGD performs poorly for small batch-sizes. The\nperformance of SIGN SGD is always worse than EF-\nSIGN SGD indicating that scaling is insufﬁcient and that\nerror-feedback is crucial for performance. Further all met-\nrics (train and test, loss and accuracy) increasingly become\nworse as the batch-size decreases indicating that SIGN SGD\nis indeed a brittle algorithm. In fact for batch-size 8, the\nalgorithm becomes extremely unstable.\nSIGN SGD M performs poorly on some datasets and for\nsmaller batch-sizes. We were surprised that the training\nperformance of SIGN SGD M is signiﬁcantly worse than even\nSIGN SGD on CIFAR-100 for batch-sizes 128 and 32. On\nCIFAR-10, on the other hand, SIGN SGD M manages to be\nfaster than SGD M (though still slower than EF-SIGN SGD).\nWe believe this may be due to SIGN SGD M being sensitive\nto the weight-decay parameter as was noted in (Bernstein\net al., 2018). We do not tune the weight-decay parameter\nand leave it to its default value for all methods (includingEF-\nSIGN SGD ). Further the generalization gap of SIGN SGD M\ngets worse for decreasing batch-sizes with a signiﬁcant 6.6%\ndrop in accuracy for batch-size 8.\n7. Conclusion\nWe study the effect of biased compressors on the conver-\ngence and generalization of stochastic gradient algorithms\nfor non-convex optimization. We have shown that biased\ncompressors if naively used can lead to bad generalization,\nand even non-convergence. We then show that using error-\nfeedback all such adverse effects can be mitigated. Our the-\nory and experiments indicate that using error-feedback, our\ncompressed gradient algorithm EF-SGD enjoys the same\nrate of convergence as original SGD—thereby giving com-\npression for free. We believe this should have a large impact\nin the design of future compressed gradient schemes for\ndistributed and decentralized learning. Further, given the re-\nlation between sign-based methods and ADAM, we believe\nthat our results will be relevant for better understanding\nthe performance and limitations of adaptive methods. Fi-\nnally, in this work we only consider the single worker case.\nDeveloping a practical and scalable algorithm for multiple\nworkers is a fruitful direction for future work.\nError Feedback Fixes SignSGD\nAcknowledgements\nWe are grateful to Tao Lin, Thijs V ogels, and Negar Foroutan\nfor their help with the experiments. We also thank Jean-\nBaptiste Cordonnier, Konstantin Mishchenko, Jeremy Bern-\nstein, and anonymous reviewers for their suggestions which\nhelped improve our writeup. We acknowledge funding from\nSNSF grant 200021_175796, as well as a Google Focused\nResearch Award.\nReferences\nAlistarh, D., Grubic, D., Li, J., Tomioka, R., and V ojnovic,\nM. Qsgd: Communication-efﬁcient sgd via gradient quan-\ntization and encoding. In Advances in Neural Information\nProcessing Systems (NIPS), 2017.\nArpit, D., Jastrz˛ ebski, S., Ballas, N., Krueger, D., Bengio,\nE., Kanwal, M. S., Maharaj, T., Fischer, A., Courville,\nA., Bengio, Y ., et al. A closer look at memorization in\ndeep networks. In International Conference on Machine\nLearning (ICML), 2017.\nBalles, L. and Hennig, P. Dissecting adam: The sign, magni-\ntude and variance of stochastic gradients. In Internation\nConference on Machine Learning (ICML), 2018.\nBernstein, J., Wang, Y .-X., Azizzadenesheli, K., and Anand-\nkumar, A. signsgd: compressed optimisation for non-\nconvex problems. In Internation Conference on Machine\nLearning (ICML), 2018.\nBernstein, J., Zhao, J., Azizzadenesheli, K., and Anandku-\nmar, A. signSGD with majority vote is communication\nefﬁcient and fault tolerant. In International Conference\non Learning Representations (ICLR), 2019.\nBottou, L. Large-scale machine learning with stochastic\ngradient descent. In Lechevallier, Y . and Saporta, G.\n(eds.), Proceedings of COMPSTAT’2010, pp. 177–186,\nHeidelberg, 2010. Physica-Verlag HD. ISBN 978-3-7908-\n2604-3.\nCarlson, D., Cevher, V ., and Carin, L. Stochastic Spectral\nDescent for Restricted Boltzmann Machines. In Interna-\ntional Conference on Artiﬁcial Intelligence and Statistics\n(AISTATS), pp. 111–119, February 2015.\nChen, J. and Gu, Q. Padam: Closing the generalization\ngap of adaptive gradient methods in training deep neu-\nral networks. In International Conference on Learning\nRepresentations (ICLR), 2019.\nChilimbi, T. M., Suzue, Y ., Apacible, J., and Kalyanaraman,\nK. Project adam: Building an efﬁcient and scalable deep\nlearning training system. In OSDI, volume 14, pp. 571–\n582, 2014.\nCortes, C. and Vapnik, V . Support-vector networks. Ma-\nchine learning, 20(3):273–297, 1995.\nDean, J., Corrado, G., Monga, R., Chen, K., Devin, M.,\nMao, M., Senior, A., Tucker, P., Yang, K., Le, Q. V .,\net al. Large scale distributed deep networks. In Advances\nin Neural Information Processing Systems (NIPS) , pp.\n1223–1231, 2012.\nDinh, L., Pascanu, R., Bengio, S., and Bengio, Y . Sharp\nminima can generalize for deep nets. arXiv preprint\narXiv:1703.04933, 2017.\nDuchi, J., Hazan, E., and Singer, Y . Adaptive subgradient\nmethods for online learning and stochastic optimization.\nJournal of Machine Learning Research , 12(Jul):2121–\n2159, 2011.\nGoyal, P., Dollar, P., Girshick, R., Noordhuis, P.,\nWesolowski, L., Kyrola, A., Tulloch, A., Jia, Y ., and\nHe, K. Accurate, large minibatch sgd: training imagenet\nin 1 hour. arXiv preprint arXiv:1706.02677, 2017.\nGugger, S. and Howard, J. Adamw and super-\nconvergence is now the fastest way to train neu-\nral nets. https://www.fast.ai/2018/07/02/\nadam-weight-decay/, 2018. Accessed: 2019-01-\n17.\nGunasekar, S., Lee, J., Soudry, D., and Srebro, N. Character-\nizing implicit bias in terms of optimization geometry. In\nInternational Conference on Machine Learning (ICML),\n2018.\nHe, K., Zhang, X., Ren, S., and Sun, J. Deep residual learn-\ning for image recognition. In Proceedings of the IEEE\nconference on computer vision and pattern recognition\n(CVPR), pp. 770–778, 2016a.\nHe, K., Zhang, X., Ren, S., and Sun, J. Identity mappings\nin deep residual networks. In European Conference on\nComputer Vision (ECCV), pp. 630–645. Springer, 2016b.\nIm, D. J., Tao, M., and Branson, K. An empirical analysis\nof the optimization of deep network loss surfaces. arXiv\npreprint arXiv:1612.04010, 2016.\nKawaguchi, K., Kaelbling, L. P., and Bengio, Y . Generaliza-\ntion in deep learning. arXiv preprint arXiv:1710.05468,\n2017.\nKingma, D. P. and Ba, J. Adam: A method for stochastic\noptimization. In International Conference on Learning\nRepresentations (ICLR), 2015.\nKrizhevsky, A. and Hinton, G. Learning multiple layers of\nfeatures from tiny images. Technical report, Technical\nReport, University of Toronto, Toronto., 2009.\nError Feedback Fixes SignSGD\nKrizhevsky, A., Sutskever, I., and Hinton, G. E. Imagenet\nclassiﬁcation with deep convolutional neural networks.\nIn Advances in Neural Information Processing Systems\n(NIPS), pp. 1097–1105, 2012.\nLeCun, Y ., Bengio, Y ., and Hinton, G. Deep learning.nature,\n521(7553):436, 2015.\nLi, H., Xu, Z., Taylor, G., Studer, C., and Goldstein, T.\nVisualizing the loss landscape of neural nets. InAdvances\nin Neural Information Processing Systems (NeurIPS) ,\n2018.\nLi, M., Andersen, D. G., Park, J. W., Smola, A. J., Ahmed,\nA., Josifovski, V ., Long, J., Shekita, E. J., and Su, B.-Y .\nScaling distributed machine learning with the parameter\nserver. In OSDI, volume 14, pp. 583–598, 2014.\nLin, Y ., Han, S., Mao, H., Wang, Y ., and Dally, W. J.\nDeep gradient compression: Reducing the communica-\ntion bandwidth for distributed training. In International\nConference on Learning Representations (ICLR), 2018.\nLiu, S., Chen, P.-Y ., Chen, X., and Hong, M. signsgd\nvia zeroth-order oracle. In International Conference on\nLearning Representations (ICLR), 2018.\nLuo, L., Xiong, Y ., and Liu, Y . Adaptive gradient methods\nwith dynamic bound of learning rate. In International\nConference on Learning Representations (ICLR), 2019.\nPaszke, A., Gross, S., Chintala, S., and Chanan, G. Pytorch,\n2017.\nReddi, S. J., Kale, S., and Kumar, S. On the convergence\nof adam and beyond. In International Conference on\nLearning Representations (ICLR), 2018.\nRiedmiller, M. and Braun, H. A direct adaptive method for\nfaster backpropagation learning: The rprop algorithm. In\nNeural Networks, 1993., IEEE International Conference\non, pp. 586–591. IEEE, 1993.\nRobbins, H. and Monro, S. A Stochastic Approximation\nMethod. The Annals of Mathematical Statistics , 22(3):\n400–407, September 1951.\nSchmidhuber, J. Deep learning in neural networks: An\noverview. Neural networks, 61:85–117, 2015.\nSeide, F., Fu, H., Droppo, J., Li, G., and Yu, D. 1-bit stochas-\ntic gradient descent and its application to data-parallel\ndistributed training of speech dnns. In Fifteenth Annual\nConference of the International Speech Communication\nAssociation, 2014.\nSimonyan, K. and Zisserman, A. Very deep convolu-\ntional networks for large-scale image recognition. arXiv\npreprint arXiv:1409.1556, 2014.\nSoudry, D., Hoffer, E., Nacson, M. S., Gunasekar, S., and\nSrebro, N. The implicit bias of gradient descent on sep-\narable data. Journal of Machine Learning Research, 19\n(70), 2018.\nStich, S. U., Cordonnier, J.-B., and Jaggi, M. Sparsiﬁed\nsgd with memory. In Advances in Neural Information\nProcessing Systems (NeurIPS), 2018.\nStrom, N. Scalable distributed dnn training using commod-\nity gpu cloud computing. In Sixteenth Annual Conference\nof the International Speech Communication Association,\n2015.\nValiant, L. G. A theory of the learnable. Communications\nof the ACM, 27(11):1134–1142, 1984.\nWang, H., Sievert, S., Liu, S., Charles, Z., Papailiopoulos,\nD., and Wright, S. Atomo: Communication-efﬁcient\nlearning via atomic sparsiﬁcation. In Advances in Neural\nInformation Processing Systems (NeurIPS), 2018.\nWen, W., Xu, C., Yan, F., Wu, C., Wang, Y ., Chen, Y ., and\nLi, H. Terngrad: Ternary gradients to reduce communica-\ntion in distributed deep learning. In Advances in Neural\nInformation Processing Systems (NIPS), pp. 1509–1519,\n2017.\nWilson, A. C., Roelofs, R., Stern, M., Srebro, N., and Recht,\nB. The marginal value of adaptive gradient methods in\nmachine learning. In Advances in Neural Information\nProcessing Systems (NIPS), pp. 4148–4158, 2017.\nZaheer, M., Reddi, S., Sachan, D., Kale, S., and Ku-\nmar, S. Adaptive methods for nonconvex optimization.\nIn Advances in Neural Information Processing Systems\n(NeurIPS), pp. 9815–9825, 2018.\nZhang, C., Bengio, S., Hardt, M., Recht, B., and Vinyals,\nO. Understanding deep learning requires rethinking gen-\neralization. In International Conference on Learning\nRepresentations (ICLR), 2017.\nZhang, C., Vinyals, O., Munos, R., and Bengio, S. A\nstudy on overﬁtting in deep reinforcement learning.arXiv\npreprint arXiv:1804.06893, 2018.",
  "values": {
    "Collective influence": "No",
    "Critiqability": "No",
    "Explicability": "No",
    "User influence": "No",
    "Respect for Law and public interest": "No",
    "Fairness": "No",
    "Beneficence": "No",
    "Deferral to humans": "No",
    "Justice": "No",
    "Not socially biased": "No",
    "Respect for Persons": "No",
    "Privacy": "No",
    "Interpretable (to users)": "No",
    "Non-maleficence": "No",
    "Transparent (to users)": "No",
    "Autonomy (power to decide)": "No"
  }
}