{
  "pdf": "kim18b",
  "title": "Disentangling_by_factorising__ICML_.1.1",
  "author": "Hyunjik Kim, Andriy Mnih",
  "paper_id": "kim18b",
  "text": "Disentangling by Factorising\nHyunjik Kim 1 2 Andriy Mnih 1\nAbstract\nWe deﬁne and address the problem of unsuper-\nvised learning of disentangled representations on\ndata generated from independent factors of varia-\ntion. We propose FactorV AE, a method that dis-\nentangles by encouraging the distribution of rep-\nresentations to be factorial and hence independent\nacross the dimensions. We show that it improves\nupon β-V AE by providing a better trade-off be-\ntween disentanglement and reconstruction quality.\nMoreover, we highlight the problems of a com-\nmonly used disentanglement metric and introduce\na new metric that does not suffer from them.\n1. Introduction\nLearning interpretable representations of data that expose\nsemantic meaning has important consequences for artiﬁcial\nintelligence. Such representations are useful not only for\nstandard downstream tasks such as supervised learning and\nreinforcement learning, but also for tasks such as transfer\nlearning and zero-shot learning where humans excel but\nmachines struggle (Lake et al., 2016). There have been\nmultiple efforts in the deep learning community towards\nlearning factors of variation in the data, commonly referred\nto as learning a disentangled representation. While there is\nno canonical deﬁnition for this term, we adopt the one due\nto Bengio et al. (2013): a representation where a change\nin one dimension corresponds to a change in one factor\nof variation, while being relatively invariant to changes in\nother factors. In particular, we assume that the data has\nbeen generated from a ﬁxed number of independent factors\nof variation.3 We focus on image data, where the effect of\nfactors of variation is easy to visualise.\nUsing generative models has shown great promise in learn-\ning disentangled representations in images. Notably, semi-\n1DeepMind, UK 2Department of Statistics, University\nof Oxford. Correspondence to: Hyunjik Kim <hyun-\njikk@google.com>.\nProceedings of the 35 th International Conference on Machine\nLearning, Stockholm, Sweden, PMLR 80, 2018. Copyright 2018\nby the author(s).\n3We discuss the limitations of this assumption in Section 4.\nCross-entropy loss\nfor classifying samples\nfrom each class\n - encouraging        to \n   be factorised\nrandomly permute \neach dimension\nacross batch\nVAE\n-\n+\nDiscriminator\nInput\nFactorVAE objective = VAE objective -       cross-entropy loss\nFigure 1. Architecture of FactorV AE, a Variational Autoencoder\n(V AE) that encourages the code distribution to be factorial. The\ntop row is a V AE with convolutional encoder and decoder, and\nthe bottom row is an MLP classiﬁer, the discriminator, that dis-\ntinguishes whether the input was drawn from the marginal code\ndistribution or the product of its marginals.\nsupervised approaches that require implicit or explicit\nknowledge about the true underlying factors of the data have\nexcelled at disentangling (Kulkarni et al., 2015; Kingma\net al., 2014; Reed et al., 2014; Siddharth et al., 2017; Hinton\net al., 2011; Mathieu et al., 2016; Goroshin et al., 2015; Hsu\net al., 2017; Denton & Birodkar, 2017). However, ideally\nwe would like to learn these in an unsupervised manner, due\nto the following reasons: 1. Humans are able to learn factors\nof variation unsupervised (Perry et al., 2010). 2. Labels are\ncostly as obtaining them requires a human in the loop. 3.\nLabels assigned by humans might be inconsistent or leave\nout the factors that are difﬁcult for humans to identify.\nβ-V AE (Higgins et al., 2016) is a popular method for un-\nsupervised disentangling based on the Variational Autoen-\ncoder (V AE) framework (Kingma & Welling, 2014; Rezende\net al., 2014) for generative modelling. It uses a modiﬁed ver-\nsion of the V AE objective with a larger weight (β > 1) on\nthe KL divergence between the variational posterior and the\nprior, and has proven to be an effective and stable method\nfor disentangling. One drawback of β-V AE is that recon-\nstruction quality (compared to V AE) must be sacriﬁced\nin order to obtain better disentangling. The goal of our\nwork is to obtain a better trade-off between disentanglement\nand reconstruction, allowing to achieve better disentangle-\nment without degrading reconstruction quality. In this work,\nwe analyse the source of this trade-off and propose Factor-\nV AE, which augments the V AE objective with a penalty that\nDisentangling by Factorising\nencourages the marginal distribution of representations to\nbe factorial without substantially affecting the quality of\nreconstructions. This penalty is expressed as a KL diver-\ngence between this marginal distribution and the product\nof its marginals, and is optimised using a discriminator net-\nwork following the divergence minimisation view of GANs\n(Nowozin et al., 2016; Mohamed & Lakshminarayanan,\n2016). Our experimental results show that this approach\nachieves better disentanglement than β-V AE for the same\nreconstruction quality. We also point out the weaknesses\nin the disentangling metric of Higgins et al. (2016), and\npropose a new metric that addresses these shortcomings.\nA popular alternative to β-V AE is InfoGAN (Chen et al.,\n2016), which is based on the Generative Adversarial Net\n(GAN) framework (Goodfellow et al., 2014) for generative\nmodelling. InfoGAN learns disentangled representations by\nrewarding the mutual information between the observations\nand a subset of latents. However at least in part due to its\ntraining stability issues (Higgins et al., 2016), there has been\nlittle empirical comparison between V AE-based methods\nand InfoGAN. Taking advantage of the recent developments\nin the GAN literature that help stabilise training, we include\nInfoWGAN-GP, a version of InfoGAN that uses Wasser-\nstein distance (Arjovsky et al., 2017) and gradient penalty\n(Gulrajani et al., 2017), in our experimental evaluation.\nIn summary, we make the following contributions: 1) We\nintroduce FactorV AE, a method for disentangling that gives\nhigher disentanglement scores than β-V AE for the same\nreconstruction quality. 2) We identify the weaknesses of\nthe disentanglement metric of Higgins et al. (2016) and\npropose a more robust alternative. 3) We give quantitative\ncomparisons of FactorV AE andβ-V AE against InfoGAN’s\nWGAN-GP counterpart for disentanglement.\n2. Trade-off between Disentanglement and\nReconstruction inβ-V AE\nWe motivate our approach by analysing where the disen-\ntanglement and reconstruction trade-off arises in the β-\nV AE objective. First, we introduce notation and archi-\ntecture of our V AE framework. We assume that observa-\ntions x(i) ∈ X , i = 1, . . . , N are generated by combining\nK underlying factors f = ( f1, . . . , fK). These observa-\ntions are modelled using a real-valued latent/code vector\nz ∈ Rd, interpreted as the representation of the data. The\ngenerative model is deﬁned by the standard Gaussian prior\np(z) = N (0, I), intentionally chosen to be a factorised\ndistribution, and the decoder pθ(x|z) parameterised by a\nneural net. The variational posterior for an observation is\nqθ(z|x) = ∏d\nj=1 N (zj|µj(x), σ2\nj (x)), with the mean and\nvariance produced by the encoder, also parameterised by\na neural net.1 The variational posterior can be seen as the\ndistribution of the representation corresponding to the data\npoint x. The distribution of representations for the entire\ndata set is then given by\nq(z) = Epdata(x)[q(z|x)] = 1\nN\nN∑\ni=1\nq(z|x(i)), (1)\nwhich is known as the marginal posterior or aggregate pos-\nterior, where pdata is the empirical data distribution. A\ndisentangled representation would have each zj correspond\nto precisely one underlying factor fk. Since we assume that\nthese factors vary independently, we wish for a factorial\ndistribution q(z) =∏d\nj=1 q(zj).\nThe β-V AE objective\n1\nN\nN∑\ni=1\n[\nEq(z|x(i))[log p(x(i)|z)] − βKL(q(z|x(i))||p(z))\n]\nis a variational lower bound on Epdata(x)[log p(x(i))] for\nβ ≥ 1, reducing to the V AE objective forβ = 1. Its ﬁrst\nterm can be interpreted as the negative reconstruction error,\nand the second term as the complexity penalty that acts as\na regulariser. We may further break down this KL term as\n(Hoffman & Johnson, 2016; Makhzani & Frey, 2017)\nEpdata(x)[KL(q(z|x)||p(z))] = I(x; z)+ KL(q(z)||p(z)),\nwhere I(x; z) is the mutual information between x and z un-\nder the joint distribution pdata(x)q(z|x). See Appendix C\nfor the derivation. Penalising the KL(q(z)||p(z)) term\npushes q(z) towards the factorial prior p(z), encouraging\nindependence in the dimensions of z and thus disentangling.\nPenalising I(x; z), on the other hand, reduces the amount of\ninformation about x stored in z, which can lead to poor re-\nconstructions for high values of β (Makhzani & Frey, 2017).\nThus making β larger than 1, penalising both terms more,\nleads to better disentanglement but reduces reconstruction\nquality. When this reduction is severe, there is insufﬁcient\ninformation about the observation in the latents, making it\nimpossible to recover the true factors. Therefore there exists\na value of β > 1 that gives highest disentanglement, but\nresults in a higher reconstruction error than a V AE.\n3. Total Correlation Penalty and FactorV AE\nPenalising I(x; z) more than a V AE does might be neither\nnecessary nor desirable for disentangling. For example,\nInfoGAN disentangles by encouraging I(x; c) to be high\nwhere c is a subset of the latent variables z 2. Hence we\n1In the rest of the paper we will omit the dependence ofp and\nq on their parametersθ for notational convenience.\n2Note however thatI(x;z) inβ-V AE is deﬁned under the joint\ndistribution of data and their encoding distributionpdata(x)q(z|x),\nwhereasI(x;c) in InfoGAN is deﬁned on the joint distribution of\nthe prior onc and the decoding distributionp(c)p(x|c).\nDisentangling by Factorising\nmotivate FactorV AE by augmenting the V AE objective with\na term that directly encourages independence in the code\ndistribution, arriving at the following objective:\n1\nN\nN∑\ni=1\n[\nEq(z|x(i))[log p(x(i)|z)] − KL(q(z|x(i))||p(z))\n]\n− γKL(q(z)||¯q(z)), (2)\nwhere ¯q(z) :=∏d\nj=1 q(zj). Note that this is also a lower\nbound on the marginal log likelihood Epdata(x)[log p(x)].\nKL(q(z)||¯q(z)) is known as Total Correlation (TC, Watan-\nabe, 1960), a popular measure of dependence for multiple\nrandom variables. In our case this term is intractable since\nboth q(z) and ¯q(z) involve mixtures with a large number of\ncomponents, and the direct Monte Carlo estimate requires a\npass through the entire data set for each q(z) evaluation.3.\nHence we take an alternative approach for optimizing this\nterm. We start by observing we can sample from q(z) efﬁ-\nciently by ﬁrst choosing a datapointx(i) uniformly at ran-\ndom and then sampling from q(z|x(i)). We can also sample\nfrom ¯q(z) by generating d samples from q(z) and then ig-\nnoring all but one dimension for each sample. A more\nefﬁcient alternative involves sampling a batch from q(z)\nand then randomly permuting across the batch for each la-\ntent dimension (see Alg. 1). This is a standard trick used in\nthe independence testing literature (Arcones & Gine, 1992)\nand as long as the batch is large enough, the distribution of\nthese samples samples will closely approximate ¯q(z).\nHaving access to samples from both distributions allows\nus to minimise their KL divergence using the density-ratio\ntrick (Nguyen et al., 2010; Sugiyama et al., 2012) which\ninvolves training a classiﬁer/discriminator to approximate\nthe density ratio that arises in the KL term. Suppose we\nhave a discriminator D (in our case an MLP) that outputs\nan estimate of the probability D(z) that its input is a sample\nfrom q(z) rather than from ¯q(z). Then we have\nT C(z) = KL(q(z)||¯q(z)) = Eq(z)\n[\nlog q(z)\n¯q(z)\n]\n≈ Eq(z)\n[\nlog D(z)\n1 − D(z)\n]\n. (3)\nWe train the discriminator and the V AE jointly. In particu-\nlar, the V AE parameters are updated using the objective in\nEqn. (2), with the TC term replaced using the discriminator-\nbased approximation from Eqn. (3). The discriminator is\ntrained to classify between samples from q(z) and ¯q(z),\nthus learning to approximate the density ratio needed for\nestimating TC. See Alg. 2 for pseudocode of FactorV AE.\nIt is important to note that low TC is necessary but not\nsufﬁcient for meaningful disentangling. For example, when\n3We have also tried using a batch estimate ofq(z), but this did\nnot work. See Appendix D for details.\nAlgorithm 1 permute dims\nInput: {z(i) ∈ Rd : i = 1, . . . , B}\nfor j = 1 to d do\nπ ← random permutation on {1, . . . , B}\n(z(i)\nj )B\ni=1 ← (z(π(i))\nj )B\ni=1\nend for\nOutput: {z(i) : i = 1, . . . , B}\nAlgorithm 2 FactorV AE\nInput: observations (x(i))N\ni=1, batch size m, latent di-\nmension d, γ, V AE/Discriminator optimisers:g, gD\nInitialize V AE and discriminator parametersθ, ψ.\nrepeat\nRandomly select batch (x(i))i∈B of size m\nSample z(i)\nθ ∼ qθ(z|x(i)) ∀i ∈ B\nθ ← g(∇θ 1\nm\n∑\ni∈B\n[log pθ(x(i),z(i)\nθ )\nqθ(z(i)\nθ |x(i)) − γ log Dψ(z(i)\nθ )\n1−Dψ(z(i)\nθ )])\nRandomly select batch (x(i))i∈B′ of size m\nSample z′(i)\nθ ∼ qθ(z|x(i)) for i ∈ B′\n(z′(i)\nperm)i∈B′ ← permute dims((z′(i)\nθ )i∈B′)\nψ ← gD(∇ψ 1\n2m[∑\ni∈B\nlog(Dψ(z(i)\nθ ))\n+ ∑\ni∈B′\nlog(1 − Dψ(z′(i)\nperm))])\nuntil convergence of objective.\nq(z|x) = p(z), TC=0 but z carries no information about the\ndata. Thus having low TC is only meaningful when we can\npreserve information in the latents, which is why controlling\nfor reconstruction error is important.\nIn the GAN literature, divergence minimisation is usually\ndone between two distributions over the data space, which\nis often very high dimensional (e.g. images). As a result, the\ntwo distributions often have disjoint support, making train-\ning unstable, especially when the discriminator is strong.\nHence it is necessary to use tricks to weaken the discrim-\ninator such as instance noise (Sønderby et al., 2016) or to\nreplace the discriminator with a critic, as in Wasserstein\nGANs (Arjovsky et al., 2017). In this work, we minimise\ndivergence between two distributions over the latent space\n(as in e.g. (Mescheder et al., 2017)), which is typically much\nlower dimensional and the two distributions have overlap-\nping support. We observe that training is stable for sufﬁ-\nciently large batch sizes (e.g. 64 worked well for d = 10),\nallowing us to use a strong discriminator.\n4. A New Metric for Disentanglement\nThe deﬁnition of disentanglement we use in this paper,\nwhere a change in one dimension of the representation cor-\nresponds to a change in exactly one factor of variation, is\nDisentangling by Factorising\nGenerate data with\nfixed    , random\nGet\nrepresentation\nT ake absolute\nvalue of difference\nT ake mean\nOne training point for\nlinear classifier\nGenerate data with\nfixed    , random\nGet rescaled\nrepresentation\nT ake empirical variance\nin each dimension T ake argmin\nOne training point for\nmajority-vote classifier\nFix one factor\nFix one factor\nFigure 2. Top: Metric in (Higgins et al., 2016). Bottom: Our new\nmetric, wheres∈ Rd is the scale (empirical standard deviation)\nof latent representations of the full data (or large enough random\nsubset).\nclearly a simplistic one. It does not allow correlations among\nthe factors or hierarchies over them. Thus this deﬁnition\nseems more suited to synthetic data with independent fac-\ntors of variation than to most realistic data sets. However,\nas we will show below, robust disentanglement is not a fully\nsolved problem even in this simple setting. One obstacle\non the way to this ﬁrst milestone is the absence of a sound\nquantitative metric for measuring disentanglement.\nA popular method of measuring disentanglement is by in-\nspecting latent traversals: visualising the change in recon-\nstructions while traversing one dimension of the latent space\nat a time. Although latent traversals can be a useful indicator\nof when a model has failed to disentangle, the qualitative\nnature of this approach makes it unsuitable for comparing\nalgorithms reliably. Doing this would require inspecting\na multitude of latent traversals over multiple reference im-\nages, random seeds, and points during training. Having a\nhuman in the loop to assess the traversals is also too time-\nconsuming and subjective. Unfortunately, for data sets that\ndo not have the ground truth factors of variation available,\ncurrently this is the only viable option for assessing disen-\ntanglement.\nHiggins et al. (2016) proposed a supervised metric that at-\ntempts to quantify disentanglement when the ground truth\nfactors of a data set are given. The metric is the error rate\nof a linear classiﬁer that is trained as follows. Choose a\nfactor k; generate data with this factor ﬁxed but all other\nfactors varying randomly; obtain their representations (de-\nﬁned to be the mean ofq(z|x)); take the absolute value of\nthe pairwise differences of these representations. Then the\nmean of these statistics across the pairs gives one training\ninput for the classiﬁer, and the ﬁxed factor index k is the\ncorresponding training output (see top of Figure 2). So if the\nrepresentations were perfectly disentangled, we would see\nzeros in the dimension of the training input that corresponds\nto the ﬁxed factor of variation, and the classiﬁer would learn\nto map the index of the zero value to the index of the factor.\nHowever this metric has several weaknesses. Firstly, it\ncould be sensitive to hyperparameters of the linear classiﬁer\noptimisation, such as the choice of the optimiser and its\norigreconstr\nFigure 3. A β-V AE model trained on the 2D Shapes data that\nscores 100% on metric in Higgins et al. (2016) (ignoring the shape\nfactor). First row: originals. Second row: reconstructions. Re-\nmaining rows: reconstructions of latent traversals. The model only\nuses three latent units to capturex-position,y-position, scale and\nignores orientation, yet achieves a perfect score on the metric.\nhyperparameters, weight initialisation, and the number of\ntraining iterations. Secondly, having a linear classiﬁer is\nnot so intuitive – we could get representations where each\nfactor corresponds to a linear combination of dimensions\ninstead of a single dimension. Finally and most importantly,\nthe metric has a failure mode: it gives 100% accuracy even\nwhen only K − 1 factors out of K have been disentangled;\nto predict the remaining factor, the classiﬁer simply learns\nto detect when all the values corresponding to the K − 1\nfactors are non-zero. An example of such a case is shown in\nFigure 3.\nTo address these weaknesses, we propose a new disentan-\nglement metric as follows. Choose a factor k; generate data\nwith this factor ﬁxed but all other factors varying randomly;\nobtain their representations; normalise each dimension by\nits empirical standard deviation over the full data (or a large\nenough random subset); take the empirical variance in each\ndimension4 of these normalised representations. Then the\nindex of the dimension with the lowest variance and the\ntarget index k provide one training input/output example for\nthe classiﬁer (see bottom of Figure 2). Thus if the repre-\nsentation is perfectly disentangled, the empirical variance\nin the dimension corresponding to the ﬁxed factor will be\n0. We normalise the representations so that the arg min\nis invariant to rescaling of the representations in each di-\nmension. Since both inputs and outputs lie in a discrete\nspace, the optimal classiﬁer is the majority-vote classiﬁer\n(see Appendix B for details), and the metric is the error rate\nof the classiﬁer. The resulting classiﬁer is a deterministic\nfunction of the training data, hence there are no optimisation\nhyperparameters to tune. We also believe that this metric\nis conceptually simpler and more natural than the previous\none. Most importantly, it circumvents the failure mode of\nthe earlier metric, since the classiﬁer needs to see the lowest\nvariance in a latent dimension for a given factor to classify\nit correctly.\nWe think developing a reliable unsupervised disentangling\nmetric that does not use the ground truth factors is an im-\nportant direction for future research, since unsupervised\n4We can use Gini’s deﬁnition of variance for discrete latents\n(Gini, 1971). See Appendix B for details.\nDisentangling by Factorising\ndisentangling is precisely useful for the scenario where we\ndo not have access to the ground truth factors. With this in\nmind, we believe that having a reliable supervised metric is\nstill valuable as it can serve as a gold standard for evaluating\nunsupervised metrics.\n5. Related Work\nThere are several recent works that use a discriminator to\noptimise a divergence to encourage independence in the\nlatent codes. Adversarial Autoencoder (AAE, Makhzani\net al., 2015) removes the I(x; z) term in the V AE objec-\ntive and maximizes the negative reconstruction error minus\nKL(q(z)||p(z)) via the density-ratio trick, showing appli-\ncations in semi-supervised classiﬁcation and unsupervised\nclustering. This means that the AAE objective is not a lower\nbound on the log marginal likelihood. Although optimising\na lower bound is not strictly necessary for disentangling, it\ndoes ensure that we have a valid generative model; having\na generative model with disentangled latents has the ben-\neﬁt of being a single model that can be useful for various\ntasks e.g. planning for model-based RL, visual concept\nlearning and semi-supervised learning, to name a few. In\nPixelGAN Autoencoders (Makhzani & Frey, 2017), the\nsame objective is used to study the decomposition of in-\nformation between the latent code and the decoder. The\nauthors state that adding noise to the inputs of the encoder\nis crucial, which suggests that limiting the information that\nthe code contains about the input is essential and that the\nI(x; z) term should not be dropped from the V AE objective.\nBrakel & Bengio (2017) also use a discriminator to penalise\nthe Jensen-Shannon Divergence between the distribution of\ncodes and the product of its marginals. However, they use\nthe GAN loss with deterministic encoders and decoders and\nonly explore their technique in the context of Independent\nComponent Analysis source separation.\nEarly works on unsupervised disentangling include (Schmid-\nhuber, 1992) which attempts to disentangle codes in an au-\ntoencoder by penalising predictability of one latent dimen-\nsion given the others and (Desjardins et al., 2012) where a\nvariant of a Boltzmann Machine is used to disentangle two\nfactors of variation in the data. More recently, Achille &\nSoatto (2018) have used a loss function that penalises TC\nin the context of supervised learning. They show that their\napproach can be extended to the V AE setting, but do not\nperform any experiments on disentangling to support the the-\nory. In a concurrent work, Kumar et al. (2018) used moment\nmatching in V AEs to penalise the covariance between the\nlatent dimensions, but did not constrain the mean or higher\nmoments. We provide the objectives used in these related\nmethods and show experimental results on disentangling\nperformance, including AAE, in Appendix F.\nThere have been various works that use the notion of pre-\ndictability to quantify disentanglement, mostly predicting\nthe value of ground truth factors f = ( f1, . . . , fK) from\nthe latent code z. This dates back to Yang & Amari (1997)\nwho learn a linear map from representations to factors in the\ncontext of linear ICA, and quantify how close this map is to\na permutation matrix. More recently Eastwood & Williams\n(2018) have extended this idea to disentanglement by train-\ning a Lasso regressor to map z to f and using its trained\nweights to quantify disentanglement. Like other regression-\nbased approaches, this one introduces hyperparameters such\nas the optimiser and the Lasso penalty coefﬁcient. The met-\nric of Higgins et al. (2016) as well as the one we proposed,\npredict the factor k from the z of images with a ﬁxed fk\nbut f−k varying randomly. Schmidhuber (1992) quantiﬁes\npredictability between the different dimensions of z, using\na predictor that is trained to predict zj from z−j.\nInvariance and equivariance are frequently considered to\nbe desirable properties of representations in the literature\n(Goodfellow et al., 2009; Kivinen & Williams, 2011; Lenc\n& Vedaldi, 2015). A representation is said to be invariant\nfor a particular task if it does not change when nuisance fac-\ntors of the data, that are irrelevant to the task, are changed.\nAn equivariant representation changes in a stable and pre-\ndictable manner when altering a factor of variation. A dis-\nentangled representation, in the sense used in the paper,\nis equivariant, since changing one factor of variation will\nchange one dimension of a disentangled representation in a\npredictable manner. Given a task, it will be easy to obtain\nan invariant representation from the disentangled represen-\ntation by ignoring the dimensions encoding the nuisance\nfactors for the task (Cohen & Welling, 2014).\nBuilding on a preliminary version of this paper, (Chen et al.,\n2018) recently proposed a minibatch-based alternative to\nour density-ratio-trick-based method for estimating the Total\nCorrelation and introduced an information-theoretic disen-\ntangling metric.\n6. Experiments\nWe compare FactorV AE toβ-V AE on the following data sets\nwith i) known generative factors: 1) 2D Shapes (Matthey\net al., 2017): 737,280 binary 64 × 64 images of 2D shapes\nwith ground truth factors[number of values]: shape[3],\nscale[6], orientation[40], x-position[32], y-position[32]. 2)\n3D Shapes data: 480,000 RGB 64 × 64 × 3 images of 3D\nshapes with ground truth factors: shape[4], scale[8], orienta-\ntion[15], ﬂoor colour[10], wall colour[10], object colour[10]\nii) unknown generative factors: 3) 3D Faces (Paysan et al.,\n2009): 239,840 grey-scale 64 × 64 images of 3D Faces. 4)\n3D Chairs (Aubry et al., 2014): 86,366 RGB 64 × 64 × 3\nimages of chair CAD models. 5) CelebA (cropped ver-\nsion) (Liu et al., 2015): 202,599 RGB 64 × 64 × 3 images\nof celebrity faces. The experimental details such as en-\nDisentangling by Factorising\ncoder/decoder architectures and hyperparameter settings are\nin Appendix A. The details of the disentanglement met-\nrics, along with a sensitivity analysis with respect to their\nhyperparameters, are given in Appendix B.\nreconstruction errornew disentanglement metric old disentanglement metric\niteration iteration\n \nFigure 4. Reconstruction error (top), metric in Higgins et al. (2016)\n(middle), our metric (bottom). β-V AE (left), FactorV AE (right).\nThe colours correspond to different values ofβ andγ respectively,\nand conﬁdence intervals are over 10 random seeds.\nBetter\nFigure 5. Reconstruction error plotted against our disentanglement\nmetric, both averaged over 10 random seeds at the end of training.\nThe numbers at each point are values of β andγ. Note that we\nwant low reconstruction error and a high disentanglement metric.\n-VAE FactorVAE\norigreconstr\nFigure 6. First row: originals. Second row: reconstructions. Re-\nmaining rows: reconstructions of latent traversals across each\nlatent dimension sorted byKL(q(zj|x)||p(zj)), for the best scor-\ning models on our disentanglement metric. Left: β-V AE, score:\n0.814,β = 4. Right: FactorV AE, score: 0.889,γ = 35.\nFrom Figure 4, we see that FactorV AE gives much better\nTrue TC\nDiscriminator TC estimate\niteration iteration\nFigure 7. Total Correlation values for FactorV AE on 2D Shapes.\nLeft: True TC value. Right: Discriminator’s estimate of TC.\ndisentanglement scores than V AEs (β = 1), while barely\nsacriﬁcing reconstruction error, highlighting the disentan-\ngling effect of adding the Total Correlation penalty to the\nV AE objective. The best disentanglement scores for Factor-\nV AE are noticeably better than those forβ-V AE given the\nsame reconstruction error. This can be seen more clearly\nin Figure 5 where the best mean disentanglement of Fac-\ntorV AE (γ = 40) is around 0.82, signiﬁcantly higher than\nthe one for β-V AE (β = 4 ), which is around 0.73, both\nwith reconstruction error around 45. From Figure 6, we\ncan see that both models are capable of ﬁndingx-position,\ny-position, and scale, but struggle to disentangle orienta-\ntion and shape, β-V AE especially. For this data set, neither\nmethod can robustly capture shape, the discrete factor of\nvariation5.\nAs a sanity check, we also evaluated the correlation between\nour metric and the metric in Higgins et al. (2016): Pearson\n(linear correlation coefﬁcient): 0.404,Kendall (proportion\nof pairs that have the same ordering): 0.310, Spearman\n(linear correlation of the rankings): 0.444, all with p-value\n0.000. Hence the two metrics show a fairly high positive\ncorrelation as expected.\nWe have also examined how the discriminator’s estimate of\nthe Total Correlation (TC) behaves and the effect of γ on\nthe true TC. From Figure 7, observe that the discriminator\nis consistently underestimating the true TC, also conﬁrmed\nin (Rosca et al., 2018). However the true TC decreases\nthroughout training, and a higher γ leads to lower TC, so\nthe gradients obtained using the discriminator are sufﬁcient\nfor encouraging independence in the code distribution.\nWe then evaluated InfoWGAN-GP, the counterpart of Info-\nGAN that uses Wasserstein distance and gradient penalty.\nSee Appendix G for an overview. One advantage of Info-\nGAN is that the Monte Carlo estimate of its objective is\ndifferentiable with respect to its parameters even for dis-\ncrete codes c, which makes gradient-based optimisation\nstraightforward. In contrast, V AE-based methods that rely\non the reprameterisation trick for gradient-based optimisa-\ntion require z to be a reparameterisable continuous random\nvariable and alternative approaches require various vari-\n5This is partly due to the fact that learning discrete factors\nwould require using discrete latent variables instead of Gaussians,\nbut jointly modelling discrete and continuous factors of variation\nis a non-trivial problem that needs further research.\nDisentangling by Factorising\nance reduction techniques for gradient estimation (Mnih\n& Rezende, 2016; Maddison et al., 2017). Thus we might\nexpect Info(W)GAN(-GP) to show better disentangling in\ncases where some factors are discrete. Hence we use 4\ncontinuous latents (one for each continuous factor) and one\ncategorical latent of 3 categories (one for each shape). We\ntuned for λ, the weight of the mutual information term\nin Info(W)GAN(-GP), ∈ { 0.0, 0.1, 0.2, . . . ,1.0}, number\nof noise variables ∈ {5, 10, 20, 40, 80, 160} and the learn-\ning rates of the generator ∈ { 10−3, 10−4}, discriminator\n∈ {10−4, 10−5}.\nold disentanglement metric\nnew disentanglement metric\niteration iteration\nFigure 8. Disentanglement scores for InfoWGAN-GP on 2D\nShapes for 10 random seeds per hyperparameter setting. Left:\nMetric in Higgins et al. (2016). Right: Our metric.\nFigure 9. Latent traversals for InfoWGAN-GP on 2D Shapes\nacross four continuous codes (ﬁrst four rows) and categorical code\n(last row) for run with best disentanglement score (λ = 0.2).\nHowever from Figure 8 we can see that the disentanglement\nscores are disappointingly low. From the latent traversals\nin Figure 9, we can see that the model learns only the scale\nfactor, and tries to put positional information in the discrete\nlatent code, which is one reason for the low disentanglement\nscore. Using 5 continuous codes and no categorical codes\ndid not improve the disentanglement scores however. Info-\nGAN with early stopping (before training instability occurs\n– see Appendix H) also gave similar results. The fact that\nsome latent traversals give blank reconstructions indicates\nthat the model does not generalise well to all parts of the\ndomain of p(z).\nOne reason InfoWGAN-GP’s poor performance on this data\nset could be that InfoGAN is sensitive to the generator and\ndiscriminator architecture, which is one thing we did not\ntune extensively. We use a similar architecture to the V AE-\nbased approaches for 2D shapes for a fair comparison, but\nhave also tried a bigger architecture which gave similar\nresults (see Appendix H). If architecture search is indeed\nimportant, this would be a weakness of InfoGAN relative to\nBetter\nFigure 10. Same as Figure 5 for 3D Shapes data.\nreconstr orig\nwall \ncolour\nfloor\ncolour\nobject\ncolour\norient-\nation\nsize\nshape\nFigure 11. Same as Figure 6 but for 3D Shapes data. Left: β-V AE,\nscore: 1.00,β = 32. Right: FactorV AE, score: 1.00,γ = 7.\nFactorV AE andβ-V AE, which are both much more robust\nto architecture choice. In Appendix H, we check that we\ncan replicate the results of Chen et al. (2016) on MNIST\nusing InfoWGAN-GP, verify that it makes training stable\ncompared to InfoGAN, and give implementation details with\nfurther empirical studies of InfoGAN and InfoWGAN-GP.\nWe now show results on the 3D Shapes data, which is a more\ncomplex data set of 3D scenes with additional features such\nas shadows and background (sky). We train both β-V AE\nand FactorV AE for 1M iterations. Figure 10 again shows\nthat FactorV AE achieves much better disentanglement with\nbarely any increase in reconstruction error compared to V AE.\nMoreover, while the top mean disentanglement scores for\nFactorV AE andβ-V AE are similar, the reconstruction error\nis lower for FactorV AE: 3515 (γ = 36) as compared to 3570\n(β = 24). The latent traversals in Figure 11 show that both\nmodels are able to capture the factors of variation in the\nbest-case scenario. Looking at latent traversals across many\nrandom seeds, however, makes it evident that both models\nstruggled to disentangle the factors for shape and scale.\nTo show that FactorV AE also gives a valid generative model\nfor both 2D Shapes and 3D Shapes, we present the log\nmarginal likelihood evaluated on the entire data set together\nwith samples from the generative model in Appendix E.\nWe also show results forβ-V AE and FactorV AE experiments\non the data sets with unknown generative factors, namely\n3D Chairs, 3D Faces, and CelebA. Note that inspecting la-\ntent traversals is the only evaluation method possible here.\nWe can see from Figure 12 (and Figures 38 and 39 in Ap-\npendix I) that FactorV AE has smaller reconstruction error\ncompared to β-V AE, and is capable of learning sensible\nDisentangling by Factorising\nfactors of variation, as shown in the latent traversals in Fig-\nures 13, 14 and 15. Unfortunately, as explained in Section 4,\nlatent traversals tell us little about the robustness of our\nmethod.\nreconstruction error\niteration iteration\nFigure 12. Plots of reconstruction error ofβ-V AE (left) and Fac-\ntorV AE (right) for different values ofβ andγ on 3D Faces data\nover 5 random seeds.\nsize\nazimuth\nback length\nleg style\n-VAE\nsize\nazimuth\nback length\nFactorVAE\nFigure 13.β-V AE and FactorV AE latent traversals across each\nlatent dimension sorted by KL on 3D Chairs, with annotations of\nthe factor of variation corresponding to each latent unit.\nazimuth\nelevation\nazimuth\nlighting\n-VAE\nFactorVAE\nazimuth\nelevation\nlighting\nFigure 14. Same as Figure 13 but for 3D Faces.\nbackground brightness\nazimuth\nskin tone\nhair length\nbackground blueness\nfringe\nhead shape\nFactorVAE\n-VAE\nbackground brightness\nhair colour\nazimuth\nhair colour\nskin tone\nhair length\nFigure 15. Same as Figure 13 but for CelebA.\n7. Conclusion and Discussion\nWe have introduced FactorV AE, a novel method for disen-\ntangling that achieves better disentanglement scores than\nβ-V AE on the 2D Shapes and 3D Shapes data sets for the\nsame reconstruction quality. Moreover, we have identiﬁed\nweaknesses of the commonly used disentanglement metric\nof Higgins et al. (2016), and proposed an alternative metric\nthat is conceptually simpler, is free of hyperparameters, and\navoids the failure mode of the former. Finally, we have\nperformed an experimental evaluation of disentangling for\nthe V AE-based methods and InfoWGAN-GP, a more stable\nvariant of InfoGAN, and identiﬁed its weaknesses relative\nto the V AE-based methods.\nOne of the limitations of our approach is that low Total\nCorrelation is necessary but not sufﬁcient for disentangling\nof independent factors of variation. For example, if all\nbut one of the latent dimensions were to collapse to the\nprior, the TC would be 0 but the representation would not\nbe disentangled. Our disentanglement metric also requires\nus to be able to generate samples holding one factor ﬁxed,\nwhich may not always be possible, for example when our\ntraining set does not cover all possible combinations of\nfactors. The metric is also unsuitable for data with non-\nindependent factors of variation.\nFor future work, we would like to use discrete latent vari-\nables to model discrete factors of variation and investigate\nhow to reliably capture combinations of discrete and contin-\nuous factors using discrete and continuous latents.\nDisentangling by Factorising\nAcknowledgements\nWe thank Chris Burgess and Nick Watters for providing the\ndata sets and helping to set them up, and thank Guillaume\nDesjardins, Sergey Bartunov, Mihaela Rosca, Irina Higgins\nand Yee Whye Teh for helpful discussions.\nReferences\nAchille, A. and Soatto, S. Information Dropout: Learn-\ning optimal representations through noisy computation.\nIEEE Transactions on Pattern Analysis and Machine In-\ntelligence, 2018.\nArcones, M. A. and Gine, E. On the bootstrap of U and V\nstatistics. The Annals of Statistics, pp. 655–674, 1992.\nArjovsky, M., Chintala, S., and Bottou, L. Wasserstein\nGenerative Adversarial Networks. In ICML, 2017.\nAubry, M., Maturana, D., Efros, A. A., Russell, B. C., and\nSivic, J. Seeing 3D chairs: exemplar part-based 2D-3D\nalignment using a large dataset of cad models. In CVPR,\n2014.\nBa, J. L., Kiros, J. R., and Hinton, G. E. Layer normalization.\narXiv preprint arXiv:1607.06450, 2016.\nBengio, Y ., Courville, A., and Vincent, P. Representation\nlearning: A review and new perspectives. IEEE transac-\ntions on Pattern Analysis and Machine Intelligence, 35\n(8):1798–1828, 2013.\nBrakel, P. and Bengio, Y . Learning independent features\nwith adversarial nets for non-linear ICA. arXiv preprint\narXiv:1710.05050, 2017.\nChen, T. Q., Li, X., Grosse, R., and Duvenaud, D. Isolating\nsources of disentanglement in variational autoencoders.\narXiv preprint arXiv:1802.04942, 2018.\nChen, X., Duan, Y ., Houthooft, R., Schulman, J., Sutskever,\nI., and Abbeel, P. InfoGAN: Interpretable representation\nlearning by information maximizing Generative Adver-\nsarial Nets. In NIPS, 2016.\nCohen, T. and Welling, M. Learning the irreducible repre-\nsentations of commutative lie groups. In ICML, 2014.\nDenton, E. L. and Birodkar, V . Unsupervised learning of\ndisentangled representations from video. In NIPS, 2017.\nDesjardins, G., Courville, A., and Bengio, Y . Disentan-\ngling factors of variation via generative entangling. arXiv\npreprint arXiv:1210.5474, 2012.\nDuchi, J., Hazan, E., and Singer, Y . Adaptive subgradient\nmethods for online learning and stochastic optimization.\nJMLR, 12(Jul):2121–2159, 2011.\nEastwood, C. and Williams, C. A framework for the quanti-\ntative evaluation of disentangled representations. InICLR,\n2018.\nGini, C. W. Variability and mutability, contribution to the\nstudy of statistical distributions and relations. Journal of\nAmerican Statistical Association, 66:534–544, 1971.\nGoodfellow, I., Lee, H., Le, Q. V ., Saxe, A., and Ng, A. Y .\nMeasuring invariances in deep networks. In NIPS, 2009.\nGoodfellow, I., Pouget-Abadie, J., Mirza, M., Xu, B.,\nWarde-Farley, D., Ozair, S., Courville, A., and Bengio, Y .\nGenerative Adversarial Nets. In NIPS, 2014.\nGoroshin, R., Bruna, J., Tompson, J., Eigen, D., and LeCun,\nY . Unsupervised learning of spatiotemporally coherent\nmetrics. In ICCV, 2015.\nGulrajani, I., Ahmed, F., Arjovsky, M., Dumoulin, V ., and\nCourville, A. Improved training of wasserstein GANs. In\nNIPS, 2017.\nHiggins, I., Matthey, L., Pal, A., Burgess, C., Glorot, X.,\nBotvinick, M., Mohamed, S., and Lerchner, A. Beta-\nV AE: Learning basic visual concepts with a constrained\nvariational framework. 2016.\nHinton, G. E., Krizhevsky, A., and Wang, S. D. Trans-\nforming auto-encoders. In International Conference on\nArtiﬁcial Neural Networks, pp. 44–51. Springer, 2011.\nHoffman, M. D. and Johnson, M. J. ELBO surgery:\nyet another way to carve up the variational evidence\nlower bound. In Workshop in Advances in Approximate\nBayesian Inference, NIPS, 2016.\nHsu, W. N., Zhang, Y ., and Glass, J. Unsupervised learning\nof disentangled and interpretable representations from\nsequential data. In NIPS, 2017.\nIoffe, S. and Szegedy, C. Batch normalization: Accelerating\ndeep network training by reducing internal covariate shift.\nIn ICML, 2015.\nKingma, D. P. and Ba, J. Adam: A method for stochastic\noptimization. In ICLR, 2015.\nKingma, D. P. and Welling, M. Auto-encoding variational\nBayes. 2014.\nKingma, D. P., Mohamed, S., Rezende, D. J., and Welling,\nM. Semi-supervised learning with deep generative mod-\nels. In NIPS, 2014.\nKivinen, J. J. and Williams, C. Transformation equivariant\nboltzmann machines. In International Conference on\nArtiﬁcial Neural Networks, 2011.\nDisentangling by Factorising\nKulkarni, T., Whitney, W. F., Kohli, P., and Tenenbaum, J.\nDeep convolutional inverse graphics network. In NIPS,\n2015.\nKumar, A., Sattigeri, P., and Balakrishnan, A. Variational\ninference of disentangled latent concepts from unlabeled\nobservations. In ICLR, 2018.\nLake, B. M., Ullman, T. D., Tenenbaum, J. B., and Gersh-\nman, S. J. Building machines that learn and think like\npeople. Behavioral and Brain Sciences, pp. 1–101, 2016.\nLenc, K. and Vedaldi, A. Understanding image represen-\ntations by measuring their equivariance and equivalence.\nIn CVPR, 2015.\nLiu, Z., Luo, P., Wang, X., and Tang, X. Deep learning\nface attributes in the wild. In Proceedings of the IEEE\nInternational Conference on Computer Vision, pp. 3730–\n3738, 2015.\nMaddison, C. J., Mnih, A., and Teh, Y . W. The CONCRETE\ndistribution: A continuous relaxation of discrete random\nvariables. In ICLR, 2017.\nMakhzani, A. and Frey, B. PixelGAN autoencoders. In\nNIPS, 2017.\nMakhzani, A., Shlens, J., Jaitly, N., Goodfellow, I., and\nFrey, B. Adversarial autoencoders. arXiv preprint\narXiv:1511.05644, 2015.\nMathieu, M. F., Zhao, J. J., Ramesh, A., Sprechmann, P.,\nand LeCun, Y . Disentangling factors of variation in deep\nrepresentation using adversarial training. In NIPS, 2016.\nMatthey, L., Higgins, I., Hassabis, D., and Lerchner,\nA. dSprites: Disentanglement testing Sprites dataset.\nhttps://github.com/deepmind/dsprites-dataset/, 2017.\nMescheder, L., Nowozin, S., and Geiger, A. Adversarial\nvariational Bayes: Unifying Variational Autoencoders\nand Generative Adversarial Networks. In ICML, 2017.\nMnih, A. and Rezende, D. J. Variational inference for Monte\nCarlo objectives. In ICML, 2016.\nMohamed, S. and Lakshminarayanan, B. Learn-\ning in implicit generative models. arXiv preprint\narXiv:1610.03483, 2016.\nNguyen, X., Wainwright, M. J., and Jordan, M. I. Estimating\ndivergence functionals and the likelihood ratio by convex\nrisk minimization. IEEE Transactions on Information\nTheory, 2010.\nNowozin, S., Cseke, B., and Tomioka, R. f-GAN: Training\ngenerative neural samplers using variational divergence\nminimization. In NIPS, 2016.\nPaysan, P., Knothe, R., Amberg, B., Romdhani, S., and\nVetter, T. A 3D face model for pose and illumination\ninvariant face recognition. In Proceedings of the IEEE\nInternational Conference on Advanced Video and Signal\nbased Surveillance, pp. 296–301, 2009.\nPerry, G., Rolls, E. T., and Stringer, S. M. Continuous trans-\nformation learning of translation invariant representations.\nExperimental Brain Research, 204(2):255–270, 2010.\nReed, S., Sohn, K., Zhang, Y ., and Lee, H. Learning to\ndisentangle factors of variation with manifold interaction.\nIn ICML, 2014.\nRezende, D. J., Mohamed, S., and Wierstra, D. Stochas-\ntic backpropagation and approximate inference in deep\ngenerative models. In ICML, 2014.\nRosca, M., Lakshminarayanan, B., and Mohamed, S. Distri-\nbution matching in variational inference. arXiv preprint\narXiv:1802.06847, 2018.\nSchmidhuber, J. Learning factorial codes by predictability\nminimization. Neural Computation, 4(6):863–879, 1992.\nSiddharth, N., Paige, B., Van de Meent, J. W., Desmaison,\nA., Wood, F., Goodman, N. D., Kohli, P., and Torr, P.\nH. S. Learning disentangled representations with semi-\nsupervised deep generative models. In NIPS, 2017.\nSønderby, C. K., Caballero, J., Theis, L., Shi, W., and\nHusz´ar, F. Amortised MAP inference for image super-\nresolution. In ICLR, 2016.\nSugiyama, M., Suzuki, T., and Kanamori, T. Density-ratio\nmatching under the Bregman divergence: a uniﬁed frame-\nwork of density-ratio estimation. Annals of the Institute\nof Statistical Mathematics, 64(5):1009–1044, 2012.\nWatanabe, S. Information theoretical analysis of multivari-\nate correlation. IBM Journal of research and development,\n4(1):66–82, 1960.\nYang, H. H. and Amari, S. I. Adaptive online learning\nalgorithms for blind separation: maximum entropy and\nminimum mutual information. Neural computation, 9(7):\n1457–1482, 1997.",
  "values": {
    "Interpretable (to users)": "No",
    "Critiqability": "No",
    "Explicability": "No",
    "User influence": "No",
    "Privacy": "No",
    "Non-maleficence": "No",
    "Collective influence": "No",
    "Beneficence": "No",
    "Not socially biased": "No",
    "Respect for Persons": "No",
    "Fairness": "No",
    "Justice": "No",
    "Transparent (to users)": "No",
    "Respect for Law and public interest": "No",
    "Autonomy (power to decide)": "No",
    "Deferral to humans": "No"
  }
}