{
  "pdf": "koloskova19a",
  "title": "Decentralized Stochastic Optimization and Gossip Algorithms with Compressed Communication",
  "author": "Anastasia Koloskova, Sebastian U. Stich, Martin Jaggi",
  "paper_id": "koloskova19a",
  "text": "Decentralized Stochastic Optimization and Gossip Algorithms\nwith Compressed Communication\nAnastasia Koloskova * 1 Sebastian U. Stich * 1 Martin Jaggi 1\nAbstract\nWe consider decentralized stochastic optimiza-\ntion with the objective function (e.g. data samples\nfor machine learning tasks) being distributed over\nn machines that can only communicate to their\nneighbors on a ﬁxed communication graph. To\naddress the communication bottleneck, the nodes\ncompress (e.g. quantize or sparsify) their model\nupdates. We cover both unbiased and biased com-\npression operators with quality denoted byδ≤ 1\n(δ = 1 meaning no compression).\nWe (i) propose a novel gossip-based stochastic\ngradient descent algorithm, CHOCO -SGD , that\nconverges at rateO\n(\n1/(nT ) + 1/(Tρ 2δ)2)\nfor\nstrongly convex objectives, whereT denotes the\nnumber of iterations and ρ the eigengap of the\nconnectivity matrix. We (ii) present a novel\ngossip algorithm, CHOCO -G OSSIP , for the av-\nerage consensus problem that converges in time\nO(1/(ρ2δ) log(1/ε)) for accuracyε> 0. This is\n(up to our knowledge) the ﬁrst gossip algorithm\nthat supports arbitrary compressed messages for\nδ >0 and still exhibits linear convergence. We\n(iii) show in experiments that both of our algo-\nrithms do outperform the respective state-of-the-\nart baselines and CHOCO -SGD can reduce com-\nmunication by at least two orders of magnitudes.\n1. Introduction\nDecentralized machine learning methods are becoming core\naspects of many important applications, both in view of\nscalability to larger datasets and systems, but also from the\nperspective of data locality, ownership and privacy. We con-\nsider decentralized optimization methods that do not rely\non a central coordinator (e.g. parameter server) but instead\nonly require on-device computation and local communica-\n*Equal contribution 1EPFL, Lausanne, Switzerland. Correspon-\ndence to: Anastasia Koloskova<anastasia.koloskova@epﬂ.ch>.\nProceedings of the 36 th International Conference on Machine\nLearning, Long Beach, California, PMLR 97, 2019. Copyright\n2019 by the author(s).\ntion with neighboring devices. This covers for instance the\nclassic setting of training machine learning models in large\ndata-centers, but also emerging applications were the com-\nputations are executed directly on the consumer devices,\nwhich keep their part of the data private at all times.1\nFormally, we consider optimization problems distributed\nacrossn devices or nodes of the form\nf⋆ := min\nx∈Rd\n[\nf(x) := 1\nn\nn∑\ni=1\nfi(x)\n]\n, (1)\nwherefi : Rd→ R fori∈ [n] :={1,...,n } are the objec-\ntives deﬁned by the data available locally on each node. We\nalso allow each local objective fi to have stochastic opti-\nmization (or sum) structure, covering the important case of\nempirical risk minimization in distributed machine learning\nand deep learning applications.\nDecentralized Communication. We model the network\ntopology as a graph where edges represent the communica-\ntion links along which messages (e.g. model updates) can\nbe exchanged. The decentralized setting is motivated by\ncentralized topologies (corresponding to a star graph) often\nnot being possible, and otherwise often posing a signiﬁ-\ncant bottleneck on the central node in terms of communica-\ntion latency, bandwidth and fault tolerance. Decentralized\ntopologies avoid these bottlenecks and thereby offer hugely\nimproved potential in scalability. For example, while the\nmaster node in the centralized setting receives (and sends)\nin each round messages from all workers, Θ(n) in total2, in\ndecentralized topologies the maximal degree of the network\nis often constant (e.g. ring or torus) or a slowly growing\nfunction inn (e.g. scale-free networks).\nDecentralized Optimization. For the case of determin-\nistic (full-gradient) optimization, recent seminal theoreti-\ncal advances show that the network topology only affects\nhigher-order terms of the convergence rate of decentralized\noptimization algorithms on convex problems (Scaman et al.,\n1Note the optimization process itself (as for instance the com-\nputed result) might leak information about the data of other nodes.\nWe do not focus on quantifying notions of privacy in this work.\n2For better connected topologies sometimes more efﬁcient all-\nreduce and broadcast implementations are available.\nDecentralized Stochastic Optimization and Gossip Algorithms with Compressed Communication\n2017; 2018). We prove the ﬁrst analogue result for the im-\nportant case of decentralized stochastic gradient descent\n(SGD), proving convergence at rateO(1/(nT )) (ignoring\nfor now higher order terms) on strongly convex functions\nwhereT denotes the number of iterations.\nThis result is signiﬁcant since stochastic methods are highly\npreferred for their efﬁciency over deterministic gradient\nmethods in machine learning applications. Our algorithm,\nCHOCO -SGD , is as efﬁcient in terms of iterations as cen-\ntralized mini-batch SGD (and consequently also achieves\na speedup of factor n compared to the serial setting on a\nsingle node) but avoids the communication bottleneck that\ncentralized algorithms suffer from.\nCommunication Compression. In distributed training,\nmodel updates (or gradient vectors) have to be exchanged\nbetween the worker nodes. To reduce the amount of data\nthat has to be sent, gradientcompression has become a popu-\nlar strategy. These ideas have recently been introduced also\nto the decentralized setting by Tang et al. (2018a). How-\never, their analysis only covers unbiased compression op-\nerators with very (unreasonably) high accuracy constraints.\nHere we propose the ﬁrst method that supports arbitrary low\naccuracy and even biased compression operators, such as\nin (Alistarh et al., 2018; Lin et al., 2018; Stich et al., 2018).\nContributions. Our contributions can be summarized as:\n• We show that the proposed CHOCO -SGD converges at\nrateO(1/(nT )+1/(Tρ 2δ)2), whereT denotes the num-\nber of iterations,n the number of workers,ρ the eigengap\nof the gossip (connectivity) matrix and δ≤ 1 the com-\npression quality factor (δ = 1 meaning no compression).\nDespite ρ and δ affecting the higher order terms, the\nﬁrst term in the rate,O(1/(nT )), is the same as for the\ncentralized baseline with exact communication, achiev-\ning the same speedup as centralized mini-batch SGD\nwhen the number n of workers grows. This is veriﬁed\nexperimentally on the ring topology and by reducing the\ncommunication by a factor of 100 (δ = 1\n100).\n• We present the ﬁrst linearly-converging gossip algorithm\nwith communication compression for the distributed aver-\nage consensus problem. Our algorithm, CHOCO -G OSSIP ,\nconverges at linear rateO(1/(ρ2δ) log(1/ε)) for accu-\nracyε >0, and allows arbitrary communication com-\npression operators (including biased and unbiased ones).\nIn contrast, previous works either exhibited sublinear\nconvergence, or required very high-precision quantiza-\ntionδ≈ 1, or could only show convergence towards a\nneighborhood of the optimal solution.\n• CHOCO -SGD signiﬁcantly outperforms state-of-the-art\nmethods for decentralized optimization with gradient\ncompression, such as ECD-SGD and DCD-SGD intro-\nduced in (Tang et al., 2018a), in all our experiments.\n2. Related Work\nStochastic gradient descent (SGD) (Robbins & Monro,\n1951; Bottou, 2010) and variants thereof are the standard\nalgorithms for machine learning problems of the form (1),\nthough it is an inherently serial algorithm that does not take\nthe distributed setting into account. Mini-batch SGD (Dekel\net al., 2012) is the natural parallelization of SGD for (1) in\nthe centralized setting, i.e. when a master node collects the\nupdates from all worker nodes, and serves a baseline here.\nDecentralized Optimization. The study of decentralized\noptimization algorithms can be tracked back at least to the\n1980s (Tsitsiklis, 1984). Decentralized algorithms are some-\ntimes referred to as gossip algorithms (Kempe et al., 2003;\nXiao & Boyd, 2004; Boyd et al., 2006) as the information\nis not broadcasted by a central entity, but spreads—similar\nto gossip—along the edges speciﬁed by the communica-\ntion graph. The most popular algorithms are based on\n(sub)gradient descent (Nedi ´c & Ozdaglar, 2009; Johans-\nson et al., 2010), alternating direction method of multipliers\n(ADMM) (Wei & Ozdaglar, 2012; Iutzeler et al., 2013) or\ndual averaging (Duchi et al., 2012; Nedi´c et al., 2015). He\net al. (2018) address the more speciﬁc problem class of gen-\neralized linear models.\nFor the deterministic (non-stochastic) convex version of (1)\na recent line of work developed optimal algorithms based\non acceleration (Jakoveti ´c et al., 2014; Scaman et al., 2017;\n2018; Uribe et al., 2018). Reisizadeh et al. (2018) and Doan\net al. (2018) studied quantization in this setting. Reisizadeh\net al. (2018) could achieve only sublinear rate for smooth\nand strongly convex objectives, while (Doan et al., 2018)\nconsidered non-smooth objectives and provided sublinear\nrates, matching optimal rates up to logarithmic factor (Sca-\nman et al., 2018). Rates for the stochastic setting are derived\nin (Shamir & Srebro, 2014; Rabbat, 2015), under the as-\nsumption that the distributions on all nodes are equal. Such\nan i.i.d. assumption is a strong restriction which prohibits\nmost distributed machine learning applications, for example\nalso federated learning setting (McMahan et al., 2017). Our\nalgorithm CHOCO -SGD overcomes this and is free of i.i.d.\nassumptions. Also, (Rabbat, 2015) requires multiple com-\nmunication rounds per stochastic gradient computation and\nso is not suited for sparse communication, as the required\nnumber of communication rounds would increase propor-\ntionally to the sparsity. Lan et al. (2018) applied gradient\nsliding techniques allowing to skip some of the communica-\ntion rounds. Assran et al. (2019) have studied time-varying\nnetworks; (Yu et al., 2019) in the case of parameter servers.\nLian et al. (2017); Tang et al. (2018b;a); Assran et al. (2019)\nconsider the non-convex setting with Tang et al. (2018a)\nalso applying gradient quantization techniques to reduce\nthe communication cost. However, their algorithms require\nvery high precision quantization, a constraint we overcome.\nDecentralized Stochastic Optimization and Gossip Algorithms with Compressed Communication\nGradient Compression. Instead of transmitting a full di-\nmensional (gradient) vector g∈ Rd, methods with gradient\ncompression transmit a compressed vector Q(g) instead,\nwhereQ: Rd→ Rd is a (random) operator chosen such\nthatQ(g) can be more efﬁciently represented, for instance\nby using limited bit representation (quantization) or enforc-\ning sparsity. A class of very common quantization opera-\ntors is based on random dithering (Goodall, 1951; Roberts,\n1962) that is in addition also unbiased, Eξ Q(x) = x,\n∀x∈ Rd, see (Alistarh et al., 2017; Wen et al., 2017; Zhang\net al., 2017). Much sparser vectors can be obtained by\nrandom sparsiﬁcation techniques that randomly mask the\ninput vectors and only preserve a constant number of coor-\ndinates (Wangni et al., 2018; Konecny & Richt´arik, 2018;\nStich et al., 2018). Techniques that do not directly quantize\ngradients, but instead maintain additional states are known\nto perform better in theory and practice (Seide et al., 2014;\nLin et al., 2018; Stich et al., 2018), an approach that we pick\nup here. Our analysis also covers deterministic and biased\ncompression operators, such as in (Alistarh et al., 2018;\nStich et al., 2018). We will not further distinguish between\nsparsiﬁcation and quantization approaches, and refer to both\nof them as compression operators in the following.\nDistributed Average Consensus. The average consensus\nproblem consists in ﬁnding the average vector ofn local vec-\ntors (see (2) below for a formal deﬁnition). The problem is\nan important sub-routine of many decentralized algorithms.\nGossip-type algorithms converge linearly for average con-\nsensus (Kempe et al., 2003; Xiao & Boyd, 2004; Olfati-\nSaber & Murray, 2004; Boyd et al., 2006). However, for\nconsensus algorithms with compressed communication it\nhas been remarked that the standard gossip algorithm does\nnot converge to the correct solution (Xiao et al., 2005). The\nproposed schemes in (Carli et al., 2007; Nedi´c et al., 2008;\nAysal et al., 2008; Carli et al., 2010b; Yuan et al., 2012) do\nonly converge to a neighborhood (whose size depends on\nthe compression accuracy) of the solution.\nIn order to converge, adaptive schemes (with varying com-\npression accuracy) have been proposed (Carli et al., 2010a;\nFang & Li, 2010; Li et al., 2011; Thanou et al., 2013). How-\never, these approaches fall back to full (uncompressed) com-\nmunication to reach high accuracy. In contrast, our method\nconverges linearly, even for arbitrary compressed commu-\nnication, without requiring adaptive accuracy. We are not\naware of a method in the literature with similar guarantees.\n3. Average Consensus with Communication\nCompression\nIn this section we present CHOCO -G OSSIP , a novel gos-\nsip algorithm for distributed average consensus with com-\npressed communication. The average consensus problem is\nan important special case of type (1), and formalized as\nx := 1\nn\nn∑\ni=1\nxi, (2)\nfor vectors xi ∈ Rd distributed on n nodes (consider\nfi(x) = 1\n2∥x− xi∥2 in (1)). Our proposed algorithm will\nlater serve as a crucial primitive in our optimization algo-\nrithm for the general optimization problem (1), but is of\nindependent interest for any average consensus problem\nwith communication constraints.\nIn Sections 3.1–3.3 below we ﬁrst review existing schemes\nthat we later consider as baselines for the numerical com-\nparison. The novel algorithm follows in Section 3.4.\n3.1. Gossip algorithms\nThe classic decentralized algorithms for the average consen-\nsus problem are gossip type algorithms (see e.g. (Xiao &\nBoyd, 2004)) that generate sequences\n{\nx(t)\ni\n}\nt≥0 on every\nnodei∈ [n] by iterations of the form\nx(t+1)\ni := x(t)\ni +γ\nn∑\nj=1\nwij∆(t)\nij . (3)\nHereγ∈ (0, 1] denotes a stepsize parameter,wij∈ [0, 1]\naveraging weights and ∆(t)\nij ∈ Rd denotes a vector that is\nsent from node j to nodei in iterationt. No communica-\ntion is required when wij = 0 . If we assume symmetry,\nwij =wji, the weights naturally deﬁne the communication\ngraphG = ([n],E ) with edges{i,j}∈ E ifwij > 0 and\nself-loops{i}∈ E fori∈ [n]. The convergence rate of\nscheme (3) crucially depends on the connectivity matrix\nW∈ Rn×n of the network deﬁned as (W )ij = wij, also\ncalled the interaction or gossip matrix.\nDeﬁnition 1 (Gossip matrix) . We assume that W ∈\n[0, 1]n×n is a symmetric ( W = W⊤) doubly stochastic\n(W 1 = 1,1⊤W = 1⊤) matrix with eigenvalues 1 =\n|λ1(W )|>|λ2(W )|≥···≥| λn(W )| and spectral gap\nρ := 1−|λ2(W )|∈ (0, 1]. (4)\nIt will also be convenient to deﬁne\nβ :=∥I−W∥2∈ [0, 2]. (5)\nTable 1 depicts values of the spectral gap for important\nnetwork topologies (with uniform averaging between the\nnodes). For the special case of uniform averaging on con-\nnected graphs it holdsρ> 0 (see e.g. (Xiao & Boyd, 2004)).\ngraph/topologyρ−1 node degree\nring O(n2) 2\n2d-torus O(n) 4\nfully connectedO(1) n−1\nTable 1. Spectral gapρ for some important network topologies on\nn nodes (see e.g. (Aldous & Fill, 2002, p. 169)) for uniformly\naveragingW , i.e.wij = 1\ndeg(i) = 1\ndeg(j) for {i,j } ∈ E.\nDecentralized Stochastic Optimization and Gossip Algorithms with Compressed Communication\n3.2. Gossip with Exact Communication\nFor a ﬁxed gossip matrixW , the classical algorithm ana-\nlyzed in (Xiao & Boyd, 2004) corresponds to the choice\nγ := 1, ∆(t)\nij := x(t)\nj − x(t)\ni , (E-G)\nin (3), with (E-G) standing for exact gossip. This scheme\ncan also conveniently be written in matrix notation as\nX (t+1) :=X (t) +γX (t)(W−I), (6)\nfor iteratesX (t) := [x(t)\n1 ,..., x(t)\nn ]∈ Rd×n.\nTheorem 1. Letγ∈ (0, 1] andρ be the spectral gap ofW .\nThen the iterates of (E-G) converge linearly to the average\nx = 1\nn\n∑n\ni=1 x(0)\ni with the rate\nn∑\ni=1\nx(t)\ni − x\n2\n≤ (1−γρ)2t\nn∑\ni=1\nx(0)\ni − x\n2\n.\nForγ = 1 the result corresponds to (Xiao & Boyd, 2004),\nhere we slightly extend the analysis for arbitrary stepsizes.\nThe short proof shows the elegance of the matrix notation\n(that we will later also adapt for the proofs that will follow).\nProof forγ = 1. LetX := [ x,..., x]∈ Rd×n. Then for\nγ = 1 the theorem follows from the observation\nX (t+1)−X\n2\nF\n(6)\n=\n(X (t)−X)W\n2\nF\n=\n(X (t)−X)(W− 1\nn 11⊤)\n2\nF\n≤\nW− 1\nn 11⊤2\n2\nX (t)−X\n2\nF\n= (1−ρ)2X (t)−X\n2\nF.\nHere on the second line we used the crucial identity\nX (t)( 1\nn 11⊤) =X, i.e. the algorithm preserves the average\nover all iterations. This can be seen from (6):\nX (t+1)( 1\nn 11⊤) =X (t)W ( 1\nn 11⊤) =X (t)( 1\nn 11⊤) =X,\nby Deﬁnition 1. The proof for arbitraryγ follows the same\nlines and is given in the appendix.\n3.3. Gossip with Quantized Communication\nIn every round of scheme (E-G) a full dimensional vector\ng∈ Rd is exchanged between two neighboring nodes for\nevery link on the communication graph (nodej sends g =\nx(t)\nj to all its neighbors i,{i,j}∈ E). A natural way to\nreduce the communication is to compress g before sending\nit, denoted asQ(g), for a (potentially random) compression\nQ: Rd → Rd. Informally, we can think of Q as either\na sparsiﬁcation operator (that enforces sparsity of Q(g))\nor a quantization operator that reduces the number of bits\nrequired to representQ(g). For instance random rounding\nto less precise ﬂoating point numbers or to integers.\nAysal et al. (2008) propose the quantized gossip (Q1-G),\nγ := 1, ∆(t)\nij :=Q(x(t)\nj )− x(t)\ni , (Q1-G)\nin scheme (3), i.e. to apply the compression operator directly\non the message that is send out from node j to node i.\nHowever, this algorithm does not preserve the average of\nthe iterates over the iterations, 1\nn\n∑n\ni=1 x(0)\ni ̸= 1\nn\n∑n\ni=1 x(t)\ni\nfort≥ 1, and as a consequence does not converge to the\noptimal solution x of (2).\nAn alternative proposal by Carli et al. (2007) alleviates this\ndrawback. The scheme\nγ := 1, ∆(t)\nij :=Q(x(t)\nj )−Q(x(t)\ni ), (Q2-G)\npreserves the average of the iterates over the iterations. How-\never, the scheme also fails to converge for arbitrary preci-\nsion. If x̸= 0, the noise introduced by the compression,Q(x(t)\nj )\n, does not vanish fort→∞ . As a consequence,\nthe iterates oscillate around x when compression error be-\ncomes larger than the suboptimality\nx(t)\ni − x\n.\nBoth these schemes have been theoretically studied in (Carli\net al., 2010b) under assumption of unbiasendness, i.e. as-\nsuming EQ Q(x) = x for all x∈ Rd. We will later adopt\nthis theoretically understood setting in our experiments.\n3.4. Proposed Method for Compressed Communication\nWe propose the novel compressed gossip scheme CHOCO -\nGOSSIP that supports a much larger class of compression\noperators, beyond unbiased quantization as for the schemes\nabove. The algorithm can be summarized as\n∆(t)\nij := ˆx(t)\nj − ˆx(t)\ni ,\nˆx(t+1)\nj := ˆx(t)\nj +Q(x(t+1)\nj − ˆx(t)\nj ),\n(CHOCO -G)\nfor a stepsizeγ <1 depending on the speciﬁc compression\noperatorQ (this will be detailed below). Here ˆx(t)\ni ∈ Rd\ndenote additional variables that are stored by all neighbors\nj of nodei,{i,j}∈ E, as well as on nodei itself.\nWe will show in Theorem 2 below that this scheme (i) pre-\nserves the averages of the iterates x(t)\ni , i∈ [n] over the\niterationst≥ 0. Moreover, (ii) the noise introduced by the\ncompression operator vanishes ast→∞ . Precisely, we will\nshow that (x(t)\ni , ˆx(t)\ni )→ (x, x) fort→∞ for everyi∈ [n].\nConsequently, the argument forQ in (CHOCO -G) goes to\nzero, and the noise introduced byQ can be controlled.\nThe proposed scheme is summarized in Algorithm 1. Every\nworkeri∈ [n] stores and updates its own local variable xi\nas well as the variables ˆxj for all neighbors (including itself)\nj :{i,j}∈ E. This seems to require each machine to store\ndeg(i) + 2 vectors. This is not necessary and the algorithm\nDecentralized Stochastic Optimization and Gossip Algorithms with Compressed Communication\nAlgorithm 1 CHOCO -GOSSIP\ninput : Initial values x(0)\ni ∈ Rd on each node i∈ [n],\nstepsize γ, communication graph G = ([n],E ) and\nmixing matrixW , initialize ˆx(0)\ni := 0∀i\n1: fort in 0...T −1 do in parallel for all workersi∈ [n]\n2: x(t+1)\ni := x(t)\ni +γ∑\nj:{i,j}∈Ewij\n(ˆx(t)\nj − ˆx(t)\ni\n)\n3: q(t)\ni :=Q(x(t+1)\ni − ˆx(t)\ni )\n4: for neighborsj :{i,j}∈ E (including{i}∈ E) do\n5: Send q(t)\ni and receive q(t)\nj\n6: ˆx(t+1)\nj := ˆx(t)\nj + q(t)\nj\n7: end for\n8: end for\ncould be re-written in a way that every node stores onlythree\nvectors: xi, ˆxi and si =∑\nj:{i,j}∈Ewij ˆxj. For simplicity,\nwe omit this modiﬁcation here and refer to Appendix E for\nthe exact form of the memory-efﬁcient algorithm.\n3.5. Convergence Analysis for CHOCO -G OSSIP\nWe analyze Algorithm 1 under the following general quality\nnotion for the compression operatorQ.\nAssumption 1 (Compression operator). We assume that the\ncompression operatorQ: Rd→ Rd satisﬁes\nEQ∥Q(x)− x∥ 2≤ (1−δ)∥x∥2, ∀x∈ Rd, (7)\nfor a parameter δ >0. Here EQ denotes the expectation\nover the internal randomness of operatorQ.\nExample operators that satisfy (7) include\n• sparsiﬁcation: Randomly selectingk out ofd coordinates\n(randk), or thek coordinates with highest magnitude val-\nues (topk) giveδ = k\nd (Stich et al., 2018, Lemma A.1).\n• randomized gossip: Setting Q(x) = x with probability\np∈ (0, 1] andQ(x) = 0 otherwise, givesδ =p.\n• rescaled unbiased estimators: suppose EQ Q(x) = x,\n∀x∈ Rd and EQ∥Q(x)∥2 ≤ τ∥x∥2, then Q′(x) :=\n1\nτQ(x) satisﬁes (7) withδ = 1\nτ .\n• random quantization: For precision (levels)s∈ N+, and\nτ = (1 + min{d/s2,\n√\nd/s}) the quantization operator\nqsgds(x) = sign(x)·∥ x∥\nsτ ·\n⌊\ns|x|\n∥x∥ +ξ\n⌋\n,\nfor random variable ξ ∼u.a.r. [0, 1]d satisﬁes (7) with\nδ = 1\nτ (Alistarh et al., 2017, Lemma 3.1).\nTheorem 2. CHOCO -G OSSIP (Algorithm 1) converges lin-\nearly for average consensus:\net≤\n(\n1− ρ2δ\n82\n)t\ne0,\nwhen using the stepsize γ := ρ2δ\n16ρ+ρ2+4β2+2ρβ2−8ρδ ,\nwhereδ is the compression factor as in Assumption 1, and\net = EQ\n∑n\ni=1\n(x(t)\ni − x\n2\n+\nx(t)\ni − ˆx(t)\ni\n2)\n.\nFor the proof we refer to the appendix. For the exact com-\nmunication caseδ = 1 we recover the rate from Theorem 1\nfor stepsizeγ <1 up to constant factors (which seems to be\na small artifact of our proof technique). The theorem shows\nconvergence for arbitraryδ >0, showing the superiority of\nscheme (CHOCO -G) over (Q1-G) and (Q2-G).\n4. Decentralized Stochastic Optimization\nWe now leverage our proposed average consensus Alg. 1 to\nachieve consensus among the compute nodes in a decentral-\nized optimization setting with communication restrictions.\nIn the decentralized optimization setting (1), not only does\nevery node have a different local objectivefi, but we also\nallow eachfi to have stochastic optimization (or sum) struc-\nture, that is fi(x) := Eξi∼Di Fi(x,ξi) , (8)\nfor a loss function Fi : Rd× Ω → R and distributions\nD1,..., Dn which can be different on every node. Our\nframework therefore covers both stochastic optimization\n(e.g. when allDi are identical) and empirical risk minimiza-\ntion when theDi’s are discrete with disjoint support.\n4.1. Proposed Scheme for Decentralized Optimization\nOur proposed method CHOCO -SGD —Communication-\nCompressed Decentralized SGD—is stated in Algorithm 2.\nAlgorithm 2 CHOCO -SGD\ninput : Initial values x(0)\ni ∈ Rd on each node i∈ [n],\nconsensus stepsizeγ, SGD stepsizes{ηt}t≥0, commu-\nnication graph G = ([n],E ) and mixing matrix W ,\ninitialize ˆx(0)\ni := 0∀i\n1: fort in 0...T −1 do in parallel for all workersi∈ [n]\n2: Sampleξ(t)\ni , compute gradient g(t)\ni :=∇Fi(x(t)\ni ,ξ (t)\ni )\n3: x\n(t+ 1\n2 )\ni := x(t)\ni −ηtg(t)\ni\n4: x(t+1)\ni := x\n(t+ 1\n2 )\ni +γ∑\nj:{i,j}∈Ewij\n(ˆx(t)\nj − ˆx(t)\ni\n)\n5: q(t)\ni :=Q(x(t+1)\ni − ˆx(t)\ni )\n6: for neighborsj :{i,j}∈ E (including{i}∈ E) do\n7: Send q(t)\ni and receive q(t)\nj\n8: ˆx(t+1)\nj := q(t)\nj + ˆx(t)\nj\n9: end for\n10: end for\nThe algorithm consists of four parts. The stochastic gradient\nstep in line 3, iterate update in step 4, application of the\ncompression operator in step 5, followed by the(CHOCO -G)\nlocal communication in lines 6–9.\nDecentralized Stochastic Optimization and Gossip Algorithms with Compressed Communication\nRemark 3. As a special case for δ = 1 and consensus\nstepsizeγ = 1, CHOCO -SGD (Algorithm 2) recovers the\nfollowing standard variant of decentralized SGD with gossip\n(similar e.g. to (Sirb & Ye, 2016; Lian et al., 2017)), given\nfor illustration in Algorithm 3.\nAlgorithm 3 PLAIN DECENTRALIZED SGD\n1: fort in 0...T −1 do in parallel for all workersi∈ [n]\n2: Sampleξ(t)\ni , compute gradient g(t)\ni :=∇Fi(x(t)\ni ,ξ (t)\ni )\n3: x\n(t+ 1\n2 )\ni := x(t)\ni −ηtg(t)\ni\n4: x(t+1)\ni :=∑n\ni=1wijx\n(t+ 1\n2 )\nj\n5: end for\n4.2. Convergence Analysis for CHOCO -SGD\nAssumption 2. We assume that each functionfi : Rd→ R\nfori∈ [n] isL-smooth andµ-strongly convex and that the\nvariance on each worker is bounded\nEξi∥∇Fi(x,ξi)−∇fi(x)∥2≤σ2\ni , ∀x∈ Rd,i∈ [n],\nEξi∥∇Fi(x,ξi)∥ 2≤G2, ∀x∈ Rd,i∈ [n],\nwhere Eξi[·] denotes the expectation overξi∼D i. It will\nbe also convenient to denote\nσ2 := 1\nn\nn∑\ni=1\nσ2\ni.\nFor the (standard) deﬁnitions of smoothness and strong\nconvexity we refer to Appendix A.1. The assumptions above\ncould be relaxed to only hold for x∈\n{\nx(t)\ni\n}T\nt=1, the set of\niterates of Algorithm 2.\nTheorem 4. Under Assumption 2, Algorithm 2 with SGD\nstepsizesηt := 4\nµ(a+t) for parametera≥ max\n{\n410\nρ2δ, 16κ\n}\nfor condition numberκ = L\nµ and consensus stepsizeγ :=\nγ(ρ,δ ) chosen as in Theorem 2, converges with the rate\nEΥ(T ) =O\n( σ2\nµnT\n)\n+O\n( κG2\nµδ2ρ4T 2\n)\n+O\n( G2\nµδ3ρ6T 3\n)\n,\nwhere Υ(T ) := f(x(T )\navg)− f⋆ for an averaged iterate\nx(T )\navg = 1\nST\n∑T−1\nt=0 wtx(t) with weightswt = (a +t)2, and\nST =∑T−1\nt=0 wt. As reminder, ρ denotes the eigengap of\nW , andδ the compression ratio.\nFor the proof we refer to the appendix. WhenT andσ are\nsufﬁciently large, the second two terms become negligible\ncompared toO\n(σ2\nµnT\n)\n—and we recover the convergence\nrate of of mini-batch SGD in the centralized setting and with\nexact communication. This is because topology (parameter\nρ) and compression (parameter δ) only affect the higher-\norder terms in the rate. We also see that we obtain in this\nsetting an× speed up compared to the serial implementation\nof SGD on only one worker.\n1\n2\n3\n4\n5\n6\n1 2 3\n4 5 6\n7 8 9\nFigure 1. Ring topology (left) and Torus topology (right).\n4.3. Distinction to Previous Baselines\nUnlike the previous methods DCD-SGD and ECD-SGD\nfrom (Tang et al., 2018a), CHOCO -SGD converges under\narbitrary high compression. As main difference to those\nschemes, CHOCO -SGD tries to carefully compensate quan-\ntization errors, while DCD- and ECD-SGD ignore them.\nIn both, DCD- and ECD-SGD, the local variable on each\nworker is updated using only copies on the neighbors (in\nDCD copies and local variables are the same), which do\nnot carry information about the true uncompressed values.\nErrors made previously are lost and cannot be corrected in\nlater iterations. In CHOCO -SGD , the shared copy ˆxi is in\ngeneral different from the private copy xi, allowing to carry\non the true values to the next iterations, and to compensate\nfor errors made in previous quantization steps.\n5. Experiments\nWe ﬁrst compareCHOCO -G OSSIP to the gossip baselines\nfrom Sec. 5.2 and then compare theCHOCO -SGD to state of\nthe art decentralized stochastic optimization schemes (that\nalso support compressed communication) in Sec. 5.3.\n5.1. Shared Experimental Setup\nFor our experiments we always report the number of iter-\nations of the respective scheme, as well as the number of\ntransmitted bits. These quantities are independent of sys-\ntems architectures and network bandwidth.\nDatasets. We rely on the epsilon (Sonnenburg et al.,\n2008) andrcv1 (Lewis et al., 2004) datasets (cf. Table 2).\nCompression operators. We use the (randk), (topk) and\n(qsgds) compression operators introduced in Sec. 3.5, with\nk set to 1% of all coordinates ands∈{ 24, 28}. Details of\ncounting number of bits are presented in Appendix F.2\nIn contrast to CHOCO -GOSSIP , the earlier schemes (Q1-G)\nand (Q2-G) were both analyzed for unbiased compression\noperators (Carli et al., 2010b). In order to reﬂect this theo-\nretically understood setting we use the rescaled operators\n(d\nk· randk) and (τ· qsgds) in combination with those.\n5.2. Average Consensus\nWe compare the performance of the gossip schemes (E-G)\n(exact communication), (Q1-G), (Q2-G) (both with unbi-\nDecentralized Stochastic Optimization and Gossip Algorithms with Compressed Communication\n0 100 200 300 400\nIteration\n10−9\n10−7\n10−5\n10−3\n10−1\nError\nqsgd 8bit\n(E-G)\n(Q1-G), qsgd 8bit\n(Q2-G), qsgd 8bit\nCHOCO, qsgd 8bit\n0.0 0.5 1.0 1.5 2.0 2.5 3.0\nNumber of transmitted bits 1e8\n10−8\n10−6\n10−4\n10−2\n100\nError\nqsgd 8bit\n(E-G)\n(Q1-G), qsgd 8bit\n(Q2-G), qsgd 8bit\nCHOCO, qsgd 8bit\nFigure 2. Average consensus on the ring topology with n = 25\nnodes,d = 2000 and (qsgd256) compression.\n0 10000 20000 30000\nIteration\n10−9\n10−7\n10−5\n10−3\n10−1\n101\nError\nrand1%/top1%\n(E-G)\n(Q1-G), rand1%\n(Q2-G), rand1%\nCHOCO, rand1%\nCHOCO, top1%\n0.0 0.5 1.0 1.5 2.0 2.5\nNumber of transmitted bits 1e9\n10−9\n10−7\n10−5\n10−3\n10−1\n101\nError\nrand1%/top1%\n(E-G)\n(Q1-G), rand1%\n(Q2-G), rand1%\nCHOCO, rand1%\nCHOCO, top1%\nFigure 3. Average consensus on the ring topology with n = 25\nnodes,d = 2000 and (rand1%) and (top1%) compression.\nased compression), and our scheme(CHOCO -G) in Figure 2\nfor the (qsgd256) compression scheme and in Figure 3 for\nthe random (rand1%) compression scheme. In addition, we\nalso depict the performance of CHOCO -GOSSIP with biased\n(top1%) compression. We use ring topology with uniformly\naveraging mixing matrix W as in Figure 1, left. The step-\nsizes γ that were used for CHOCO -G OSSIP are listed in\nTable 3. We consider here the consensus problem(2) with\ndata (xi + 1)∈ Rd on the i-machine with xi being the\ni-th vector in the epsilon dataset. The shift was added to\nmove the average away from 0, as some of the schemes\nare biased towards this special output. We depict the errors\n1\nn\n∑n\ni=1\nx(t)\ni − x\n2\n. For more details, plots of the full error\net and additional experiments we refer to Appendix G.1.\nThe proposed scheme (CHOCO -G) with 8 bit quantization\n(qsgd256) converges with the same rate as(E-G) that uses ex-\nact communications (Fig. 2, left), while it requires much less\ndata to be transmitted (Fig. 2, right). The schemes (Q1-G)\nand (Q2-G) do not converge in both settings (Fig. 2, 3,\nright). (Q1-G) diverges because the quantization error is too\nlarge already after the ﬁrst step.\nWith sparsiﬁed communication (rand1%), i.e. transmitting\nonly 1% of all the coordinates, the scheme (Q1-G) quickly\nzeros out all the coordinates (Fig. 3). CHOCO -G OSSIP\nproves to be more robust and converges. The observed\nrate matches with the theoretical ﬁndings, as we expect the\nscheme with factor 100× compression to be 100× slower\nthan (E-G) without compression. In terms of total data\ntransmitted, both schemes converge at approximately same\nspeed (Fig. 3, right). We also see that (rand1%) sparsiﬁca-\ntion can give additional gains and comes out as the most\ndata-efﬁcient method in these experiments.\ndataset m d density\nepsilon 400000 2000 100%\nrcv1 20242 47236 0.15%\nTable 2. Size (m,d ) and density of\nthe datasets.\nexperiment γ\nCHOCO,(qsgd256) 1\nCHOCO, (rand1%) 0.011\nCHOCO, (top1%) 0.046\nTable 3. Tuned stepsizesγ\nfor averaging in Figs. 2– 3.\nepsilon rcv1\nalgorithm a b γ a b γ\nPLAIN 0.1 d - 1 1 -\nCHOCO,(qsgd16) 0.1 d 0.34 1 1 0.078\nCHOCO, (rand1%) 0.1 d 0.01 1 1 0.016\nCHOCO, (top1%) 0.1 d 0.04 1 1 0.04\nDCD, (rand1%) 10−15 d - 10−10 d -\nDCD,(qsgd16) 0.01 d - 10−10 d -\nECD, (rand1%) 10−10 d - 10−10 d -\nECD, (qsgd16) 10−12 d - 10−10 d -\nTable 4. SGD learning rates ηt = ma\nt+b and consensus learning\nratesγ used in the experiments in Figs. 5–6. The parameters where\ntuned separately for each algorithm, tuning details can be found\nin Appendix F.1. The ECD and DCD stepsizes are small because\nthe algorithms were observed to diverge for larger choices.\n5.3. Decentralized SGD\nWe assess the performance of CHOCO -SGD on logistic\nregression, deﬁned as 1\nm\n∑m\nj=1 log(1 + exp(−bja⊤\nj x)) +\n1\n2m∥x∥2, where aj ∈ Rd andbj ∈{− 1, 1} are the data\nsamples andm denotes the number of samples in the dataset.\nWe distribute them data samples evenly among then work-\ners and consider two settings: (i) randomly shufﬂed, where\ndatapoints are randomly assigned to workers, and the more\ndifﬁcult (ii)sorted setting, where each worker only gets data\nsamples just from one class (with the possible exception of\none worker that gets two labels assigned). Moreover, we\ntry to make the setting as difﬁcult as possible, meaning that\ne.g. on the ring topology the machines with the same label\nform two connected clusters. We repeat each experiment\nthree times and depict the mean curve and the area corre-\nsponding to one standard deviation. We plot suboptimality,\ni.e.f(x(t))−f⋆ (obtained by the LogisticSGD optimizer\nfrom scikit-learn (Pedregosa et al., 2011)) versus number\nof iterations and the number of transmitted bits between\nworkers, which is proportional to the actual running time if\ncommunication is a bottleneck.\nAlgorithms. As baselines we consider Alg. 3 with exact\ncommunication (denoted as ‘plain’) and the communication\nefﬁcient state-of-the-art optimization schemes DCD-SGD\nand ECD-SGD recently proposed in (Tang et al., 2018a)\n(for unbiased quantization operators) and compare them to\nCHOCO -SGD . We use decaying stepsizeηt = ma\nt+b where\nthe parametersa,b are individually tuned for each algorithm\nand compression scheme, with values given in Table 4. Con-\nsensus learning ratesγ were tuned on the simpler problem\nseparately from optimization (see appendix F.1 for details,\nTable 4 for ﬁnal values).\nDecentralized Stochastic Optimization and Gossip Algorithms with Compressed Communication\nFigure 4. Performance of Algorithm 3 on ring, torus and fully connected topologies for n ∈ { 9, 25, 64} nodes. Here we consider the\nsorted setting, whilst the performance for randomly shufﬂed data is depicted in the Appendix G.\nFigure 5. Comparison of Algorithm 3 (plain), ECD-SGD, DCD-\nSGD and CHOCO -SGD with (rand1%) sparsiﬁcation (in addition\n(top1%) for CHOCO -SGD ), forepsilon (top) andrcv1 (bottom)\nin terms of iterations (left) and communication cost (right),n = 9.\nImpact of Topology. In Figure 4 we show the perfor-\nmance of the baseline Algorithm 3 with exact communica-\ntion on different topologies (ring, torus and fully-connected;\nFig. 1) with uniformly averaging mixing matrixW . Note\nthat Algorithm 3 for fully-connected graph corresponds to\nmini-batch SGD. Increasing the number of workers from\nn = 9 ton = 25 andn = 64 shows the mild effect of the\nnetwork topology on the convergence. We observe that the\nsorted setting is more difﬁcult than therandomly shufﬂed\nsetting (see Fig. 11 in the Appendix G), where the conver-\ngence behavior remains almost unaffected. In the following\nwe focus on the hardest case, i.e. the ring topology.\nComparison to Baselines. Figures 5 and 6 depict the per-\nformance of the algorithms on the ring topology withn = 9\nnodes for sorted data of the epsilon and rcv1 datasets.\nCHOCO -SGD performs almost as good as the exact Alg. 3,\nbut uses 100× less communication with ( rand1%) sparsi-\nﬁcation (Fig. 5, right) and approximately 13× less com-\nmunication for (qsgd16) quantization. The (top1%) variant\nperforms slightly better than (rand1%) sparsiﬁcation.\nCHOCO -SGD consistently outperforms DCD-SGD in all\nsettings. We also observed that DCD-SGD starts to per-\nform better for larger number of levelss in the (qsgds) in\nFigure 6. Comparison of Algorithm 3 (plain), ECD-SGD, DCD-\nSGD and CHOCO -SGD with (qsgd16) quantization, forepsilon\n(top) andrcv1 (bottom) in terms of iterations (left) and communi-\ncation cost (right), onn = 9 nodes on a ring topology.\nthe quantiﬁcation operator (increasing communication cost).\nThis is consistent with the reporting in (Tang et al., 2018a)\nthat assumed high precision quantization. As a surprise to\nus, ECD-SGD, which was proposed in (Tang et al., 2018a)\nas the preferred alternative over DCD-SGD for less precise\nquantization operators, always performs worse than DCD-\nSGD, and often diverges.\nFigures for randomly shufﬂed data can be found in the Ap-\npendix G. In that case CHOCO -SGD performs exactly as\nwell as the exact Algorithm 3 in all situations.\nConclusion. The experiments verify our theoretical ﬁnd-\nings: CHOCO -GOSSIP is the ﬁrst linearly convergent gossip\nalgorithm with quantized communication andCHOCO -SGD\nconsistently outperforms the baselines for decentralized op-\ntimization, reaching almost the same performance as exact\ncommunication, while signiﬁcantly reducing communica-\ntion cost. In view of the striking popularity of SGD as\nopposed to full-gradient methods for deep-learning, the ap-\nplication of CHOCO -SGD to decentralized deep learning—\nan instance of problem (1)—is a promising direction. We\nleave the analysis of CHOCO -SGD on non-convex func-\ntion for future work. We believe that most of techniques\npresented here should carry over to the smooth non-convex\nsetting as well.\nDecentralized Stochastic Optimization and Gossip Algorithms with Compressed Communication\nAcknowledgements\nWe acknowledge funding from SNSF grant 200021 175796,\nas well as a Google Focused Research Award.\nReferences\nAldous, D. and Fill, J. A. Reversible markov chains and random\nwalks on graphs, 2002. Monograph, recompiled 2014.\nAlistarh, D., Grubic, D., Li, J., Tomioka, R., and V ojnovic, M.\nQSGD: Communication-efﬁcient SGD via gradient quantiza-\ntion and encoding. NIPS - Advances in Neural Information\nProcessing Systems 30, pp. 1709–1720. 2017.\nAlistarh, D., Hoeﬂer, T., Johansson, M., Konstantinov, N., Khirirat,\nS., and Renggli, C. The convergence of sparsiﬁed gradient\nmethods. NeurIPS - Advances in Neural Information Processing\nSystems 31, pp. 5977–5987. 2018.\nAssran, M., Loizou, N., Ballas, N., and Rabbat, M. Stochastic\nGradient Push for Distributed Deep Learning. ICML, 2019.\nAysal, T. C., Coates, M. J., and Rabbat, M. G. Distributed average\nconsensus with dithered quantization. IEEE Transactions on\nSignal Processing, 56(10):4905–4918, 2008.\nBottou, L. Large-scale machine learning with stochastic gradient\ndescent. In Lechevallier, Y . and Saporta, G. (eds.),Proceedings\nof COMPSTAT’2010, pp. 177–186.\nBoyd, S., Ghosh, A., Prabhakar, B., and Shah, D. Randomized\ngossip algorithms. IEEE/ACM Trans. Netw., 14(SI):2508–2530,\n2006.\nCarli, R., Fagnani, F., Frasca, P., Taylor, T., and Zampieri, S. Av-\nerage consensus on networks with transmission noise or quan-\ntization. In 2007 European Control Conference (ECC) , pp.\n1852–1857, 2007.\nCarli, R., Bullo, F., and Zampieri, S. Quantized average consensus\nvia dynamic coding/decoding schemes. International Journal\nof Robust and Nonlinear Control, 20:156–175, 2010a.\nCarli, R., Frasca, P., Fagnani, F., and Zampieri, S. Gossip consen-\nsus algorithms via quantized communication. Automatica, 46:\n70–80, 2010b.\nDekel, O., Gilad-Bachrach, R., Shamir, O., and Xiao, L. Optimal\ndistributed online prediction using mini-batches.J. Mach. Learn.\nRes., 13(1):165–202, 2012.\nDoan, T. T., Theja Maguluri, S., and Romberg, J. Accelerating the\nConvergence Rates of Distributed Subgradient Methods with\nAdaptive Quantization. arXiv e-prints, art. arXiv:1810.13245,\n2018.\nDuchi, J. C., Agarwal, A., and Wainwright, M. J. Dual averaging\nfor distributed optimization: Convergence analysis and network\nscaling. IEEE Transactions on Automatic Control, 57(3):592–\n606, 2012.\nFang, J. and Li, H. Distributed estimation of gauss - markov ran-\ndom ﬁelds with one-bit quantized data.IEEE Signal Processing\nLetters, 17(5):449–452, 2010.\nGoodall, W. M. Television by pulse code modulation. The Bell\nSystem Technical Journal, 30(1):33–49, 1951.\nHe, L., Bian, A., and Jaggi, M. Cola: Decentralized linear learning.\nIn Advances in Neural Information Processing Systems 31, pp.\n4541–4551. 2018.\nIutzeler, F., Bianchi, P., Ciblat, P., and Hachem, W. Asynchronous\ndistributed optimization using a randomized alternating direc-\ntion method of multipliers. In Proceedings of the 52nd IEEE\nConference on Decision and Control, CDC 2013, Firenze, Italy,\npp. 3671–3676. IEEE, 2013.\nJakoveti´c, D., Xavier, J., and Moura, J. M. F. Fast distributed\ngradient methods. IEEE Transactions on Automatic Control, 59\n(5):1131–1146, 2014.\nJohansson, B., Rabi, M., and Johansson, M. A randomized in-\ncremental subgradient method for distributed optimization in\nnetworked systems. SIAM Journal on Optimization , 20(3):\n1157–1170, 2010.\nKempe, D., Dobra, A., and Gehrke, J. Gossip-based computation\nof aggregate information. In Proceedings of the 44th Annual\nIEEE Symposium on Foundations of Computer Science, FOCS\n’03, pp. 482–, Washington, DC, USA, 2003. IEEE Computer\nSociety.\nKonecny, J. and Richt´arik, P. Randomized Distributed Mean Es-\ntimation: Accuracy vs. Communication. Frontiers in Applied\nMathematics and Statistics, 4:1502, 2018.\nLan, G., Lee, S., and Zhou, Y . Communication-efﬁcient algorithms\nfor decentralized and stochastic optimization. Mathematical\nProgramming, 2018.\nLewis, D. D., Yang, Y ., Rose, T. G., and Li, F. Rcv1: A new\nbenchmark collection for text categorization research. J. Mach.\nLearn. Res., 5:361–397, 2004.\nLi, T., Fu, M., Xie, L., and Zhang, J. Distributed consensus\nwith limited communication data rate. IEEE Transactions on\nAutomatic Control, 56(2):279–292, 2011.\nLian, X., Zhang, C., Zhang, H., Hsieh, C.-J., Zhang, W., and\nLiu, J. Can decentralized algorithms outperform centralized\nalgorithms? A case study for decentralized parallel stochastic\ngradient descent. In NIPS - Advances in Neural Information\nProcessing Systems 30, pp. 5330–5340. 2017.\nLin, Y ., Han, S., Mao, H., Wang, Y ., and Dally, B. Deep gradient\ncompression: Reducing the communication bandwidth for dis-\ntributed training. In ICLR 2018 - International Conference on\nLearning Representations, 2018.\nMcMahan, B., Moore, E., Ramage, D., Hampson, S., and Arcas,\nB. A. y. Communication-Efﬁcient Learning of Deep Networks\nfrom Decentralized Data. In AISTATS 2017 - Proceedings of\nthe 20th International Conference on Artiﬁcial Intelligence and\nStatistics, pp. 1273–1282, 2017.\nNedi´c, A. and Ozdaglar, A. Distributed subgradient methods for\nmulti-agent optimization. IEEE Transactions on Automatic\nControl, 54(1):48–61, 2009.\nNedi´c, A., Olshevsky, A., Ozdaglar, A., and Tsitsiklis, J. N. Dis-\ntributed subgradient methods and quantization effects. In Pro-\nceedings of the 47th IEEE Conference on Decision and Control,\nCDC 2008, pp. 4177–4184, 2008.\nDecentralized Stochastic Optimization and Gossip Algorithms with Compressed Communication\nNedi´c, A., Lee, S., and Raginsky, M. Decentralized online opti-\nmization with global objectives and local communication. In\n2015 American Control Conference (ACC) , pp. 4497–4503,\n2015.\nOlfati-Saber, R. and Murray, R. M. Consensus problems in net-\nworks of agents with switching topology and time-delays. IEEE\nTransactions on Automatic Control, 49(9):1520–1533, 2004.\nPedregosa, F., Varoquaux, G., Gramfort, A., Michel, V ., Thirion,\nB., Grisel, O., Blondel, M., Prettenhofer, P., Weiss, R., Dubourg,\nV ., Vanderplas, J., Passos, A., Cournapeau, D., Brucher, M.,\nPerrot, M., and Duchesnay, E. Scikit-learn: Machine learning in\nPython. Journal of Machine Learning Research, 12:2825–2830,\n2011.\nRabbat, M. Multi-agent mirror descent for decentralized stochastic\noptimization. In 2015 IEEE 6th International Workshop on\nComputational Advances in Multi-Sensor Adaptive Processing\n(CAMSAP), pp. 517–520, 2015.\nRakhlin, A., Shamir, O., and Sridharan, K. Making gradient de-\nscent optimal for strongly convex stochastic optimization. In\nProceedings of the 29th International Coference on Interna-\ntional Conference on Machine Learning, ICML’12, pp. 1571–\n1578, USA, 2012. Omnipress.\nReisizadeh, A., Mokhtari, A., Hassani, S. H., and Pedarsani,\nR. Quantized decentralized consensus optimization. CoRR,\nabs/1806.11536, 2018.\nRobbins, H. and Monro, S. A Stochastic Approximation Method.\nThe Annals of Mathematical Statistics, 22(3):400–407, 1951.\nRoberts, L. Picture coding using pseudo-random noise. IRE\nTransactions on Information Theory, 8(2):145–154, February\n1962. ISSN 0096-1000. doi: 10.1109/TIT.1962.1057702.\nScaman, K., Bach, F., Bubeck, S., Lee, Y . T., and Massouli´e, L.\nOptimal algorithms for smooth and strongly convex distributed\noptimization in networks. In ICML - Proceedings of the 34th\nInternational Conference on Machine Learning, pp. 3027–3036,\n2017.\nScaman, K., Bach, F., Bubeck, S., Massouli ´e, L., and Lee, Y . T.\nOptimal algorithms for non-smooth distributed optimization\nin networks. In Advances in Neural Information Processing\nSystems 31, pp. 2745–2754. 2018.\nSeide, F., Fu, H., Droppo, J., Li, G., and Yu, D. 1-bit stochastic\ngradient descent and its application to data-parallel distributed\ntraining of speech DNNs. In Li, H., Meng, H. M., Ma, B., Chng,\nE., and Xie, L. (eds.), INTERSPEECH, pp. 1058–1062. ISCA,\n2014.\nShamir, O. and Srebro, N. Distributed stochastic optimization\nand learning. 2014 52nd Annual Allerton Conference on Com-\nmunication, Control, and Computing (Allerton), pp. 850–857,\n2014.\nSirb, B. and Ye, X. Consensus optimization with delayed and\nstochastic gradients on decentralized networks. In 2016 IEEE\nInternational Conference on Big Data (Big Data), pp. 76–85,\n2016.\nSonnenburg, S., Franc, V ., Yom-Tov, E., and Sebag, M. Pascal\nlarge scale learning challenge. 25th International Conference\non Machine Learning (ICML2008) Workshop. J. Mach. Learn.\nRes, 10:1937–1953, 01 2008.\nStich, S. U. Local SGD Converges Fast and Communicates Little.\narXiv e-prints, art. arXiv:1805.09767, May 2018.\nStich, S. U., Cordonnier, J.-B., and Jaggi, M. Sparsiﬁed SGD\nwith memory. In NeurIPS - Advances in Neural Information\nProcessing Systems 31, pp. 4452–4463. 2018.\nTang, H., Gan, S., Zhang, C., Zhang, T., and Liu, J. Commu-\nnication compression for decentralized training. In Advances\nin Neural Information Processing Systems 31, pp. 7663–7673.\n2018a.\nTang, H., Lian, X., Yan, M., Zhang, C., and Liu, J.d2: Decentral-\nized training over decentralized data. In ICML - Proceedings\nof the 35th International Conference on Machine Learning, pp.\n4848–4856, 2018b.\nThanou, D., Kokiopoulou, E., Pu, Y ., and Frossard, P. Distributed\naverage consensus with quantization reﬁnement.IEEE Transac-\ntions on Signal Processing, 61(1):194–205, 2013.\nTsitsiklis, J. N. Problems in decentralized decision making and\ncomputation. PhD thesis, Massachusetts Institute of Technology,\n1984.\nUribe, C. A., Lee, S., and Gasnikov, A. A Dual Approach for\nOptimal Algorithms in Distributed Optimization over Networks.\narXiv, September 2018.\nWangni, J., Wang, J., Liu, J., and Zhang, T. Gradient sparsiﬁca-\ntion for communication-efﬁcient distributed optimization. In\nNeurIPS - Advances in Neural Information Processing Systems\n31, pp. 1306–1316. 2018.\nWei, E. and Ozdaglar, A. Distributed alternating direction method\nof multipliers. In 2012 IEEE 51st IEEE Conference on Decision\nand Control (CDC), pp. 5445–5450, 2012.\nWen, W., Xu, C., Yan, F., Wu, C., Wang, Y ., Chen, Y ., and Li,\nH. Terngrad: Ternary gradients to reduce communication in\ndistributed deep learning. In NIPS - Advances in Neural Infor-\nmation Processing Systems 30, pp. 1509–1519. 2017.\nXiao, L. and Boyd, S. Fast linear iterations for distributed averag-\ning. Systems & Control Letters, 53(1):65–78, 2004.\nXiao, L., Boyd, S., and Lall, S. A scheme for robust distributed\nsensor fusion based on average consensus. InIPSN 2005. Fourth\nInternational Symposium on Information Processing in Sensor\nNetworks, 2005., pp. 63–70, 2005.\nYu, C., Tang, H., Renggli, C., Kassing, S., Singla, A., Alistarh,\nD., Zhang, C., and Liu, J. Distributed Learning over Unreliable\nNetworks. ICML 2019, 2019.\nYuan, D., Xu, S., Zhao, H., and Rong, L. Distributed dual averag-\ning method for multi-agent optimization with quantized com-\nmunication. Systems & Control Letters, 61(11):1053 – 1061,\n2012.\nZhang, H., Li, J., Kara, K., Alistarh, D., Liu, J., and Zhang, C.\nZipML: Training linear models with end-to-end low precision,\nand a little bit of deep learning. In ICML - Proceedings of\nthe 34th International Conference on Machine Learning , pp.\n4035–4043, 2017.",
  "values": {
    "Privacy": "No",
    "Explicability": "No",
    "Critiqability": "No",
    "Non-maleficence": "No",
    "Collective influence": "No",
    "Deferral to humans": "No",
    "Interpretable (to users)": "No",
    "Beneficence": "No",
    "Transparent (to users)": "No",
    "User influence": "No",
    "Respect for Persons": "No",
    "Fairness": "No",
    "Respect for Law and public interest": "No",
    "Not socially biased": "No",
    "Autonomy (power to decide)": "No",
    "Justice": "No"
  }
}