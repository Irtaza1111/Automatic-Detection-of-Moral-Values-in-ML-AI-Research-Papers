{
  "pdf": "mescheder18a",
  "title": "mescheder18a",
  "author": "Unknown",
  "paper_id": "mescheder18a",
  "text": "Which Training Methods for GANs do actually Converge?\nLars Mescheder 1 Andreas Geiger 1 2 Sebastian Nowozin 3\nAbstract\nRecent work has shown local convergence of\nGAN training for absolutely continuous data and\ngenerator distributions. In this paper, we show\nthat the requirement of absolute continuity is nec-\nessary: we describe a simple yet prototypical\ncounterexample showing that in the more real-\nistic case of distributions that are not absolutely\ncontinuous, unregularized GAN training is not\nalways convergent. Furthermore, we discuss reg-\nularization strategies that were recently proposed\nto stabilize GAN training. Our analysis shows\nthat GAN training with instance noise or zero-\ncentered gradient penalties converges. On the\nother hand, we show that Wasserstein-GANs and\nWGAN-GP with a ﬁnite number of discriminator\nupdates per generator update do not always con-\nverge to the equilibrium point. We discuss these\nresults, leading us to a new explanation for the\nstability problems of GAN training. Based on\nour analysis, we extend our convergence results\nto more general GANs and prove local conver-\ngence for simpliﬁed gradient penalties even if the\ngenerator and data distributions lie on lower di-\nmensional manifolds. We ﬁnd these penalties to\nwork well in practice and use them to learn high-\nresolution generative image models for a variety\nof datasets with little hyperparameter tuning.\n1. Introduction\nGenerative Adversarial Networks (GANs) (Goodfellow\net al., 2014) are powerful latent variable models that can be\nused to learn complex real-world distributions. Especially\nfor images, GANs have emerged as one of the dominant\napproaches for generating new realistically looking samples\nafter the model has been trained on some dataset.\n1MPI Tübingen, Germany 2ETH Zürich, Switzerland\n3Microsoft Research, Cambridge, UK. Correspondence to: Lars\nMescheder <lars.mescheder@tue.mpg.de>.\nProceedings of the 35 th International Conference on Machine\nLearning, Stockholm, Sweden, PMLR 80, 2018. Copyright 2018\nby the author(s).\nMethod Local\nconvergence\n(a.c. case)\nLocal\nconvergence\n(general case)\nunregularized (Goodfellow et al., 2014)\u0013 \u0017\nWGAN (Arjovsky et al., 2017) \u0017 \u0017\nWGAN-GP (Gulrajani et al., 2017) \u0017 \u0017\nDRAGAN (Kodali et al., 2017) \u0013 \u0017\nInstance noise (Sønderby et al., 2016) \u0013 \u0013\nConOpt (Mescheder et al., 2017) \u0013 \u0013\nGradient penalties (Roth et al., 2017) \u0013 \u0013\nGradient penalty on real data only \u0013 \u0013\nGradient penalty on fake data only \u0013 \u0013\nTable 1. Convergence properties of different GAN training algo-\nrithms for general GAN-architectures. Here, we distinguish be-\ntween the case where both the data and generator distributions are\nabsolutely continuous (a.c.) and the general case where they may\nlie on lower dimensional manifolds.\nHowever, while very powerful, GANs can be hard to train\nand in practice it is often observed that gradient descent\nbased GAN optimization does not lead to convergence. As\na result, a lot of recent research has focused on ﬁnding\nbetter training algorithms (Arjovsky et al., 2017; Gulrajani\net al., 2017; Kodali et al., 2017; Sønderby et al., 2016; Roth\net al., 2017) for GANs as well as gaining better theoretically\nunderstanding of their training dynamics (Arjovsky et al.,\n2017; Arjovsky & Bottou, 2017; Mescheder et al., 2017;\nNagarajan & Kolter, 2017; Heusel et al., 2017).\nDespite practical advances, the training dynamics of GANs\nare still not completely understood. Recently, Mescheder\net al. (2017) and Nagarajan & Kolter (2017) showed that\nlocal convergence and stability properties of GAN train-\ning can be analyzed by examining the eigenvalues of the\nJacobian of the the associated gradient vector ﬁeld: if the\nJacobian has only eigenvalues with negative real-part at the\nequilibrium point, GAN training converges locally for small\nenough learning rates. On the other hand, if the Jacobian has\neigenvalues on the imaginary axis, it is generally not locally\nconvergent. Moreover, Mescheder et al. (2017) showed that\nif there are eigenvalues close but not on the imaginary axis,\nthe training algorithm can require intractably small learning\nrates to achieve convergence. While Mescheder et al. (2017)\nobserved eigenvalues close to the imaginary axis in practice,\nthis observation does not answer the question if eigenvalues\nclose to the imaginary axis are a general phenomenon and if\nyes, whether they are indeed the root cause for the training\nWhich Training Methods for GANs do actually Converge?\ninstabilities that people observe in practice.\nA partial answer to this question was given by Nagarajan\n& Kolter (2017), who showed that for absolutely continu-\nous data and generator distributions1 all eigenvalues of the\nJacobian have negative real-part. As a result, GANs are lo-\ncally convergent for small enough learning rates in this case.\nHowever, the assumption of absolute continuity is not true\nfor common use cases of GANs, where both distributions\nmay lie on lower dimensional manifolds (Sønderby et al.,\n2016; Arjovsky & Bottou, 2017).\nIn this paper we show that this assumption is indeed nec-\nessary: by considering a simple yet prototypical example\nof GAN training we analytically show that (unregularized)\nGAN training is not always locally convergent. We also\ndiscuss how recent techniques for stabilizing GAN train-\ning affect local convergence on our example problem. Our\nﬁndings show that neither Wasserstein GANs (WGANs) (Ar-\njovsky et al., 2017) nor Wasserstein GANs with Gradient\nPenalty (WGAN-GP) (Gulrajani et al., 2017) nor DRAGAN\n(Kodali et al., 2017) converge on this simple example for a\nﬁxed number of discriminator updates per generator update.\nOn the other hand, we show that instance noise (Sønderby\net al., 2016; Arjovsky & Bottou, 2017), zero-centered gradi-\nent penalties (Roth et al., 2017) and consensus optimization\n(Mescheder et al., 2017) lead to local convergence.\nBased on our analysis, we give a new explanation for the\ninstabilities commonly observed when training GANs based\non discriminator gradients orthogonal to the tangent space\nof the data manifold. We also introduce simpliﬁed gradient\npenalties for which we prove local convergence. We ﬁnd\nthat these gradient penalties work well in practice, allowing\nus to learn high-resolution image based generative models\nfor a variety of datasets with little hyperparameter tuning.\nIn summary, our contributions are as follows:\n• We identify a simple yet prototypical counterexample\nshowing that (unregularized) gradient descent based\nGAN optimization is not always locally convergent\n• We discuss if and how recently introduced regulariza-\ntion techniques stabilize the training\n• We introduce simpliﬁed gradient penalties and prove\nlocal convergence for the regularized GAN training\ndynamics\nAll proofs can be found in the supplementary material.\n1Nagarajan & Kolter (2017) also proved local convergence for\na slightly more general family of probability distributions where\nthe support of the generator distribution is equal to the support of\nthe true data distribution near the equilibrium point. Alternatively,\nthey showed that their results also hold when the discriminator\nsatisﬁes certain (strong) smoothness conditions. However, these\nconditions are usually hard to satisfy in practice without prior\nknowledge about the support of the true data distribution.\n2. Instabilities in GAN training\n2.1. Background\nGANs are deﬁned by a min-max two-player game between\na discriminative network Dψ(x) and generative network\nGθ(z). While the discriminator tries to distinguish between\nreal data point and data points produced by the generator,\nthe generator tries to fool the discriminator. It can be shown\n(Goodfellow et al., 2014) that if both the generator and\ndiscriminator are powerful enough to approximate any real-\nvalued function, the unique Nash-equilibrium of this two\nplayer game is given by a generator that produces the true\ndata distribution and a discriminator which is 0 everywhere\non the data distribution.\nFollowing the notation of Nagarajan & Kolter (2017), the\ntraining objective for the two players can be described by\nan objective function of the form\nL(θ, ψ) = Ep(z) [f(Dψ(Gθ(z)))]\n+ EpD(x) [f(−Dψ(x))] (1)\nfor some real-valued function f. The common choice\nf(t) =− log(1 + exp(−t)) leads to the loss function con-\nsidered in the original GAN paper (Goodfellow et al., 2014).\nFor technical reasons we assume that f is continuously dif-\nferentiable and satisﬁesf′(t)̸= 0 for all t∈ R.\nThe goal of the generator is to minimize this loss whereas\nthe discriminator tries to maximize it. Our goal when train-\ning GANs is to ﬁnd a Nash-equilibrium, i.e. a parameter\nassignment (θ∗, ψ∗) where neither the discriminator nor the\ngenerator can improve their utilities unilaterally.\nGANs are usually trained using Simultaneous or Alternating\nGradient Descent (SimGD and AltGD). Both algorithms\ncan be described as ﬁxed point algorithms (Mescheder et al.,\n2017) that apply some operator Fh(θ, ψ) to the parameter\nvalues (θ, ψ) of the generator and discriminator, respectively.\nFor example, simultaneous gradient descent corresponds to\nthe operator Fh(θ, ψ) = (θ, ψ) + h v(θ, ψ), where v(θ, ψ)\ndenotes the gradient vector ﬁeld\nv(θ, ψ) :=\n(−∇θL(θ, ψ)\n∇ψL(θ, ψ)\n)\n. (2)\nSimilarly, alternating gradient descent can be described by\nan operator Fh = F2,h◦ F1,h where F1,h and F2,h perform\nan update for the generator and discriminator, respectively.\nRecently, it was shown (Mescheder et al., 2017) that local\nconvergence of GAN training near an equilibrium point\n(θ∗, ψ∗) can be analyzed by looking at the spectrum of the\nJacobian F′\nh(θ∗, ψ∗) at the equilibrium: if F′\nh(θ∗, ψ∗) has\neigenvalues with absolute value bigger than 1 , the training\nalgorithm will generally not converge to (θ∗, ψ∗). On the\nother hand, if all eigenvalues have absolute value smaller\nWhich Training Methods for GANs do actually Converge?\npD=δ0 pθ=δθ\nDψ(x)\nx\ny\n(a) t = t0\npD=δ0 pθ=δθ\nDψ(x)\nx\ny\n(b) t = t1\nFigure 1. Visualization of the counterexample showing that gra-\ndient descent based GAN optimization is not always convergent:\n(a) In the beginning, the discriminator pushes the generator towards\nthe true data distribution and the discriminator’s slope increases.\n(b) When the generator reaches the target distribution, the slope of\nthe discriminator is largest, pushing the generator away from the\ntarget distribution. This results in oscillatory training dynamics\nthat never converge.\nthan 1, the training algorithm will converge to(θ∗, ψ∗) with\nlinear rate O(|λmax|k) where λmax is the eigenvalue of\nF′(θ∗, ψ∗) with the biggest absolute value. If all eigenval-\nues of F′(θ∗, ψ∗) are on the unit circle, the algorithm can\nbe convergent, divergent or neither, but if it is convergent\nit will generally converge with a sublinear rate. A similar\nresult (Khalil, 1996; Nagarajan & Kolter, 2017) also holds\nfor the (idealized) continuous system\n( ˙θ(t)\n˙ψ(t)\n)\n=\n(\n−∇ψL(θ, ψ)\n∇θL(θ, ψ)\n)\n(3)\nwhich corresponds to training the GAN with inﬁnitely small\nlearning rate: if all eigenvalues of the Jacobian v′(θ∗, ψ∗)\nat a stationary point (θ∗, ψ∗) have negative real-part, the\ncontinuous system converges locally to (θ∗, ψ∗) with lin-\near convergence rate. On the other hand, if v′(θ∗, ψ∗) has\neigenvalues with positive real-part, the continuous system\nis not locally convergent. If all eigenvalues have zero real-\npart, it can be convergent, divergent or neither, but if it is\nconvergent, it will generally converge with a sublinear rate.\nFor simultaneous gradient descent linear convergence can\nbe achieved if and only if all eigenvalues of the Jacobian\nof the gradient vector ﬁeldv(θ, ψ) have negative real part\n(Mescheder et al., 2017). This situation was also considered\nby Nagarajan & Kolter (2017) who examined the asymptotic\ncase of step sizesh that go to0 and proved local convergence\nfor absolutely continuous generator and data distributions\nunder certain regularity assumptions.\n2.2. The Dirac-GAN\nSimple experiments, simple theorems are the building\nblocks that help us understand more complicated systems.\nAli Rahimi - Test of Time Award speech, NIPS 2017\nIn this section, we describe a simple yet prototypical coun-\nterexample which shows that in the general case unregular-\nized GAN training is neither locally nor globally convergent.\nDeﬁnition 2.1. The Dirac-GAN consists of a (univariate)\ngenerator distribution pθ = δθ and a linear discriminator\nDψ(x) = ψ· x. The true data distribution pD is given by a\nDirac-distribution concentrated at 0.\nNote that for the Dirac-GAN, both the generator and the\ndiscriminator have exactly one parameter. This situation\nis visualized in Figure 1. In this setup, the GAN training\nobjective (1) is given by\nL(θ, ψ) = f(ψθ) + f(0) (4)\nWhile using linear discriminators might appear restrictive,\nthe class of linear discriminators is in fact as powerful as\nthe class of all real-valued functions for this example: when\nwe use f(t) =− log(1 + exp(−t)) and we take the supre-\nmum over ψ in (4), we obtain (up to scalar and additive\nconstants) the Jensen-Shannon divergence between pθ and\npD. The same holds true for the Wasserstein-divergence,\nwhen we use f(t) = t and put a Lipschitz constraint on the\ndiscriminator (see Section 3.1).\nWe show that the training dynamics of GANs do not con-\nverge in this simple setup.\nLemma 2.2. The unique equilibrium point of the training\nobjective in (4) is given by θ = ψ = 0 . Moreover, the\nJacobian of the gradient vector ﬁeld at the equilibrium point\nhas the two eigenvalues ±f′(0) i which are both on the\nimaginary axis.\nWe now take a closer look at the training dynamics produced\nby various algorithms for training the Dirac-GAN. First, we\nconsider the (idealized) continuous system in (3): while\nLemma 2.2 shows that the continuous system is generally\nnot linearly convergent to the equilibrium point, it could\nin principle converge with a sublinear convergence rate.\nHowever, this is not the case as the next lemma shows:\nLemma 2.3. The integral curves of the gradient vector ﬁeld\nv(θ, ψ) do not converge to the Nash-equilibrium. More\nspeciﬁcally, every integral curve(θ(t), ψ(t)) of the gradient\nvector ﬁeld v(θ, ψ) satisﬁes θ(t)2 + ψ(t)2 = const for all\nt∈ [0,∞).\nNote that our results do not contradict the results of Nagara-\njan & Kolter (2017) and Heusel et al. (2017): our example\nviolates Assumption IV in Nagarajan & Kolter (2017) that\nthe support of the generator distribution is equal to the sup-\nport of the true data distribution near the equilibrium. It\nalso violates the assumption 2 in Heusel et al. (2017) that\nthe optimal discriminator parameter vector is a continuous\nfunction of the current generator parameters. In fact, unless\n2This assumption is usually even violated by Wasserstein-\nGANs, as the optimal discriminator parameter vector as a function\nof the current generator parameters can have discontinuities near\nthe Nash-equilibrium. See Section 3.1 for details.\nWhich Training Methods for GANs do actually Converge?\n(a) SimGD\n (b) AltGD\nFigure 2. Training behavior of the Dirac-GAN. The starting iterate\nis marked in red.\nθ = 0, there is not even an optimal discriminator parameter\nfor the Dirac-GAN. Indeed, we found that two-time scale\nupdates as suggested by Heusel et al. (2017) do not help con-\nvergence towards the Nash-equilibrium (see Figure 22 in the\nsupplementary material). However, our example seems to\nbe a prototypical situation for (unregularized) GAN training\nwhich usually deals with distributions that are concentrated\non lower dimensional manifolds (Arjovsky & Bottou, 2017).\nWe now take a closer look at the discretized system.\nLemma 2.4. For simultaneous gradient descent, the Ja-\ncobian of the update operator Fh(θ, ψ) has eigenvalues\nλ1/2 = 1± hf′(0)i with absolute values\n√\n1 + h2f′(0)2\nat the Nash-equilibrium. Independently of the learning rate,\nsimultaneous gradient descent is therefore not stable near\nthe equilibrium. Even stronger, for every initial condition\nand learning rate h > 0, the norm of the iterates (θk, ψk)\nobtained by simultaneous gradient descent is monotonically\nincreasing.\nThe behavior of simultaneous gradient descent for our ex-\nample problem is visualized in Figure 2a.\nSimilarly, for alternating gradient descent we have\nLemma 2.5. For alternating gradient descent with ng gen-\nerator and nd discriminator updates, the Jacobian of the\nupdate operator Fh(θ, ψ) has eigenvalues\nλ1/2 = 1− α2\n2 ±\n√(\n1− α2\n2\n)2\n− 1. (5)\nwith α :=√ngndhf′(0). For α≤ 2, all eigenvalues are\nhence on the unit circle. Moreover for α > 2, there are\neigenvalues outside the unit circle.\nEven though Lemma 2.5 shows that alternating gradient\ndescent does not converge linearly to the Nash-equilibrium,\nit could in principle converge with a sublinear convergence\nrate. However, this is very unlikely because – as Lemma 2.3\nshows – even the continuous system does not converge. In-\ndeed, we empirically found that alternating gradient descent\noscillates in stable cycles around the equilibrium and shows\nno sign of convergence (Figure 2b).\n2.3. Where do instabilities come from?\nOur simple example shows that naive gradient based GAN\noptimization does not always converge to the equilibrium\npoint. To get a better understanding of what can go wrong\nfor more complicated GANs, it is instructive to analyze\nthese instabilities in depth for this simple example problem.\nTo understand the instabilities, we have to take a closer\nlook at the oscillatory behavior that GANs exhibit both for\nthe Dirac-GAN and for more complex systems. An intu-\nitive explanation for the oscillations is given in Figure 1:\nwhen the generator is far from the true data distribution,\nthe discriminator pushes the generator towards the true data\ndistribution. At the same time, the discriminator becomes\nmore certain, which increases the discriminator’s slope (Fig-\nure 1a). Now, when the generator reaches the target distri-\nbution (Figure 1b), the slope of the discriminator is largest,\npushing the generator away from the target distribution. As\na result, the generator moves away again from the true data\ndistribution and the discriminator has to change its slope\nfrom positive to negative. After a while, we end up with a\nsimilar situation as in the beginning of training, only on the\nother side of the true data distribution. This process repeats\nindeﬁnitely and does not converge.\nAnother way to look at this is to consider the local behavior\nof the training algorithm near the Nash-equilibrium. Indeed,\nnear the Nash-equilibrium, there is nothing that pushes the\ndiscriminator towards having zero slope on the true data\ndistribution. Even if the generator is initialized exactly on\nthe target distribution, there is no incentive for the discrimi-\nnator to move to the equilibrium discriminator. As a result,\ntraining is unstable near the equilibrium point.\nThis phenomenon of discriminator gradients orthogonal to\nthe data distribution can also arise for more complex exam-\nples: as long as the data distribution is concentrated on a\nlow dimensional manifold and the class of discriminators\nis big enough, there is no incentive for the discriminator to\nproduce zero gradients orthogonal to the tangent space of\nthe data manifold and hence converge to the equilibrium\ndiscriminator. Even if the generator produces exactly the\ntrue data distribution, there is no incentive for the discrim-\ninator to produce zero gradients orthogonal to the tangent\nspace. When this happens, the discriminator does not pro-\nvide useful gradients for the generator orthogonal to the data\ndistribution and the generator does not converge.\nNote that these instabilities can only arise if the true data\ndistribution is concentrated on a lower dimensional man-\nifold. Indeed, Nagarajan & Kolter (2017) showed that -\nunder some suitable assumptions - gradient descent based\nGAN optimization is locally convergent for absolutely con-\ntinuous distributions. Unfortunately, this assumption may\nnot be satisﬁed for data distributions like natural images to\nWhich Training Methods for GANs do actually Converge?\n(a) Standard GAN\n (b) Non-saturating GAN\n(c) WGAN (nd = 5)\n (d) WGAN-GP (nd = 5)\n(e) Consensus optimization\n (f) Instance noise\n(g) Gradient penalty\n (h) Gradient penalty (CR)\nFigure 3. Convergence properties of different GAN training al-\ngorithms using alternating gradient descent with recommended\nnumber of discriminator updates per generator update ( nd = 1\nif not noted otherwise). The shaded area in Figure 3c visualizes\nthe set of forbidden values for the discriminator parameter ψ. The\nstarting iterate is marked in red.\nwhich GANs are commonly applied (Arjovsky & Bottou,\n2017). Moreover, even if the data distribution is absolutely\ncontinuous but concentrated along some lower dimensional\nmanifold, the eigenvalues of the Jacobian of the gradient\nvector ﬁeld will be very close to the imaginary axis, result-\ning in a highly ill-conditioned problem. This was observed\nby Mescheder et al. (2017) who examined the spectrum\nof the Jacobian for a data distribution given by a circular\nmixture of Gaussians with small variance.\n3. Regularization strategies\nAs we have seen in Section 2, unregularized GAN training\ndoes not always converge to the Nash-equilibrium. In this\nsection, we discuss how several regularization techniques\nthat have recently been proposed, inﬂuence convergence of\nthe Dirac-GAN.\nInterestingly, we also ﬁnd that the non-saturating loss pro-\nposed in the original GAN paper (Goodfellow et al., 2014)\nleads to convergence of the continuous system, albeit with\nan extremely slow convergence rate. A more detailed discus-\nsion and an analysis of consensus optimization (Mescheder\net al., 2017) can be found in the supplementary material.\n3.1. Wasserstein GAN\nThe two-player GAN game can be interpreted as minimizing\na probabilistic divergence between the true data distribution\nand the distribution produced by the generator (Nowozin\net al., 2016; Goodfellow et al., 2014). This divergence is\nobtained by considering the best-response strategy for the\ndiscriminator, resulting in an objective function that only\ncontains the generator parameters. Many recent regular-\nization techniques for GANs are based on the observation\n(Arjovsky & Bottou, 2017) that this divergence may be dis-\ncontinuous with respect to the parameters of the generator\nor may even take on inﬁnite values if the support of the data\ndistribution and the generator distribution do not match.\nTo make the divergence continuous with respect to the pa-\nrameters of the generator, Wasserstein GANs (WGANs)\nArjovsky et al. (2017) replace the Jensen-Shannon diver-\ngence used in the original derivation of GANs (Goodfellow\net al., 2014) with the Wasserstein-divergence. As a result,\nArjovsky et al. (2017) propose to use f(t) = t and restrict\nthe class of discriminators to Lipschitz continuous functions\nwith Lipschitz constant equal to some g0 > 0. While a\nWGAN converges if the discriminator is always trained un-\ntil convergence, in practice WGANs are usually trained by\nrunning only a ﬁxed ﬁnite number of discriminator updates\nper generator update. However, near the Nash-equilibrium\nthe optimal discriminator parameters can have a disconti-\nnuity as a function of the current generator parameters: for\nthe Dirac-GAN, the optimal discriminator has to move from\nψ =−1 to ψ = 1 when θ changes signs. As the gradients\nget smaller near the equilibrium point, the gradient updates\ndo not lead to convergence for the discriminator. Overall,\nthe training dynamics are again determined by the Jacobian\nof the gradient vector ﬁeld near the Nash-equilibrium:\nLemma 3.1. A WGAN trained with simultaneous or alter-\nnating gradient descent with a ﬁxed number of discrimina-\ntor updates per generator update and a ﬁxed learning rate\nh > 0 does generally not converge to the Nash equilibrium\nfor the Dirac-GAN.\nThe training behavior of the WGAN is visualized in Fig-\nure 3c. We stress that this analysis only holds if the discrimi-\nnator is trained with a ﬁxed number of discriminator updates\n(as it is usually done in practice). More careful training that\nensures that the discriminator is kept exactly optimal or\ntwo-timescale training (Heusel et al., 2017) might be able\nto ensure convergence for WGANs.\nWhich Training Methods for GANs do actually Converge?\nDψ(x)\nx\ny\n(a) Example with instance noise\n (b) Eigenvalues\nFigure 4. Dirac-GAN with instance noise. While unregularized\nGAN training is inherently unstable, instance noise can stabilize it:\n(a) Near the Nash-equilibrium, the discriminator is pushed towards\nthe zero discriminator. (b) As we increase the noise level σ from\n0 to σcritical, the real part of the eigenvalues at the equilibrium\npoint becomes negative and the absolute value of the imaginary\npart becomes smaller. For noise levels bigger than σcritical all\neigenvalues are real-valued and GAN training hence behaves like\na normal optimization problem.\nThe convergence properties of WGANs were also consid-\nered by Nagarajan & Kolter (2017) who showed that even\nfor absolutely continuous densities and inﬁnitesimal learn-\ning rates, WGANs are not always locally convergent.\nWe also found that WGAN-GP (Gulrajani et al., 2017) does\nnot converge for the Dirac-GAN (Figure 3d). Please see the\nsupplementary material for details.3\n3.2. Instance noise\nA common technique to stabilize GANs is to add instance\nnoise (Sønderby et al., 2016; Arjovsky & Bottou, 2017), i.e.\nindependent Gaussian noise, to the data points. While the\noriginal motivation was to make the probabilistic divergence\nbetween data and generator distribution well-deﬁned for dis-\ntributions that do not have common support, this does not\nclarify the effects of instance noise on thetraining algorithm\nitself and its ability to ﬁnd a Nash-equilibrium. Interestingly,\nhowever, it was recently shown (Nagarajan & Kolter, 2017)\nthat in the case of absolutely continuous distributions, gra-\ndient descent based GAN optimization is - under suitable\nassumptions - locally convergent.\nIndeed, for the Dirac-GAN we have:\nLemma 3.2. When using Gaussian instance noise with stan-\ndard deviation σ, the eigenvalues of the Jacobian of the\ngradient vector ﬁeld are given by\nλ1/2 = f′′(0)σ2±\n√\nf′′(0)2σ4− f′(0)2. (6)\nIn particular, all eigenvalues of the Jacobian have negative\nreal-part at the Nash-equilibrium if f′′(0) < 0 and σ > 0.\nHence, simultaneous and alternating gradient descent are\nboth locally convergent for small enough learning rates.\n3Despite these negative results, WGAN-GP has been success-\nfully applied in practice (Gulrajani et al., 2017; Karras et al., 2017)\nand we leave a theoretical analysis of these empirical results to\nfuture research.\nInterestingly, Lemma 3.2 shows that there is a critical noise\nlevel given by σ2\ncritical =|f′(0)|/|f′′(0)|. If the noise level\nis smaller than the critical noise level, the eigenvalues of\nthe Jacobian have non-zero imaginary part which results\nin a rotational component in the gradient vector ﬁeld near\nthe equilibrium point. If the noise level is larger than the\ncritical noise level, all eigenvalues of the Jacobian become\nreal-valued and the rotational component in the gradient\nvector ﬁeld disappears. The optimization problem is best\nbehaved when we select σ = σcritical: in this case we can\neven achieve quadratic convergence for h =|f′(0)|−1. The\neffect of instance noise on the eigenvalues is visualized in\nFigure 4b, which shows the traces of the two eigenvalues as\nwe increase σ from 0 to 2σcritical.\nFigure 3f shows the training behavior of the GAN with\ninstance noise, showing that instance noise indeed creates a\nstrong radial component in the gradient vector ﬁeld which\nmakes the training algorithm converge.\n3.3. Zero-centered gradient penalties\nMotivated by the success of instance noise to make the f-\ndivergence between two distributions well-deﬁned, Roth\net al. (2017) derived a local approximation to instance noise\nthat results in a zero-centered4 gradient penalty for the dis-\ncriminator.\nFor the Dirac-GAN, a penalty on the squared norm of the\ngradients of the discriminator (no matter where) results in\nthe regularizer\nR(ψ) = γ\n2 ψ2. (7)\nThis regularizer does not include the weighting terms con-\nsidered by Roth et al. (2017). However, the same analysis\ncan also be applied to the regularizer with the additional\nweighting, yielding almost exactly the same results (see\nSection D.2 of the supplementary material).\nLemma 3.3. The eigenvalues of the Jacobian of the gradi-\nent vector ﬁeld for the gradient-regularized Dirac-GAN at\nthe equilibrium point are given by\nλ1/2 =− γ\n2±\n√\nγ2\n4 − f′(0)2. (8)\nIn particular, for γ > 0 all eigenvalues have negative real\npart. Hence, simultaneous and alternating gradient descent\nare both locally convergent for small enough learning rates.\nLike for instance noise, there is a critical regularization pa-\nrameter γcritical = 2|f′(0)| that results in a locally rotation\nfree vector ﬁeld. A visualization of the training behavior of\nthe Dirac-GAN with gradient penalty is shown in Figure 3g.\nFigure 3h illustrates the training behavior of the GAN with\n4In contrast to the gradient regularizers used in WGAN-GP and\nDRAGAN which are not zero-centered.\nWhich Training Methods for GANs do actually Converge?\ngradient penalty and critical regularization (CR). In particu-\nlar, we see that near the Nash-equilibrium the vector ﬁeld\ndoes not have a rotational component anymore and hence\nbehaves like a normal optimization problem.\n4. General convergence results\nIn Section 3 we analyzed the convergence properties of var-\nious regularization strategies for the Dirac-GAN. In this\nsection, we consider general GANs. First, we introduce two\nsimpliﬁed versions of the zero-centered gradient penalty\nproposed by Roth et al. (2017). We then show that these gra-\ndient penalties allow us to extend the convergence proof by\nNagarajan & Kolter (2017) to the case where the generator\nand data distribution do not locally have the same support.5\nAs a result, our convergence proof for the regularized train-\ning dynamics also holds for the more realistic case where\nboth the generator and data distributions may lie on lower\ndimensional manifolds.\n4.1. Simpliﬁed gradient penalties\nOur analysis suggests that the main effect of the zero-\ncentered gradient penalties proposed by Roth et al. (2017)\non local stability is to penalize the discriminator for deviat-\ning from the Nash-equilibrium. The simplest way to achieve\nthis is to penalize the gradient on real data alone: when the\ngenerator distribution produces the true data distribution\nand the discriminator is equal to 0 on the data manifold, the\ngradient penalty ensures that the discriminator cannot create\na non-zero gradient orthogonal to the data manifold without\nsuffering a loss in the GAN game.\nThis leads to the following regularization term:\nR1(ψ) := γ\n2 EpD(x)\n[\n∥∇Dψ(x)∥2]\n. (9)\nNote that this regularizer is a simpliﬁed version of to the\nregularizer derived by Roth et al. (2017). However, our\nregularizer does not contain the additional weighting terms\nand penalizes the discriminator gradients only on the true\ndata distribution.\nWe also consider a similar regularization term given by\nR2(θ, ψ) := γ\n2 Epθ(x)\n[\n∥∇Dψ(x)∥2]\n(10)\nwhere we penalize the discriminator gradients on the current\ngenerator distribution instead of the true data distribution.\nNote that for the Dirac-GAN from Section 2, both regulariz-\ners reduce to the gradient penalty from Section 3.3 whose\nbehavior is visualized in Figure 3g and Figure 3h.\n5Assumption IV in Nagarajan & Kolter (2017)\n4.2. Convergence\nIn this section we present convergence results for the regular-\nized GAN-training dynamics for both regularization terms\nR1(ψ) and R2(ψ) under some suitable assumptions.6\nLet (θ∗, ψ∗) denote an equilibrium point of the regularized\ntraining dynamics. In our convergence analysis, we consider\nthe realizable case, i.e. we assume that there are generator\nparameters that make the generator produce the true data\ndistribution:\nAssumption I. We have pθ∗ = pD and Dψ∗(x) = 0 in\nsome local neighborhood of supp pD.\nLike Nagarajan & Kolter (2017), we assume that f satisﬁes\nthe following property:\nAssumption II. We have f′(0)̸= 0 and f′′(0) < 0.\nAn extension of our convergence proof to f(t) = t (as in\nWGANs) can be found in the supplementary material.\nThe convergence proof is complicated by the fact that for\nneural networks, there generally is not a single equilibrium\npoint (θ∗, ψ∗), but a submanifold of equivalent equilibria\ncorresponding to different parameterizations of the same\nfunction. We therefore deﬁne thereparameterization mani-\nfoldsMG andMD. To this end, let\nh(ψ) := EpD(x)\n[\n|Dψ(x)|2 +∥∇xDψ(x)∥2]\n. (11)\nThe reparameterization manifolds are then deﬁned as\nMG :={θ| pθ = pD} MD :={ψ| h(ψ) = 0}. (12)\nTo prove local convergence, we have to assume some reg-\nularity properties forMG andMD near the equilibrium\npoint. To state these assumptions, we need\ng(θ) := Epθ(x) [∇ψDψ(x)|ψ=ψ∗] . (13)\nAssumption III. There are ϵ-balls Bϵ(θ∗) and Bϵ(ψ∗)\naround θ∗ and ψ∗ so thatMG∩ Bϵ(θ∗) andMD∩ Bϵ(ψ∗)\ndeﬁneC1- manifolds. Moreover, the following holds:\n(i) if v∈ Rn is not in the tangent space of MD at ψ∗,\nthen ∂2\nv h(ψ∗)̸= 0.\n(ii) if w∈ Rm is not in the tangent space of MG at θ∗,\nthen ∂wg(θ∗)̸= 0.\nWhile formally similar, the two conditions in Assumption III\nhave very different meanings: the ﬁrst condition is a simple\nregularity property that means that the geometry of MD\ncan be locally described by the second derivative of h. The\nsecond condition implies that the discriminator is strong\n6Our results also hold for any convex combination of R1 and\nR2 and the regularizer with the additional weighting terms derived\nby Roth et al. (2017). See the supplementary material for details.\nWhich Training Methods for GANs do actually Converge?\nenough so that it can detect any deviation from the equilib-\nrium generator distribution. Indeed, this is the only point\nwhere we assume that the class of representable discrimi-\nnators is sufﬁciently expressive (and excludes, for example,\nthe trivial case Dψ = 0 for all ψ).\nWe are now ready to state our main convergence result. To\nthis end, consider the regularized gradient vector ﬁeld\n˜vi(θ, ψ) :=\n( −∇θL(θ, ψ)\n∇ψL(θ, ψ)−∇ ψRi(θ, ψ)\n)\n. (14)\nTheorem 4.1. Assume Assumption I, II and III hold for\n(θ∗, ψ∗). For small enough learning rates, simultaneous\nand alternating gradient descent for ˜v1 and ˜v2 are both\nconvergent toMG×M D in a neighborhood of (θ∗, ψ∗).\nMoreover, the rate of convergence is at least linear.\nTheorem 4.1 shows that GAN training with our gradient\npenalties is convergent when initialized sufﬁciently close\nto the equilibrium point. While this does not show that the\nmethod is globally convergent, it at least shows that near the\nequilibrium the method is well-behaved.\n4.3. Stable equilibria for unregularized GAN training\nAs we have seen in Section 2, unregularized GAN training\ndoes not always converge to the Nash-equilibrium. However,\nthis does not rule out the existence of stable equilibria for\nevery GAN architecture. In Section E of the supplementary\nmaterial, we identify two forms of stable equilibria that may\nexist for unregularized GAN training (energy solutions and\nfull-rank solutions). However, it is not yet clear under what\nconditions such solutions exist for high dimensional data\ndistributions.\n5. Experiments\n2D-Problems Measuring convergence for GANs is hard\nfor high dimensional problems, because we lack a metric\nthat can reliably detect non-convergent behavior. We there-\nfore ﬁrst examine the behavior of the different regularizers\non simple 2D examples where we can assess convergence\nusing an estimate of the Wasserstein-1-distance.\nTo this end, we run 5 different training algorithms on 4 dif-\nferent 2D-examples for 6 different GAN architectures. For\neach method, we try both stochastic gradient descent and\nRMS-Prop with 4 different learning rates. For the R1-, R2-\nand WGAN-GP-regularizers we try 3 different regulariza-\ntion parameters. We train all methods for 50k iterations and\nreport the results for the best hyperparameter setup. Please\nsee the supplementary material for details.\nThe results are shown in Figure 5. We see that the R1- and\nR2-regularizers perform similarly and they achieve slightly\nbetter results than unregularized training or training with\n(a) 2D Gaussian\n (b) Line segment\n(c) Circle\n (d) Four line segments\nFigure 5. Wasserstein-1-distance to true data distribution for 4 dif-\nferent 2D-data-distributions, 6 different architectures (small bars)\nand 5 different training methods. Here, we abbreviate WGAN-\nGP with 1 and 5 discriminator update(s) per generator update as\nWGP-1 and WGP-5.\nWGAN-GP. In the supplementary material we show that the\nR1- and R2-regularizers ﬁnd solutions where the discrim-\ninator is 0 in a neighborhood of the true data distribution,\nwhereas unregularized training and WGAN-GP converge to\nenergy solutions.\nImages To test how well the gradient penalties from Sec-\ntion 4.1 perform on more complicated tasks, we train con-\nvolutional GANs on a variety of datasets, including a gener-\native model for all 1000 Imagenet classes and a generative\nmodel for the celebA-HQ dataset (Karras et al., 2017) at res-\nolution 1024×1024. While we ﬁnd that unregularized GAN\ntraining quickly leads to mode-collapse for these problems,\nour simple R1-regularizer enables stable training. Random\nsamples from the models and more details on the experi-\nmental setup can be found in the supplementary material.\n6. Conclusion\nIn this paper, we analyzed the stability of GAN training on\na simple yet prototypical example. Due to the simplicity of\nthe example, we were able to analyze the convergence prop-\nerties of the training dynamics analytically and we showed\nthat (unregularized) gradient based GAN optimization is\nnot always locally convergent. Our ﬁndings also show that\nWGANs and WGAN-GP do not always lead to local con-\nvergence whereas instance noise and zero-centered gradient\npenalties do. Based on our analysis, we extended our results\nto more general GANs and we proved local convergence for\nsimpliﬁed zero-centered gradient penalties under suitable\nassumptions. In the future, we would like to extend our\ntheory to the non-realizable case and examine the effect of\nﬁnite sampling sizes on the GAN training dynamics.\nWhich Training Methods for GANs do actually Converge?\nAcknowledgements\nWe would like to thank Vaishnavh Nagarajan and Kevin\nRoth for insightful discussions. We also thank Vaishnavh\nNagarajan for giving helpful feedback on an early draft of\nthis manuscript. We thank NVIDIA for donating the GPUs\nfor the experiments presented in the supplementary material.\nThis work was supported by Microsoft Research through its\nPhD Scholarship Programme.\nReferences\nAbadi, M., Barham, P., Chen, J., Chen, Z., Davis, A., Dean,\nJ., Devin, M., Ghemawat, S., Irving, G., Isard, M., Kud-\nlur, M., Levenberg, J., Monga, R., Moore, S., Murray,\nD. G., Steiner, B., Tucker, P. A., Vasudevan, V ., Warden,\nP., Wicke, M., Yu, Y ., and Zheng, X. Tensorﬂow: A\nsystem for large-scale machine learning. In 12th USENIX\nSymposium on Operating Systems Design and Implemen-\ntation, OSDI 2016, Savannah, GA, USA, November 2-4,\n2016., pp. 265–283, 2016.\nArjovsky, M. and Bottou, L. Towards principled meth-\nods for training generative adversarial networks. CoRR,\nabs/1701.04862, 2017.\nArjovsky, M., Chintala, S., and Bottou, L. Wasserstein\nGAN. CoRR, abs/1701.07875, 2017.\nBarratt, S. and Sharma, R. A note on the inception score.\nCoRR, abs/1801.01973, 2018.\nBerthelot, D., Schumm, T., and Metz, L. BEGAN: bound-\nary equilibrium generative adversarial networks. CoRR,\nabs/1703.10717, 2017.\nBertsekas, D. P. Nonlinear programming. Athena scientiﬁc\nBelmont, 1999.\nGidel, G., Berard, H., Vincent, P., and Lacoste-Julien, S. A\nvariational inequality perspective on generative adversar-\nial nets. CoRR, abs/1802.10551, 2018.\nGoodfellow, I. J., Pouget-Abadie, J., Mirza, M., Xu, B.,\nWarde-Farley, D., Ozair, S., Courville, A. C., and Bengio,\nY . Generative adversarial nets. InAdvances in Neural In-\nformation Processing Systems 27: Annual Conference on\nNeural Information Processing Systems 2014, December\n8-13 2014, Montreal, Quebec, Canada, pp. 2672–2680,\n2014.\nGulrajani, I., Ahmed, F., Arjovsky, M., Dumoulin, V ., and\nCourville, A. C. Improved training of wasserstein gans. In\nAdvances in Neural Information Processing Systems 30:\nAnnual Conference on Neural Information Processing\nSystems 2017, 4-9 December 2017, Long Beach, CA,\nUSA, pp. 5769–5779, 2017.\nHe, K., Zhang, X., Ren, S., and Sun, J. Deep residual learn-\ning for image recognition. In Proceedings of the IEEE\nconference on computer vision and pattern recognition,\npp. 770–778, 2016.\nHeusel, M., Ramsauer, H., Unterthiner, T., Nessler, B., and\nHochreiter, S. Gans trained by a two time-scale update\nrule converge to a local nash equilibrium. In Advances\nin Neural Information Processing Systems 30: Annual\nConference on Neural Information Processing Systems\n2017, 4-9 December 2017, Long Beach, CA, USA , pp.\n6629–6640, 2017.\nHjelm, R. D., Jacob, A. P., Che, T., Cho, K., and Bengio,\nY . Boundary-seeking generative adversarial networks.\nCoRR, abs/1702.08431, 2017.\nKarras, T., Aila, T., Laine, S., and Lehtinen, J. Progres-\nsive growing of gans for improved quality, stability, and\nvariation. CoRR, abs/1710.10196, 2017.\nKhalil, H. K. Nonlinear systems. Prentice-Hall, New Jersey,\n2(5):5–1, 1996.\nKodali, N., Abernethy, J. D., Hays, J., and Kira, Z. How to\ntrain your DRAGAN. CoRR, abs/1705.07215, 2017.\nKrizhevsky, A. and Hinton, G. Learning multiple layers of\nfeatures from tiny images. 2009.\nLiu, Z., Luo, P., Wang, X., and Tang, X. Deep learning face\nattributes in the wild. In Proceedings of International\nConference on Computer Vision (ICCV), 2015.\nMescheder, L. M., Nowozin, S., and Geiger, A. The numer-\nics of gans. In Advances in Neural Information Process-\ning Systems 30: Annual Conference on Neural Informa-\ntion Processing Systems 2017, 4-9 December 2017, Long\nBeach, CA, USA, pp. 1823–1833, 2017.\nMiyato, T., Kataoka, T., Koyama, M., and Yoshida, Y . Spec-\ntral normalization for generative adversarial networks.\nCoRR, abs/1802.05957, 2018.\nNagarajan, V . and Kolter, J. Z. Gradient descent GAN\noptimization is locally stable. In Advances in Neural\nInformation Processing Systems 30: Annual Conference\non Neural Information Processing Systems 2017, 4-9\nDecember 2017, Long Beach, CA, USA, pp. 5591–5600,\n2017.\nNowozin, S., Cseke, B., and Tomioka, R. f-gan: Training\ngenerative neural samplers using variational divergence\nminimization. In Advances in Neural Information Pro-\ncessing Systems 29: Annual Conference on Neural Infor-\nmation Processing Systems 2016, December 5-10, 2016,\nBarcelona, Spain, pp. 271–279, 2016.\nWhich Training Methods for GANs do actually Converge?\nOdena, A., Olah, C., and Shlens, J. Conditional image\nsynthesis with auxiliary classiﬁer gans. InProceedings of\nthe 34th International Conference on Machine Learning,\nICML 2017, Sydney, NSW, Australia, 6-11 August 2017,\npp. 2642–2651, 2017.\nRadford, A., Metz, L., and Chintala, S. Unsupervised rep-\nresentation learning with deep convolutional generative\nadversarial networks. CoRR, abs/1511.06434, 2015.\nRoth, K., Lucchi, A., Nowozin, S., and Hofmann, T. Stabi-\nlizing training of generative adversarial networks through\nregularization. In Advances in Neural Information Pro-\ncessing Systems 30: Annual Conference on Neural In-\nformation Processing Systems 2017, 4-9 December 2017,\nLong Beach, CA, USA, pp. 2015–2025, 2017.\nRussakovsky, O., Deng, J., Su, H., Krause, J., Satheesh, S.,\nMa, S., Huang, Z., Karpathy, A., Khosla, A., Bernstein,\nM., Berg, A. C., and Fei-Fei, L. ImageNet Large Scale\nVisual Recognition Challenge. International Journal of\nComputer Vision (IJCV) , 115(3):211–252, 2015. doi:\n10.1007/s11263-015-0816-y.\nSalimans, T., Goodfellow, I. J., Zaremba, W., Cheung, V .,\nRadford, A., and Chen, X. Improved techniques for\ntraining gans. In Advances in Neural Information Pro-\ncessing Systems 29: Annual Conference on Neural Infor-\nmation Processing Systems 2016, December 5-10, 2016,\nBarcelona, Spain, pp. 2226–2234, 2016.\nSønderby, C. K., Caballero, J., Theis, L., Shi, W., and\nHuszár, F. Amortised MAP inference for image super-\nresolution. CoRR, abs/1610.04490, 2016.\nTieleman, T. and Hinton, G. Lecture 6.5-rmsprop: Divide\nthe gradient by a running average of its recent magnitude,\n2012.\nYazici, Y ., Foo, C. S., Winkler, S., Yap, K., Piliouras, G., and\nChandrasekhar, V . The unusual effectiveness of averaging\nin GAN training. CoRR, abs/1806.04498, 2018.\nYu, F., Zhang, Y ., Song, S., Seff, A., and Xiao, J. LSUN:\nconstruction of a large-scale image dataset using deep\nlearning with humans in the loop. CoRR, abs/1506.03365,\n2015.\nZhao, J. J., Mathieu, M., and LeCun, Y . Energy-based\ngenerative adversarial network. CoRR, abs/1609.03126,\n2016.",
  "values": {
    "Transparent (to users)": "Yes",
    "Interpretable (to users)": "Yes",
    "Explicability": "Yes",
    "Justice": "Yes",
    "Beneficence": "Yes",
    "Respect for Law and public interest": "Yes",
    "Critiqability": "Yes",
    "Privacy": "Yes",
    "Not socially biased": "Yes",
    "Non-maleficence": "Yes",
    "Respect for Persons": "Yes",
    "User influence": "Yes",
    "Fairness": "Yes",
    "Deferral to humans": "Yes",
    "Autonomy (power to decide)": "Yes",
    "Collective influence": "Yes"
  }
}