{
  "pdf": "pham18a",
  "title": "pham18a",
  "author": "Unknown",
  "paper_id": "pham18a",
  "text": "Efﬁcient Neural Architecture Search via Parameter Sharing\nHieu Pham * 1 2 Melody Y. Guan* 3 Barret Zoph 1 Quoc V . Le1 Jeff Dean 1\nAbstract\nWe propose Efﬁcient Neural Architecture Search\n(ENAS), a fast and inexpensive approach for au-\ntomatic model design. ENAS constructs a large\ncomputational graph, where each subgraph repre-\nsents a neural network architecture, hence forcing\nall architectures to share their parameters. A con-\ntroller is trained with policy gradient to search for\na subgraph that maximizes the expected reward on\na validation set. Meanwhile a model correspond-\ning to the selected subgraph is trained to minimize\na canonical cross entropy loss. Sharing parame-\nters among child models allows ENAS to deliver\nstrong empirical performances, whilst using much\nfewer GPU-hours than existing automatic model\ndesign approaches, and notably, 1000x less ex-\npensive than standard Neural Architecture Search.\nOn Penn Treebank, ENAS discovers a novel ar-\nchitecture that achieves a test perplexity of 56.3,\non par with the existing state-of-the-art among\nall methods without post-training processing. On\nCIFAR-10, ENAS ﬁnds a novel architecture that\nachieves 2.89% test error, which is on par with the\n2.65% test error of NASNet (Zoph et al., 2018).\n1. Introduction\nNeural architecture search (NAS) has been successfully ap-\nplied to design model architectures for image classiﬁcation\nand language models (Zoph & Le, 2017; Zoph et al., 2018;\nCai et al., 2018; Liu et al., 2017; 2018). In NAS, a con-\ntroller is trained in a loop: the controller ﬁrst samples a\ncandidate architecture, i.e. a child model, trains it to conver-\ngence, and measure its performance on the task of desire.\nThe controller then uses the performance as a guiding signal\nto ﬁnd more promising architectures. This process is re-\npeated for many iterations. Despite its impressive empirical\nperformance, NAS is computationally expensive and time\n*Equal contribution 1Google Brain 2Language Technology In-\nstitute, Carnegie Mellon University 3Department of Computer\nScience, Stanford University. Correspondence to: Hieu Pham\n<hyhieu@cmu.edu>, Melody Y . Guan<mguan@stanford.edu>.\nProceedings of the 35 th International Conference on Machine\nLearning, Stockholm, Sweden, PMLR 80, 2018. Copyright 2018\nby the author(s).\nconsuming, e.g. Zoph et al. (2018) use 450 GPUs for 3-4\ndays (i.e. 32,400-43,200 GPU hours). Meanwhile, using\nless resource tends to produce less compelling results (Ne-\ngrinho & Gordon, 2017; Baker et al., 2017a). We observe\nthat the computational bottleneck of NAS is the training\nof each child model to convergence, only to measure its\naccuracy whilst throwing away all the trained weights.\nThe main contribution of this work is to improve the efﬁ-\nciency of NAS by forcing all child models to share weights\nto eschew training each child model from scratch to con-\nvergence. The idea has apparent complications, as different\nchild models might utilize their weights differently, but was\nencouraged by previous work on transfer learning and multi-\ntask learning, which established that parameters learned for\na particular model on a particular task can be used for other\nmodels on other tasks, with little to no modiﬁcations (Raza-\nvian et al., 2014; Zoph et al., 2016; Luong et al., 2016).\nWe empirically show that not only is sharing parameters\namong child models possible, but it also allows for very\nstrong performance. Speciﬁcally, on CIFAR-10, our method\nachieves a test error of 2.89%, compared to 2.65% by NAS.\nOn Penn Treebank, our method achieves a test perplexity of\n56.3, which signiﬁcantly outperforms NAS’s test perplexity\nof 62.4 (Zoph & Le, 2017) and which is on par with the\nexisting state-of-the-art among Penn Treebank’s approaches\nthat do not utilize post-training processing (56.0; Yang et al.\n(2018)). Importantly, in all of our experiments, for which\nwe use a single Nvidia GTX 1080Ti GPU, the search for\narchitectures takes less than 16 hours. Compared to NAS,\nthis is a reduction of GPU-hours by more than 1000x. Due\nto its efﬁciency, we name our methodEfﬁcient Neural Ar-\nchitecture Search (ENAS).\n2. Methods\nCentral to the idea of ENAS is the observation that all of\nthe graphs which NAS ends up iterating over can be viewed\nas sub-graphs of a larger graph. In other words, we can\nrepresent NAS’s search space using asingle directed acyclic\ngraph (DAG). Figure 2 illustrates a generic example DAG,\nwhere an architecture can be realized by taking a subgraph\nof the DAG. Intuitively, ENAS’s DAG is the superposition\nof all possible child models in a search space of NAS, where\nthe nodes represent the local computations and the edges\nEfﬁcient Neural Architecture Search via Parameter Sharing\nFigure 1. An example of a recurrent cell in our search space with 4 computational nodes. Left: The computational DAG that corresponds\nto the recurrent cell. The red edges represent the ﬂow of information in the graph. Middle: The recurrent cell. Right: The outputs of the\ncontroller RNN that result in the cell in the middle and the DAG on the left. Note that nodes 3 and 4 are never sampled by the RNN, so\ntheir results are averaged and are treated as the cell’s output.\nFigure 2. The graph represents the entire search space while the\nred arrows deﬁne a model in the search space, which is decided by\na controller. Here, node 1 is the input to the model whereas nodes\n3 and 6 are the model’s outputs.\nrepresent the ﬂow of information. The local computations at\neach node have their own parameters, which are used only\nwhen the particular computation is activated. Therefore,\nENAS’s design allows parameters to be shared among all\nchild models, i.e. architectures, in the search space.\nIn the following, we facilitate the discussion of ENAS with\nan example that illustrates how to design a cell for recurrent\nneural networks from a speciﬁed DAG and a controller (Sec-\ntion 2.1). We will then explain how to train ENAS and how\nto derive architectures from ENAS’s controller (Section 2.2).\nFinally, we will explain our search space for designing con-\nvolutional architectures (Sections 2.3 and 2.4).\n2.1. Designing Recurrent Cells\nTo design recurrent cells, we employ a DAG withN nodes,\nwhere the nodes represent local computations, and the edges\nrepresent the ﬂow of information between the N nodes.\nENAS’s controller is an RNN that decides: 1) which edges\nare activated and 2) which computations are performed at\neach node in the DAG. This design of our search space for\nRNN cells is different from the search space for RNN cells\nin Zoph & Le (2017), where the authors ﬁx the topology\nof their architectures as a binary tree and only learn the\noperations at each node of the tree. In contrast, our search\nspace allows ENAS to design both the topology and the\noperations in RNN cells, and hence is more ﬂexible.\nTo create a recurrent cell, the controller RNN samples N\nblocks of decisions. Here we illustrate the ENAS mech-\nanism via a simple example recurrent cell with N = 4\ncomputational nodes (visualized in Figure 1). Let xt be the\ninput signal for a recurrent cell (e.g. word embedding), and\nht−1 be the output from the previous time step. We sample\nas follows.\n1. At node 1: The controller ﬁrst samples an activation function.\nIn our example, the controller chooses the tanh activation\nfunction, which means that node1 of the recurrent cell should\ncompute k1 = tanh (xt · W(x) + ht−1 · W(h)\n1 ).\n2. At node 2: The controller then samples a previous index\nand an activation function. In our example, it chooses the\nprevious index 1 and the activation function ReLU. Thus,\nnode 2 of the cell computes k2 = ReLU(k1 · W(h)\n2,1 ).\n3. At node 3: The controller again samples a previous index and\nan activation function. In our example, it chooses the pre-\nvious index 2 and the activation function ReLU. Therefore,\nk3 = ReLU(k2 · W(h)\n3,2 ).\n4. At node 4: The controller again samples a previous index\nand an activation function. In our example, it chooses the\nprevious index 1 and the activation function tanh, leading to\nk4 = tanh (k1 · W(h)\n4,1 ).\n5. For the output, we simply average all the loose ends, i.e. the\nnodes that are not selected as inputs to any other nodes. In\nour example, since the indices 3 and 4 were never sampled to\nbe the input for any node, the recurrent cell uses their average\n(k3 + k4)/2 as its output. In other words, ht = (k3 + k4)/2.\nIn the example above, we note that for each pair of nodes\nj <ℓ, there is an independent parameter matrix W(h)\nℓ,j . As\nshown in the example, by choosing the previous indices, the\ncontroller also decides which parameter matrices are used.\nTherefore, in ENAS, all recurrent cells in a search space\nshare the same set of parameters.\nOur search space includes an exponential number of con-\nﬁgurations. Speciﬁcally, if the recurrent cell hasN nodes\nand we allow 4 activation functions (namely tanh, ReLU,\nidentity, and sigmoid), then the search space has4N ×(N −\n1)! conﬁgurations. In our experiments, N = 12 , which\nmeans there are approximately 1014 models in our search\nspace.\nEfﬁcient Neural Architecture Search via Parameter Sharing\n2.2. Training ENAS and Deriving Architectures\nOur controller network is an LSTM with 100 hidden\nunits (Hochreiter & Schmidhuber, 1997). This LSTM sam-\nples decisions via softmax classiﬁers, in an autoregressive\nfashion: the decision in the previous step is fed as input\nembedding into the next step. At the ﬁrst step, the controller\nnetwork receives an empty embedding as input.\nIn ENAS, there are two sets of learnable parameters: the\nparameters of the controller LSTM, denoted byθ, and the\nshared parameters of the child models, denoted byω. The\ntraining procedure of ENAS consists of two interleaving\nphases. The ﬁrst phase trains ω, the shared parameters\nof the child models, on a whole pass through the training\ndata set. For our Penn Treebank experiments,ω is trained\nfor about 400 steps, each on a minibatch of 64 examples,\nwhere the gradient ∇ω is computed using back-propagation\nthrough time, truncated at 35 time steps. Meanwhile, for\nCIFAR-10,ω is trained on 45, 000 training images, sepa-\nrated into minibatches of size 128, where ∇ω is computed\nusing standard back-propagation. The second phase trainsθ,\nthe parameters of the controller LSTM, for a ﬁxed number\nof steps, typically set to 2000 in our experiments. These two\nphases are alternated during the training of ENAS. More\ndetails are as follows.\nTraining the shared parameters ω of the child models.\nIn this step, we ﬁx the controller’s policyπ(m;θ) and per-\nform stochastic gradient descent (SGD) onω to minimize\nthe expected loss function Em∼π [L(m;ω)]. Here, L(m;ω)\nis the standard cross-entropy loss, computed on a minibatch\nof training data, with a model m sampled from π(m;θ).\nThe gradient is computed using the Monte Carlo estimate\n∇ωEm∼π(m;θ) [L(m;ω)] ≈ 1\nM\nM∑\ni=1\n∇ωL(mi,ω ), (1)\nwhere mi’s are sampled fromπ(m;θ) as described above.\nEqn 1 provides an unbiased estimate of the gradient\n∇ωEm∼π(m;θ) [L(m;ω)]. However, this estimate has a\nhigher variance than the standard SGD gradient, wherem is\nﬁxed. Nevertheless – and this is perhaps surprising – we ﬁnd\nthatM = 1 works just ﬁne,i.e. we can updateω using the\ngradient from any single model m sampled fromπ(m;θ).\nAs mentioned, we trainω during a entire pass through the\ntraining data.\nTraining the controller parameters θ. In this step, we\nﬁxω and update the policy parametersθ, aiming to maxi-\nmize the expected reward Em∼π(m;θ) [R(m,ω )]. We em-\nploy the Adam optimizer (Kingma & Ba, 2015), for which\nthe gradient is computed using REINFORCE (Williams,\n1992), with a moving average baseline to reduce variance.\nThe reward R(m,ω ) is computed on the validation set ,\nrather than on the training set, to encourage ENAS to select\nmodels that generalize well rather than models that overﬁt\nthe training set well. In our language model experiment,\nthe reward function isc/valid ppl, where the perplexity is\ncomputed on a minibatch of validation data. In our im-\nage classiﬁcation experiments, the reward function is the\naccuracy on a minibatch of validation images.\nDeriving Architectures. We discuss how to derive novel\narchitectures from a trained ENAS model. We ﬁrst sample\nseveral models from the trained policyπ(m,θ ). For each\nsampled model, we compute its reward on a single mini-\nbatch sampled from the validation set. We then take only\nthe model with the highest reward to re-train from scratch.\nIt is possible to improve our experimental results by training\nall the sampled models from scratch and selecting the model\nwith the highest performance on a separated validation set,\nas done by other works (Zoph & Le, 2017; Zoph et al., 2018;\nLiu et al., 2017; 2018). However, our method yields similar\nperformance whilst being much more economical.\n2.3. Designing Convolutional Networks\nFigure 3. An example run of a recurrent cell in our search space\nwith 4 computational nodes, which represent 4 layers in a convo-\nlutional network. Top: The output of the controller RNN. Bottom\nLeft: The computational DAG corresponding to the network’s\narchitecture. Red arrows denote the active computational paths.\nBottom Right: The complete network. Dotted arrows denote skip\nconnections.\nWe now discuss the search space for convolutional architec-\ntures. Recall that in the search space of the recurrent cell,\nthe controller RNN samples two decisions at each decision\nblock: 1) what previous node to connect to and 2) what\nactivation function to use. In the search space for convolu-\ntional models, the controller RNN also samples two sets of\ndecisions at each decision block: 1) what previous nodes to\nconnect to and 2) what computation operation to use. These\ndecisions construct a layer in the convolutional model.\nThe decision of what previous nodes to connect to allows the\nmodel to form skip connections (He et al., 2016a; Zoph &\nEfﬁcient Neural Architecture Search via Parameter Sharing\nLe, 2017). Speciﬁcally, at layerk, up tok − 1 mutually dis-\ntinct previous indices are sampled, leading to 2k−1 possible\ndecisions at layerk. We provide an illustrative example of\nsampling a convolutional network in Figure 3. In this exam-\nple, at layerk = 4, the controller samples previous indices\n{1, 3}, so the outputs of layers 1 and 3 are concatenated\nalong their depth dimension and sent to layer 4.\nMeanwhile, the decision of what computation operation\nto use sets a particular layer into convolution or average\npooling or max pooing. The 6 operations available for the\ncontroller are: convolutions with ﬁlter sizes3 × 3 and 5 × 5,\ndepthwise-separable convolutions with ﬁlter sizes3 × 3 and\n5 × 5 (Chollet, 2017), and max pooling and average pooling\nof kernel size 3 × 3. As for recurrent cells, each operation at\neach layer in our ENAS convolutional network has a distinct\nset of parameters.\nMaking the described set of decisions for a total ofL times,\nwe can sample a network of L layers. Since all decisions\nare independent, there are 6L × 2L(L−1)/2 networks in the\nsearch space. In our experiments, L = 12 , resulting in\n1.6 × 1029 possible networks.\n2.4. Designing Convolutional Cells\nRather than designing the entire convolutional network, one\ncan design smaller modules and then connect them together\nto form a network (Zoph et al., 2018). Figure 4 illustrates\nthis design, where the convolutional cell and reduction cell\narchitectures are to be designed. We now discuss how to use\nENAS to search for the architectures of these cells.\nFigure 4. Connecting 3 blocks, each with N convolution cells and\n1 reduction cell, to make the ﬁnal network.\nWe utilize the ENAS computational DAG with B nodes\nto represent the computations that happen locally in a cell.\nIn this DAG, node 1 and node 2 are treated as the cell’s\ninputs, which are the outputs of the two previous cells in\nthe ﬁnal network (see Figure 4). For each of the remaining\nB − 2 nodes, we ask the controller RNN to make two sets\nof decisions: 1) two previous nodes to be used as inputs\nto the current node and 2) two operations to apply to the\ntwo sampled nodes. The 5 available operations are: identity,\nseparable convolution with kernel size 3 × 3 and 5 × 5, and\naverage pooling and max pooling with kernel size 3 × 3. At\neach node, after the previous nodes and their corresponding\noperations are sampled, the operations are applied on the\nprevious nodes, and their results are added.\nAs before, we illustrate the mechanism of our search space\nFigure 5. An example run of the controller for our search space\nover convolutional cells. Top: the controller’s outputs. In our\nsearch space for convolutional cells, node 1 and node 2 are the\ncell’s inputs, so the controller only has to design node3 and node 4.\nBottom Left: The corresponding DAG, where red edges represent\nthe activated connections. Bottom Right: the convolutional cell\naccording to the controller’s sample.\nwith an example, here withB = 4 nodes (refer to Figure 5).\nDetails are as follows.\n1. Nodes 1, 2 are input nodes, so no decisions are needed for\nthem. Let h1, h2 be the outputs of these nodes.\n2. At node 3: the controller samples two previous nodes and\ntwo operations. In Figure 5 Top Left, it samples node 2,\nnode 2, separable conv 5x5, and identity. This means that\nh3 = sep conv 5x5(h2) + id(h2).\n3. At node 4: the controller samples node 3 , node 1 ,\navg pool 3x3, and sep conv 3x3. This means that h4 =\navg pool 3x3(h3) + sep conv 3x3(h1).\n4. Since all nodes but h4 were used as inputs to at least another\nnode, the only loose end, h4, is treated as the cell’s output.\nIf there are multiple loose ends, they will be concatenated\nalong the depth dimension to form the cell’s output.\nA reduction cell can also be realized from the search space\nwe discussed, simply by: 1) sampling a computational graph\nfrom the search space, and 2) applying all operations with\na stride of 2. A reduction cell thus reduces the spatial di-\nmensions of its input by a factor of 2. Following Zoph et al.\n(2018), we sample the reduction cell conditioned on the\nEfﬁcient Neural Architecture Search via Parameter Sharing\nconvolutional cell, hence making the controller RNN run\nfor a total of 2(B − 2) blocks.\nFinally, we estimate the complexity of this search space.\nAt node i (3 ≤ i ≤ B), the controller can select any two\nnodes from thei − 1 previous nodes, and any two operations\nfrom 5 operations. As all decisions are independent, there\nare (5 × (B − 2)!)2 possible cells. Since we independently\nsample for a convolutional cell and a reduction cell, the ﬁnal\nsize of the search space is(5×(B −2)!)4. WithB = 7 as in\nour experiments, the search space can realize1.3×1011 ﬁnal\nnetworks, making it signiﬁcantly smaller than the search\nspace for entire convolutional networks (Section 2.3).\n3. Experiments\nWe ﬁrst present our experimental results from employing\nENAS to design recurrent cells on the Penn Treebank dataset\nand convolutional architectures on the CIFAR-10 dataset.\nWe then present an ablation study which asserts the role of\nENAS in discovering novel architectures.\n3.1. Language Model with Penn Treebank\nDataset and Settings. Penn Treebank (Marcus et al.,\n1994) is a well-studied benchmark for language model. We\nuse the standard pre-processed version of the dataset, which\nis also used by previous works, e.g. Zaremba et al. (2014).\nSince the goal of our work is to discover cell architectures,\nwe only employ the standard training and test process on\nPenn Treebank, and do not utilize post-training techniques\nsuch as neural cache (Grave et al., 2017) and dynamic eval-\nuation (Krause et al., 2017). Additionally, as Collins et al.\n(2017) have established that RNN models with more param-\neters can learn to store more information, we limit the size\nof our ENAS cell to 24M parameters. We also do not tune\nour hyper-parameters extensively like Melis et al. (2017),\nnor do we train multiple architectures and select the best one\nbased on their validation perplexities like Zoph & Le (2017).\nTherefore, ENAS is not at any advantage, compared to Zoph\n& Le (2017); Yang et al. (2018); Melis et al. (2017), and its\nimproved performance is only due to the cell’s architecture.\nTraining details. Our controller is trained using Adam,\nwith a learning rate of 0.00035. To prevent premature con-\nvergence, we also use a tanh constant of 2.5 and a tempera-\nture of 5.0 for the sampling logits (Bello et al., 2017a;b), and\nadd the controller’s sample entropy to the reward, weighted\nby 0.0001. Additionally, we augment the simple transforma-\ntions between nodes in the constructed recurrent cell with\nhighway connections (Zilly et al., 2017). For instance, in-\nstead of having k2 = ReLU(k1 · W(h)\n2,1 ) as shown in the\nexample from Section 2.1, we havek2 =c2 ⊗ ReLU(k1 ·\nW(h)\n2,1 ) + (1 −c2) ⊗k1, where c2 = sigmoid(k1 · W(c)\n2,1)\nand ⊗ denotes elementwise multiplication.\nThe shared parameters of the child models ω are trained\nusing SGD with a learning rate of 20.0, decayed by a factor\nof 0.96 after every epoch starting at epoch 15, for a total of\n150 epochs. We clip the norm of the gradient ∇ω at 0.25.\nWe ﬁnd that using a large learning rate whilst clipping the\ngradient norm at a small threshold makes the updates on\nω more stable. We utilize three regularization techniques\nonω: an ℓ2 regularization weighted by 10−7; variational\ndropout (Gal & Ghahramani, 2016); and tying word embed-\ndings and softmax weights (Inan et al., 2017). More details\nare in Appendix A.\nResults. Running on a single Nvidia GTX 1080Ti GPU,\nENAS ﬁnds a recurrent cell in about10 hours. In Table 1,\nwe present the performance of the ENAS cell as well as\nother baselines that do not employ post-training processing.\nThe ENAS cell achieves a test perplexity of 56.3, which is\non par with the existing state-of-the-art of 56.0 achieved\nby Mixture of Softmaxes (MoS) (Yang et al., 2018). Note\nthat we do not apply MoS to the ENAS cell. Importantly,\nENAS cell outperforms NAS (Zoph & Le, 2017) by more\nthan 6 perplexity points, whilst the search process of ENAS,\nin terms of GPU hours, is more than 1000x faster.\nArchitecture Additional TechniquesParams Test\n(million)PPL\nLSTM (Zaremba et al., 2014)Vanilla Dropout 66 78.4\nLSTM (Gal & Ghahramani, 2016)VD 66 75.2\nLSTM (Inan et al., 2017) VD, WT 51 68.5\nRHN (Zilly et al., 2017) VD, WT 24 66.0\nLSTM (Melis et al., 2017) Hyper-parameters Search24 59.5\nLSTM (Yang et al., 2018) VD, WT,ℓ2, AWD, MoC 22 57.6\nLSTM (Merity et al., 2017) VD, WT,ℓ2, AWD 24 57.3\nLSTM (Yang et al., 2018) VD, WT,ℓ2, AWD, MoS 22 56.0\nNAS (Zoph & Le, 2017) VD, WT 54 62.4\nENAS VD, WT,ℓ2 24 56.3\nTable 1. Test perplexity on Penn Treebank of ENAS and other base-\nlines. Abbreviations: RHN is Recurrent Highway Network, VD\nis Variational Dropout; WT is Weight Tying; ℓ2 is Weight Penalty;\nAWD is Averaged Weight Drop; MoC is Mixture of Contexts; MoS\nis Mixture of Softmaxes.\nFigure 6. The RNN cell ENAS discovered for Penn Treebank.\nOur ENAS cell, visualized in Figure 6, has a few interesting\nproperties. First, all non-linearities in the cell are either\nReLU or tanh, even though the search space also has two\nother functions: identity and sigmoid. Second, we suspect\nEfﬁcient Neural Architecture Search via Parameter Sharing\nthis cell is a local optimum, similar to the observations made\nby Zoph & Le (2017). When we randomly pick some nodes\nand switch the non-linearity into identity or sigmoid, the\nperplexity increases up to 8 points. Similarly, when we\nrandomly switch some ReLU nodes into tanh or vice versa,\nthe perplexity also increases, but only up to 3 points. Third,\nas shown in Figure 6, the output of our ENAS cell is an\naverage of 6 nodes. This behavior is similar to that of Mix-\nture of Contexts (MoC) (Yang et al., 2018). Not only does\nENAS independently discover MoC, but it also learns to\nbalance between i) the number of contexts to mix, which\nincreases the model’s expressiveness, and ii) the depth of\nthe recurrent cell, which learns more complex transforma-\ntions (Zilly et al., 2017).\n3.2. Image Classiﬁcation on CIFAR-10\nDataset. The CIFAR-10 dataset (Krizhevsky, 2009) con-\nsists of 50, 000 training images and 10, 000 test images. We\nuse the standard data pre-processing and augmentation tech-\nniques, i.e. subtracting the channel mean and dividing the\nchannel standard deviation, centrally padding the training\nimages to 40 × 40 and randomly cropping them back to\n32 × 32, and randomly ﬂipping them horizontally.\nSearch spaces. We apply ENAS to two search spaces: 1)\nthe macro search space over entire convolutional models\n(Section 2.3); and 2) the micro search space over convolu-\ntional cells (Section 2.4).\nTraining details. The shared parameters ω are trained\nwith Nesterov momentum (Nesterov, 1983), where the learn-\ning rate follows the cosine schedule with lmax = 0 .05,\nlmin = 0.001,T0 = 10, andTmul = 2 (Loshchilov & Hutter,\n2017). Each architecture search is run for 310 epochs. We\ninitializeω with He initialization (He et al., 2015). We also\napply anℓ2 weight decay of 10−4. We train the architectures\nrecommended by the controller using the same settings.\nThe policy parameters θ are initialized uniformly in\n[−0.1, 0.1], and trained with Adam at a learning rate of\n0.00035. Similar to the procedure in Section 3.1, we apply\na tanh constant of 2.5 and a temperature of 5.0 to the con-\ntroller’s logits, and add the controller entropy to the reward,\nweighted by 0.1. Additionally, in the macro search space,\nwe enforce the sparsity in the skip connections by adding to\nthe reward the KL divergence between: 1) the skip connec-\ntion probability between any two layers and 2) our chosen\nprobabilityρ = 0.4, which represents the prior belief of a\nskip connection being formed. This KL divergence term is\nweighted by 0.8. More training details are in Appendix B.\nResults. Table 2 summarizes the test errors of ENAS and\nother approaches. In this table, the ﬁrst block presents the\nresults of DenseNet (Huang et al., 2016), one of the highest-\nperforming architectures that are designed by human experts.\nWhen trained with a strong regularization technique, such as\nShake-Shake (Gastaldi, 2016), and a data augmentation tech-\nnique, such as CutOut (DeVries & Taylor, 2017), DenseNet\nimpressively achieves the test error of 2.56%.\nThe second block of Table 2 presents the performances of\napproaches that attempt to design an entire convolutional\nnetwork, along with the the number of GPUs and the time\nthese methods take to discover their ﬁnal models. As shown,\nENAS ﬁnds a network architecture, which we visualize in\nFigure 7, and which achieves 4.23% test error. This test\nerror is better than the error of 4.47%, achieved by the\nsecond best NAS model (Zoph & Le, 2017). If we keep\nthe architecture, but increase the number of ﬁlters in the\nnetwork’s highest layer to 512, then the test error decreases\nto 3.87%, which is not far away from NAS’s best model,\nwhose test error is 3.65%. Impressively, ENAS takes about\n7 hours to ﬁnd this architecture, reducing the number of\nGPU-hours by more than 50,000x compared to NAS.\nThe third block of Table 2 presents the performances of\napproaches that attempt to design one more more modules\nand then connect them together to form the ﬁnal networks.\nENAS takes 11.5 hours to discover the convolution cell\nand the reduction cell, which are visualized in Figure 8.\nWith the convolutional cell replicated for N = 6 times\n(c.f. Figure 4), ENAS achieves 3.54% test error, on par with\nthe 3.41% error of NASNet-A (Zoph et al., 2018). With\nCutOut (DeVries & Taylor, 2017), ENAS’s error decreases\nto 2.89%, compared to 2.65% by NASNet-A.\nIn addition to ENAS’s strong performance, we also ﬁnd\nthat the models found by ENAS are, in a sense, the local\nminimums in their search spaces. In particular, in the model\nthat ENAS ﬁnds from the marco search space, if we replace\nall separable convolutions with normal convolutions, and\nthen adjust the model size so that the number of parameters\nstay the same, then the test error increases by 1.7%. Simi-\nlarly, if we randomly change several connections in the cells\nthat ENAS ﬁnds in the micro search space, the test error\nincreases by 2.1%. This behavior is also observed when\nENAS searches for recurrent cells (c.f. Section 3.1), as well\nas in Zoph & Le (2017). We thus believe that the controller\nRNN learned by ENAS is as good as the controller RNN\nlearned by NAS, and that the performance gap between NAS\nand ENAS is due to the fact that we do not sample multiple\narchitectures from our trained controller, train them, and\nthen select the best architecture on the validation data. This\nextra step beneﬁts NAS’s performance.\n3.3. The Importance of ENAS\nA question regarding ENAS’s importance is whether\nENAS is actually capable of ﬁnding good architectures, or\nif it is the design of the search spaces that leads to ENAS’s\nEfﬁcient Neural Architecture Search via Parameter Sharing\nMethod GPUs Times Params Error\n(days) (million) ( %)\nDenseNet-BC (Huang et al., 2016) − − 25.6 3.46\nDenseNet + Shake-Shake (Gastaldi, 2016) − − 26.2 2.86\nDenseNet + CutOut (DeVries & Taylor, 2017) − − 26.2 2.56\nBudgeted Super Nets (Veniat & Denoyer, 2017) − − − 9.21\nConvFabrics (Saxena & Verbeek, 2016) − − 21.2 7.43\nMacro NAS + Q-Learning (Baker et al., 2017a) 10 8-10 11.2 6.92\nNet Transformation (Cai et al., 2018) 5 2 19.7 5.70\nFractalNet (Larsson et al., 2017) − − 38.6 4.60\nSMASH (Brock et al., 2018) 1 1.5 16.0 4.03\nNAS (Zoph & Le, 2017) 800 21-28 7.1 4.47\nNAS + more ﬁlters (Zoph & Le, 2017) 800 21-28 37.4 3.65\nENAS + macro search space 1 0.32 21.3 4.23\nENAS + macro search space + more channels 1 0.32 38.0 3.87\nHierarchical NAS (Liu et al., 2018) 200 1.5 61.3 3.63\nMicro NAS + Q-Learning (Zhong et al., 2018) 32 3 − 3.60\nProgressive NAS (Liu et al., 2017) 100 1.5 3.2 3.63\nNASNet-A (Zoph et al., 2018) 450 3-4 3.3 3.41\nNASNet-A + CutOut (Zoph et al., 2018) 450 3-4 3.3 2.65\nENAS + micro search space 1 0.45 4.6 3.54\nENAS + micro search space + CutOut 1 0.45 4.6 2.89\nTable 2. Classiﬁcation errors of ENAS and baselines on CIFAR-10. In this table, the ﬁrst block presents DenseNet, one of the state-of-the-\nart architectures designed by human experts. The second block presents approaches that design the entire network. The last block presents\ntechniques that design modular cells which are combined to build the ﬁnal network.\nFigure 7. ENAS’s discovered network from the macro search space for image classiﬁcation.\nstrong empirical performance.\nComparing to Guided Random Search. We uniformly\nsample a recurrent cell, an entire convolutional network,\nand a pair of convolutional and reduction cells from their\nsearch spaces and train them to convergence using the same\nsettings as the architectures found by ENAS. For the macro\nspace over entire networks, we sample the skip connections\nwith an activation probability of 0.4, effectively balancing\nENAS’s advantage from the KL divergence term in its re-\nward (see Section 3.2). Our random recurrent cell achieves\nthe test perplexity of 81.2 on Penn Treebank, which is far\nworse than ENAS’s perplexity of56.3. Our random convolu-\ntional network reaches 5.86% test error, and our two random\ncells reache 6.77% on CIFAR-10, while ENAS achieves\n4.23% and 3.54%, respectively.\nDisabling ENAS Search. In addition to random search,\nwe attempt to train only the shared parameters ω without\nupdating the controller. We conduct this study for our macro\nsearch space (Section 2.3), where the effect of an untrained\nrandom controller is similar to dropout with a rate of 0.5\non the skip connections, and to drop-path on the opera-\ntions (Zoph et al., 2018; Larsson et al., 2017). At con-\nvergence, the model has the error rate of 8.92%. On the\nvalidation set, an ensemble of 250 Monte Carlo conﬁgura-\ntions of this trained model can only reach 5.49% test error.\nWe therefore conclude that the appropriate training of the\nENAS controller is crucial for good performance.\nEfﬁcient Neural Architecture Search via Parameter Sharing\nFigure 8. ENAS cells discovered in the micro search space.\n4. Related Work and Discussions\nThere is a growing interest in improving the efﬁciency of\nNAS. Concurrent to our work are the promising ideas of\nusing performance prediction (Baker et al., 2017b; Deng\net al., 2017), using iterative search method for architectures\nof growing complexity (Liu et al., 2017), and using hier-\narchical representation of architectures (Liu et al., 2018).\nTable 2 shows that ENAS is signiﬁcantly more efﬁcient than\nthese other methods, in GPU hours.\nENAS’s design of sharing weights between architectures\nis inspired by the concept of weight inheritance in neu-\nral model evolution (Real et al., 2017; 2018). Addition-\nally, ENAS’s choice of representing computations using a\nDAG is inspired by the concept of stochastic computational\ngraph (Schulman et al., 2015), which introduces nodes with\nstochastic outputs into a computational graph. ENAS’s uti-\nlizes such stochastic decisions in a network to make discrete\narchitectural decisions that govern subsequent computations\nin the network, trains the decision maker, i.e. the controller,\nand ﬁnally harvests the decisions to derive architectures.\nClosely related to ENAS is SMASH (Brock et al., 2018),\nwhich designs an architecture and then uses a hypernet-\nwork (Ha et al., 2017) to generate its weight. Such us-\nage of the hypernetwork in SMASH inherently restricts\nthe weights of SMASH’s child architectures to a low-rank\nspace. This is because the hypernetwork generates weights\nfor SMASH’s child models via tensor products (Ha et al.,\n2017), which suffer from a low-rank restriction as for ar-\nbitrary matrices A and B, one always has the inequality:\nrank(A · B) ≤ min {rank(A), rank(B)}. Due to this limit,\nSMASH will ﬁnd architectures that perform well in the\nrestricted low-rank space of their weights, rather than ar-\nchitectures that perform well in the normal training setups,\nwhere the weights are no longer restricted. Meanwhile,\nENAS allows the weights of its child models to be arbitrary,\neffectively avoiding such restriction. We suspect this is the\nreason behind ENAS’s superior empirical performance to\nSMASH. In addition, it can be seen from our experiments\nthat ENAS can be ﬂexibly applied to multiple search spaces\nand disparate domains, e.g. the space of RNN cells for the\ntext domain, the macro search space of entire networks, and\nthe micro search space of convolutional cells for the image\ndomain.\n5. Conclusion\nNAS is an important advance that automatizes the designing\nprocess of neural networks. However, NAS’s computational\nexpense prevents it from being widely adopted. In this\npaper, we presented ENAS, a novel method that speeds up\nNAS by more than 1000x, in terms of GPU hours. ENAS’s\nkey contribution is the sharing of parameters across child\nmodels during the search for architectures. This insight is\nimplemented by searching for a subgraph within a larger\ngraph that incorporates architectures in a search space. We\nshowed that ENAS works well on both CIFAR-10 and Penn\nTreebank datasets.\nAcknowledgements\nThe authors want to thank Jaime Carbonell, Zihang Dai,\nLukasz Kaiser, Azalia Mirhoseini, Ashwin Paranjape,\nDaniel Selsam, and Xinyi Wang for their suggestions on\nimproving the paper.\nReferences\nBaker, Bowen, Gupta, Otkrist, Naik, Nikhil, and Raskar,\nRamesh. Designing neural network architectures using\nreinforcement learning. In ICLR, 2017a.\nBaker, Bowen, Otkrist, Gupta, Raskar, Ramesh, and Naik,\nNikhil. Accelerating neural architecture search using\nEfﬁcient Neural Architecture Search via Parameter Sharing\nperformance prediction. Arxiv, 1705.10823, 2017b.\nBello, Irwan, Pham, Hieu, Le, Quoc V ., Norouzi, Moham-\nmad, and Bengio, Samy. Neural combinatorial optimiza-\ntion with reinforcement learning. In ICLR Workshop,\n2017a.\nBello, Irwan, Zoph, Barret, Vasudevan, Vijay, and Le,\nQuoc V . Neural optimizer search with reinforcement\nlearning. In ICML, 2017b.\nBrock, Andrew, Lim, Theodore, Ritchie, James M., and\nWeston, Nick. SMASH: one-shot model architecture\nsearch through hypernetworks. ICLR, 2018.\nCai, Han, Chen, Tianyao, Zhang, Weinan, Yu, Yong., and\nWang, Jun. Efﬁcient architecture search by network trans-\nformation. In AAAI, 2018.\nChollet, Francois. Xception: Deep learning with depthwise\nseparable convolutions. In CVPR, 2017.\nCollins, Jasmine, Sohl-Dickstein, Jascha, and Sussillo,\nDavid. Capacity and trainability in recurrent neural net-\nworks. In ICLR, 2017.\nDeng, Boyang, Yan, Junjie, and Lin, Dahua. Peephole:\nPredicting network performance before training. Arxiv,\n1705.10823, 2017.\nDeVries, Terrance and Taylor, Graham W. Improved regu-\nlarization of convolutional neural networks with cutout.\nArxiv, 1708.04552, 2017.\nGal, Yarin and Ghahramani, Zoubin. A theoretically\ngrounded application of dropout in recurrent neural net-\nworks. In NIPS, 2016.\nGastaldi, Xavier. Shake-shake regularization of 3-branch\nresidual networks. In ICLR Workshop Track, 2016.\nGrave, Edouard, Joulin, Armand, and Usunier, Nicolas. Im-\nproving neural language models with a continuous cache.\nIn ICLR, 2017.\nHa, David, Dai, Andrew, and Le, Quoc V . Hypernetworks.\nIn ICLR, 2017.\nHe, Kaiming, Zhang, Xiangyu, Rein, Shaoqing, and Sun,\nJian. Delving deep into rectiﬁers: Surpassing human-\nlevel performance on imagenet classiﬁcation. InCVPR,\n2015.\nHe, Kaiming, Zhang, Xiangyu, Ren, Shaoqing, and Sun,\nJian. Deep residual learning for image recognition. In\nCPVR, 2016a.\nHe, Kaiming, Zhang, Xiangyu, Ren, Shaoqing, and Sun,\nJian. Identity mappings in deep residual networks. In\nCPVR, 2016b.\nHochreiter, Sepp and Schmidhuber, J¨urgen. Long short-term\nmemory. In Neural Computations, 1997.\nHuang, Gao, Liu, Zhuang, van der Maaten, Laurens, and\nWeinberger, Kilian Q. Densely connected convolutional\nnetworks. In CVPR, 2016.\nInan, Hakan, Khosravi, Khashayar, and Socher, Richard.\nTying word vectors and word classiﬁers: a loss framework\nfor language modeling. In ICLR, 2017.\nIoffe, Sergey and Szegedy, Christian. Batch normalization:\nAccelerating deep network training by reducing internal\ncovariate shift. In ICML, 2015.\nKingma, Diederik P. and Ba, Jimmy Lei. Adam: A method\nfor stochastic optimization. In ICLR, 2015.\nKrause, Ben, Kahembwe, Emmanuel, Murray, Iain, and\nRenals, Steve. Dynamic evaluation of neural sequence\nmodels. Arxiv, 1709.07432, 2017.\nKrizhevsky, Alex. Learning multiple layers of features from\ntiny images. Technical report, 2009.\nLarsson, Gustav, Maire, Michael, and Shakhnarovich, Gre-\ngory. Fractalnet: Ultra-deep neural networks without\nresiduals. In ICLR, 2017.\nLin, Min, Chen, Qiang, and Yan, Shuicheng. Network in\nnetwork. Arxiv, 1312.4400, 2013.\nLiu, Chenxi, Zoph, Barret, Shlens, Jonathon, Hua, Wei, Li,\nLi-Jia, Fei-Fei, Li, Yuille, Alan, Huang, Jonathan, and\nMurphy, Kevin. Progressive neural architecture search.\nArxiv, 1712.00559, 2017.\nLiu, Hanxiao, Simonyan, Karen, Vinyals, Oriol, Fernando,\nChrisantha, and Kavukcuoglu, Koray. Hierarchical rep-\nresentations for efﬁcient architecture search. In ICLR,\n2018.\nLoshchilov, Ilya and Hutter, Frank. Sgdr: Stochastic gradi-\nent descent with warm restarts. In ICLR, 2017.\nLuong, Minh-Thang, Le, Quoc V ., Sutskever, Ilya, Vinyals,\nOriol, and Kaiser, Lukasz. Multi-task sequence to se-\nquence learning. In ICLR, 2016.\nMarcus, Mitchell, Kim, Grace, Marcinkiewicz, Mary Ann,\nMacIntyre, Robert, Bies, Ann, Ferguson, Mark, Katz,\nKaren, and Schasberger, Britta. The penn treebank: An-\nnotating predicate argument structure. In Proceedings of\nthe Workshop on Human Language Technology, 1994.\nMelis, G´abor, Dyer, Chris, and Blunsom, Phil. On the state\nof the art of evaluation in neural language models. Arxiv,\n1707.05589, 2017.\nEfﬁcient Neural Architecture Search via Parameter Sharing\nMerity, Stephen, Keskar, Nitish Shirish, and Socher,\nRichard. Regularizing and optimizing LSTM language\nmodels. Arxiv, 1708.02182, 2017.\nNegrinho, Renato and Gordon, Geoff. Deeparchitect: Au-\ntomatically designing and training deep architectures. In\nCPVR, 2017.\nNesterov, Yurii E. A method for solving the convex pro-\ngramming problem with convergence rateo(1/k2). Soviet\nMathematics Doklady, 1983.\nRazavian, Ali Sharif, Azizpour, Hossein, Josephine, Sulli-\nvan, and Carlsson, Stefan. Cnn features off-the-shelf: an\nastounding baseline for recognition. In CVPR, 2014.\nReal, Esteban, Moore, Sherry, Selle, Andrew, Saxena,\nSaurabh, Leon, Yutaka Suematsu, Tan, Jie, Le, Quoc,\nand Kurakin, Alex. Large-scale evolution of image clas-\nsiﬁers. In ICML, 2017.\nReal, Esteban, Aggarwal, Alok, Huang, Yanping, and Le,\nQuoc V . Peephole: Predicting network performance be-\nfore training. Arxiv, 1802.01548, 2018.\nSaxena, Shreyas and Verbeek, Jakob. Convolutional neural\nfabrics. In NIPS, 2016.\nSchulman, John, Heess, Nicolas, Weber, Theophane, and\nAbbeel, Pieter. Gradient estimation using stochastic com-\nputation graphs. In NIPS, 2015.\nVeniat, Tom and Denoyer, Ludovic. Learning time-efﬁcient\ndeep architectures with budgeted super networks. Arxiv,\n1706.00046, 2017.\nWilliams, Ronald J. Simple statistical gradient-following\nalgorithms for connectionist reinforcement learning. Ma-\nchine Learning, 1992.\nYang, Zhilin, Dai, Zihang, Salakhutdinov, Ruslan, and Co-\nhen, William. Breaking the softmax bottleneck: A high-\nrank rnn language model. In ICLR, 2018.\nZaremba, Wojciech, Sutskever, Ilya, and Vinyals, Oriol. Re-\ncurrent neural network regularization. Arxiv, 1409.2329,\n2014.\nZhong, Zhao, Yan, Junjie, and Liu, Cheng-Lin. Practical\nnetwork blocks design with q-learning. AAAI, 2018.\nZilly, Julian Georg, Srivastava, Rupesh Kumar, Koutn´ık, Jan,\nand Schmidhuber, J¨urgen. Recurrent highway networks.\nIn ICML, 2017.\nZoph, Barret and Le, Quoc V . Neural architecture search\nwith reinforcement learning. In ICLR, 2017.\nZoph, Barret, Yuret, Deniz, May, Jonathan, and Knight,\nKevin. Transfer learning for low-resource neural machine\ntranslation. In EMNLP, 2016.\nZoph, Barret, Vasudevan, Vijay, Shlens, Jonathon, and Le,\nQuoc V . Learning transferable architectures for scalable\nimage recognition. In CVPR, 2018.",
  "values": {
    "Non-maleficence": "No",
    "Respect for Law and public interest": "No",
    "Autonomy (power to decide)": "No",
    "Interpretable (to users)": "No",
    "Deferral to humans": "No",
    "Transparent (to users)": "No",
    "Privacy": "No",
    "Justice": "No",
    "Explicability": "No",
    "Critiqability": "No",
    "Not socially biased": "No",
    "Beneficence": "No",
    "Respect for Persons": "No",
    "User influence": "No",
    "Fairness": "No",
    "Collective influence": "No"
  }
}