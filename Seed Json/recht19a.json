{
  "pdf": "recht19a",
  "title": "main_icml",
  "author": "Vaishaal Shankar",
  "paper_id": "recht19a",
  "text": "Do ImageNet Classiﬁers Generalize to ImageNet?\nBenjamin Recht⇤ 1 Rebecca Roelofs 1 Ludwig Schmidt 1 Vaishaal Shankar 1\nAbstract\nWe build new test sets for the CIFAR-10 and Ima-\ngeNet datasets. Both benchmarks have been the\nfocus of intense research for almost a decade, rais-\ning the danger of overﬁtting to excessively re-used\ntest sets. By closely following the original dataset\ncreation processes, we test to what extent current\nclassiﬁcation models generalize to new data. We\nevaluate a broad range of models and ﬁnd accu-\nracy drops of 3% – 15% on CIFAR-10 and 11%\n– 14% on ImageNet. However, accuracy gains\non the original test sets translate to larger gains\non the new test sets. Our results suggest that the\naccuracy drops are not caused by adaptivity, but\nby the models’ inability to generalize to slightly\n“harder” images than those found in the original\ntest sets.\n1. Introduction\nThe overarching goal of machine learning is to produce\nmodels that generalize. We usually quantify generalization\nby measuring the performance of a model on a held-out\ntest set. What does good performance on the test set then\nimply? At the very least, one would hope that the model also\nperforms well on a new test set assembled from the same\ndata source by following the same data cleaning protocol.\nIn this paper, we realize this thought experiment by repli-\ncating the dataset creation process for two prominent\nbenchmarks, CIFAR-10 and ImageNet (\nDeng et al., 2009;\nKrizhevsky, 2009). In contrast to the ideal outcome, we ﬁnd\nthat a wide range of classiﬁcation models fail to reach their\noriginal accuracy scores. The accuracy drops range from\n3% to 15% on CIFAR-10 and 11% to 14% on ImageNet.\nOn ImageNet, the accuracy loss amounts to approximately\nﬁve years of progress in a highly active period of machine\nlearning research.\n⇤ Authors ordered alphabetically. Ben did none of the work.\n1Department of Computer Science, University of California Berke-\nley, Berkeley, California, USA. Correspondence to: Benjamin\nRecht <brecht@berkeley.edu>.\nProceedings of the 36 th International Conference on Machine\nLearning, Long Beach, California, PMLR 97, 2019. Copyright\n2019 by the author(s).\nConventional wisdom suggests that such drops arise because\nthe models have been adapted to the speciﬁc images in the\noriginal test sets, e.g., via extensive hyperparameter tuning.\nHowever, our experiments show that the relative order of\nmodels is almost exactly preserved on our new test sets:\nthe models with highest accuracy on the original test sets\nare still the models with highest accuracy on the new test\nsets. Moreover, there are no diminishing returns in accuracy.\nIn fact, every percentage point of accuracy improvement\non the original test set translates to a larger improvement\non our new test sets. So although later models could have\nbeen adapted more to the test set, they see smaller drops in\naccuracy. These results provide evidence that exhaustive\ntest set evaluations are an effective way to improve image\nclassiﬁcation models. Adaptivity is therefore an unlikely\nexplanation for the accuracy drops.\nInstead, we propose an alternative explanation based on\nthe relative difﬁculty of the original and new test sets. We\ndemonstrate that it is possible to recover the original Im-\nageNet accuracies almost exactly if we only include the\neasiest images from our candidate pool. This suggests that\nthe accuracy scores of even the best image classiﬁers are\nstill highly sensitive to minutiae of the data cleaning process.\nThis brittleness puts claims about human-level performance\ninto context (He et al. , 2015; Karpathy, 2011; Russakovsky\net al., 2015). It also shows that current classiﬁers still do\nnot generalize reliably even in the benign environment of a\ncarefully controlled reproducibility experiment.\nFigure 1 shows the main result of our experiment. Before\nwe describe our methodology in Section 3, the next section\nprovides relevant background. To enable future research, we\nrelease both our new test sets and the corresponding code. 1\n2. Potential Causes of Accuracy Drops\nWe adopt the standard classiﬁcation setup and posit the\nexistence of a “true” underlying data distribution D over\nlabeled examples (x, y). The overall goal in classiﬁcation\n1https://github.com/modestyachts/CIFAR-10\n.1 and https://github.com/modestyachts/ImageN\netV2\nDo ImageNet Classiﬁers Generalize to ImageNet?\nFigure 1. Model accuracy on the original test sets vs. our new test sets. Each data point corresponds to one model in our testbed (shown\nwith 95% Clopper-Pearson conﬁdence intervals). The plots reveal two main phenomena: (i) There is a signiﬁcant drop in accuracy from\nthe original to the new test sets. (ii) The model accuracies closely follow a linear function with slope greater than 1 (1.7 for CIFAR-10\nand 1.1 for ImageNet). This means that every percentage point of progress on the original test set translates into more than one percentage\npoint on the new test set. The two plots are drawn so that their aspect ratio is the same, i.e., the slopes of the lines are visually comparable.\nThe red shaded region is a 95% conﬁdence region for the linear ﬁt from 100,000 bootstrap samples.\nis to ﬁnd a model ˆf that minimizes the population loss\nLD( ˆf )= E\n(x,y)⇠ D\nh\nI[ ˆf (x) 6= y]\ni\n. (1)\nSince we usually do not know the distribution D, we instead\nmeasure the performance of a trained classiﬁer via a test set\nS drawn from the distribution D:\nLS( ˆf )= 1\n|S|\nX\n(x,y)2 S\nI[ ˆf (x) 6= y] . (2)\nWe then use this test error LS( ˆf ) as a proxy for the popu-\nlation loss LD( ˆf ). If a model ˆf achieves a low test error,\nwe assume that it will perform similarly well on future ex-\namples from the distribution D. This assumption underlies\nessentially all empirical evaluations in machine learning\nsince it allows us to argue that the model ˆf generalizes.\nIn our experiments, we test this assumption by collecting a\nnew test set S0 from a data distribution D0 that we carefully\ncontrol to resemble the original distribution D. Ideally, the\noriginal test accuracy LS( ˆf ) and new test accuracy LS0( ˆf )\nwould then match up to the random sampling error. In\ncontrast to this idealized view, our results in Figure 1 show\na large drop in accuracy from the original test set S set to\nour new test set S0. To understand this accuracy drop in\nmore detail, we decompose the difference between LS( ˆf )\nand LS0( ˆf ) into three parts (dropping the dependence on ˆf\nto simplify notation):\nLS \u0000 LS0 =( LS \u0000 LD)| {z }\nAdaptivity gap\n+( LD \u0000 LD0)| {z }\nDistribution Gap\n+( LD0 \u0000 LS0)| {z }\nGeneralization gap\n.\nWe now discuss to what extent each of the three terms can\nlead to accuracy drops.\nGeneralization Gap.\nBy construction, our new test set\nS0 is independent of the existing classiﬁer ˆf . Hence the\nthird term LD0 \u0000 LS0 is the standard generalization gap\ncommonly studied in machine learning. It is determined\nsolely by the random sampling error.\nA ﬁrst guess is that this inherent sampling error sufﬁces\nto explain the accuracy drops in Figure 1 (e.g., the new\ntest set S0 could have sampled certain “harder” modes of\nthe distribution D more often). However, random ﬂuctu-\nations of this magnitude are unlikely for the size of our\ntest sets. With 10,000 data points (as in our new ImageNet\ntest set), a Clopper-Pearson 95% conﬁdence interval for\nthe test accuracy has size of at most\n± 1%. Increasing the\nconﬁdence level to 99.99% yields a conﬁdence interval of\nsize at most ± 2%. Moreover, these conﬁdence intervals\nbecome smaller for higher accuracies, which is the rele-\nvant regime for the best-performing models. Hence random\nchance alone cannot explain the accuracy drops observed in\nour experiments.2\nAdaptivity Gap. We call the term LS \u0000 LD the adaptivity\ngap. It measures how much adapting the model ˆf to the\ntest set S causes the test error LS to underestimate the\npopulation loss LD. If we assumed that our model ˆf is\nindependent of the test set S, this terms would follow the\n2We remark that the sampling process for the new test set S0\ncould indeed systematically sample harder modes more often than\nunder the original data distribution D. Such a systematic change\nin the sampling process would not be an effect of random chance\nbut captured by the distribution gap described below.\nDo ImageNet Classiﬁers Generalize to ImageNet?\nsame concentration laws as the generalization gap LD0\u0000 LS0\nabove. But this assumption is undermined by the common\npractice of tuning model hyperparameters directly on the\ntest set, which introduces dependencies between the model\nˆf and the test set S. In the extreme case, this can be seen\nas training directly on the test set. But milder forms of\nadaptivity may also artiﬁcially inﬂate accuracy scores by\nincreasing the gap between\nLS and LD beyond the purely\nrandom error.\nDistribution Gap.\nWe call the term LD \u0000 LD0 the distri-\nbution gap . It quantiﬁes how much the change from the\noriginal distribution D to our new distribution D0 affects\nthe model ˆf . Note that this term is not inﬂuenced by ran-\ndom effects but quantiﬁes the systematic difference between\nsampling the original and new test sets. While we went to\ngreat lengths to minimize such systematic differences, in\npractice it is hard to argue whether two high-dimensional\ndistributions are exactly the same. We typically lack a pre-\ncise deﬁnition of either distribution, and collecting a real\ndataset involves a plethora of design choices.\n2.1. Distinguishing Between the Two Mechanisms\nFor a single model ˆf , it is unclear how to disentangle the\nadaptivity and distribution gaps. To gain a more nuanced\nunderstanding, we measure accuracies for multiple models\nˆf1,..., ˆfk. This provides additional insights because it\nallows us to determine how the two gaps have evolved over\ntime.\nFor both CIFAR-10 and ImageNet, the classiﬁcation models\ncome from a long line of papers that incrementally improved\naccuracy scores over the past decade. A natural assumption\nis that later models have experienced more adaptive over-\nﬁtting since they are the result of more successive hyperpa-\nrameter tuning on the same test set. Their higher accuracy\nscores would then come from an increasing adaptivity gap\nand reﬂect progress only on the speciﬁc examples in the\ntest set\nS but not on the actual distribution D. In an ex-\ntreme case, the population accuracies LD( ˆfi) would plateau\n(or even decrease) while the test accuracies LS( ˆfi) would\ncontinue to grow for successive models ˆfi.\nHowever, this idealized scenario is in stark contrast to our\nresults in Figure 1. Later models do not see diminishing re-\nturns but an increased advantage over earlier models. Hence\nwe view our results as evidence that the accuracy drops\nmainly stem from a large distribution gap. After presenting\nour results in more detail in the next section, we will further\ndiscuss this point in Section 5.\n3. Summary of Our Experiments\nWe now give an overview of the main steps in our repro-\nducibility experiment. Appendices C and D describe our\nmethodology in more detail. We begin with the ﬁrst deci-\nsion, which was to choose informative datasets.\n3.1. Choice of Datasets\nWe focus on image classiﬁcation since it has become the\nmost prominent task in machine learning and underlies a\nbroad range of applications. The cumulative progress on\nImageNet is often cited as one of the main breakthroughs\nin computer vision and machine learning (\nMalik, 2017).\nState-of-the-art models now surpass human-level accuracy\nby some measure (He et al., 2015; Russakovsky et al., 2015).\nThis makes it particularly important to check if common\nimage classiﬁcation models can reliably generalize to new\ndata from the same source.\nWe decided on CIFAR-10 and ImageNet, two of the most\nwidely-used image classiﬁcation benchmarks ( Hamner,\n2017). Both datasets have been the focus of intense research\nfor almost ten years now. Due to the competitive nature of\nthese benchmarks, they are an excellent example for test-\ning whether adaptivity has led to overﬁtting. In addition to\ntheir popularity, their carefully documented dataset creation\nprocess makes them well suited for a reproducibility exper-\niment (Deng et al., 2009; Krizhevsky, 2009; Russakovsky\net al., 2015).\nEach of the two datasets has speciﬁc features that make it\nespecially interesting for our replication study. CIFAR-10\nis small enough so that many researchers developed and\ntested new models for this dataset. In contrast, ImageNet\nrequires signiﬁcantly more computational resources, and\nexperimenting with new architectures has long been out of\nreach for many research groups. As a result, CIFAR-10 has\nlikely experienced more hyperparameter tuning, which may\nalso have led to more adaptive overﬁtting.\nOn the other hand, the limited size of CIFAR-10 could also\nmake the models more susceptible to small changes in the\ndistribution. Since the CIFAR-10 models are only exposed\nto a constrained visual environment, they may be unable to\nlearn a robust representation. In contrast, ImageNet captures\na much broader variety of images: it contains about 24⇥\nmore training images than CIFAR-10 and roughly 100⇥\nmore pixels per image. So conventional wisdom (such as\nthe claims of human-level performance) would suggest that\nImageNet models also generalize more reliably .\nAs we will see, neither of these conjectures is supported\nby our data: CIFAR-10 models do not suffer from more\nadaptive overﬁtting, and ImageNet models do not appear to\nbe signiﬁcantly more robust.\n3.2. Dataset Creation Methodology\nOne way to test generalization would be to evaluate existing\nmodels on new i.i.d. data from the original test distribution.\nFor example, this would be possible if the original dataset\nauthors had collected a larger initial dataset and randomly\nDo ImageNet Classiﬁers Generalize to ImageNet?\nsplit it into two test sets, keeping one of the test sets hidden\nfor several years. Unfortunately, we are not aware of such a\nsetup for CIFAR-10 or ImageNet.\nIn this paper, we instead mimic the original distribution as\nclosely as possible by repeating the dataset curation process\nthat selected the original test set 3 from a larger data source.\nWhile this introduces the difﬁculty of disentangling the\nadaptivity gap from the distribution gap, it also enables us\nto check whether independent replication affects current\naccuracy scores. In spite of our efforts, we found that it is\nastonishingly hard to replicate the test set distributions of\nCIFAR-10 and ImageNet. At a high level, creating a new\ntest set consists of two parts:\nGathering Data.\nTo obtain images for a new test set, a\nsimple approach would be to use a different dataset, e.g.,\nOpen Images (\nKrasin et al., 2017). However, each dataset\ncomes with speciﬁc biases ( Torralba and Efros, 2011). For\ninstance, CIFAR-10 and ImageNet were assembled in the\nlate 2000s, and some classes such as car or cell_phone\nhave changed signiﬁcantly over the past decade. We avoided\nsuch biases by drawing new images from the same source as\nCIFAR-10 and ImageNet. For CIFAR-10, this was the larger\nTiny Image dataset (\nTorralba et al., 2008). For ImageNet, we\nfollowed the original process of utilizing the Flickr image\nhosting service and only considered images uploaded in\na similar time frame as for ImageNet. In addition to the\ndata source and the class distribution, both datasets also\nhave rich structure within each class. For instance, each\nclass in CIFAR-10 consists of images from multiple speciﬁc\nkeywords in Tiny Images. Similarly, each class in ImageNet\nwas assembled from the results of multiple queries to the\nFlickr API. We relied on the documentation of the two\ndatasets to closely match the sub-class distribution as well.\nCleaning Data. Many images in Tiny Images and the\nFlickr results are only weakly related to the query (or not at\nall). To obtain a high-quality dataset with correct labels, it\nis therefore necessary to manually select valid images from\nthe candidate pool. While this step may seem trivial, our\nresults in Section\n4 will show that it has major impact on\nthe model accuracies.\nThe authors of CIFAR-10 relied on paid student labelers\nto annotate their dataset. The researchers in the ImageNet\nproject utilized Amazon Mechanical Turk (MTurk) to han-\ndle the large size of their dataset. We again replicated both\nannotation processes. Two graduate students authors of\nthis paper impersonated the CIFAR-10 labelers, and we\nemployed MTurk workers for our new ImageNet test set.\n3For ImageNet, we repeat the creation process of the valida-\ntion set because most papers developed and tested models on the\nvalidation set. We discuss this point in more detail in Appendix\nD.1. In the context to this paper, we use the terms “validation set”\nand “test set” interchangeably for ImageNet.\nFor both datasets, we also followed the original labeling\ninstructions, MTurk task format, etc.\nAfter collecting a set of correctly labeled images, we sam-\npled our ﬁnal test sets from the ﬁltered candidate pool. We\ndecided on a test set size of 2,000 for CIFAR-10 and 10,000\nfor ImageNet. While these are smaller than the original\ntest sets, the sample sizes are still large enough to obtain\n95% conﬁdence intervals of about ±1%. Moreover, our aim\nwas to avoid bias due to CIFAR-10 and ImageNet possibly\nleaving only “harder” images in the respective data sources.\nThis effect is minimized by building test sets that are small\ncompared to the original datasets (about 3% of the overall\nCIFAR-10 dataset and less than 1% of the overall ImageNet\ndataset).\n3.3. Results on the New Test Sets\nAfter assembling our new test sets, we evaluated a broad\nrange of image classiﬁcation models spanning a decade of\nmachine learning research. The models include the sem-\ninal AlexNet (\nKrizhevsky et al. , 2012), widely used con-\nvolutional networks ( He et al. , 2016a; Huang et al. , 2017;\nSimonyan and Zisserman, 2014; Szegedy et al., 2016), and\nthe state-of-the-art ( Cubuk et al. , 2018; Liu et al. , 2018).\nFor all deep architectures, we used code previously pub-\nlished online. We relied on pre-trained models whenever\npossible and otherwise ran the training commands from\nthe respective repositories. In addition, we also evaluated\nthe best-performing approaches preceding convolutional\nnetworks on each dataset. These are random features for\nCIFAR-10 (\nCoates et al. , 2011; Rahimi and Recht , 2009)\nand Fisher vectors for ImageNet ( Perronnin et al., 2010).4\nWe wrote our own implementations for these models, which\nwe also release publicly. 5\nOverall, the top-1 accuracies range from 83% to 98% on\nthe original CIFAR-10 test set and 21% to 83% on the\noriginal ImageNet validation set. We refer the reader to\nAppendices\nD.4.3 and C.3.2 for a full list of models and\nsource repositories.\nFigure 1 in the introduction plots original vs. new accuracies,\nand Table 1 in this section summarizes the numbers of key\nmodels. The remaining accuracy scores can be found in\nAppendices C.3.3 and D.4.4. We now brieﬂy describe the\n4We remark that our implementation of Fisher vectors yields\ntop-5 accuracy numbers that are 17% lower than the published\nnumbers in ILSVRC 2012 (\nRussakovsky et al. , 2015). Unfortu-\nnately, there is no publicly available reference implementation of\nFisher vector models achieving this accuracy score. Hence our\nimplementation should not be seen as an exact reproduction of the\nstate-of-the-art Fisher vector model, but as a baseline inspired by\nthis approach. The main goal of including Fisher vector models\nin our experiment is to investigate if they follow the same overall\ntrends as convolutional neural networks.\n5https://github.com/modestyachts/nondeep\nDo ImageNet Classiﬁers Generalize to ImageNet?\nCIFAR-10\nOrig. New\nRank Model Orig. Accuracy New Accuracy Gap Rank ∆ Rank\n1 autoaug_pyramid_net_tf 98.4 [98.1, 98.6] 95.5 [94.5, 96.4] 2.9 1 0\n6 shake_shake_64d_cutout 97.1 [96.8, 97.4] 93.0 [91.8, 94.1] 4.1 5 1\n16 wide_resnet_28_10 95.9 [95.5, 96.3] 89.7 [88.3, 91.0] 6.2 14 2\n23 resnet_basic_110 93.5 [93.0, 93.9] 85.2 [83.5, 86.7] 8.3 24 -1\n27 vgg_15_BN_64 93.0 [92.5, 93.5] 84.9 [83.2, 86.4] 8.1 27 0\n30 cudaconvnet 88.5 [87.9, 89.2] 77.5 [75.7, 79.3] 11.0 30 0\n31 random_features_256k_aug 85.6 [84.9, 86.3] 73.1 [71.1, 75.1] 12.5 31 0\nImageNet Top-1\nOrig. New\nRank Model Orig. Accuracy New Accuracy Gap Rank ∆ Rank\n1 pnasnet_large_tf 82.9 [82.5, 83.2] 72.2 [71.3, 73.1] 10.7 3 -2\n4 nasnetalarge 82.5 [82.2, 82.8] 72.2 [71.3, 73.1] 10.3 1 3\n21 resnet152 78.3 [77.9, 78.7] 67.0 [66.1, 67.9] 11.3 21 0\n23 inception_v3_tf 78.0 [77.6, 78.3] 66.1 [65.1, 67.0] 11.9 24 -1\n30 densenet161 77.1 [76.8, 77.5] 65.3 [64.4, 66.2] 11.8 30 0\n43 vgg19_bn 74.2 [73.8, 74.6] 61.9 [60.9, 62.8] 12.3 44 -1\n64 alexnet 56.5 [56.1, 57.0] 44.0 [43.0, 45.0] 12.5 64 0\n65 fv_64k 35.1 [34.7, 35.5] 24.1 [23.2, 24.9] 11.0 65 0\nTable 1. Model accuracies on the original CIFAR-10 test set, the original ImageNet validation set, and our new test sets. ∆ Rank is the\nrelative difference in the ranking from the original test set to the new test set in the full ordering of all models (see Appendices C.3.3 and\nD.4.4). For example, ∆Rank = −2 means that a model dropped by two places on the new test set compared to the original test set. The\nconﬁdence intervals are 95% Clopper-Pearson intervals. Due to space constraints, references for the models can be found in Appendices\nC.3.2 and D.4.3.\ntwo main trends and discuss the results further in Section 5.\nA Signiﬁcant Drop in Accuracy. All models see a large\ndrop in accuracy from the original test sets to our new test\nsets. For widely used architectures such as VGG ( Simonyan\nand Zisserman , 2014) and ResNet ( He et al. , 2016a), the\ndrop is 8% on CIFAR-10 and 11% on ImageNet. On CIFAR-\n10, the state of the art ( Cubuk et al. , 2018) is more robust\nand only drops by 3% from 98.4% to 95.5%. In contrast,\nthe best model on ImageNet ( Liu et al., 2018) sees an 11%\ndrop from 83% to 72% in top-1 accuracy and a 6% drop\nfrom 96% to 90% in top-5 accuracy. So the top-1 drop on\nImageNet is larger than what we observed on CIFAR-10.\nTo put these accuracy numbers into perspective, we note that\nthe best model in the ILSVRC 6 2013 competition achieved\n89% top-5 accuracy, and the best model from ILSVRC\n2014 achieved 93% top-5 accuracy. So the 6% drop in\ntop-5 accuracy from the 2018 state-of-the-art corresponds\nto approximately ﬁve years of progress in a very active\nperiod of machine learning research.\n6ILSVRC is the ImageNet Large Scale Visual Recognition\nChallenge (Russakovsky et al., 2015).\nFew Changes in the Relative Order . When sorting the\nmodels in order of their original and new accuracy, there\nare few changes in the respective rankings. Models with\ncomparable original accuracy tend to see a similar decrease\nin performance. In fact, Figure 1 shows that the original ac-\ncuracy is highly predictive of the new accuracy and that the\nrelationship can be summarized well with a linear function.\nOn CIFAR-10, the new accuracy of a model is approxi-\nmately given by the following formula:\naccnew =1 .69 · accorig \u0000 72.7% .\nOn ImageNet, the top-1 accuracy of a model is given by\naccnew =1 .11 · accorig \u0000 20.2% .\nComputing a 95% conﬁdence interval from 100,000\nbootstrap samples gives\n[1.63, 1.76] for the slope and\n[\u0000 78.6, \u0000 67.5] for the offset on CIFAR-10, and [1.07, 1.19]\nand [\u0000 26.0, \u0000 17.8] respectively for ImageNet.\nOn both datasets, the slope of the linear ﬁt is greater than 1.\nSo models with higher original accuracy see a smaller drop\non the new test sets. In other words, model robustness\nimproves with increasing accuracy. This effect is less pro-\nnounced on ImageNet (slope 1.1) than on CIFAR-10 (slope\nDo ImageNet Classiﬁers Generalize to ImageNet?\n1.7). In contrast to a scenario with strong adaptive overﬁt-\nting, neither dataset sees diminishing returns in accuracy\nscores when going from the original to the new test sets.\n3.4. Experiments to Test Follow-Up Hypotheses\nSince the drop from original to new accuracies is concern-\ningly large, we investigated multiple hypotheses for explain-\ning this drop. Appendices\nC.2 and D.3 list a range of follow-\nup experiments we conducted, e.g., re-tuning hyperparam-\neters, training on part of our new test set, or performing\ncross-validation. However, none of these effects can explain\nthe size of the drop. We conjecture that the accuracy drops\nstem from small variations in the human annotation process.\nAs we will see in the next section, the resulting changes in\nthe test sets can signiﬁcantly affect model accuracies.\n4. Understanding the Impact of Data\nCleaning on ImageNet\nA crucial aspect of ImageNet is the use of MTurk. There\nis a broad range of design choices for the MTurk tasks and\nhow the resulting annotations determine the ﬁnal dataset.\nTo better understand the impact of these design choices,\nwe assembled three different test sets for ImageNet. All\nof these test sets consist of images from the same Flickr\ncandidate pool, are correctly labeled, and selected by more\nthan 70% of the MTurk workers on average. Nevertheless,\nthe resulting model accuracies vary by 14%. To put these\nnumbers in context, we ﬁrst describe our MTurk annotation\npipeline.\nMTurk Tasks.\nWe designed our MTurk tasks and user\ninterface to closely resemble those originally used for Im-\nageNet. As in ImageNet, each MTurk task contained a\ngrid of 48 candidate images for a given target class. The\ntask description was derived from the original ImageNet\ninstructions and included the deﬁnition of the target class\nwith a link to a corresponding Wikipedia page. We asked\nthe MTurk workers to select images belonging to the target\nclass regardless of “occlusions, other objects, and clutter or\ntext in the scene” and to avoid drawings or paintings (both\nas in ImageNet). Appendix D.4.1 shows a screenshot of our\nUI and a screenshot of the original UI for comparison.\nFor quality control, we embedded at least six randomly\nselected images from the original validation set in each\nMTurk task (three from the same class, three from a class\nthat is nearby in the WordNet hierarchy). These images\nappeared in random locations of the image grid for each\ntask. In total, we collected sufﬁcient MTurk annotations\nso that we have at least 20 annotated validation images for\neach class.\nThe main outcome of the MTurk tasks is a selection fre-\nquency for each image, i.e., what fraction of MTurk workers\nselected the image in a task for its target class. We recruited\nat least ten MTurk workers for each task (and hence for\neach image), which is similar to ImageNet. Since each task\ncontained original validation images, we could also estimate\nhow often images from the original dataset were selected by\nour MTurk workers.\nSampling Strategies.\nIn order to understand how the\nMTurk selection frequency affects the model accuracies,\nwe explored three sampling strategies.\n• MatchedFrequency: First, we estimated the selec-\ntion frequency distribution for each class from the anno-\ntated original validation images. We then sampled ten\nimages from our candidate pool for each class accord-\ning to these class-speciﬁc distributions (see Appendix\nD.1.2 for details).\n• Threshold0.7: For each class, we sampled ten im-\nages with selection frequency at least 0.7.\n• TopImages: For each class, we chose the ten images\nwith highest selection frequency.\nIn order to minimize labeling errors, we manually reviewed\neach dataset and removed incorrect images. The average\nselection frequencies of the three ﬁnal datasets range from\n0.93 for T opImagesover 0.85 for Threshold0.7 to 0.73\nfor MatchedFrequency. For comparison, the original val-\nidation set has an average selection frequency of 0.71 in\nour experiments. Hence all three of our new test sets have\nhigher selection frequencies than the original ImageNet val-\nidation set. In the preceding sections, we presented results\non MatchedFrequency for ImageNet since it is closest to\nthe validation set in terms of selection frequencies.\nResults.\nTable 2 shows that the MTurk selection fre-\nquency has signiﬁcant impact on both top-1 and top-5 ac-\ncuracy. In particular, T opImageshas the highest average\nMTurk selection frequency and sees a small increase of\nabout 2% in both average top-1 and top-5 accuracy com-\npared to the original validation set. This is in stark contrast\nto MatchedFrequency, which has the lowest average se-\nlection frequency and exhibits a signiﬁcant drop of 12%\nand 8%, respectively. The Threshold0.7 dataset is in the\nmiddle and sees a small decrease of 3% in top-1 and 1% in\ntop-5 accuracy.\nIn total, going from T opImagesto MatchedFrequency\ndecreases the accuracies by about 14% (top-1) and 10%\n(top-5). For comparison, note that after excluding AlexNet\n(and the SqueezeNet models tuned to match AlexNet ( Ian-\ndola et al. , 2016)), the range of accuracies spanned by all\nremaining convolutional networks is roughly 14% (top-1)\nand 8% (top-5). So the variation in accuracy caused by\nthe three sampling strategies is larger than the variation in\naccuracy among all post-AlexNet models we tested.\nFigure 2 plots the new vs. original top-1 accuracies on\nThreshold0.7 and T opImages, similar to Figure 1 for\nMatchedFrequency before. For easy comparison of top-1\nDo ImageNet Classiﬁers Generalize to ImageNet?\nSampling Strategy A verage MTurk\nSelection Freq.\nA verage Top-1 Accuracy\nChange\nA verage Top-5 Accuracy\nChange\nMatchedFrequency 0.73 -11.8% -8.2%\nThreshold0.7 0.85 -3.2% -1.2%\nT opImages 0.93 +2.1% +1.8%\nTable 2. Impact of the three sampling strategies for our ImageNet test sets. The table shows the average MTurk selection frequency in the\nresulting datasets and the average changes in model accuracy compared to the original validation set. We refer the reader to Section 4 for\na description of the three sampling strategies. All three test sets have an average selection frequency of more than 0.7, yet the model\naccuracies still vary widely. For comparison, the original ImageNet validation set has an average selection frequency of 0.71 in our MTurk\nexperiments. The changes in average accuracy span 14% and 10% in top-1 and top-5, respectively. This shows that details of the sampling\nstrategy have large inﬂuence on the resulting accuracies.\nand top-5 accuracy plots on all three datasets, we refer the\nreader to Figure 1 in Appendix D.4.4. All three plots show\na good linear ﬁt.\n5. Discussion\nDue to space constraints, we defer a discussion of related\nwork to Appendix A. Furthermore, Appendix B contains\na theoretical model for the accurate linear ﬁt observed in\nFigures 1 and 2. Here, we return to the main question from\nSection 2: What causes the accuracy drops? As before, we\ndistinguish between two possible mechanisms.\n5.1. Adaptivity Gap\nIn its prototypical form, adaptive overﬁtting would manifest\nitself in diminishing returns observed on the new test set\n(see Section 2.1). However, we do not observe this pattern\non either CIFAR-10 or ImageNet. On both datasets, the\nslope of the linear ﬁt is greater than 1, i.e., each point of\naccuracy improvement on the original test set translates to\nmore than 1% on the new test set. This is the opposite of the\nstandard overﬁtting scenario. So at least on CIFAR-10 and\nImageNet, multiple years of competitive test set adaptivity\ndid not lead to diminishing accuracy numbers.\nWhile our experiments rule out the most dangerous form of\nadaptive overﬁtting, we remark that they do not exclude all\nvariants. For instance, it could be that any test set adaptiv-\nity leads to a roughly constant drop in accuracy. Then all\nmodels are affected equally and we would see no diminish-\ning returns since later models could still be better. Testing\nfor this form of adaptive overﬁtting likely requires a new\ntest set that is truly i.i.d. and not the result of a separate\ndata collection effort. Finding a suitable dataset for such an\nexperiment is an interesting direction for future research.\nThe lack of adaptive overﬁtting contradicts conventional\nwisdom in machine learning. We now describe two mecha-\nnisms that could have prevented adaptive overﬁtting:\nThe Ladder Mechanism.\nBlum and Hardt introduced the\nLadder algorithm to protect machine learning competitions\nagainst adaptive overﬁtting ( Blum and Hardt , 2015). The\ncore idea is that constrained interaction with the test set can\nallow a large number of model evaluations to succeed, even\nif the models are chosen adaptively. Due to the natural form\nof their algorithm, the authors point out that it can also be\nseen as a mechanism that the machine learning community\nimplicitly follows.\nLimited Model Class. Adaptivity is only a problem if we\ncan choose among models for which the test set accuracy dif-\nfers signiﬁcantly from the population accuracy. Importantly,\nthis argument does not rely on the number of all possible\nmodels (e.g., all parameter settings of a neural network), but\nonly on those models that could actually be evaluated on the\ntest set. For instance, the standard deep learning workﬂow\nonly produces models trained with SGD-style algorithms\non a ﬁxed training set, and requires that the models achieve\nhigh training accuracy (otherwise we would not consider\nthe corresponding hyperparameters). Hence the number of\ndifferent models arising from the current methodology may\nbe small enough so that uniform convergence holds.\nOur experiments offer little evidence for favoring one ex-\nplanation over the other. One observation is that the convo-\nlutional networks shared many errors on CIFAR-10, which\ncould be an indicator that the models are rather similar. But\nto gain a deeper understanding into adaptive overﬁtting, it is\nlikely necessary to gather further data from more machine\nlearning benchmarks, especially in scenarios where adaptive\noverﬁtting does occur naturally.\n5.2. Distribution Gap\nThe lack of diminishing returns in our experiments points\ntowards the distribution gap as the primary reason for the ac-\ncuracy drops. Moreover, our results on ImageNet show that\nchanges in the sampling strategy can indeed affect model\naccuracies by a large amount, even if the data source and\nother parts of the dataset creation process stay the same.\nSo in spite of our efforts to match the original dataset cre-\nation process, the distribution gap is still our leading hy-\npothesis for the accuracy drops. This demonstrates that it\nDo ImageNet Classiﬁers Generalize to ImageNet?\nFigure 2. Model accuracy on the original ImageNet validation set vs. accuracy on two variants of our new test set. We refer the reader to\nSection 4 for a description of these test sets. Each data point corresponds to one model in our testbed (shown with 95% Clopper-Pearson\nconﬁdence intervals). On Threshold0.7, the model accuracies are 3% lower than on the original test set. On T opImages, which contains\nthe images most frequently selected by MTurk workers, the models perform 2% better than on the original test set. The accuracies on\nboth datasets closely follow a linear function, similar to MatchedFrequency in Figure 1. The red shaded region is a 95% conﬁdence\nregion for the linear ﬁt from 100,000 bootstrap samples.\nis surprisingly hard to accurately replicate the distribution\nof current image classiﬁcation datasets. The main difﬁculty\nlikely is the subjective nature of the human annotation step.\nThere are many parameters that can affect the quality of\nhuman labels such as the annotator population (MTurk vs.\nstudents, qualiﬁcations, location & time, etc.), the exact task\nformat, and compensation. Moreover, there are no exact def-\ninitions for many classes in ImageNet (e.g., see Appendix\nD.4.8). Understanding these aspects in more detail is an im-\nportant direction for designing future datasets that contain\nchallenging images while still being labeled correctly.\nThe difﬁculty of clearly deﬁning the data distribution, com-\nbined with the brittle behavior of the tested models, calls\ninto question whether the black-box and i.i.d. framework\nof learning can produce reliable classiﬁers. Our analysis of\nselection frequencies in Figure 15 (Appendix D.4.7) shows\nthat we could create a new test set with even lower model ac-\ncuracies. The images in this hypothetical dataset would still\nbe correct, from Flickr, and selected by more than half of\nthe MTurk labelers on average. So in spite of the impressive\naccuracy scores on the original validation set, current Ima-\ngeNet models still have difﬁculty generalizing from “easy”\nto “hard” images.\n6. Conclusion & Future Work\nThe expansive growth of machine learning rests on the aspi-\nration to deploy trained systems in a variety of challenging\nenvironments. Common examples include autonomous ve-\nhicles, content moderation, and medicine. In order to use\nmachine learning in these areas responsibly, it is important\nthat we can both train models with sufﬁcient generalization\nabilities, and also reliably measure their performance. As\nour results show, these goals still pose signiﬁcant hurdles\neven in a benign environment.\nOur experiments are only a ﬁrst step in addressing this relia-\nbility challenge. One important question is whether other\nmachine learning tasks are also resilient to adaptive overﬁt-\nting, but similarly brittle under natural variations in the data.\nAnother direction is developing methods for more compre-\nhensive yet still realistic evaluations of machine learning\nsystems. Of course, the overarching goal is to develop learn-\ning algorithms that generalize reliably. While this is often\na vague goal, our new test sets offer a well-deﬁned instan-\ntiation of this challenge that is beyond the reach of current\nmethods. Generalizing from our “easy” to slightly “harder”\nimages will hopefully serve as a starting point towards a\nfuture generation of more reliable models.\nAcknowledgements\nWe would like to thank Tudor Achim, Alex Berg, Orianna\nDeMasi, Jia Deng, Alexei Efros, David Fouhey, Moritz\nHardt, Piotr Indyk, Esther Rolf, and Olga Russakovsky for\nhelpful discussions while working on this paper. Moritz\nHardt has been particularly helpful in all stages of this\nproject and – among other invaluable advice – suggested\nthe title of this paper and a precursor to the data model in\nSection\nB. We also thank the participants of our human\naccuracy experiment in Appendix C.2.5 (whose names we\nkeep anonymous following our IRB protocol).\nThis research was generously supported in part by\nONR awards N00014-17-1-2191, N00014-17-1-2401,\nand N00014-18-1-2833, the DARPA Assured Autonomy\n(FA8750-18-C-0101) and Lagrange (W911NF-16-1-0552)\nprograms, an Amazon AWS AI Research Award, and a gift\nfrom Microsoft Research. In addition, LS was supported by\na Google PhD fellowship and a Microsoft Research Fellow-\nship at the Simons Institute for the Theory of Computing.\nDo ImageNet Classiﬁers Generalize to ImageNet?\nReferences\nAlex Berg. Personal communication, 2018.\nBattista Biggio and Fabio Roli. Wild patterns: Ten years\nafter the rise of adversarial machine learning. Pattern\nRecognition, 2018. https://arxiv.org/abs/17\n12.03141.\nAvrim Blum and Moritz Hardt. The Ladder: A reliable\nleaderboard for machine learning competitions. In Inter-\nnational Conference on Machine Learning (ICML) , 2015.\nhttp://arxiv.org/abs/1502.04585.\nY unpeng Chen, Jianan Li, Huaxin Xiao, Xiaojie Jin,\nShuicheng Y an, and Jiashi Feng. Dual path networks.\nIn Neural Information Processing Systems (NIPS) , 2017.\nhttps://arxiv.org/abs/1707.01629.\nFrançois Chollet. Xception: Deep learning with depth-\nwise separable convolutions. In Conference on Com-\nputer Vision and Pattern Recognition (CVPR) , 2017.\nhttps://arxiv.org/abs/1610.02357.\nStephane Clinchant, Gabriela Csurka, Florent Perronnin,\nand Jean-Michel Renders. XRCE’s participation to Im-\nagEval.\nhttp://citeseerx.ist.psu.edu/vi\newdoc/download?doi=10.1.1.102.6670&r\nep=rep1&type=pdf\n, 2007.\nAdam Coates, Andrew Ng, and Honglak Lee. An analysis\nof single-layer networks in unsupervised feature learning.\nIn Conference on Artiﬁcial Intelligence and Statistics\n(AISTATS), 2011. http://proceedings.mlr.pr\ness/v15/coates11a.html.\nEkin D. Cubuk, Barret Zoph, Dandelion Mane, Vijay V a-\nsudevan, and Quoc V . Le. AutoAugment: Learning aug-\nmentation policies from data. https://arxiv.org/\nabs/1805.09501, 2018.\nJia Deng. Large Scale Visual Recognition . PhD thesis,\nPrinceton University, 2012. ftp://ftp.cs.princ\neton.edu/techreports/2012/923.pdf.\nJia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and\nLi Fei-Fei. ImageNet: A large-scale hierarchical image\ndatabase. In Conference on Computer Vision and Pattern\nRecognition (CVPR), 2009. http://www.image-ne\nt.org/papers/imagenet_cvpr09.pdf.\nTerrance DeVries and Graham W Taylor. Improved regu-\nlarization of convolutional neural networks with Cutout.\nhttps://arxiv.org/abs/1708.04552, 2017.\nLogan Engstrom, Dimitris Tsipras, Ludwig Schmidt, and\nAleksander Madry. A rotation and a translation sufﬁce:\nFooling CNNs with simple transformations. http://\narxiv.org/abs/1712.02779, 2017.\nMark Everingham, Luc Gool, Christopher K. Williams, John\nWinn, and Andrew Zisserman. The Pascal Visual Object\nClasses (VOC) challenge. International Journal of Com-\nputer Vision, 2010. http://dx.doi.org/10.10\n07/s11263-009-0275-4 .\nAlhussein Fawzi and Pascal Frossard. Manitest: Are clas-\nsiﬁers really invariant? In British Machine Vision Con-\nference (BMVC), 2015. https://arxiv.org/abs/\n1507.06535.\nLi Fei-Fei, Rob Fergus, and Pietro Perona. Learning gen-\nerative visual models from few training examples: An\nincremental Bayesian approach tested on 101 object cate-\ngories. Computer Vision and Image Understanding, 2007.\nhttp://dx.doi.org/10.1016/j.cviu.2005\n.09.012\n.\nXavier Gastaldi. Shake-shake regularization. https:\n//arxiv.org/abs/1705.07485, 2017.\nRobert Geirhos, Carlos R. M. Temme, Jonas Rauber,\nHeiko H. Schütt, Matthias Bethge, and Felix A. Wich-\nmann. Generalisation in humans and deep neural net-\nworks. In Advances in Neural Information Processing\nSystems (NeurIPS) . 2018.\nhttp://papers.nips.\ncc/paper/7982-generalisation-in-huma\nns-and-deep-neural-networks.pdf\n.\nBen Hamner. Popular datasets over time. https://www.\nkaggle.com/benhamner/popular-dataset\ns-over-time/data\n, 2017.\nDongyoon Han, Jiwhan Kim, and Junmo Kim. Deep\npyramidal residual networks. In Conference on Com-\nputer Vision and Pattern Recognition (CVPR) , 2017.\nhttps://arxiv.org/abs/1610.02915.\nKaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian\nSun. Delving deep into rectiﬁers: Surpassing human-\nlevel performance on imagenet classiﬁcation. In Inter-\nnational Conference on Computer Vision (ICCV) , 2015.\nhttps://arxiv.org/abs/1502.01852.\nKaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun.\nDeep residual learning for image recognition. In Con-\nference on Computer Vision and Pattern Recognition\n(CVPR), 2016a. https://arxiv.org/abs/15\n12.03385.\nKaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun.\nIdentity mappings in deep residual networks. In European\nConference on Computer Vision (ECCV), 2016b. https:\n//arxiv.org/abs/1603.05027.\nDan Hendrycks and Thomas Dietterich. Benchmarking\nneural network robustness to common corruptions and\nperturbations. In International Conference on Learning\nDo ImageNet Classiﬁers Generalize to ImageNet?\nRepresentations (ICLR), 2019. https://arxiv.or\ng/abs/1807.01697.\nHossein Hosseini and Radha Poovendran. Semantic ad-\nversarial examples. In Conference on Computer Vi-\nsion and Pattern Recognition (CVPR) Workshops, 2018.\nhttps://arxiv.org/abs/1804.00499.\nAndrew G Howard, Menglong Zhu, Bo Chen, Dmitry\nKalenichenko, Weijun Wang, Tobias Weyand, Marco\nAndreetto, and Hartwig Adam. Mobilenets: Efﬁcient\nconvolutional neural networks for mobile vision applica-\ntions. https://arxiv.org/abs/1704.04861 ,\n2017.\nJie Hu, Li Shen, and Gang Sun. Squeeze-and-excitation\nnetworks. In Conference on Computer Vision and Pattern\nRecognition (CVPR), 2018. https://arxiv.org/\nabs/1709.01507.\nGao Huang, Zhuang Liu, Kilian Q Weinberger, and Laurens\nvan der Maaten. Densely connected convolutional net-\nworks. In Conference on Computer Vision and Pattern\nRecognition (CVPR), 2017. https://arxiv.org/\nabs/1608.06993.\nForrest N. Iandola, Song Han, Matthew W. Moskewicz,\nKhalid Ashraf, William J. Dally, and Kurt Keutzer.\nSqueezeNet: AlexNet-level accuracy with 50x fewer pa-\nrameters and <0.5MB model size. https://arxiv.\norg/abs/1602.07360, 2016.\nSergey Ioffe and Christian Szegedy. Batch normalization:\nAccelerating deep network training by reducing internal\ncovariate shift. In International Conference on Machine\nLearning (ICML), 2015. https://arxiv.org/ab\ns/1502.03167.\nCan Kanbak, Seyed-Mohsen Moosavi-Dezfooli, and Pas-\ncal Frossard. Geometric robustness of deep networks:\nAnalysis and improvement. In Conference on Com-\nputer Vision and Pattern Recognition (CVPR) , 2018.\nhttps://arxiv.org/abs/1711.09115.\nAndrej Karpathy. Lessons learned from manually classify-\ning CIFAR-10. http://karpathy.github.io/2\n011/04/27/manually-classifying-cifar\n10/, 2011.\nKenji Kawaguchi, Leslie Pack Kaelbling, and Y oshua Ben-\ngio. Generalization in deep learning. https://arxi\nv.org/abs/1710.05468, 2017.\nSimon Kornblith, Jonathon Shlens, and Quoc V . Le. Do\nbetter ImageNet models transfer better? http://ar\nxiv.org/abs/1805.08974, 2018.\nIvan Krasin, Tom Duerig, Neil Alldrin, Vittorio Ferrari,\nSami Abu-El-Haija, Alina Kuznetsova, Hassan Rom,\nJasper Uijlings, Stefan Popov, Shahab Kamali, Matteo\nMalloci, Jordi Pont-Tuset, Andreas V eit, Serge Belongie,\nVictor Gomes, Abhinav Gupta, Chen Sun, Gal Chechik,\nDavid Cai, Zheyun Feng, Dhyanesh Narayanan, and\nKevin Murphy. Openimages: A public dataset for large-\nscale multi-label and multi-class image classiﬁcation.\nhttps://storage.googleapis.com/openi\nmages/web/index.html\n, 2017.\nAlex Krizhevsky. Learning multiple layers of features from\ntiny images. https://www.cs.toronto.edu\n/~kriz/learning-features-2009-TR.pdf ,\n2009.\nAlex Krizhevsky, Ilya Sutskever, and Geoffrey E Hinton.\nImagenet classiﬁcation with deep convolutional neural\nnetworks. In Advances in Neural Information Processing\nSystems (NIPS), 2012. https://papers.nips.cc\n/paper/4824-imagenet-classification-\nwith-deep-convolutional-neural-netwo\nrks.\nFei-Fei Li and Jia Deng. ImageNet: Where have we been?\nwhere are we going? http://image-net.org/ch\nallenges/talks_2017/imagenet_ilsvrc2\n017_v1.0.pdf\n, 2017.\nTsung-Yi Lin, Michael Maire, Serge J. Belongie,\nLubomir D. Bourdev, Ross B. Girshick, James Hays,\nPietro Perona, Deva Ramanan, Piotr Dollár, and\nC. Lawrence Zitnick. Microsoft COCO: Common objects\nin context. In European Conference on Computer Vision\n(ECCV), 2014. https://arxiv.org/abs/1405\n.0312.\nChenxi Liu, Barret Zoph, Maxim Neumann, Jonathon\nShlens, Wei Hua, Li-Jia Li, Li Fei-Fei, Alan Y uille,\nJonathan Huang, and Kevin Murphy. Progressive neural\narchitecture search. In European Conference on Com-\nputer Vision (ECCV), 2018. https://arxiv.org/\nabs/1712.00559.\nShuying Liu and Weihong Deng. V ery deep convolutional\nneural network based image classiﬁcation using small\ntraining sample size. In Asian Conference on Pattern\nRecognition (ACPR), 2015.\nhttps://ieeexplore\n.ieee.org/document/7486599/.\nAleksander Madry, Aleksandar Makelov, Ludwig Schmidt,\nDimitris Tsipras, and Adrian Vladu. Towards deep learn-\ning models resistant to adversarial attacks. In Interna-\ntional Conference on Learning Representations (ICLR) ,\n2018. https://arxiv.org/abs/1706.06083.\nJitendra Malik. Technical perspective: What led computer\nvision to deep learning? Communications of the ACM ,\n2017. http://doi.acm.org/10.1145/306538\n4.\nDo ImageNet Classiﬁers Generalize to ImageNet?\nGeorge A. Miller. Wordnet: A lexical database for english.\nCommunications of the ACM , 1995. URL http://do\ni.acm.org/10.1145/219717.219748.\nFlorent Perronnin, Jorge Sánchez, and Thomas Mensink.\nImproving the Fisher kernel for large-scale image clas-\nsiﬁcation. In European Conference on Computer Vision\n(ECCV), 2010. URL https://www.robots.ox.\nac.uk/~vgg/rg/papers/peronnin_etal_E\nCCV10.pdf\n.\nJean Ponce, Tamara L. Berg, Mark Everingham, David A.\nForsyth, Martial Hebert, Sveltana Lazebnik, Marcin\nMarszalek, Cordelia Schmid, Bryan C. Russell, Antio-\nnio Torralba, Chris. K. I. Williams, Jianguo Zhang, and\nAndrew Zisserman. Dataset issues in object recognition .\n2006. https://link.springer.com/chapte\nr/10.1007/11957959_2.\nAli Rahimi and Benjamin Recht. Weighted Sums of Ran-\ndom Kitchen Sinks: Replacing minimization with ran-\ndomization in learning. In Advances in Neural Infor-\nmation Processing Systems (NIPS) , 2009.\nhttps:\n//papers.nips.cc/paper/3495-weighted\n-sums-of-random-kitchen-sinks-replac\ning-minimization-with-randomization-\nin-learning\n.\nEsteban Real, Alok Aggarwal, Y anping Huang, and Quoc V .\nLe. Regularized evolution for image classiﬁer architecture\nsearch. http://arxiv.org/abs/1802.01548 ,\n2018.\nOlga Russakovsky, Jia Deng, Hao Su, Jonathan Krause, San-\njeev Satheesh, Sean Ma, Zhiheng Huang, Andrej Karpa-\nthy, Aditya Khosla, Michael Bernstein, Alexander C.\nBerg, and Fei-Fei Li. ImageNet large scale visual recogni-\ntion challenge. International Journal of Computer Vision,\n2015. https://arxiv.org/abs/1409.0575.\nKaren Simonyan and Andrew Zisserman. V ery deep con-\nvolutional networks for large-scale image recognition.\nhttps://arxiv.org/abs/1409.1556, 2014.\nChristian Szegedy, Wojciech Zaremba, Ilya Sutskever, Joan\nBruna, Dumitru Erhan, Ian J. Goodfellow, and Rob Fer-\ngus. Intriguing properties of neural networks. In Interna-\ntional Conference on Learning Representations (ICLR) ,\n2013. http://arxiv.org/abs/1312.6199.\nChristian Szegedy, Wei Liu, Y angqing Jia, Pierre Sermanet,\nScott Reed, Dragomir Anguelov, Dumitru Erhan, Vincent\nV anhoucke, and Andrew Rabinovich. Going deeper with\nconvolutions. In Conference on Computer Vision and\nPattern Recognition (CVPR), 2015. https://arxiv.\norg/abs/1409.4842v1.\nChristian Szegedy, Vincent V anhoucke, Sergey Ioffe, Jon\nShlens, and Zbigniew Wojna. Rethinking the Incep-\ntion architecture for computer vision. In Conference on\nComputer Vision and Pattern Recognition (CVPR), 2016.\nhttps://arxiv.org/abs/1512.00567.\nChristian Szegedy, Sergey Ioffe, Vincent V anhoucke, and\nAlexander A. Alemi. Inception-v4, Inception-Resnet\nand the impact of residual connections on learning. In\nConference On Artiﬁcial Intelligence (AAAI) , 2017. ht\ntps://arxiv.org/abs/1602.07261.\nAntonio Torralba and Alexei A. Efros. Unbiased look at\ndataset bias. In Conference on Computer Vision and\nPattern Recognition (CVPR), 2011. http://people\n.csail.mit.edu/torralba/publications\n/datasets_cvpr11.pdf.\nAntonio Torralba, Rob Fergus, and William. T. Freeman.\n80 Million Tiny Images: A Large Data Set for Nonpara-\nmetric Object and Scene Recognition. IEEE Transac-\ntions on Pattern Analysis and Machine Intelligence , 2008.\nhttps://ieeexplore.ieee.org/document\n/4531741/.\nZhou Wang, Alan C. Bovik, Hamid R. Sheikh, and Eero P .\nSimoncelli. Image quality assessment: from error visibil-\nity to structural similarity. IEEE Transactions on Image\nProcessing, 2004. http://www.cns.nyu.edu/pu\nb/lcv/wang03-preprint.pdf.\nChaowei Xiao, Jun-Y an Zhu, Bo Li, Warren He, Mingyan\nLiu, and Dawn Song. Spatially transformed adversarial\nexamples. In International Conference on Learning Rep-\nresentations (ICLR), 2018. https://arxiv.org/\nabs/1801.02612.\nSaining Xie, Ross Girshick, Piotr Dollár, Zhuowen Tu, and\nKaiming He. Aggregated residual transformations for\ndeep neural networks. In Conference on Computer Vision\nand Pattern Recognition (CVPR), 2017. https://ar\nxiv.org/abs/1611.05431.\nY oshihiro Y amada, Masakazu Iwamura, and Koichi Kise.\nShakedrop regularization. https://arxiv.org/ab\ns/1802.02375, 2018.\nBenjamin Z. Y ao, Xiong Y ang, and Song-Chun Zhu. In-\ntroduction to a large-scale general purpose ground truth\ndatabase: methodology, annotation tool and benchmarks.\nIn Energy Minimization Methods in Computer Vision\nand Pattern Recognition (EMMCVPR) , 2007. https:\n//link.springer.com/chapter/10.1007/\n978-3-540-74198-5_14 .\nSergey Zagoruyko and Nikos Komodakis. Wide residual\nnetworks. In British Machine Vision Conference (BMVC),\n2016. https://arxiv.org/abs/1605.07146.\nDo ImageNet Classiﬁers Generalize to ImageNet?\nXingcheng Zhang, Zhizhong Li, Chen Change Loy, and\nDahua Lin. Polynet: A pursuit of structural diversity in\nvery deep networks. In Conference on Computer Vision\nand Pattern Recognition (CVPR), 2017. https://ar\nxiv.org/abs/1611.05725.\nBarret Zoph, Vijay V asudevan, Jonathon Shlens, and Quoc V\nLe. Learning transferable architectures for scalable im-\nage recognition. In Conference on Computer Vision and\nPattern Recognition (CVPR), 2018. https://arxiv.\norg/abs/1707.07012.",
  "values": {
    "Interpretable (to users)": "No",
    "Transparent (to users)": "No",
    "Deferral to humans": "No",
    "User influence": "No",
    "Collective influence": "No",
    "Explicability": "No",
    "Not socially biased": "No",
    "Critiqability": "No",
    "Fairness": "No",
    "Beneficence": "No",
    "Respect for Persons": "No",
    "Justice": "No",
    "Privacy": "No",
    "Non-maleficence": "No",
    "Autonomy (power to decide)": "No",
    "Respect for Law and public interest": "No"
  }
}