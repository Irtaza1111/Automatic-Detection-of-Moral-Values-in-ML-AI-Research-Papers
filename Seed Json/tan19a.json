{
  "pdf": "tan19a",
  "title": "EfficientNet: Rethinking Model Scaling for Convolutional Neural Networks",
  "author": "Mingxing Tan, Quoc V. Le",
  "paper_id": "tan19a",
  "text": "EfﬁcientNet: Rethinking Model Scaling for Convolutional Neural Networks\nMingxing Tan1 Quoc V . Le1\nAbstract\nConvolutional Neural Networks (ConvNets) are\ncommonly developed at a ﬁxed resource budget,\nand then scaled up for better accuracy if more\nresources are available. In this paper, we sys-\ntematically study model scaling and identify that\ncarefully balancing network depth, width, and res-\nolution can lead to better performance. Based\non this observation, we propose a new scaling\nmethod that uniformly scales all dimensions of\ndepth/width/resolution using a simple yet highly\neffective compound coefﬁcient. We demonstrate\nthe effectiveness of this method on scaling up\nMobileNets and ResNet.\nTo go even further, we use neural architecture\nsearch to design a new baseline network and scale\nit up to obtain a family of models, called Efﬁcient-\nNets, which achieve much better accuracy and efﬁ-\nciency than previous ConvNets. In particular, our\nEfﬁcientNet-B7 achieves state-of-the-art 84.4%\ntop-1 / 97.1% top-5 accuracy on ImageNet, while\nbeing 8.4x smaller and 6.1x faster on inference\nthan the best existing ConvNet. Our EfﬁcientNets\nalso transfer well and achieve state-of-the-art ac-\ncuracy on CIFAR-100 (91.7%), Flowers (98.8%),\nand 3 other transfer learning datasets, with an\norder of magnitude fewer parameters.\n1. Introduction\nScaling up ConvNets is widely used to achieve better accu-\nracy. For example, ResNet (He et al., 2016) can be scaled\nup from ResNet-18 to ResNet-200 by using more layers;\nRecently, GPipe (Huang et al., 2018) achieved 84.3% Ima-\ngeNet top-1 accuracy by scaling up a baseline model four\ntime larger. However, the process of scaling up ConvNets\nhas never been well understood and there are currently many\n1Google Research, Brain Team, Mountain View, CA. Corre-\nspondence to: Mingxing Tan<tanmingxing@google.com>.\nProceedings of the 36 th International Conference on Machine\nLearning, Long Beach, California, PMLR 97, 2019. Copyright\n2019 by the author(s).\n0 20 40 60 80 100 120 140 160 180\nNumber of Parameters (Millions)\n74\n76\n78\n80\n82\n84Imagenet Top 1 Accuracy (%)\nResNet-34\nResNet-50\nResNet-152\nDenseNet-201\nInception-v2\nInception-ResNet-v2\nNASNet-A\nNASNet-A\nResNeXt-101\nXception\nAmoebaNet-A AmoebaNet-C\nSENet\nB0\nB3\nB4\nB5\nB6\nEfﬁcientNet-B7\nTop1 Acc. #ParamsResNet-152 (He et al., 2016)77.8% 60MEfﬁcientNet-B1 78.8% 7.8MResNeXt-101 (Xie et al., 2017)80.9% 84MEfﬁcientNet-B3 81.1% 12MSENet (Hu et al., 2018)82.7% 146MNASNet-A (Zoph et al., 2018)82.7% 89MEfﬁcientNet-B4 82.6% 19MGPipe (Huang et al., 2018)† 84.3% 556MEfﬁcientNet-B7 84.4% 66M†Not plotted\nFigure 1. Model Size vs. ImageNet Accuracy. All numbers are\nfor single-crop, single-model. Our EfﬁcientNets signiﬁcantly out-\nperform other ConvNets. In particular, EfﬁcientNet-B7 achieves\nnew state-of-the-art 84.4% top-1 accuracy but being 8.4x smaller\nand 6.1x faster than GPipe. EfﬁcientNet-B1 is 7.6x smaller and\n5.7x faster than ResNet-152. Details are in Table 2 and 4.\nways to do it. The most common way is to scale up Con-\nvNets by their depth (He et al., 2016) or width (Zagoruyko &\nKomodakis, 2016). Another less common, but increasingly\npopular, method is to scale up models by image resolution\n(Huang et al., 2018). In previous work, it is common to scale\nonly one of the three dimensions – depth, width, and image\nsize. Though it is possible to scale two or three dimensions\narbitrarily, arbitrary scaling requires tedious manual tuning\nand still often yields sub-optimal accuracy and efﬁciency.\nIn this paper, we want to study and rethink the process\nof scaling up ConvNets. In particular, we investigate the\ncentral question: is there a principled method to scale up\nConvNets that can achieve better accuracy and efﬁciency?\nOur empirical study shows that it is critical to balance all\ndimensions of network width/depth/resolution, and surpris-\ningly such balance can be achieved by simply scaling each\nof them with constant ratio. Based on this observation, we\npropose a simple yet effective compound scaling method.\nUnlike conventional practice that arbitrary scales these fac-\ntors, our method uniformly scales network width, depth,\nand resolution with a set of ﬁxed scaling coefﬁcients. For\nexample, if we want to use 2N times more computational\nEfﬁcientNet: Rethinking Model Scaling for Convolutional Neural Networks\n(a) baseline (b) width scaling (c) depth scaling (d) resolution scaling (e) compound scaling\n#channels\nlayer_i\nresolution HxW\nwider\ndeeper\nhigher \nresolution higher \nresolution\ndeeper\nwider\nFigure 2. Model Scaling. (a) is a baseline network example; (b)-(d) are conventional scaling that only increases one dimension of network\nwidth, depth, or resolution. (e) is our proposed compound scaling method that uniformly scales all three dimensions with a ﬁxed ratio.\nresources, then we can simply increase the network depth by\nαN , width byβN , and image size byγN , whereα,β,γ are\nconstant coefﬁcients determined by a small grid search on\nthe original small model. Figure 2 illustrates the difference\nbetween our scaling method and conventional methods.\nIntuitively, the compound scaling method makes sense be-\ncause if the input image is bigger, then the network needs\nmore layers to increase the receptive ﬁeld and more channels\nto capture more ﬁne-grained patterns on the bigger image. In\nfact, previous theoretical (Raghu et al., 2017; Lu et al., 2018)\nand empirical results (Zagoruyko & Komodakis, 2016) both\nshow that there exists certain relationship between network\nwidth and depth, but to our best knowledge, we are the\nﬁrst to empirically quantify the relationship among all three\ndimensions of network width, depth, and resolution.\nWe demonstrate that our scaling method work well on exist-\ning MobileNets (Howard et al., 2017; Sandler et al., 2018)\nand ResNet (He et al., 2016). Notably, the effectiveness of\nmodel scaling heavily depends on the baseline network; to\ngo even further, we use neural architecture search (Zoph &\nLe, 2017; Tan et al., 2019) to develop a new baseline net-\nwork, and scale it up to obtain a family of models, calledEfﬁ-\ncientNets. Figure 1 summarizes the ImageNet performance,\nwhere our EfﬁcientNets signiﬁcantly outperform other Con-\nvNets. In particular, our EfﬁcientNet-B7 surpasses the best\nexisting GPipe accuracy (Huang et al., 2018), but using\n8.4x fewer parameters and running 6.1x faster on inference.\nCompared to the widely used ResNet (He et al., 2016), our\nEfﬁcientNet-B4 improves the top-1 accuracy from 76.3%\nof ResNet-50 to 82.6% with similar FLOPS. Besides Ima-\ngeNet, EfﬁcientNets also transfer well and achieve state-of-\nthe-art accuracy on 5 out of 8 widely used datasets, while\nreducing parameters by up to 21x than existing ConvNets.\n2. Related Work\nConvNet Accuracy: Since AlexNet (Krizhevsky et al.,\n2012) won the 2012 ImageNet competition, ConvNets have\nbecome increasingly more accurate by going bigger: while\nthe 2014 ImageNet winner GoogleNet (Szegedy et al., 2015)\nachieves 74.8% top-1 accuracy with about 6.8M parameters,\nthe 2017 ImageNet winner SENet (Hu et al., 2018) achieves\n82.7% top-1 accuracy with 145M parameters. Recently,\nGPipe (Huang et al., 2018) further pushes the state-of-the-art\nImageNet top-1 validation accuracy to 84.3% using 557M\nparameters: it is so big that it can only be trained with a\nspecialized pipeline parallelism library by partitioning the\nnetwork and spreading each part to a different accelera-\ntor. While these models are mainly designed for ImageNet,\nrecent studies have shown better ImageNet models also per-\nform better across a variety of transfer learning datasets\n(Kornblith et al., 2019), and other computer vision tasks\nsuch as object detection (He et al., 2016; Tan et al., 2019).\nAlthough higher accuracy is critical for many applications,\nwe have already hit the hardware memory limit, and thus\nfurther accuracy gain needs better efﬁciency.\nConvNet Efﬁciency: Deep ConvNets are often over-\nparameterized. Model compression (Han et al., 2016; He\net al., 2018; Yang et al., 2018) is a common way to re-\nduce model size by trading accuracy for efﬁciency. As mo-\nbile phones become ubiquitous, it is also common to hand-\ncraft efﬁcient mobile-size ConvNets, such as SqueezeNets\n(Iandola et al., 2016; Gholami et al., 2018), MobileNets\n(Howard et al., 2017; Sandler et al., 2018), and ShufﬂeNets\n(Zhang et al., 2018; Ma et al., 2018). Recently, neural archi-\ntecture search becomes increasingly popular in designing\nefﬁcient mobile-size ConvNets (Tan et al., 2019; Cai et al.,\nEfﬁcientNet: Rethinking Model Scaling for Convolutional Neural Networks\n2019), and achieves even better efﬁciency than hand-crafted\nmobile ConvNets by extensively tuning the network width,\ndepth, convolution kernel types and sizes. However, it is\nunclear how to apply these techniques for larger models that\nhave much larger design space and much more expensive\ntuning cost. In this paper, we aim to study model efﬁciency\nfor super large ConvNets that surpass state-of-the-art accu-\nracy. To achieve this goal, we resort to model scaling.\nModel Scaling: There are many ways to scale a Con-\nvNet for different resource constraints: ResNet (He et al.,\n2016) can be scaled down (e.g., ResNet-18) or up (e.g.,\nResNet-200) by adjusting network depth (#layers), while\nWideResNet (Zagoruyko & Komodakis, 2016) and Mo-\nbileNets (Howard et al., 2017) can be scaled by network\nwidth (#channels). It is also well-recognized that bigger\ninput image size will help accuracy with the overhead of\nmore FLOPS. Although prior studies (Raghu et al., 2017;\nLin & Jegelka, 2018; Sharir & Shashua, 2018; Lu et al.,\n2018) have shown that network deep and width are both\nimportant for ConvNets’ expressive power, it still remains\nan open question of how to effectively scale a ConvNet to\nachieve better efﬁciency and accuracy. Our work systemati-\ncally and empirically studies ConvNet scaling for all three\ndimensions of network width, depth, and resolutions.\n3. Compound Model Scaling\nIn this section, we will formulate the scaling problem, study\ndifferent approaches, and propose our new scaling method.\n3.1. Problem Formulation\nA ConvNet Layer i can be deﬁned as a function: Yi =\nFi(Xi), whereFi is the operator,Yi is output tensor,Xi is\ninput tensor, with tensor shape⟨Hi,Wi,Ci⟩1, whereHi and\nWi are spatial dimension andCi is the channel dimension.\nA ConvNetN can be represented by a list of composed lay-\ners:N =Fk⊙...⊙F 1⊙F 1(X1) =⨀\nj=1...kFj(X1). In\npractice, ConvNet layers are often partitioned into multiple\nstages and all layers in each stage share the same architec-\nture: for example, ResNet (He et al., 2016) has ﬁve stages,\nand all layers in each stage has the same convolutional type\nexcept the ﬁrst layer performs down-sampling. Therefore,\nwe can deﬁne a ConvNet as:\nN =\n⨀\ni=1...s\nFLi\ni\n(\nX⟨Hi,Wi,Ci⟩\n)\n(1)\nwhereFLi\ni denotes layerFi is repeatedLi times in stagei,\n⟨Hi,Wi,Ci⟩ denotes the shape of input tensor X of layer\ni. Figure 2(a) illustrate a representative ConvNet, where\nthe spatial dimension is gradually shrunk but the channel\n1For the sake of simplicity, we omit batch dimension.\ndimension is expanded over layers, for example, from initial\ninput shape⟨224, 224, 3⟩ to ﬁnal output shape⟨7, 7, 512⟩.\nUnlike regular ConvNet designs that mostly focus on ﬁnd-\ning the best layer architectureFi, model scaling tries to ex-\npand the network length (Li), width (Ci), and/or resolution\n(Hi,Wi) without changingFi predeﬁned in the baseline\nnetwork. By ﬁxingFi, model scaling simpliﬁes the design\nproblem for new resource constraints, but it still remains\na large design space to explore differentLi,Ci,Hi,Wi for\neach layer. In order to further reduce the design space, we\nrestrict that all layers must be scaled uniformly with con-\nstant ratio. Our target is to maximize the model accuracy\nfor any given resource constraints, which can be formulated\nas an optimization problem:\nmax\nd,w,r\nAccuracy\n(\nN (d,w,r )\n)\ns.t. N (d,w,r ) =\n⨀\ni=1...s\nˆFd· ˆLi\ni\n(\nX⟨r· ˆHi,r· ˆWi,w· ˆCi⟩\n)\nMemory(N )≤ target memory\nFLOPS(N )≤ target ﬂops\n(2)\nwhere w,d,r are coefﬁcients for scaling network width,\ndepth, and resolution; ˆFi, ˆLi, ˆHi, ˆWi, ˆCi are predeﬁned pa-\nrameters in baseline network (see Table 1 as an example).\n3.2. Scaling Dimensions\nThe main difﬁculty of problem 2 is that the optimald,w,r\ndepend on each other and the values change under different\nresource constraints. Due to this difﬁculty, conventional\nmethods mostly scale ConvNets in one of these dimensions:\nDepth (ddd): Scaling network depth is the most common way\nused by many ConvNets (He et al., 2016; Huang et al., 2017;\nSzegedy et al., 2015; 2016). The intuition is that deeper\nConvNet can capture richer and more complex features, and\ngeneralize well on new tasks. However, deeper networks\nare also more difﬁcult to train due to the vanishing gradient\nproblem (Zagoruyko & Komodakis, 2016). Although sev-\neral techniques, such as skip connections (He et al., 2016)\nand batch normalization (Ioffe & Szegedy, 2015), alleviate\nthe training problem, the accuracy gain of very deep network\ndiminishes: for example, ResNet-1000 has similar accuracy\nas ResNet-101 even though it has much more layers. Figure\n3 (middle) shows our empirical study on scaling a baseline\nmodel with different depth coefﬁcientd, further suggesting\nthe diminishing accuracy return for very deep ConvNets.\nWidth (www): Scaling network width is commonly used for\nsmall size models (Howard et al., 2017; Sandler et al., 2018;\nEfﬁcientNet: Rethinking Model Scaling for Convolutional Neural Networks\n0 2 4 6 8\nFLOPS (Billions)\n75\n76\n77\n78\n79\n80\n81ImageNet Top-1 Accuracy(%)\nw=1.0\nw=1.4\nw=1.8\nw=2.6\nw=3.8\nw=5.0\n0 1 2 3 4\nFLOPS (Billions)\n75\n76\n77\n78\n79\n80\n81\nd=1.0\nd=2.0\nd=3.0d=4.0\nd=6.0 d=8.0\n0 1 2 3\nFLOPS (Billions)\n75\n76\n77\n78\n79\n80\n81\nr=1.0\nr=1.3\nr=1.5\nr=1.7\nr=1.9\nr=2.2 r=2.5\nFigure 3. Scaling Up a Baseline Model with Different Network Width ( w), Depth (d), and Resolution ( r) Coefﬁcients. Bigger\nnetworks with larger width, depth, or resolution tend to achieve higher accuracy, but the accuracy gain quickly saturate after reaching\n80%, demonstrating the limitation of single dimension scaling. Baseline network is described in Table 1.\nTan et al., 2019) 2. As discussed in (Zagoruyko & Ko-\nmodakis, 2016), wider networks tend to be able to capture\nmore ﬁne-grained features and are easier to train. However,\nextremely wide but shallow networks tend to have difﬁcul-\nties in capturing higher level features. Our empirical results\nin Figure 3 (left) show that the accuracy quickly saturates\nwhen networks become much wider with largerw.\nResolution (rrr): With higher resolution input images, Con-\nvNets can potentially capture more ﬁne-grained patterns.\nStarting from 224x224 in early ConvNets, modern Con-\nvNets tend to use 299x299 (Szegedy et al., 2016) or 331x331\n(Zoph et al., 2018) for better accuracy. Recently, GPipe\n(Huang et al., 2018) achieves state-of-the-art ImageNet ac-\ncuracy with 480x480 resolution. Higher resolutions, such as\n600x600, are also widely used in object detection ConvNets\n(He et al., 2017; Lin et al., 2017). Figure 3 (right) shows the\nresults of scaling network resolutions, where indeed higher\nresolutions improve accuracy, but the accuracy gain dimin-\nishes for very high resolutions (r = 1.0 denotes resolution\n224x224 andr = 2.5 denotes resolution 560x560).\nThe above analyses lead us to the ﬁrst observation:\nObservation 1 – Scaling up any dimension of network\nwidth, depth, or resolution improves accuracy, but the accu-\nracy gain diminishes for bigger models.\n3.3. Compound Scaling\nWe empirically observe that different scaling dimensions are\nnot independent. Intuitively, for higher resolution images,\nwe should increase network depth, such that the larger re-\nceptive ﬁelds can help capture similar features that include\nmore pixels in bigger images. Correspondingly, we should\nalso increase network depth when resolution is higher, in\n2In some literature, scaling number of channels is called “depth\nmultiplier”, which means the same as our width coefﬁcientw.\n0 5 10 15 20 25\nFLOPS (billions)\n76\n77\n78\n79\n80\n81\n82ImageNet Top1 Accuracy (%)\nd=1.0, r=1.0\nd=1.0, r=1.3\nd=2.0, r=1.0\nd=2.0, r=1.3\nFigure 4. Scaling Network Width for Different Baseline Net-\nworks. Each dot in a line denotes a model with different width\ncoefﬁcient (w). All baseline networks are from Table 1. The ﬁrst\nbaseline network (d=1.0,r=1.0) has 18 convolutional layers with\nresolution 224x224, while the last baseline (d=2.0,r=1.3) has 36\nlayers with resolution 299x299.\norder to capture more ﬁne-grained patterns with more pixels\nin high resolution images. These intuitions suggest that we\nneed to coordinate and balance different scaling dimensions\nrather than conventional single-dimension scaling.\nTo validate our intuitions, we compare width scaling under\ndifferent network depths and resolutions, as shown in Figure\n4. If we only scale network width w without changing\ndepth (d=1.0) and resolution (r=1.0), the accuracy saturates\nquickly. With deeper (d=2.0) and higher resolution (r=2.0),\nwidth scaling achieves much better accuracy under the same\nFLOPS cost. These results lead us to the second observation:\nObservation 2 – In order to pursue better accuracy and\nefﬁciency, it is critical to balance all dimensions of network\nwidth, depth, and resolution during ConvNet scaling.\nEfﬁcientNet: Rethinking Model Scaling for Convolutional Neural Networks\nIn fact, a few prior work (Zoph et al., 2018; Real et al., 2019)\nhave already tried to arbitrarily balance network width and\ndepth, but they all require tedious manual tuning.\nIn this paper, we propose a newcompound scaling method,\nwhich use a compound coefﬁcient φ to uniformly scales\nnetwork width, depth, and resolution in a principled way:\ndepth:d =αφ\nwidth:w =βφ\nresolution:r =γφ\ns.t.α·β2·γ2≈ 2\nα≥ 1,β≥ 1,γ ≥ 1\n(3)\nwhere α,β,γ are constants that can be determined by a\nsmall grid search. Intuitively, φ is a user-speciﬁed coefﬁ-\ncient that controls how many more resources are available\nfor model scaling, whileα,β,γ specify how to assign these\nextra resources to network width, depth, and resolution re-\nspectively. Notably, the FLOPS of a regular convolution op\nis proportional to d,w2,r2, i.e., doubling network depth\nwill double FLOPS, but doubling network width or resolu-\ntion will increase FLOPS by four times. Since convolution\nops usually dominate the computation cost in ConvNets,\nscaling a ConvNet with equation 3 will approximately in-\ncrease total FLOPS by\n(\nα·β2·γ2)φ\n. In this paper, we\nconstraintα·β2·γ2≈ 2 such that for any newφ, the total\nFLOPS will approximately3 increase by 2φ.\n4. EfﬁcientNet Architecture\nSince model scaling does not change layer operators ˆFi\nin baseline network, having a good baseline network is\nalso critical. We will evaluate our scaling method using\nexisting ConvNets, but in order to better demonstrate the\neffectiveness of our scaling method, we have also developed\na new mobile-size baseline, called EfﬁcientNet.\nInspired by (Tan et al., 2019), we develop our baseline net-\nwork by leveraging a multi-objective neural architecture\nsearch that optimizes both accuracy and FLOPS. Speciﬁ-\ncally, we use the same search space as (Tan et al., 2019),\nand useACC (m)×[FLOPS (m)/T ]w as the optimization\ngoal, where ACC (m) andFLOPS (m) denote the accu-\nracy and FLOPS of model m,T is the target FLOPS and\nw=-0.07 is a hyperparameter for controlling the trade-off\nbetween accuracy and FLOPS. Unlike (Tan et al., 2019;\nCai et al., 2019), here we optimize FLOPS rather than la-\ntency since we are not targeting any speciﬁc hardware de-\nvice. Our search produces an efﬁcient network, which we\nname EfﬁcientNet-B0. Since we use the same search space\nas (Tan et al., 2019), the architecture is similar to Mnas-\n3FLOPS may differ from theocratic value due to rounding.\nTable 1. EfﬁcientNet-B0 baseline network – Each row describes\na stagei with ˆLi layers, with input resolution ⟨ ˆHi, ˆWi⟩ and output\nchannels ˆCi. Notations are adopted from equation 2.\nStage Operator Resolution#Channels#Layers\ni ˆFi ˆHi× ˆWi ˆCi ˆLi\n1 Conv3x3 224×224 32 1\n2 MBConv1, k3x3 112×112 16 1\n3 MBConv6, k3x3 112×112 24 2\n4 MBConv6, k5x5 56×56 40 2\n5 MBConv6, k3x3 28×28 80 3\n6 MBConv6, k5x5 28×28 112 3\n7 MBConv6, k5x5 14×14 192 4\n8 MBConv6, k3x3 7×7 320 1\n9 Conv1x1 & Pooling & FC7×7 1280 1\nNet, except our EfﬁcientNet-B0 is slightly bigger due to\nthe larger FLOPS target (our FLOPS target is 400M). Ta-\nble 1 shows the architecture of EfﬁcientNet-B0. Its main\nbuilding block is mobile inverted bottleneck MBConv (San-\ndler et al., 2018; Tan et al., 2019), to which we also add\nsqueeze-and-excitation optimization (Hu et al., 2018).\nStarting from the baseline EfﬁcientNet-B0, we apply our\ncompound scaling method to scale it up with two steps:\n• STEP 1: we ﬁrst ﬁxφ = 1, assuming twice more re-\nsources available, and do a small grid search ofα,β,γ\nbased on Equation 2 and 3. In particular, we ﬁnd\nthe best values for EfﬁcientNet-B0 areα = 1.2,β =\n1.1,γ = 1.15, under constraint ofα·β2·γ2≈ 2.\n• STEP 2: we then ﬁxα,β,γ as constants and scale up\nbaseline network with differentφ using Equation 3, to\nobtain EfﬁcientNet-B1 to B7 (Details in Table 2).\nNotably, it is possible to achieve even better performance by\nsearching forα,β,γ directly around a large model, but the\nsearch cost becomes prohibitively more expensive on larger\nmodels. Our method solves this issue by only doing search\nonce on the small baseline network (step 1), and then use\nthe same scaling coefﬁcients for all other models (step 2).\n5. Experiments\nIn this section, we will ﬁrst evaluate our scaling method on\nexisting ConvNets and the new proposed EfﬁcientNets.\n5.1. Scaling Up MobileNets and ResNets\nAs a proof of concept, we ﬁrst apply our scaling method\nto the widely-used MobileNets (Howard et al., 2017; San-\ndler et al., 2018) and ResNet (He et al., 2016). Table 3\nshows the ImageNet results of scaling them in different\nways. Compared to other single-dimension scaling methods,\nour compound scaling method improves the accuracy on all\nthese models, suggesting the effectiveness of our proposed\nscaling method for general existing ConvNets.\nEfﬁcientNet: Rethinking Model Scaling for Convolutional Neural Networks\nTable 2. EfﬁcientNet Performance Results on ImageNet (Russakovsky et al., 2015). All EfﬁcientNet models are scaled from our\nbaseline EfﬁcientNet-B0 using different compound coefﬁcientφ in Equation 3. ConvNets with similar top-1/top-5 accuracy are grouped\ntogether for efﬁciency comparison. Our scaled EfﬁcientNet models consistently reduce parameters and FLOPS by an order of magnitude\n(up to 8.4x parameter reduction and up to 16x FLOPS reduction) than existing ConvNets.\nModel Top-1 Acc. Top-5 Acc. #Params Ratio-to-EfﬁcientNet#FLOPS Ratio-to-EfﬁcientNet\nEfﬁcientNet-B0 76.3% 93.2% 5.3M 1x 0.39B 1x\nResNet-50 (He et al., 2016) 76.0% 93.0% 26M 4.9x 4.1B 11x\nDenseNet-169 (Huang et al., 2017) 76.2% 93.2% 14M 2.6x 3.5B 8.9x\nEfﬁcientNet-B1 78.8% 94.4% 7.8M 1x 0.70B 1x\nResNet-152 (He et al., 2016) 77.8% 93.8% 60M 7.6x 11B 16x\nDenseNet-264 (Huang et al., 2017) 77.9% 93.9% 34M 4.3x 6.0B 8.6x\nInception-v3 (Szegedy et al., 2016) 78.8% 94.4% 24M 3.0x 5.7B 8.1x\nXception (Chollet, 2017) 79.0% 94.5% 23M 3.0x 8.4B 12x\nEfﬁcientNet-B2 79.8% 94.9% 9.2M 1x 1.0B 1x\nInception-v4 (Szegedy et al., 2017) 80.0% 95.0% 48M 5.2x 13B 13x\nInception-resnet-v2 (Szegedy et al., 2017)80.1% 95.1% 56M 6.1x 13B 13x\nEfﬁcientNet-B3 81.1% 95.5% 12M 1x 1.8B 1x\nResNeXt-101 (Xie et al., 2017) 80.9% 95.6% 84M 7.0x 32B 18x\nPolyNet (Zhang et al., 2017) 81.3% 95.8% 92M 7.7x 35B 19x\nEfﬁcientNet-B4 82.6% 96.3% 19M 1x 4.2B 1x\nSENet (Hu et al., 2018) 82.7% 96.2% 146M 7.7x 42B 10x\nNASNet-A (Zoph et al., 2018) 82.7% 96.2% 89M 4.7x 24B 5.7x\nAmoebaNet-A (Real et al., 2019) 82.8% 96.1% 87M 4.6x 23B 5.5x\nPNASNet (Liu et al., 2018) 82.9% 96.2% 86M 4.5x 23B 6.0x\nEfﬁcientNet-B5 83.3% 96.7% 30M 1x 9.9B 1x\nAmoebaNet-C (Cubuk et al., 2019) 83.5% 96.5% 155M 5.2x 41B 4.1x\nEfﬁcientNet-B6 84.0% 96.9% 43M 1x 19B 1x\nEfﬁcientNet-B7 84.4% 97.1% 66M 1x 37B 1x\nGPipe (Huang et al., 2018) 84.3% 97.0% 557M 8.4x - -\nWe omit ensemble and multi-crop models (Hu et al., 2018), or models pretrained on 3.5B Instagram images (Mahajan et al., 2018).\nTable 3. Scaling Up MobileNets and ResNet.\nModel FLOPS Top-1 Acc.\nBaseline MobileNetV1 (Howard et al., 2017)0.6B 70.6%\nScale MobileNetV1 by width (w=2) 2.2B 74.2%\nScale MobileNetV1 by resolution (r=2) 2.2B 72.7%\ncompound scale (ddd=1.4,www=1.2,rrr=1.3) 2.3B 75.6%\nBaseline MobileNetV2 (Sandler et al., 2018)0.3B 72.0%\nScale MobileNetV2 by depth (d=4) 1.2B 76.8%\nScale MobileNetV2 by width (w=2) 1.1B 76.4%\nScale MobileNetV2 by resolution (r=2) 1.2B 74.8%\nMobileNetV2 compound scale 1.3B 77.4%\nBaseline ResNet-50 (He et al., 2016) 4.1B 76.0%\nScale ResNet-50 by depth (d=4) 16.2B 78.1%\nScale ResNet-50 by width (w=2) 14.7B 77.7%\nScale ResNet-50 by resolution (r=2) 16.4B 77.5%\nResNet-50 compound scale 16.7B 78.8%\nTable 4. Inference Latency Comparison – Latency is measured\nwith batch size 1 on a single core of Intel Xeon CPU E5-2690.\nAcc. @ Latency Acc. @ Latency\nResNet-152 77.8% @ 0.554s GPipe 84.3% @ 19.0s\nEfﬁcientNet-B1 78.8% @ 0.098sEfﬁcientNet-B7 84.4% @ 3.1s\nSpeedup 5.7x Speedup 6.1x\n0 5 10 15 20 25 30 35 40 45\nFLOPS (Billions)\n74\n76\n78\n80\n82\n84Imagenet Top 1 Accuracy (%)\nResNet-34\nResNet-50\nResNet-152\nDenseNet-201\nInception-v2\nInception-ResNet-v2\nNASNet-A\nNASNet-A\nResNeXt-101\nXception\nAmeobaNet-A\nAmoebaNet-C\nSENet\nB0\nB3\nB4\nB5\nEfﬁcientNet-B6\nTop1 Acc. FLOPSResNet-152 (Xie et al., 2017)77.8% 11BEfﬁcientNet-B1 78.8% 0.7BResNeXt-101 (Xie et al., 2017)80.9% 32BEfﬁcientNet-B3 81.1% 1.8BSENet (Hu et al., 2018)82.7% 42BNASNet-A (Zoph et al., 2018)80.7% 24BEfﬁcientNet-B4 82.6% 4.2BAmeobaNet-C (Cubuk et al., 2019)83.5% 41BEfﬁcientNet-B5 83.3% 9.9B\nFigure 5. FLOPS vs. ImageNet Accuracy.\n5.2. ImageNet Results for EfﬁcientNet\nWe train our EfﬁcientNet models on ImageNet using simi-\nlar settings as (Tan et al., 2019): RMSProp optimizer with\ndecay 0.9 and momentum 0.9; batch norm momentum 0.99;\nweight decay 1e-5; initial learning rate 0.256 that decays\nby 0.97 every 2.4 epochs. We also use swish activation\nEfﬁcientNet: Rethinking Model Scaling for Convolutional Neural Networks\nTable 5. EfﬁcientNet Performance Results on Transfer Learning Datasets. Our scaled EfﬁcientNet models achieve new state-of-the-\nart accuracy for 5 out of 8 datasets, with 9.6x fewer parameters on average.\nComparison to best public-available results Comparison to best reported results\nModel Acc. #Param Our Model Acc. #Param(ratio) Model Acc. #Param Our Model Acc. #Param(ratio)\nCIFAR-10 NASNet-A 98.0% 85M EfﬁcientNet-B0 98.1% 4M (21x)†Gpipe99.0% 556M EfﬁcientNet-B7 98.9% 64M (8.7x)\nCIFAR-100 NASNet-A 87.5% 85M EfﬁcientNet-B0 88.1% 4M (21x)Gpipe 91.3% 556M EfﬁcientNet-B791.7% 64M (8.7x)\nBirdsnap Inception-v4 81.8% 41M EfﬁcientNet-B5 82.0% 28M (1.5x)GPipe 83.6% 556M EfﬁcientNet-B784.3% 64M (8.7x)\nStanford CarsInception-v4 93.4% 41M EfﬁcientNet-B3 93.6% 10M (4.1x)‡DAT 94.8% - EfﬁcientNet-B7 94.7% -\nFlowers Inception-v4 98.5% 41M EfﬁcientNet-B5 98.5% 28M (1.5x)DAT 97.7% - EfﬁcientNet-B7 98.8% -\nFGVC AircraftInception-v4 90.9% 41M EfﬁcientNet-B3 90.7% 10M (4.1x)DAT 92.9% - EfﬁcientNet-B7 92.9% -\nOxford-IIIT PetsResNet-152 94.5% 58M EfﬁcientNet-B4 94.8% 17M (5.6x)GPipe 95.9% 556M EfﬁcientNet-B6 95.4% 41M (14x)\nFood-101 Inception-v4 90.8% 41M EfﬁcientNet-B4 91.5% 17M (2.4x)GPipe 93.0% 556M EfﬁcientNet-B793.0% 64M (8.7x)\nGeo-Mean (4.7x) (9.6x)\n†GPipe (Huang et al., 2018) trains giant models with specialized pipeline parallelism library.\n‡DAT denotes domain adaptive transfer learning (Ngiam et al., 2018). Here we only compare ImageNet-based transfer learning results.\nTransfer accuracy and #params for NASNet (Zoph et al., 2018), Inception-v4 (Szegedy et al., 2017), ResNet-152 (He et al., 2016) are from (Kornblith et al., 2019).\n0.0 0.2 0.4 0.6 0.8 1.0\nNumber of Parameters (Millions, log-scale)\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\n101 102 103\n97\n98\n99Accuracy(%)\nCIFAR10\n101 102 103\n84\n86\n88\n90\n92\nCIFAR100\n101 102 103\n70\n75\n80\n85\nBirdsnap\n101 102 103\n91\n92\n93\n94\nStanford Cars\n101 102 103\n97.0\n97.5\n98.0\n98.5\nAccuracy(%)\nFlowers\n101 102 103\n82.5\n85.0\n87.5\n90.0\n92.5\nFGVC Aircraft\n101 102 103\n92\n94\n96\nOxford-IIIT Pets\n101 102 103\n86\n88\n90\n92\nFood-101\nInception-v1\nInception-v3\nInception-v4\nInception-ResNet-v2\nResNet-50\nResNet-101\nResNet-152\nDenseNet-169\nDenseNet-201\nNASNet-A\nGPIPE\nEfﬁcientNet\nFigure 6. Model Parameters vs. Transfer Learning Accuracy – All models are pretrained on ImageNet and ﬁnetuned on new datasets.\n(Ramachandran et al., 2018; Elfwing et al., 2018), ﬁxed Au-\ntoAugment policy (Cubuk et al., 2019), and stochastic depth\n(Huang et al., 2016) with drop connect ratio 0.3. As com-\nmonly known that bigger models need more regularization,\nwe linearly increase dropout (Srivastava et al., 2014) ratio\nfrom 0.2 for EfﬁcientNet-B0 to 0.5 for EfﬁcientNet-B7.\nTable 2 shows the performance of all EfﬁcientNet models\nthat are scaled from the same baseline EfﬁcientNet-B0. Our\nEfﬁcientNet models generally use an order of magnitude\nfewer parameters and FLOPS than other ConvNets with\nsimilar accuracy. In particular, our EfﬁcientNet-B7 achieves\n84.4% top1 / 97.1% top-5 accuracy with 66M parameters\nand 37B FLOPS, being more accurate but8.4x smaller than\nthe previous best GPipe (Huang et al., 2018).\nFigure 1 and Figure 5 illustrates the parameters-accuracy\nand FLOPS-accuracy curve for representative ConvNets,\nwhere our scaled EfﬁcientNet models achieve better accu-\nracy with much fewer parameters and FLOPS than other\nConvNets. Notably, our EfﬁcientNet models are not only\nsmall, but also computational cheaper. For example, our\nEfﬁcientNet-B3 achieves higher accuracy than ResNeXt-\n101 (Xie et al., 2017) using 18x fewer FLOPS.\nTo validate the computational cost, we have also mea-\nsured the inference latency on a real CPU as shown in\nTable 4, where we report average latency over 20 runs.\nOur EfﬁcientNet-B1 runs5.7x faster than the widely used\nResNet-152 (He et al., 2016), while EfﬁcientNet-B7 runs\nabout 6.1x faster than GPipe (Huang et al., 2018), suggest-\ning our EfﬁcientNets are indeed fast on real hardware.\n5.3. Transfer Learning Results for EfﬁcientNet\nWe have also evaluated our EfﬁcientNet on a list of com-\nmonly used transfer learning datasets, as shown in Table\n6. We borrow the same training settings from (Kornblith\net al., 2019) and (Huang et al., 2018), which take ImageNet\npretrained checkpoints and ﬁnetune on new datasets.\nEfﬁcientNet: Rethinking Model Scaling for Convolutional Neural Networks\nbakeshop\noriginal image\n baseline model\n deeper (d=4)\n wider (w=2)\n higher resolution (r=2)\n compound scaling\nmaze\nFigure 7. Class Activation Map (CAM) (Zhou et al., 2016) for Models with different scaling methods – Our compound scaling\nmethod allows the scaled model (last column) to focus on more relevant regions with more object details.\nTable 6. Transfer Learning Datasets.\nDataset Train Size Test Size #Classes\nCIFAR-10 (Krizhevsky & Hinton, 2009)50,000 10,000 10\nCIFAR-100 (Krizhevsky & Hinton, 2009)50,000 10,000 100\nBirdsnap (Berg et al., 2014)47,386 2,443 500\nStanford Cars (Krause et al., 2013)8,144 8,041 196\nFlowers (Nilsback & Zisserman, 2008)2,040 6,149 102\nFGVC Aircraft (Maji et al., 2013)6,667 3,333 100\nOxford-IIIT Pets (Parkhi et al., 2012)3,680 3,369 37\nFood-101 (Bossard et al., 2014)75,750 25,250 101\nTable 5 shows the transfer learning performance: (1) Com-\npared to public available models, such as NASNet-A (Zoph\net al., 2018) and Inception-v4 (Szegedy et al., 2017), our Ef-\nﬁcientNet models achieve better accuracy with 4.7x average\n(up to 21x) parameter reduction. (2) Compared to state-\nof-the-art models, including DAT (Ngiam et al., 2018) that\ndynamically synthesizes training data and GPipe (Huang\net al., 2018) that is trained with specialized pipeline paral-\nlelism, our EfﬁcientNet models still surpass their accuracy\nin 5 out of 8 datasets, but using 9.6x fewer parameters\nFigure 6 compares the accuracy-parameters curve for a va-\nriety of models. In general, our EfﬁcientNets consistently\nachieve better accuracy with an order of magnitude fewer pa-\nrameters than existing models, including ResNet (He et al.,\n2016), DenseNet (Huang et al., 2017), Inception (Szegedy\net al., 2017), and NASNet (Zoph et al., 2018).\n6. Discussion\nTo disentangle the contribution of our proposed scaling\nmethod from the EfﬁcientNet architecture, Figure 8 com-\npares the ImageNet performance of different scaling meth-\nods for the same EfﬁcientNet-B0 baseline network. In gen-\neral, all scaling methods improve accuracy with the cost\nof more FLOPS, but our compound scaling method can\nfurther improve accuracy, by up to 2.5%, than other single-\ndimension scaling methods, suggesting the importance of\nour proposed compound scaling.\n0 1 2 3 4 5\nFLOPS (Billions)\n75\n76\n77\n78\n79\n80\n81\n82\n83ImageNet Top-1 Accuracy(%)\nscale by width\nscale by depth\nscale by resolution\ncompound scaling\nFigure 8. Scaling Up EfﬁcientNet-B0 with Different Methods.\nIn order to further understand why our compound scaling\nmethod is better than others, Figure 7 compares the class ac-\ntivation map for a few representative models with different\nscaling methods. All these models are scaled from the same\nEfﬁcientNet-B0 baseline with about 4x more FLOPS than\nthe baseline. Images are randomly picked from ImageNet\nvalidation set. As shown in the ﬁgure, the model with com-\npound scaling tends to focus on more relevant regions with\nmore object details, while other models are either lack of\nobject details or unable to capture all objects in the images.\n7. Conclusion\nIn this paper, we systematically study ConvNet scaling and\nidentify that carefully balancing network width, depth, and\nresolution is an important but missing piece, preventing us\nfrom better accuracy and efﬁciency. To address this issue,\nwe propose a simple and highly effective compound scaling\nmethod, which enables us to easily scale up a baseline Con-\nvNet to any target resource constraints in a more principled\nway, while maintaining model efﬁciency. Powered by this\ncompound scaling method, we demonstrate that a mobile-\nsize EfﬁcientNet model can be scaled up very effectively,\nsurpassing state-of-the-art accuracy with an order of magni-\ntude fewer parameters and FLOPS, on both ImageNet and\nﬁve commonly used transfer learning datasets.\nEfﬁcientNet: Rethinking Model Scaling for Convolutional Neural Networks\nAcknowledgements\nWe thank Ruoming Pang, Vijay Vasudevan, Alok Aggarwal,\nBarret Zoph, Hongkun Yu, Xiaodan Song, Samy Bengio,\nJeff Dean, and Google Brain team for their help.\nReferences\nBerg, T., Liu, J., Woo Lee, S., Alexander, M. L., Jacobs,\nD. W., and Belhumeur, P. N. Birdsnap: Large-scale\nﬁne-grained visual categorization of birds. CVPR, pp.\n2011–2018, 2014.\nBossard, L., Guillaumin, M., and Van Gool, L. Food-101–\nmining discriminative components with random forests.\nECCV, pp. 446–461, 2014.\nCai, H., Zhu, L., and Han, S. Proxylessnas: Direct neural\narchitecture search on target task and hardware. ICLR,\n2019.\nChollet, F. Xception: Deep learning with depthwise separa-\nble convolutions. CVPR, pp. 1610–02357, 2017.\nCubuk, E. D., Zoph, B., Mane, D., Vasudevan, V ., and Le,\nQ. V . Autoaugment: Learning augmentation policies\nfrom data. CVPR, 2019.\nElfwing, S., Uchibe, E., and Doya, K. Sigmoid-weighted\nlinear units for neural network function approximation\nin reinforcement learning. Neural Networks, 107:3–11,\n2018.\nGholami, A., Kwon, K., Wu, B., Tai, Z., Yue, X., Jin, P.,\nZhao, S., and Keutzer, K. Squeezenext: Hardware-aware\nneural network design. ECV Workshop at CVPR’18 ,\n2018.\nHan, S., Mao, H., and Dally, W. J. Deep compression:\nCompressing deep neural networks with pruning, trained\nquantization and huffman coding. ICLR, 2016.\nHe, K., Zhang, X., Ren, S., and Sun, J. Deep residual\nlearning for image recognition. CVPR, pp. 770–778,\n2016.\nHe, K., Gkioxari, G., Doll ´ar, P., and Girshick, R. Mask\nr-cnn. ICCV, pp. 2980–2988, 2017.\nHe, Y ., Lin, J., Liu, Z., Wang, H., Li, L.-J., and Han, S.\nAmc: Automl for model compression and acceleration\non mobile devices. ECCV, 2018.\nHoward, A. G., Zhu, M., Chen, B., Kalenichenko, D., Wang,\nW., Weyand, T., Andreetto, M., and Adam, H. Mobilenets:\nEfﬁcient convolutional neural networks for mobile vision\napplications. arXiv preprint arXiv:1704.04861, 2017.\nHu, J., Shen, L., and Sun, G. Squeeze-and-excitation net-\nworks. CVPR, 2018.\nHuang, G., Sun, Y ., Liu, Z., Sedra, D., and Weinberger,\nK. Q. Deep networks with stochastic depth. ECCV, pp.\n646–661, 2016.\nHuang, G., Liu, Z., Van Der Maaten, L., and Weinberger,\nK. Q. Densely connected convolutional networks. CVPR,\n2017.\nHuang, Y ., Cheng, Y ., Chen, D., Lee, H., Ngiam, J., Le,\nQ. V ., and Chen, Z. Gpipe: Efﬁcient training of giant\nneural networks using pipeline parallelism.arXiv preprint\narXiv:1808.07233, 2018.\nIandola, F. N., Han, S., Moskewicz, M. W., Ashraf, K.,\nDally, W. J., and Keutzer, K. Squeezenet: Alexnet-level\naccuracy with 50x fewer parameters and<0.5 mb model\nsize. arXiv preprint arXiv:1602.07360, 2016.\nIoffe, S. and Szegedy, C. Batch normalization: Accelerating\ndeep network training by reducing internal covariate shift.\nICML, pp. 448–456, 2015.\nKornblith, S., Shlens, J., and Le, Q. V . Do better imagenet\nmodels transfer better? CVPR, 2019.\nKrause, J., Deng, J., Stark, M., and Fei-Fei, L. Collecting a\nlarge-scale dataset of ﬁne-grained cars.Second Workshop\non Fine-Grained Visual Categorizatio, 2013.\nKrizhevsky, A. and Hinton, G. Learning multiple layers of\nfeatures from tiny images. Technical Report, 2009.\nKrizhevsky, A., Sutskever, I., and Hinton, G. E. Imagenet\nclassiﬁcation with deep convolutional neural networks.\nIn NIPS, pp. 1097–1105, 2012.\nLin, H. and Jegelka, S. Resnet with one-neuron hidden\nlayers is a universal approximator. NeurIPS, pp. 6172–\n6181, 2018.\nLin, T.-Y ., Doll´ar, P., Girshick, R., He, K., Hariharan, B.,\nand Belongie, S. Feature pyramid networks for object\ndetection. CVPR, 2017.\nLiu, C., Zoph, B., Shlens, J., Hua, W., Li, L.-J., Fei-Fei, L.,\nYuille, A., Huang, J., and Murphy, K. Progressive neural\narchitecture search. ECCV, 2018.\nLu, Z., Pu, H., Wang, F., Hu, Z., and Wang, L. The expres-\nsive power of neural networks: A view from the width.\nNeurIPS, 2018.\nMa, N., Zhang, X., Zheng, H.-T., and Sun, J. Shufﬂenet v2:\nPractical guidelines for efﬁcient cnn architecture design.\nECCV, 2018.\nEfﬁcientNet: Rethinking Model Scaling for Convolutional Neural Networks\nMahajan, D., Girshick, R., Ramanathan, V ., He, K., Paluri,\nM., Li, Y ., Bharambe, A., and van der Maaten, L. Explor-\ning the limits of weakly supervised pretraining. arXiv\npreprint arXiv:1805.00932, 2018.\nMaji, S., Rahtu, E., Kannala, J., Blaschko, M., and Vedaldi,\nA. Fine-grained visual classiﬁcation of aircraft. arXiv\npreprint arXiv:1306.5151, 2013.\nNgiam, J., Peng, D., Vasudevan, V ., Kornblith, S., Le, Q. V .,\nand Pang, R. Domain adaptive transfer learning with spe-\ncialist models. arXiv preprint arXiv:1811.07056, 2018.\nNilsback, M.-E. and Zisserman, A. Automated ﬂower clas-\nsiﬁcation over a large number of classes. ICVGIP, pp.\n722–729, 2008.\nParkhi, O. M., Vedaldi, A., Zisserman, A., and Jawahar, C.\nCats and dogs. CVPR, pp. 3498–3505, 2012.\nRaghu, M., Poole, B., Kleinberg, J., Ganguli, S., and Sohl-\nDickstein, J. On the expressive power of deep neural\nnetworks. ICML, 2017.\nRamachandran, P., Zoph, B., and Le, Q. V . Searching for\nactivation functions. arXiv preprint arXiv:1710.05941,\n2018.\nReal, E., Aggarwal, A., Huang, Y ., and Le, Q. V . Regu-\nlarized evolution for image classiﬁer architecture search.\nAAAI, 2019.\nRussakovsky, O., Deng, J., Su, H., Krause, J., Satheesh, S.,\nMa, S., Huang, Z., Karpathy, A., Khosla, A., Bernstein,\nM., et al. Imagenet large scale visual recognition chal-\nlenge. International Journal of Computer Vision, 115(3):\n211–252, 2015.\nSandler, M., Howard, A., Zhu, M., Zhmoginov, A., and\nChen, L.-C. Mobilenetv2: Inverted residuals and linear\nbottlenecks. CVPR, 2018.\nSharir, O. and Shashua, A. On the expressive power of\noverlapping architectures of deep learning. ICLR, 2018.\nSrivastava, N., Hinton, G., Krizhevsky, A., Sutskever, I.,\nand Salakhutdinov, R. Dropout: a simple way to prevent\nneural networks from overﬁtting.The Journal of Machine\nLearning Research, 15(1):1929–1958, 2014.\nSzegedy, C., Liu, W., Jia, Y ., Sermanet, P., Reed, S.,\nAnguelov, D., Erhan, D., Vanhoucke, V ., and Rabinovich,\nA. Going deeper with convolutions. CVPR, pp. 1–9,\n2015.\nSzegedy, C., Vanhoucke, V ., Ioffe, S., Shlens, J., and Wojna,\nZ. Rethinking the inception architecture for computer\nvision. CVPR, pp. 2818–2826, 2016.\nSzegedy, C., Ioffe, S., Vanhoucke, V ., and Alemi, A. A.\nInception-v4, inception-resnet and the impact of residual\nconnections on learning. AAAI, 4:12, 2017.\nTan, M., Chen, B., Pang, R., Vasudevan, V ., Sandler, M.,\nHoward, A., and Le, Q. V . MnasNet: Platform-aware\nneural architecture search for mobile. CVPR, 2019.\nXie, S., Girshick, R., Doll´ar, P., Tu, Z., and He, K. Aggre-\ngated residual transformations for deep neural networks.\nCVPR, pp. 5987–5995, 2017.\nYang, T.-J., Howard, A., Chen, B., Zhang, X., Go, A., Sze,\nV ., and Adam, H. Netadapt: Platform-aware neural net-\nwork adaptation for mobile applications. ECCV, 2018.\nZagoruyko, S. and Komodakis, N. Wide residual networks.\nBMVC, 2016.\nZhang, X., Li, Z., Loy, C. C., and Lin, D. Polynet: A pursuit\nof structural diversity in very deep networks. CVPR, pp.\n3900–3908, 2017.\nZhang, X., Zhou, X., Lin, M., and Sun, J. Shufﬂenet: An ex-\ntremely efﬁcient convolutional neural network for mobile\ndevices. CVPR, 2018.\nZhou, B., Khosla, A., Lapedriza, A., Oliva, A., and Torralba,\nA. Learning deep features for discriminative localization.\nCVPR, pp. 2921–2929, 2016.\nZoph, B. and Le, Q. V . Neural architecture search with\nreinforcement learning. ICLR, 2017.\nZoph, B., Vasudevan, V ., Shlens, J., and Le, Q. V . Learning\ntransferable architectures for scalable image recognition.\nCVPR, 2018.",
  "values": {
    "Privacy": "No",
    "Deferral to humans": "No",
    "Collective influence": "No",
    "User influence": "No",
    "Justice": "No",
    "Fairness": "No",
    "Transparent (to users)": "No",
    "Explicability": "No",
    "Beneficence": "No",
    "Interpretable (to users)": "No",
    "Critiqability": "No",
    "Respect for Persons": "No",
    "Non-maleficence": "No",
    "Not socially biased": "No",
    "Respect for Law and public interest": "No",
    "Autonomy (power to decide)": "No"
  }
}