{
  "pdf": "wu19e",
  "title": "Simplifying Graph Convolutional Networks",
  "author": "Felix Wu, Tianyi Zhang, Amauri Holanda de Souza Jr., Christopher Fifty, Tao Yu, Kilian Q. Weinberger",
  "paper_id": "wu19e",
  "text": "Simplifying Graph Convolutional Networks\nFelix Wu * 1 Tianyi Zhang * 1 Amauri Holanda de Souza Jr.* 1 2 Christopher Fifty 1 Tao Yu1\nKilian Q. Weinberger1\nAbstract\nGraph Convolutional Networks (GCNs) and their\nvariants have experienced signiﬁcant attention and\nhave become the de facto methods for learning\ngraph representations. GCNs derive inspiration\nprimarily from recent deep learning approaches,\nand as a result, may inherit unnecessary complex-\nity and redundant computation. In this paper,\nwe reduce this excess complexity through suc-\ncessively removing nonlinearities and collapsing\nweight matrices between consecutive layers. We\ntheoretically analyze the resulting linear model\nand show that it corresponds to a ﬁxed low-pass\nﬁlter followed by a linear classiﬁer. Notably, our\nexperimental evaluation demonstrates that these\nsimpliﬁcations do not negatively impact accuracy\nin many downstream applications. Moreover, the\nresulting model scales to larger datasets, is natu-\nrally interpretable, and yields up to two orders of\nmagnitude speedup over FastGCN.\n1. Introduction\nGraph Convolutional Networks (GCNs) (Kipf & Welling,\n2017) are an efﬁcient variant of Convolutional Neural Net-\nworks (CNNs) on graphs. GCNs stack layers of learned\nﬁrst-order spectral ﬁlters followed by a nonlinear activation\nfunction to learn graph representations. Recently, GCNs and\nsubsequent variants have achieved state-of-the-art results\nin various application areas, including but not limited to\ncitation networks (Kipf & Welling, 2017), social networks\n(Chen et al., 2018), applied chemistry (Liao et al., 2019),\nnatural language processing (Yao et al., 2019; Han et al.,\n2012; Zhang et al., 2018c), and computer vision (Wang\net al., 2018; Kampffmeyer et al., 2018).\nHistorically, the development of machine learning algo-\n*Equal contribution 1Cornell University 2Federal Insti-\ntute of Ceara (Brazil). Correspondence to: Felix Wu\n<fw245@cornell.edu>, Tianyi Zhang <tz58@cornell.edu>.\nProceedings of the 36 th International Conference on Machine\nLearning, Long Beach, California, PMLR 97, 2019. Copyright\n2019 by the author(s).\nrithms has followed a clear trend from initial simplicity to\nneed-driven complexity. For instance, limitations of the\nlinear Perceptron (Rosenblatt, 1958) motivated the develop-\nment of the more complex but also more expressive neural\nnetwork (or multi-layer Perceptrons, MLPs) (Rosenblatt,\n1961). Similarly, simple pre-deﬁned linear image ﬁlters (So-\nbel & Feldman, 1968; Harris & Stephens, 1988) eventually\ngave rise to nonlinear CNNs with learned convolutional\nkernels (Waibel et al., 1989; LeCun et al., 1989). As ad-\nditional algorithmic complexity tends to complicate theo-\nretical analysis and obfuscates understanding, it is typically\nonly introduced for applications where simpler methods are\ninsufﬁcient. Arguably, most classiﬁers in real world appli-\ncations are still linear (typically logistic regression), which\nare straight-forward to optimize and easy to interpret.\nHowever, possibly because GCNs were proposed after the\nrecent “renaissance” of neural networks, they tend to be a\nrare exception to this trend. GCNs are built upon multi-layer\nneural networks, and were never an extension of a simpler\n(insufﬁcient) linear counterpart.\nIn this paper, we observe that GCNs inherit considerable\ncomplexity from their deep learning lineage, which can\nbe burdensome and unnecessary for less demanding appli-\ncations. Motivated by the glaring historic omission of a\nsimpler predecessor, we aim to derive the simplest linear\nmodel that “could have” preceded the GCN, had a more\n“traditional” path been taken. We reduce the excess com-\nplexity of GCNs by repeatedly removing the nonlinearities\nbetween GCN layers and collapsing the resulting function\ninto a single linear transformation. We empirically show\nthat the ﬁnal linear model exhibits comparable or even su-\nperior performance to GCNs on a variety of tasks while be-\ning computationally more efﬁcient and ﬁtting signiﬁcantly\nfewer parameters. We refer to this simpliﬁed linear model\nas Simple Graph Convolution (SGC).\nIn contrast to its nonlinear counterparts, the SGC is intu-\nitively interpretable and we provide a theoretical analysis\nfrom the graph convolution perspective. Notably, feature\nextraction in SGC corresponds to a single ﬁxed ﬁlter applied\nto each feature dimension. Kipf & Welling (2017) empiri-\ncally observe that the “renormalization trick”, i.e. adding\nself-loops to the graph, improves accuracy, and we demon-\nSimplifying Graph Convolutional Networks\nNonlinearity\nLinear Transformation\nGCN\n¯\nH\n(k )\n SH\n(k \u0000 1)\n<latexit sha1_base64=\"(null)\">(null)</latexit><latexit sha1_base64=\"(null)\">(null)</latexit><latexit sha1_base64=\"(null)\">(null)</latexit><latexit sha1_base64=\"(null)\">(null)</latexit>\n¯\nH\n(k )\n \n¯\nH\n(k )\n⇥\n(k )\n<latexit sha1_base64=\"(null)\">(null)</latexit><latexit sha1_base64=\"(null)\">(null)</latexit><latexit sha1_base64=\"(null)\">(null)</latexit><latexit sha1_base64=\"(null)\">(null)</latexit>\nPredictions\nˆ\nY GCN\n= softmax( SH\n( K \u0000 1)\n⇥\n( K )\n)\n<latexit sha1_base64=\"(null)\">(null)</latexit><latexit sha1_base64=\"(null)\">(null)</latexit><latexit sha1_base64=\"(null)\">(null)</latexit><latexit sha1_base64=\"(null)\">(null)</latexit>\n⇥ ( K \u0000 1)\n<latexit sha1_base64=\"(null)\">(null)</latexit><latexit sha1_base64=\"(null)\">(null)</latexit><latexit sha1_base64=\"(null)\">(null)</latexit><latexit sha1_base64=\"(null)\">(null)</latexit>\nFeature Propagation\nLogistic Regression\nˆ\nY SGC\n= softmax\n\u0000\n¯\nX ⇥\n\u0000\n<latexit sha1_base64=\"(null)\">(null)</latexit><latexit sha1_base64=\"(null)\">(null)</latexit><latexit sha1_base64=\"(null)\">(null)</latexit><latexit sha1_base64=\"(null)\">(null)</latexit>\nPredictions\n-1 +10\nFeature Value:Class +1: Class -1: Feature Vector:\nK-step Feature Propagation\n¯\nX  S\nK\nX\n<latexit sha1_base64=\"(null)\">(null)</latexit><latexit sha1_base64=\"(null)\">(null)</latexit><latexit sha1_base64=\"(null)\">(null)</latexit><latexit sha1_base64=\"(null)\">(null)</latexit>\nSGC\nInput Graph\nx\n1\n<latexit sha1_base64=\"(null)\">(null)</latexit><latexit sha1_base64=\"(null)\">(null)</latexit><latexit sha1_base64=\"(null)\">(null)</latexit><latexit sha1_base64=\"(null)\">(null)</latexit>\nx\n2\n<latexit sha1_base64=\"(null)\">(null)</latexit><latexit sha1_base64=\"(null)\">(null)</latexit><latexit sha1_base64=\"(null)\">(null)</latexit><latexit sha1_base64=\"(null)\">(null)</latexit>\nx\n3\n<latexit sha1_base64=\"(null)\">(null)</latexit><latexit sha1_base64=\"(null)\">(null)</latexit><latexit sha1_base64=\"(null)\">(null)</latexit><latexit sha1_base64=\"(null)\">(null)</latexit>\nx\n4\n<latexit sha1_base64=\"(null)\">(null)</latexit><latexit sha1_base64=\"(null)\">(null)</latexit><latexit sha1_base64=\"(null)\">(null)</latexit><latexit sha1_base64=\"(null)\">(null)</latexit>\nx\n5\n<latexit sha1_base64=\"(null)\">(null)</latexit><latexit sha1_base64=\"(null)\">(null)</latexit><latexit sha1_base64=\"(null)\">(null)</latexit><latexit sha1_base64=\"(null)\">(null)</latexit>\nx\n6\n<latexit sha1_base64=\"(null)\">(null)</latexit><latexit sha1_base64=\"(null)\">(null)</latexit><latexit sha1_base64=\"(null)\">(null)</latexit><latexit sha1_base64=\"(null)\">(null)</latexit>\nx\n7\n<latexit sha1_base64=\"(null)\">(null)</latexit><latexit sha1_base64=\"(null)\">(null)</latexit><latexit sha1_base64=\"(null)\">(null)</latexit><latexit sha1_base64=\"(null)\">(null)</latexit>\nH\n(0)\n= X =[ x 1\n,..., x n\n]\n>\n<latexit sha1_base64=\"(null)\">(null)</latexit><latexit sha1_base64=\"(null)\">(null)</latexit><latexit sha1_base64=\"(null)\">(null)</latexit><latexit sha1_base64=\"(null)\">(null)</latexit>\nInput Graph\nx\n1\n<latexit sha1_base64=\"(null)\">(null)</latexit><latexit sha1_base64=\"(null)\">(null)</latexit><latexit sha1_base64=\"(null)\">(null)</latexit><latexit sha1_base64=\"(null)\">(null)</latexit>\nx\n2\n<latexit sha1_base64=\"(null)\">(null)</latexit><latexit sha1_base64=\"(null)\">(null)</latexit><latexit sha1_base64=\"(null)\">(null)</latexit><latexit sha1_base64=\"(null)\">(null)</latexit>\nx\n3\n<latexit sha1_base64=\"(null)\">(null)</latexit><latexit sha1_base64=\"(null)\">(null)</latexit><latexit sha1_base64=\"(null)\">(null)</latexit><latexit sha1_base64=\"(null)\">(null)</latexit>\nx\n4\n<latexit sha1_base64=\"(null)\">(null)</latexit><latexit sha1_base64=\"(null)\">(null)</latexit><latexit sha1_base64=\"(null)\">(null)</latexit><latexit sha1_base64=\"(null)\">(null)</latexit>\nx\n5\n<latexit sha1_base64=\"(null)\">(null)</latexit><latexit sha1_base64=\"(null)\">(null)</latexit><latexit sha1_base64=\"(null)\">(null)</latexit><latexit sha1_base64=\"(null)\">(null)</latexit>\nx\n6\n<latexit sha1_base64=\"(null)\">(null)</latexit><latexit sha1_base64=\"(null)\">(null)</latexit><latexit sha1_base64=\"(null)\">(null)</latexit><latexit sha1_base64=\"(null)\">(null)</latexit>\nx\n7\n<latexit sha1_base64=\"(null)\">(null)</latexit><latexit sha1_base64=\"(null)\">(null)</latexit><latexit sha1_base64=\"(null)\">(null)</latexit><latexit sha1_base64=\"(null)\">(null)</latexit>\nX =[ x 1\n,..., x n\n]\n>\n<latexit sha1_base64=\"(null)\">(null)</latexit><latexit sha1_base64=\"(null)\">(null)</latexit><latexit sha1_base64=\"(null)\">(null)</latexit><latexit sha1_base64=\"(null)\">(null)</latexit>\nH\n(k )\n ReLU(\n¯\nH\n(k )\n)\n<latexit sha1_base64=\"(null)\">(null)</latexit><latexit sha1_base64=\"(null)\">(null)</latexit><latexit sha1_base64=\"(null)\">(null)</latexit><latexit sha1_base64=\"(null)\">(null)</latexit><latexit sha1_base64=\"(null)\">(null)</latexit><latexit sha1_base64=\"(null)\">(null)</latexit><latexit sha1_base64=\"(null)\">(null)</latexit><latexit sha1_base64=\"(null)\">(null)</latexit><latexit sha1_base64=\"(null)\">(null)</latexit><latexit sha1_base64=\"(null)\">(null)</latexit><latexit sha1_base64=\"(null)\">(null)</latexit><latexit sha1_base64=\"(null)\">(null)</latexit><latexit sha1_base64=\"(null)\">(null)</latexit>\nFigure 1. Schematic layout of a GCN v.s. a SGC. Top row: The GCN transforms the feature vectors repeatedly throughout K layers\nand then applies a linear classiﬁer on the ﬁnal representation. Bottom row: the SGC reduces the entire procedure to a simple feature\npropagation step followed by standard logistic regression.\nstrate that this method effectively shrinks the graph spectral\ndomain, resulting in a low-pass-type ﬁlter when applied to\nSGC. Crucially, this ﬁltering operation gives rise to locally\nsmooth features across the graph (Bruna et al., 2014).\nThrough an empirical assessment on node classiﬁcation\nbenchmark datasets for citation and social networks, we\nshow that the SGC achieves comparable performance to\nGCN and other state-of-the-art graph neural networks. How-\never, it is signiﬁcantly faster, and even outperforms Fast-\nGCN (Chen et al., 2018) by up to two orders of magnitude\non the largest dataset (Reddit) in our evaluation. Finally,\nwe demonstrate that SGC extrapolates its effectiveness to a\nwide-range of downstream tasks. In particular, SGC rivals,\nif not surpasses, GCN-based approaches on text classiﬁ-\ncation, user geolocation, relation extraction, and zero-shot\nimage classiﬁcation tasks. The code is available on Github1.\n2. Simple Graph Convolution\nWe follow Kipf & Welling (2017) to introduce GCNs (and\nsubsequently SGC) in the context of node classiﬁcation.\nHere, GCNs take a graph with some labeled nodes as input\nand generate label predictions for all graph nodes. Let\nus formally deﬁne such a graph as G = (V, A), where V\nrepresents the vertex set consisting of nodes {v1,...,v n},\nand A ∈ Rn×n is a symmetric (typically sparse) adjacency\nmatrix whereaij denotes the edge weight between nodes\n1https://github.com/Tiiiger/SGC\nvi andvj. A missing edge is represented through aij = 0.\nWe deﬁne the degree matrix D = diag(d1,...,d n) as a\ndiagonal matrix where each entry on the diagonal is equal\nto the row-sum of the adjacency matrixdi = ∑\njaij.\nEach node vi in the graph has a corresponding d-\ndimensional feature vector xi ∈ Rd. The entire feature\nmatrix X ∈ Rn×d stacksn feature vectors on top of one\nanother, X = [ x1,..., xn]⊤. Each node belongs to one\nout ofC classes and can be labeled with aC-dimensional\none-hot vector yi ∈ {0, 1}C. We only know the labels of a\nsubset of the nodes and want to predict the unknown labels.\n2.1. Graph Convolutional Networks\nSimilar to CNNs or MLPs, GCNs learn a new feature repre-\nsentation for the featurexi of each node over multiple layers,\nwhich is subsequently used as input into a linear classiﬁer.\nFor thek-th graph convolution layer, we denote the input\nnode representations of all nodes by the matrix H(k−1) and\nthe output node representations H(k). Naturally, the initial\nnode representations are just the original input features:\nH(0) = X, (1)\nwhich serve as input to the ﬁrst GCN layer.\nAK-layer GCN is identical to applying a K-layer MLP\nto the feature vector xi of each node in the graph, except\nthat the hidden representation of each node is averaged with\nits neighbors at the beginning of each layer. In each graph\nconvolution layer, node representations are updated in three\nSimplifying Graph Convolutional Networks\nstages: feature propagation, linear transformation, and a\npointwise nonlinear activation (see Figure 1). For the sake\nof clarity, we describe each step in detail.\nFeature propagation is what distinguishes a GCN from\nan MLP. At the beginning of each layer the features hi of\neach node vi are averaged with the feature vectors in its\nlocal neighborhood,\n¯h(k)\ni ← 1\ndi + 1h(k−1)\ni +\nn∑\nj=1\naij√\n(di + 1)(dj + 1)\nh(k−1)\nj .\n(2)\nMore compactly, we can express this update over the en-\ntire graph as a simple matrix operation. Let S denote the\n“normalized” adjacency matrix with added self-loops,\nS = ˜D− 1\n2 ˜A ˜D− 1\n2, (3)\nwhere ˜A = A + I and ˜D is the degree matrix of ˜A. The\nsimultaneous update in Equation 2 for all nodes becomes a\nsimple sparse matrix multiplication\n¯H(k) ← SH(k−1). (4)\nIntuitively, this step smoothes the hidden representations lo-\ncally along the edges of the graph and ultimately encourages\nsimilar predictions among locally connected nodes.\nFeature transformation and nonlinear transition. Af-\nter the local smoothing, a GCN layer is identical to a stan-\ndard MLP. Each layer is associated with a learned weight\nmatrix Θ(k), and the smoothed hidden feature representa-\ntions are transformed linearly. Finally, a nonlinear activa-\ntion function such as ReLU is applied pointwise before\noutputting feature representation H(k). In summary, the\nrepresentation updating rule of thek-th layer is:\nH(k) ← ReLU\n(\n¯H(k)Θ(k)\n)\n. (5)\nThe pointwise nonlinear transformation of thek-th layer is\nfollowed by the feature propagation of the (k + 1)-th layer.\nClassiﬁer. For node classiﬁcation, and similar to a stan-\ndard MLP, the last layer of a GCN predicts the labels using a\nsoftmax classiﬁer. Denote the class predictions forn nodes\nas ˆY ∈ Rn×C where ˆyic denotes the probability of node\ni belongs to class c. The class prediction ˆY of aK-layer\nGCN can be written as:\nˆYGCN = softmax\n(\nSH(K−1)Θ(K)\n)\n, (6)\nwhere softmax(x) = exp(x)/ ∑C\nc=1 exp(xc) acts as a nor-\nmalizer across all classes.\n2.2. Simple Graph Convolution\nIn a traditional MLP, deeper layers increase the expressivity\nbecause it allows the creation of feature hierarchies, e.g.\nfeatures in the second layer build on top of the features of\nthe ﬁrst layer. In GCNs, the layers have a second important\nfunction: in each layer the hidden representations are aver-\naged among neighbors that are one hop away. This implies\nthat afterk layers a node obtains feature information from\nall nodes that are k−hops away in the graph. This effect\nis similar to convolutional neural networks, where depth\nincreases the receptive ﬁeld of internal features (Hariharan\net al., 2015). Although convolutional networks can bene-\nﬁt substantially from increased depth (Huang et al., 2016),\ntypically MLPs obtain little beneﬁt beyond 3 or 4 layers.\nLinearization. We hypothesize that the nonlinearity be-\ntween GCN layers is not critical - but that the majority of the\nbeneﬁt arises from the local averaging. We therefore remove\nthe nonlinear transition functions between each layer and\nonly keep the ﬁnal softmax (in order to obtain probabilistic\noutputs). The resulting model is linear, but still has the same\nincreased “receptive ﬁeld” of aK-layer GCN,\nˆY = softmax\n(\nS... SSXΘ(1)Θ(2)... Θ(K)\n)\n. (7)\nTo simplify notation we can collapse the repeated multi-\nplication with the normalized adjacency matrix S into a\nsingle matrix by raising S to the K-th power, SK. Fur-\nther, we can reparameterize our weights into a single matrix\nΘ = Θ(1)Θ(2)... Θ(K). The resulting classiﬁer becomes\nˆYSGC = softmax\n(\nSKXΘ\n)\n, (8)\nwhich we refer to as Simple Graph Convolution (SGC).\nLogistic regression. Equation 8 gives rise to a natural and\nintuitive interpretation of SGC: by distinguishing between\nfeature extraction and classiﬁer, SGC consists of a ﬁxed\n(i.e., parameter-free) feature extraction/smoothing compo-\nnent ¯X = SKX followed by a linear logistic regression\nclassiﬁer ˆY = softmax( ¯XΘ). Since the computation of ¯X\nrequires no weight it is essentially equivalent to a feature\npre-processing step and the entire training of the model re-\nduces to straight-forward multi-class logistic regression on\nthe pre-processed features ¯X.\nOptimization details. The training of logistic regression\nis a well studied convex optimization problem and can\nbe performed with any efﬁcient second order method or\nstochastic gradient descent (Bottou, 2010). Provided the\ngraph connectivity pattern is sufﬁciently sparse, SGD nat-\nurally scales to very large graph sizes and the training of\nSGC is drastically faster than that of GCNs.\nSimplifying Graph Convolutional Networks\n3. Spectral Analysis\nWe now study SGC from a graph convolution perspective.\nWe demonstrate that SGC corresponds to a ﬁxed ﬁlter on\nthe graph spectral domain. In addition, we show that adding\nself-loops to the original graph, i.e. the renormalization trick\n(Kipf & Welling, 2017), effectively shrinks the underlying\ngraph spectrum. On this scaled domain, SGC acts as a low-\npass ﬁlter that produces smooth features over the graph. As\na result, nearby nodes tend to share similar representations\nand consequently predictions.\n3.1. Preliminaries on Graph Convolutions\nAnalogous to the Euclidean domain, graph Fourier analysis\nrelies on the spectral decomposition of graph Laplacians.\nThe graph Laplacian ∆ = D −A (as well as its normalized\nversion ∆sym = D−1/2∆D−1/2) is a symmetric positive\nsemideﬁnite matrix with eigendecomposition∆ = UΛU⊤,\nwhere U ∈ Rn×n comprises orthonormal eigenvectors and\nΛ = diag(λ1,...,λ n) is a diagonal matrix of eigenvalues.\nThe eigendecomposition of the Laplacian allows us to deﬁne\nthe Fourier transform equivalent on the graph domain, where\neigenvectors denote Fourier modes and eigenvalues denote\nfrequencies of the graph. In this regard, let x ∈ Rn be a\nsignal deﬁned on the vertices of the graph. We deﬁne the\ngraph Fourier transform of x as ˆx = U⊤x, with inverse\noperation given by x = Uˆx. Thus, the graph convolution\noperation between signal x and ﬁlterg is\ng ∗ x = U\n(\n(U⊤g) ⊙ (U⊤x)\n)\n= U ˆGU⊤x, (9)\nwhere ˆG = diag (ˆg1,..., ˆgn) denotes a diagonal matrix in\nwhich the diagonal corresponds to spectral ﬁlter coefﬁcients.\nGraph convolutions can be approximated byk-th order poly-\nnomials of Laplacians\nU ˆGU⊤x ≈\nk∑\ni=0\nθi∆ix = U\n( k∑\ni=0\nθiΛi\n)\nU⊤x, (10)\nwhereθi denotes coefﬁcients. In this case, ﬁlter coefﬁcients\ncorrespond to polynomials of the Laplacian eigenvalues, i.e.,\nˆG = ∑\niθiΛi or equivalently ˆg(λj) = ∑\niθiλi\nj.\nGraph Convolutional Networks (GCNs) (Kipf & Welling,\n2017) employ an afﬁne approximation (k = 1 ) of Equa-\ntion 10 with coefﬁcientsθ0 = 2θ andθ1 = −θ from which\nwe attain the basic GCN convolution operation\ng ∗ x =θ(I + D−1/2AD−1/2)x. (11)\nIn their ﬁnal design, Kipf & Welling (2017) replace\nthe matrix I + D−1/2AD−1/2 by a normalized version\n˜D−1/2 ˜A ˜D−1/2 where ˜A = A + I and consequently\n˜D = D + I, dubbed the renormalization trick. Finally,\nby generalizing the convolution to work with multiple ﬁlters\nin ad-channel input and layering the model with nonlinear\nactivation functions between each layer, we have the GCN\npropagation rule as deﬁned in Equation 5.\n3.2. SGC and Low-Pass Filtering\nThe initial ﬁrst-order Chebyshev ﬁlter derived in GCNs\ncorresponds to the propagation matrix S1-order = I +\nD−1/2AD−1/2 (see Equation 11). Since the normalized\nLaplacian is ∆sym = I − D−1/2AD−1/2, then S1-order =\n2I − ∆sym. Therefore, feature propagation with SK\n1-order im-\nplies ﬁlter coefﬁcientsˆgi = ˆg(λi) = (2 −λi)K, whereλi\ndenotes the eigenvalues of ∆sym. Figure 2 illustrates the\nﬁltering operation related toS1-order for a varying number\nof propagation stepsK ∈ {1,..., 6}. As one may observe,\nhigh powers of S1-order lead to exploding ﬁlter coefﬁcients\nand undesirably over-amplify signals at frequenciesλi < 1.\nTo tackle potential numerical issues associated with the\nﬁrst-order Chebyshev ﬁlter, Kipf & Welling (2017) pro-\npose the renormalization trick. Basically, it consists of\nreplacing S1-order by the normalized adjacency matrix af-\nter adding self-loops for all nodes. We call the resulting\npropagation matrix the augmented normalized adjacency\nmatrix ˜Sadj = ˜D−1/2 ˜A ˜D−1/2, where ˜A = A + I and\n˜D = D + I. Correspondingly, we deﬁne the augmented\nnormalized Laplacian ˜∆sym = I − ˜D−1/2 ˜A ˜D−1/2. Thus,\nwe can describe the spectral ﬁlters associated with˜Sadj as a\npolynomial of the eigenvalues of the underlying Laplacian,\ni.e., ˆg(˜λi) = (1 − ˜λi)K, where ˜λi are eigenvalues of ˜∆sym.\nWe now analyze the spectrum of ˜∆sym and show that adding\nself-loops to graphs shrinks the spectrum (eigenvalues) of\nthe corresponding normalized Laplacian.\nTheorem 1. Let A be the adjacency matrix of an undirected,\nweighted, simple graph G without isolated nodes and with\ncorresponding degree matrixD. Let ˜A = A+γI, such that\nγ >0, be the augmented adjacency matrix with correspond-\ning degree matrix ˜D. Also, letλ1 andλn denote the smallest\nand largest eigenvalues of∆sym = I−D−1/2AD−1/2; sim-\nilarly, let ˜λ1 and ˜λn be the smallest and largest eigenvalues\nof ˜∆sym = I − ˜D−1/2 ˜A ˜D−1/2. We have that\n0 =λ1 = ˜λ1 < ˜λn <λ n. (12)\nTheorem 1 shows that the largest eigenvalue of the normal-\nized graph Laplacian becomes smaller after adding self-\nloopsγ >0 (see supplementary materials for the proof).\nFigure 2 depicts the ﬁltering operations associated with\nthe normalized adjacency Sadj = D−1/2AD−1/2 and its\naugmented variant ˜Sadj = ˜D−1/2 ˜A ˜D−1/2 on the Cora\ndataset (Sen et al., 2008). Feature propagation with Sadj cor-\nresponds to ﬁltersg(λi) = (1 −λi)K in the spectral range\nSimplifying Graph Convolutional Networks\n0 1 2\nEigenvalue (λ)\n0\n2\n4\n8Spectral Coefﬁcient\nFirst-Order Chebyshev\n0 1 2\nEigenvalue (λ)\n−1\n0\n1\nNormalized Adj.\nK = 1 K = 2 K = 3 K = 4 K = 5 K = 6\n0 1 2\nEigenvalue (λ)\n−1\n0\n1\nAugmented Normalized Adj.\nFigure 2. Feature (red) and ﬁlters (blue) spectral coefﬁcients for different propagation matrices on Cora dataset (3rd feature).\n[0, 2]; therefore odd powers of Sadj yield negative ﬁlter coef-\nﬁcients at frequenciesλi > 1. By adding self-loops (˜Sadj),\nthe largest eigenvalue shrinks from 2 to approximately 1.5\nand then eliminates the effect of negative coefﬁcients. More-\nover, this scaled spectrum allows the ﬁlter deﬁned by taking\npowersK >1 of ˜Sadj to act as a low-pass-type ﬁlters. In\nsupplementary material, we empirically evaluate different\nchoices for the propagation matrix.\n4. Related Works\n4.1. Graph Neural Networks\nBruna et al. (2014) ﬁrst propose a spectral graph-based\nextension of convolutional networks to graphs. In a follow-\nup work, ChebyNets (Defferrard et al., 2016) deﬁne graph\nconvolutions using Chebyshev polynomials to remove the\ncomputationally expensive Laplacian eigendecomposition.\nGCNs (Kipf & Welling, 2017) further simplify graph con-\nvolutions by stacking layers of ﬁrst-order Chebyshev poly-\nnomial ﬁlters with a redeﬁned propagation matrixS. Chen\net al. (2018) propose an efﬁcient variant of GCN based on\nimportance sampling, and Hamilton et al. (2017) propose\na framework based on sampling and aggregation. Atwood\n& Towsley (2016), Abu-El-Haija et al. (2018), and Liao\net al. (2019) exploit multi-scale information by raising S to\nhigher order. Xu et al. (2019) study the expressiveness of\ngraph neural networks in terms of their ability to distinguish\nany two graphs and introduce Graph Isomorphism Network,\nwhich is proved to be as powerful as the Weisfeiler-Lehman\ntest for graph isomorphism. Klicpera et al. (2019) separate\nthe non-linear transformation from propagation by using a\nneural network followed by a personalized random walk.\nThere are many other graph neural models (Monti et al.,\n2017; Duran & Niepert, 2017; Li et al., 2018); we refer to\nZhou et al. (2018); Battaglia et al. (2018); Wu et al. (2019)\nfor a more comprehensive review.\nPrevious publications have pointed out that simpler, some-\ntimes linear models can be effective for node/graph classi-\nﬁcation tasks. Thekumparampil et al. (2018) empirically\nshow that a linear version of GCN can perform competitively\nand propose an attention-based GCN variant. Cai & Wang\n(2018) propose an effective linear baseline for graph classi-\nﬁcation using node degree statistics. Eliav & Cohen (2018)\nshow that models which use linear feature/label propaga-\ntion steps can beneﬁt from self-training strategies. Li et al.\n(2019) propose a generalized version of label propagation\nand provide a similar spectral analysis of the renormaliza-\ntion trick.\nGraph Attentional Models learn to assign different edge\nweights at each layer based on node features and have\nachieved state-of-the-art results on several graph learning\ntasks (Velickovic et al., 2018; Thekumparampil et al., 2018;\nZhang et al., 2018a; Kampffmeyer et al., 2018). However,\nthe attention mechanism usually adds signiﬁcant overhead\nto computation and memory usage. We refer the readers to\nLee et al. (2018) for further comparison.\n4.2. Other Works on Graphs\nGraph methodologies can roughly be categorized into two\napproaches: graph embedding methods and graph laplacian\nregularization methods. Graph embedding methods (Weston\net al., 2008; Perozzi et al., 2014; Yang et al., 2016; Velikovi\net al., 2019) represent nodes as high-dimensional feature\nvectors. Among them, DeepWalk (Perozzi et al., 2014) and\nDeep Graph Infomax (DGI) (Velikovi et al., 2019) use un-\nsupervised strategies to learn graph embeddings. DeepWalk\nrelies on truncated random walk and uses a skip-gram model\nto generate embeddings, whereas DGI trains a graph convo-\nlutional encoder through maximizing mutual information.\nGraph Laplacian regularization (Zhu et al., 2003; Zhou et al.,\n2004; Belkin & Niyogi, 2004; Belkin et al., 2006) introduce\na regularization term based on graph structure which forces\nnodes to have similar labels to their neighbors. Label Prop-\nagation (Zhu et al., 2003) makes predictions by spreading\nlabel information from labeled nodes to their neighbors until\nconvergence.\nSimplifying Graph Convolutional Networks\n100 101 102 103\nRelative Training Time\n77\n78\n79\n80Test Acc (%)\nSGC\n1x\nFastGCN\n6x\nGCN\n28x\nGIN\n89x\nDGI\n260x\nGAT\n415x\nAdaLNet\n758x\nLNet\n909x\nPubmed\n100 101 102\nRelative Training Time\n92\n93\n94\n95\n96\n97Test F1 (%)\nSGC\n1x\nSAGE-mean\n29x\nSAGE-GCN\n32x\nFastGCN\n100x\nSAGE-LSTM\n180x\nDGI\nGaAN\nReddit\nFigure 3. Performance over training time on Pubmed and Reddit. SGC is the fastest while achieving competitive performance. We are not\nable to benchmark the training time of GaAN and DGI on Reddit because the implementations are not released.\nTable 1. Dataset statistics of the citation networks and Reddit.\nDataset # Nodes # Edges Train/Dev/Test Nodes\nCora 2, 708 5 , 429 140 /500/1, 000\nCiteseer 3, 327 4 , 732 120 /500/1, 000\nPubmed 19, 717 44 , 338 60 /500/1, 000\nReddit 233K 11.6M 152K/24K/55K\n5. Experiments and Discussion\nWe ﬁrst evaluate SGC on citation networks and social net-\nworks and then extend our empirical analysis to a wide\nrange of downstream tasks.\n5.1. Citation Networks & Social Networks\nWe evaluate the semi-supervised node classiﬁcation perfor-\nmance of SGC on the Cora, Citeseer, and Pubmed citation\nnetwork datasets (Table 2) (Sen et al., 2008). We supplement\nour citation network analysis by using SGC to inductively\npredict community structure on Reddit (Table 3), which\nconsists of a much larger graph. Dataset statistics are sum-\nmarized in Table 1.\nDatasets and experimental setup. On the citation net-\nworks, we train SGC for 100 epochs using Adam (Kingma\n& Ba, 2015) with learning rate 0.2. In addition, we use\nweight decay and tune this hyperparameter on each dataset\nusing hyperopt (Bergstra et al., 2015) for 60 iterations on\nthe public split validation set. Experiments on citation net-\nworks are conducted transductively. On the Reddit dataset,\nwe train SGC with L-BFGS (Liu & Nocedal, 1989) using\nno regularization, and remarkably, training converges in 2\nsteps. We evaluate SGC inductively by following Chen et al.\n(2018): we train SGC on a subgraph comprising only train-\ning nodes and test with the original graph. On all datasets,\nwe tune the number of epochs based on both convergence\nbehavior and validation accuracy.\nTable 2. Test accuracy (%) averaged over 10 runs on citation net-\nworks. †We remove the outliers (accuracy < 75/65/75%) when\ncalculating their statistics due to high variance.\nCora Citeseer Pubmed\nNumbers from literature:\nGCN 81.5 70.3 79.0\nGAT 83.0± 0.7 72.5± 0.7 79.0± 0.3\nGLN 81.2± 0.1 70.9± 0.1 78.9± 0.1\nAGNN 83.1± 0.1 71.7± 0.1 79.9± 0.1\nLNet 79.5± 1.8 66.2± 1.9 78.3± 0.3\nAdaLNet 80.4± 1.1 68.7± 1.0 78.1± 0.4\nDeepWalk 70.7± 0.6 51.4± 0.5 76.8± 0.6\nDGI 82.3± 0.6 71.8± 0.7 76.8± 0.6\nOur experiments:\nGCN 81.4± 0.4 70.9± 0.5 79.0± 0.4\nGAT 83.3± 0.7 72.6± 0.6 78.5± 0.3\nFastGCN 79.8± 0.3 68.8± 0.6 77.4± 0.3\nGIN 77.6± 1.1 66.1± 0.9 77.0± 1.2\nLNet 80.2± 3.0† 67.3± 0.5 78.3± 0.6†\nAdaLNet 81.9± 1.9† 70.6± 0.8† 77.8± 0.7†\nDGI 82.5± 0.7 71.6± 0.7 78.4± 0.7\nSGC 81.0± 0.0 71.9± 0.1 78.9± 0.0\nTable 3. Test Micro F1 Score (%) averaged over 10 runs on Red-\ndit. Performances of models are cited from their original papers.\nOOM: Out of memory.\nSetting Model Test F1\nSupervised\nGaAN 96.4\nSAGE-mean 95.0\nSAGE-LSTM 95.4\nSAGE-GCN 93.0\nFastGCN 93.7\nGCN OOM\nUnsupervised\nSAGE-mean 89.7\nSAGE-LSTM 90.7\nSAGE-GCN 90.8\nDGI 94.0\nNo Learning Random-Init DGI 93.3\nSGC 94.9\nSimplifying Graph Convolutional Networks\nBaselines. For citation networks, we compare against\nGCN (Kipf & Welling, 2017) GAT (Velickovic et al., 2018)\nFastGCN (Chen et al., 2018) LNet, AdaLNet (Liao et al.,\n2019) and DGI (Velikovi et al., 2019) using the publicly\nreleased implementations. Since GIN is not initially eval-\nuated on citation networks, we implement GIN following\nXu et al. (2019) and use hyperopt to tune weight decay and\nlearning rate for 60 iterations. Moreover, we tune the hidden\ndimension by hand.\nFor Reddit, we compare SGC to the reported performance of\nGaAN (Zhang et al., 2018a), supervised and unsupervised\nvariants of GraphSAGE (Hamilton et al., 2017), FastGCN,\nand DGI. Table 3 also highlights the setting of the feature\nextraction step for each method. We note that SGC involves\nno learning because the feature extraction step,SKX, has no\nparameter. Both unsupervised and no-learning approaches\ntrain logistic regression models with labels afterward.\nPerformance. Based on results in Table 2 and Table 3, we\nconclude that SGC is very competitive. Table 2 shows the\nperformance of SGC can match the performance of GCN\nand state-of-the-art graph networks on citation networks. In\nparticular on Citeseer, SGC is about 1% better than GCN,\nand we reason this performance boost is caused by SGC\nhaving fewer parameters and therefore suffering less from\noverﬁtting. Remarkably, GIN performs slight worse because\nof overﬁtting. Also, both LNet and AdaLNet are unstable\non citation networks. On Reddit, Table 3 shows that SGC\noutperforms the previous sampling-based GCN variants,\nSAGE-GCN and FastGCN by more than 1%.\nNotably, Velikovi et al. (2019) report that the performance of\na randomly initialized DGI encoder nearly matches that of a\ntrained encoder; however, both models underperform SGC\non Reddit. This result may suggest that the extra weights\nand nonlinearities in the DGI encoder are superﬂuous, if not\noutright detrimental.\nEfﬁciency. In Figure 3, we plot the performance of the\nstate-of-the-arts graph networks over their training time rel-\native to that of SGC on the Pubmed and Reddit datasets. In\nparticular, we precompute SKX and the training time of\nSGC takes into account this precomputation time. We mea-\nsure the training time on a NVIDIA GTX 1080 Ti GPU and\npresent the benchmark details in supplementary materials.\nOn large graphs (e.g. Reddit), GCN cannot be trained due\nto excessive memory requirements. Previous approaches\ntackle this limitation by either sampling to reduce neigh-\nborhood size (Chen et al., 2018; Hamilton et al., 2017) or\nlimiting their model sizes (Velikovi et al., 2019). By apply-\ning a ﬁxed ﬁlter and precomputingSKX, SGC minimizes\nmemory usage and only learns a single weight matrix during\ntraining. Since S is typically sparse andK is usually small,\nTable 4. Test Accuracy (%) on text classiﬁcation datasets. The\nnumbers are averaged over 10 runs.\nDataset Model Test Acc.↑ Time (seconds)↓\n20NG GCN 87.9± 0.2 1205 .1± 144.5\nSGC 88.5± 0.1 19 .06± 0.15\nR8 GCN 97.0± 0.2 129 .6± 9.9\nSGC 97.2± 0.1 1 .90± 0.03\nR52 GCN 93.8± 0.2 245 .0± 13.0\nSGC 94.0± 0.2 3 .01± 0.01\nOhsumed GCN 68.2± 0.4 252 .4± 14.7\nSGC 68.5± 0.3 3 .02± 0.02\nMR GCN 76.3± 0.3 16 .1± 0.4\nSGC 75.9± 0.3 4 .00± 0.04\nTable 5. Test accuracy (%) within 161 miles on semi-supervised\nuser geolocation. The numbers are averaged over 5 runs.\nDataset Model Acc.@161↑ Time↓\nGEOTEXT GCN+H 60.6± 0.2 153 .0s\nSGC 61.1± 0.1 5 .6s\nTWITTER-US GCN+H 61.9± 0.2 9 h 54m\nSGC 62.5± 0.1 4 h 33m\nTWITTER-WORLD GCN+H 53.6± 0.2 2 d 05h 17m\nSGC 54.1± 0.2 22 h 53m\nwe can exploit fast sparse-dense matrix multiplication to\ncompute SKX. Figure 3 shows that SGC can be trained up\nto two orders of magnitude faster than fast sampling-based\nmethods while having little or no drop in performance.\n5.2. Downstream Tasks\nWe extend our empirical evaluation to 5 downstream appli-\ncations — text classiﬁcation, semi-supervised user geoloca-\ntion, relation extraction, zero-shot image classiﬁcation, and\ngraph classiﬁcation — to study the applicability of SGC.\nWe describe experimental setup in supplementary materials.\nText classiﬁcation assigns labels to documents. Yao et al.\n(2019) use a 2-layer GCN to achieve state-of-the-art results\nby creating a corpus-level graph which treats both docu-\nments and words as nodes in a graph. Word-word edge\nweights are pointwise mutual information (PMI) and word-\ndocument edge weights are normalized TF-IDF scores. Ta-\nble 4 shows that an SGC (K = 2) rivals their model on 5\nbenchmark datasets, while being up to 83.6× faster.\nSemi-supervised user geolocation locates the “home”\nposition of users on social media given users’ posts, con-\nnections among users, and a small number of labelled users.\nRahimi et al. (2018) apply GCNs with highway connections\non this task and achieve close to state-of-the-art results. Ta-\nSimplifying Graph Convolutional Networks\nTable 6. Test Accuracy (%) on Relation Extraction. The numbers\nare averaged over 10 runs.\nTACRED Test Accuracy ↑\nC-GCN (Zhang et al., 2018c) 66.4\nC-GCN 66.4 ± 0.4\nC-SGC 67.0 ± 0.4\nTable 7. Top-1 accuracy (%) averaged over 10 runs in the 2-\nhop and 3-hop setting of the zero-shot image task on ImageNet.\nADGPM (Kampffmeyer et al., 2018) and EXEM 1-nns (Chang-\npinyo et al., 2018) use more powerful visual features.\nModel # Param.↓ 2-hop Acc.↑ 3-hop Acc.↑\nUnseen categories only:\nEXEM 1-nns - 27.0 7.1\nADGPM - 26.6 6.3\nGCNZ - 19.8 4.1\nGCNZ (ours) 9.5M 20.9± 0.2 4.3± 0.0\nMLP-SGCZ (ours) 4.3M 21.2± 0.2 4.4± 0.1\nUnseen categories & seen categories:\nADGPM - 10.3 2.9\nGCNZ - 9.7 2.2\nGCNZ (ours) 9.5M 10.0± 0.2 2.4± 0.0\nMLP-SGCZ (ours) 4.3M 10.5± 0.1 2.5± 0.0\nble 5 shows that SGC outperforms GCN with highway con-\nnections on GEOTEXT (Eisenstein et al., 2010), TWITTER-\nUS (Roller et al., 2012), and TWITTER-WORLD (Han\net al., 2012) under Rahimi et al. (2018)’s framework, while\nsaving 30+ hours on TWITTER-WORLD.\nRelation extraction involves predicting the relation be-\ntween subject and object in a sentence. Zhang et al.\n(2018c) propose C-GCN which uses an LSTM (Hochre-\niter & Schmidhuber, 1997) followed by a GCN and an MLP.\nWe replace GCN with SGC (K = 2) and call the resulting\nmodel C-SGC. Table 6 shows that C-SGR sets new state-of-\nthe-art on TACRED (Zhang et al., 2017).\nZero-shot image classiﬁcation consists of learning an\nimage classiﬁer without access to any images or labels from\nthe test categories. GCNZ (Wang et al., 2018) uses a GCN\nto map the category names — based on their relations in\nWordNet (Miller, 1995) — to image feature domain, and\nﬁnd the most similar category to a query image feature\nvector. Table 7 shows that replacing GCN with an MLP\nfollowed by SGC can improve performance while reducing\nthe number of parameters by 55%. We ﬁnd that an MLP\nfeature extractor is necessary in order to map the pretrained\nGloVe vectors to the space of visual features extracted by\na ResNet-50. Again, this downstream application demon-\nstrates that learned graph convolution ﬁlters are superﬂuous;\nsimilar to Changpinyo et al. (2018)’s observation that GCNs\nmay not be necessary.\nGraph classiﬁcation requires models to use graph struc-\nture to categorize graphs. Xu et al. (2019) theoretically\nshow that GCNs are not sufﬁcient to distinguish certain\ngraph structures and show that their GIN is more expressive\nand achieves state-of-the-art results on various graph classi-\nﬁcation datasets. We replace the GCN in DCGCN (Zhang\net al., 2018b) with an SGC and get 71.0% and 76.2% on\nNCI1 and COLLAB datasets (Yanardag & Vishwanathan,\n2015) respectively, which is on par with an GCN counterpart,\nbut far behind GIN. Similarly, on QM8 quantum chemistry\ndataset (Ramakrishnan et al., 2015), more advanced AdaL-\nNet and LNet (Liao et al., 2019) get 0.01 MAE on QM8,\noutperforming SGC’s 0.03 MAE by a large margin.\n6. Conclusion\nIn order to better understand and explain the mechanisms\nof GCNs, we explore the simplest possible formulation of a\ngraph convolutional model, SGC. The algorithm is almost\ntrivial, a graph based pre-processing step followed by stan-\ndard multi-class logistic regression. However, the perfor-\nmance of SGC rivals — if not surpasses — the performance\nof GCNs and state-of-the-art graph neural network mod-\nels across a wide range of graph learning tasks. Moreover\nby precomputing the ﬁxed feature extractor SK, training\ntime is reduced to a record low. For example on the Reddit\ndataset, SGC can be trained up to two orders of magnitude\nfaster than sampling-based GCN variants.\nIn addition to our empirical analysis, we analyze SGC from\na convolution perspective and manifest this method as a\nlow-pass-type ﬁlter on the spectral domain. Low-pass-type\nﬁlters capture low-frequency signals, which corresponds\nwith smoothing features across a graph in this setting. Our\nanalysis also provides insight into the empirical boost of\nthe “renormalization trick” and demonstrates how shrinking\nthe spectral domain leads to a low-pass-type ﬁlter which\nunderpins SGC.\nUltimately, the strong performance of SGC sheds light onto\nGCNs. It is likely that the expressive power of GCNs origi-\nnates primarily from the repeated graph propagation (which\nSGC preserves) rather than the nonlinear feature extraction\n(which it doesn’t.)\nGiven its empirical performance, efﬁciency, and inter-\npretability, we argue that the SGC should be highly ben-\neﬁcial to the community in at least three ways: (1) as a\nﬁrst model to try, especially for node classiﬁcation tasks;\n(2) as a simple baseline for comparison with future graph\nlearning models; (3) as a starting point for future research in\ngraph learning — returning to the historic machine learning\npractice to develop complex from simple models.\nSimplifying Graph Convolutional Networks\nAcknowledgement\nThis research is supported in part by grants from the\nNational Science Foundation (III-1618134, III-1526012,\nIIS1149882, IIS-1724282, and TRIPODS-1740822), the\nOfﬁce of Naval Research DOD (N00014-17-1-2175), Bill\nand Melinda Gates Foundation, and Facebook Research.\nWe are thankful for generous support by SAP America Inc.\nAmauri Holanda de Souza Jr. thanks CNPq (Brazilian Coun-\ncil for Scientiﬁc and Technological Development) for the\nﬁnancial support. We appreciate the discussion with Xiang\nFu, Shengyuan Hu, Shangdi Yu, Wei-Lun Chao and Geoff\nPleiss as well as the ﬁgure design support from Boyi Li.\nReferences\nAbu-El-Haija, S., Kapoor, A., Perozzi, B., and Lee, J. N-\nGCN: Multi-scale graph convolution for semi-supervised\nnode classiﬁcation. arXiv preprint arXiv:1802.08888 ,\n2018.\nAtwood, J. and Towsley, D. Diffusion-convolutional neural\nnetworks. In Lee, D. D., Sugiyama, M., Luxburg, U. V .,\nGuyon, I., and Garnett, R. (eds.), Advances in Neural In-\nformation Processing Systems 29, pp. 1993–2001. Curran\nAssociates, Inc., 2016.\nBattaglia, P. W., Hamrick, J. B., Bapst, V ., Sanchez-\nGonzalez, A., Zambaldi, V ., Malinowski, M., Tacchetti,\nA., Raposo, D., Santoro, A., Faulkner, R., et al. Rela-\ntional inductive biases, deep learning, and graph networks.\narXiv preprint arXiv:1806.01261, 2018.\nBelkin, M. and Niyogi, P. Semi-supervised learning on\nriemannian manifolds. Machine Learning, 56(1-3):209–\n239, 2004.\nBelkin, M., Niyogi, P., and Sindhwani, V . Manifold regular-\nization: A geometric framework for learning from labeled\nand unlabeled examples. Journal of Machine Learning\nResearch, 7:2399–2434, 2006.\nBergstra, J., Komer, B., Eliasmith, C., Yamins, D., and Cox,\nD. D. Hyperopt: A python library for optimizing the\nhyperparameters of machine learning algorithms. Com-\nputational Science & Discovery, 8(1), 2015.\nBottou, L. Large-scale machine learning with stochastic\ngradient descent. In Proceedings of 19th International\nConference on Computational Statistics , pp. 177–186.\nSpringer, 2010.\nBruna, J., Zaremba, W., Szlam, A., and Lecun, Y . Spectral\nnetworks and locally connected networks on graphs. In\nInternational Conference on Learning Representations\n(ICLR’2014), 2014.\nCai, C. and Wang, Y . A simple yet effective baseline for\nnon-attribute graph classiﬁcation, 2018.\nChangpinyo, S., Chao, W.-L., Gong, B., and Sha, F. Classi-\nﬁer and exemplar synthesis for zero-shot learning. arXiv\npreprint arXiv:1812.06423, 2018.\nChen, J., Ma, T., and Xiao, C. FastGCN: Fast Learning with\nGraph Convolutional Networks via Importance Sampling.\nIn International Conference on Learning Representations\n(ICLR’2018), 2018.\nDefferrard, M., Bresson, X., and Vandergheynst, P. Con-\nvolutional neural networks on graphs with fast localized\nspectral ﬁltering. In Lee, D. D., Sugiyama, M., Luxburg,\nU. V ., Guyon, I., and Garnett, R. (eds.),Advances in Neu-\nral Information Processing Systems 29, pp. 3844–3852.\nCurran Associates, Inc., 2016.\nDuran, A. G. and Niepert, M. Learning graph representa-\ntions with embedding propagation. In Guyon, I., Luxburg,\nU. V ., Bengio, S., Wallach, H., Fergus, R., Vishwanathan,\nS., and Garnett, R. (eds.), Advances in Neural Informa-\ntion Processing Systems 30, pp. 5119–5130. Curran As-\nsociates, Inc., 2017.\nEisenstein, J., O’Connor, B., Smith, N. A., and Xing, E. P.\nA latent variable model for geographic lexical variation.\nIn Proceedings of the 2010 Conference on Empirical\nMethods in Natural Language Processing, pp. 1277–1287.\nAssociation for Computational Linguistics, 2010.\nEliav, B. and Cohen, E. Bootstrapped graph diffusions: Ex-\nposing the power of nonlinearity.Proceedings of the ACM\non Measurement and Analysis of Computing Systems, 2\n(1):10:1–10:19, 2018.\nHamilton, W., Ying, Z., and Leskovec, J. Inductive repre-\nsentation learning on large graphs. In Guyon, I., Luxburg,\nU. V ., Bengio, S., Wallach, H., Fergus, R., Vishwanathan,\nS., and Garnett, R. (eds.), Advances in Neural Informa-\ntion Processing Systems 30, pp. 1024–1034. Curran As-\nsociates, Inc., 2017.\nHan, B., Cook, P., and Baldwin, T. Geolocation prediction\nin social media data by ﬁnding location indicative words.\nIn Proceedings of the 24th International Conference on\nComputational Linguistics, pp. 1045–1062, 2012.\nHariharan, B., Arbel ´aez, P., Girshick, R., and Malik, J.\nHypercolumns for object segmentation and ﬁne-grained\nlocalization. In Proceedings of the IEEE conference on\ncomputer vision and pattern recognition , pp. 447–456,\n2015.\nHarris, C. and Stephens, M. A combined corner and edge de-\ntector. In Proceedings of the 4th Alvey Vision Conference,\npp. 147–151, 1988.\nSimplifying Graph Convolutional Networks\nHochreiter, S. and Schmidhuber, J. Long Short-Term Mem-\nory. Neural Computation, 9:1735–1780, 1997.\nHuang, G., Sun, Y ., Liu, Z., Sedra, D., and Weinberger,\nK. Q. Deep networks with stochastic depth. In European\nConference on Computer Vision, pp. 646–661. Springer,\n2016.\nKampffmeyer, M., Chen, Y ., Liang, X., Wang, H., Zhang, Y .,\nand Xing, E. P. Rethinking knowledge graph propagation\nfor zero-shot learning. arXiv preprint arXiv:1805.11724,\n2018.\nKingma, D. P. and Ba, J. Adam: A method for stochastic\noptimization. In International Conference on Learning\nRepresentations (ICLR’2015), 2015.\nKipf, T. N. and Welling, M. Semi-supervised classiﬁca-\ntion with graph convolutional networks. In International\nConference on Learning Representations (ICLR’2017),\n2017.\nKlicpera, J., Bojchevski, A., and G ¨unnemann, S. Predict\nthen propagate: Graph neural networks meet personal-\nized pagerank. In International Conference on Learning\nRepresentations (ICLR’2019), 2019.\nLeCun, Y ., Boser, B., Denker, J. S., Henderson, D., Howard,\nR. E., Hubbard, W., and Jackel, L. D. Backpropaga-\ntion applied to handwritten zip code recognition. Neural\nComputation, 1(4):541–551, 1989.\nLee, J. B., Rossi, R. A., Kim, S., Ahmed, N. K., and Koh,\nE. Attention models in graphs: A survey. arXiv e-prints,\n2018.\nLi, Q., Han, Z., and Wu, X. Deeper insights into graph con-\nvolutional networks for semi-supervised learning. CoRR,\nabs/1801.07606, 2018.\nLi, Q., Wu, X.-M., Liu, H., Zhang, X., and Guan, Z. Label\nefﬁcient semi-supervised learning via graph ﬁltering. In\nIEEE/CVF Conference on Computer Vision and Pattern\nRecognition (CVPR), 2019.\nLiao, R., Zhao, Z., Urtasun, R., and Zemel, R. Lanczos-\nnet: Multi-scale deep graph convolutional networks. In\nInternational Conference on Learning Representations\n(ICLR’2019), 2019.\nLiu, D. C. and Nocedal, J. On the limited memory BFGS\nmethod for large scale optimization. Mathematical Pro-\ngramming, 45(1-3):503–528, 1989.\nMiller, G. A. Wordnet: a lexical database for english. Com-\nmunications of the ACM, 38(11):39–41, 1995.\nMonti, F., Boscaini, D., Masci, J., Rodol`a, E., Svoboda, J.,\nand Bronstein, M. M. Geometric deep learning on graphs\nand manifolds using mixture model cnns. In 2017 IEEE\nConference on Computer Vision and Pattern Recognition,\nCVPR 2017, Honolulu, HI, USA, July 21-26, 2017 , pp.\n5425–5434, 2017.\nPerozzi, B., Al-Rfou, R., and Skiena, S. Deepwalk: Online\nlearning of social representations. In Proceedings of the\n20th ACM SIGKDD International Conference on Knowl-\nedge Discovery and Data Mining, KDD’14, pp. 701–710.\nACM, 2014.\nRahimi, S., Cohn, T., and Baldwin, T. Semi-supervised\nuser geolocation via graph convolutional networks. In\nProceedings of the 56th Annual Meeting of the Associ-\nation for Computational Linguistics (Volume 1: Long\nPapers), pp. 2009–2019. Association for Computational\nLinguistics, 2018.\nRamakrishnan, R., Hartmann, M., Tapavicza, E., and von\nLilienfeld, O. A. Electronic spectra from TDDFT and\nmachine learning in chemical space. The Journal of\nchemical physics, 143(8):084111, 2015.\nRoller, S., Speriosu, M., Rallapalli, S., Wing, B., and\nBaldridge, J. Supervised text-based geolocation using\nlanguage models on an adaptive grid. In Proceedings\nof the 2012 Joint Conference on Empirical Methods in\nNatural Language Processing and Computational Natu-\nral Language Learning, pp. 1500–1510. Association for\nComputational Linguistics, 2012.\nRosenblatt, F. The Perceptron: a probabilistic model for\ninformation storage and organization in the brain. Psy-\nchological review, 65(6):386, 1958.\nRosenblatt, F. Principles of neurodynamics: Perceptrons\nand the theory of brain mechanisms. Technical report,\nCornell Aeronautical Lab Inc, 1961.\nSen, P., Namata, G., Bilgic, M., Getoor, L., Galligher, B.,\nand Eliassi-Rad, T. Collective classiﬁcation in network\ndata. AI magazine, 29(3):93, 2008.\nSobel, I. and Feldman, G. A 3x3 isotropic gradient operator\nfor image processing. A talk at the Stanford Artiﬁcial\nProject, pp. 271–272, 1968.\nThekumparampil, K. K., Wang, C., Oh, S., and Li,\nL. Attention-based graph neural network for semi-\nsupervised learning. arXiv e-prints, 2018.\nVelickovic, P., Cucurull, G., Casanova, A., Romero, A.,\nLi`o, P., and Bengio, Y . Graph Attention Networks. In\nInternational Conference on Learning Representations\n(ICLR’2018), 2018.\nSimplifying Graph Convolutional Networks\nVelikovi, P., Fedus, W., Hamilton, W. L., Li, P., Bengio, Y .,\nand Hjelm, R. D. Deep Graph InfoMax. In International\nConference on Learning Representations (ICLR’2019),\n2019.\nWaibel, A., Hanazawa, T., and Hinton, G. Phoneme recog-\nnition using time-delay neural networks. IEEE Transac-\ntions on Acoustics, Speech. and Signal Processing, 37(3),\n1989.\nWang, X., Ye, Y ., and Gupta, A. Zero-shot recognition via\nsemantic embeddings and knowledge graphs. In Com-\nputer Vision and Pattern Recognition (CVPR), pp. 6857–\n6866. IEEE Computer Society, 2018.\nWeston, J., Ratle, F., and Collobert, R. Deep learning via\nsemi-supervised embedding. In Proceedings of the 25th\nInternational Conference on Machine Learning, ICML\n’08, pp. 1168–1175, 2008.\nWu, Z., Pan, S., Chen, F., Long, G., Zhang, C., and Yu, P. S.\nA comprehensive survey on graph neural networks. arXiv\npreprint arXiv:1901.00596, 2019.\nXu, K., Hu, W., Leskovec, J., and Jegelka, S. How powerful\nare graph neural networks? In International Conference\non Learning Representations (ICLR’2019), 2019.\nYanardag, P. and Vishwanathan, S. Deep graph kernels.\nIn Proceedings of the 21th ACM SIGKDD International\nConference on Knowledge Discovery and Data Mining,\npp. 1365–1374. ACM, 2015.\nYang, Z., Cohen, W. W., and Salakhutdinov, R. Revisit-\ning semi-supervised learning with graph embeddings. In\nProceedings of the 33rd International Conference on In-\nternational Conference on Machine Learning - Volume\n48, ICML’16, pp. 40–48, 2016.\nYao, L., Mao, C., and Luo, Y . Graph convolutional networks\nfor text classiﬁcation. In Proceedings of the 33rd AAAI\nConference on Artiﬁcial Intelligence (AAAI’19), 2019.\nZhang, J., Shi, X., Xie, J., Ma, H., King, I., and Yeung,\nD. Gaan: Gated attention networks for learning on large\nand spatiotemporal graphs. In Proceedings of the Thirty-\nFourth Conference on Uncertainty in Artiﬁcial Intelli-\ngence (UAI’2018), pp. 339–349. AUAI Press, 2018a.\nZhang, M., Cui, Z., Neumann, M., and Chen, Y . An end-\nto-end deep learning architecture for graph classiﬁcation.\n2018b.\nZhang, Y ., Zhong, V ., Chen, D., Angeli, G., and Manning,\nC. D. Position-aware attention and supervised data im-\nprove slot ﬁlling. In Proceedings of the 2017 Conference\non Empirical Methods in Natural Language Processing,\npp. 35–45. Association for Computational Linguistics,\n2017.\nZhang, Y ., Qi, P., and Manning, C. D. Graph convolution\nover pruned dependency trees improves relation extrac-\ntion. In Empirical Methods in Natural Language Process-\ning (EMNLP), 2018c.\nZhou, D., Bousquet, O., Thomas, N. L., Weston, J., and\nSch¨olkopf, B. Learning with local and global consistency.\nIn Thrun, S., Saul, L. K., and Sch ¨olkopf, B. (eds.), Ad-\nvances in Neural Information Processing Systems 16, pp.\n321–328. MIT Press, 2004.\nZhou, J., Cui, G., Zhang, Z., Yang, C., Liu, Z., and Sun,\nM. Graph Neural Networks: A Review of Methods and\nApplications. arXiv e-prints, 2018.\nZhu, X., Ghahramani, Z., and Lafferty, J. Semi-supervised\nlearning using gaussian ﬁelds and harmonic functions.\nIn Proceedings of the Twentieth International Confer-\nence on International Conference on Machine Learning,\nICML’03, pp. 912–919. AAAI Press, 2003.",
  "values": {
    "Explicability": "No",
    "Deferral to humans": "No",
    "Interpretable (to users)": "No",
    "Transparent (to users)": "No",
    "Non-maleficence": "No",
    "Beneficence": "No",
    "Autonomy (power to decide)": "No",
    "Respect for Law and public interest": "No",
    "Critiqability": "No",
    "Fairness": "No",
    "Privacy": "No",
    "Not socially biased": "No",
    "Collective influence": "No",
    "Justice": "No",
    "User influence": "No",
    "Respect for Persons": "No"
  }
}