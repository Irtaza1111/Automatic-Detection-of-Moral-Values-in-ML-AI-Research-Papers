{
  "pdf": "ying19a",
  "title": "NAS-Bench-101: Towards Reproducible Neural Architecture Search",
  "author": "Chris Ying, Aaron Klein, Esteban Real, Eric Christiansen, Kevin Murphy, Frank Hutter",
  "paper_id": "ying19a",
  "text": "NAS-Bench-101: Towards Reproducible Neural Architecture Search\nChris Ying * 1 Aaron Klein * 2 Esteban Real 1 Eric Christiansen 1 Kevin Murphy 1 Frank Hutter 2\nAbstract\nRecent advances in neural architecture search\n(NAS) demand tremendous computational re-\nsources, which makes it difﬁcult to reproduce\nexperiments and imposes a barrier-to-entry to re-\nsearchers without access to large-scale computa-\ntion. We aim to ameliorate these problems by in-\ntroducing NAS-Bench-101, the ﬁrst public archi-\ntecture dataset for NAS research. To build NAS-\nBench-101, we carefully constructed a compact,\nyet expressive, search space, exploiting graph iso-\nmorphisms to identify 423k unique convolutional\narchitectures. We trained and evaluated all of\nthese architectures multiple times on CIFAR-10\nand compiled the results into a large dataset of\nover 5 million trained models. This allows re-\nsearchers to evaluate the quality of a diverse range\nof models in milliseconds by querying the pre-\ncomputed dataset. We demonstrate its utility by\nanalyzing the dataset as a whole and by bench-\nmarking a range of architecture optimization al-\ngorithms.\n1. Introduction\nMany successes in deep learning (Krizhevsky et al., 2012;\nGoodfellow et al., 2014; Sutskever et al., 2014) have re-\nsulted from novel neural network architecture designs. For\nexample, in the ﬁeld of image classiﬁcation, research has\nproduced numerous ways of combining neural network lay-\ners into unique architectures, such as Inception modules\n(Szegedy et al., 2015), residual connections (He et al., 2016),\nor dense connections (Huang et al., 2017). This prolifera-\ntion of choices has fueled research into neural architecture\nsearch (NAS), which casts the discovery of new architec-\n*Equal contribution 1Google Brain, Mountain\nView, California, USA 2Department of Computer Sci-\nence, University of Freiburg, Germany. Correspon-\ndence to: Chris Ying <contact@chrisying.net>, Aaron\nKlein <kleinaa@cs.uni-freiburg.de>, Esteban Real\n<ereal@google.com>.\nProceedings of the 36 th International Conference on Machine\nLearning, Long Beach, California, PMLR 97, 2019. Copyright\n2019 by the author(s).\ntures as an optimization problem (Baker et al., 2017; Zoph\n& Le, 2016; Real et al., 2017; Elsken et al., 2019). This\nhas resulted in state of the art performance in the domain\nof image classiﬁcation (Zoph et al., 2018; Real et al., 2018;\nHuang et al., 2018), and has shown promising results in\nother domains, such as sequence modeling (Zoph & Le,\n2016; So et al., 2019).\nUnfortunately, NAS research is notoriously hard to repro-\nduce (Li & Talwalkar, 2019; Sciuto et al., 2019). First,\nsome methods require months of compute time (e.g., Zoph\net al., 2018), making these methods inaccessible to most\nresearchers. Second, while recent improvements (Liu et al.,\n2018a; Pham et al., 2018; Liu et al., 2018b) have yielded\nmore efﬁcient methods, different methods are not compa-\nrable to each other due to different training procedures and\ndifferent search spaces, which make it difﬁcult to attribute\nthe success of each method to the search algorithm itself.\nTo address the issues above, this paper introduces NAS-\nBench-101, the ﬁrst architecture-dataset for NAS. To build\nit, we trained and evaluated a large number of different con-\nvolutional neural network (CNN) architectures on CIFAR-\n10 (Krizhevsky & Hinton, 2009), utilizing over 100 TPU\nyears of computation time. We compiled the results into a\nlarge table which maps 423k unique architectures to metrics\nincluding run time and accuracy. This enables NAS experi-\nments to be run via querying a table instead of performing\nthe usual costly train and evaluate procedure. Moreover, the\ndata, search space, and training code is fully public 1, to\nfoster reproducibility in the NAS community.\nBecause NAS-Bench-101 exhaustively evaluates a search\nspace, it permits, for the ﬁrst time, a comprehensiveanalysis\nof a NAS search space as a whole. We illustrate such po-\ntential by measuring search space properties relevant to\narchitecture search. Finally, we demonstrate its application\nto the analysis of algorithms by benchmarking a wide range\nof open source architecture/hyperparameter search meth-\nods, including evolutionary approaches, random search, and\nBayesian optimization.\nIn summary, our contributions are the following:\n• We introduce NAS-Bench-101, the ﬁrst large-scale, open-\n1 Data and code for NAS-Bench-101 available at https://\ngithub.com/google-research/nasbench.\nNAS-Bench-101\nsource architecture dataset for NAS (Section 2);\n• We illustrate how to use the dataset to analyze the nature\nof the search space, revealing insights which may guide\nthe design of NAS algorithms (Section 3);\n• We illustrate how to use the dataset to perform fast bench-\nmarking of various open-source NAS optimization algo-\nrithms (Section 4).\n2. The NASBench Dataset\nThe NAS-Bench-101 dataset is a table which maps neural\nnetwork architectures to their training and evaluation met-\nrics. Most NAS approaches to date have trained models on\nthe CIFAR-10 classiﬁcation set because its small images\nallow relatively fast neural network training. Furthermore,\nmodels which perform well on CIFAR-10 tend to perform\nwell on harder benchmarks, such as ImageNet (Krizhevsky\net al., 2012) when scaled up (Zoph et al., 2018)). For these\nreasons, we also use CNN training on CIFAR-10 as the basis\nof NAS-Bench-101.\n2.1. Architectures\nSimilar to other NAS approaches, we restrict our search\nfor neural net topologies to the space of small feedforward\nstructures, usually called cells, which we describe below.\nWe stack each cell 3 times, followed by a downsampling\nlayer, in which the image height and width are halved via\nmax-pooling and the channel count is doubled. We repeat\nthis pattern 3 times, followed by global average pooling and\na ﬁnal dense softmax layer. The initial layer of the model\nis a stem consisting of one\n3× 3 convolution with 128\noutput channels. See Figure 1, top-left, for an illustration\nof the overall network structure. Note that having a stem\nfollowed by stacks of cells is a common pattern both in\nhand-designed image classiﬁers (He et al., 2016; Huang\net al., 2017; Hu et al., 2018) and in NAS search spaces for\nimage classiﬁcation. Thus, the variation in the architectures\narises from variation in the cells.\nThe space of cell architectures consists of all possible di-\nrected acyclic graphs onV nodes, where each possible node\nhas one ofL labels, representing the corresponding opera-\ntion. Two of the vertices are specially labeled as operation\nIN and OUT, representing the input and output tensors to\nthe cell, respectively. Unfortunately, this space of labeled\nDAGs grows exponentially in both V andL. In order to\nlimit the size of the space to allow exhaustive enumeration,\nwe impose the following constraints:\n• We setL = 3, using only the following operations:\n– 3× 3 convolution\n– 1× 1 convolution\n– 3× 3 max-pool\n• We limitV ≤ 7.\n• We limit the maximum number of edges to 9.\nconv stem\nstack 1\nstack 2\nstack 3\ndownsample\ndownsample\nglobal avg pool\ndense\ncell \n2-3\ncell \n2-2\ncell \n2-1\nin\n1x1 3x3 3x3 \n3x3 \nMP \nout\nin\n1x1 \n3x3 \n3x3 \nMP \n3x3 \nout\nin\nout\nF= \n64\nF= \n64\nF= \n64\n+\n&\nF= \n64\n1x1 proj 1x1 proj\n1x1 proj +\n1x1 proj\nF=128\nF=128\nFigure 1: (top-left) The outer skeleton of each model. (top-\nright) An Inception-like cell with the original 5x5 convolu-\ntion approximated by two 3x3 convolutions (concatenation\nand projection operations omitted). (bottom-left) The cell\nthat attained the lowest mean test error (projection layers\nomitted). (bottom-right) An example cell that demonstrates\nhow channel counts are automatically determined (“+” de-\nnotes addition and “&” denotes concatenation; 1× 1 projec-\ntions are used to scale channel counts).\nAll convolutions utilize batch normalization followed by\nReLU. These constraints were chosen to ensure that the\nsearch space still contains ResNet-like and Inception-like\ncells (He et al., 2016; Szegedy et al., 2016). An example of\nan Inception-like cell is illustrated in Figure 1, top-right. We\nintentionally use convolutions instead of separable convolu-\ntions to match the original designs of ResNet and Inception,\nalthough this comes as the cost of being more parameter-\nheavy than some of the more recent state-of-the-art archi-\ntectures like AmoebaNet (Real et al., 2018).\n2.2. Cell encoding\nThere are multiple ways to encode a cell and different en-\ncodings may favor certain algorithms by biasing the search\nspace. For most of our experiments, we chose to use a very\ngeneral encoding: a 7-vertex directed acyclic graph, rep-\nresented by a 7× 7 upper-triangular binary matrix, and a\nlist of 5 labels, one for each of the 5 intermediate vertices\n(recall that the input and output vertices are ﬁxed) Since\nthere are 21 possible edges in the matrix and 3 possible\noperations for each label, there are 221∗ 35≈ 510M total\nunique models in this encoding. In Supplement S3, we also\nNAS-Bench-101\ndiscuss an alternative encoding.\nHowever, a large number of models in this space are invalid\n(i.e., there is no path from the input vertex, or the number of\ntotal edges exceeds 9). Furthermore, different graphs in this\nencoding may not be computationally unique. The method\nwhich we used to identify and enumerate unique graphs is\ndescribed in Supplement S1. After de-duplication, there are\napproximately 423k unique graphs in the search space.\n2.3. Combine semantics\nTranslating from the graph to the corresponding neural net-\nwork is straightforward, with one exception. When multiple\nedges point to the same vertex, the incoming tensors must\nbe combined. Adding them or concatenating them are both\nstandard techniques. To support both ResNet and Inception-\nlike cells and to keep the space tractable, we adopted the\nfollowing ﬁxed rule: tensors going to the output vertex\nare concatenated and those going into other vertices are\nsummed. The output tensors from the input vertex are pro-\njected in order to match the expected input channel counts\nof the subsequent operations. This is illustrated in Figure 1,\nbottom-right.\n2.4. Training\nThe training procedure forms an important part of an ar-\nchitecture search benchmark, since different training proce-\ndures can lead to very substantial performance differences.\nTo counter this issue and allow comparisons of NAS algo-\nrithms on equal grounds, we designed and open-sourced a\nsingle general training pipeline for all models in the dataset.\nChoice of hyperparameters. We utilize a single, ﬁxed set\nof hyperparameters for all NAS-Bench-101 models. This set\nof hyperparameters was chosen to be robust across different\narchitectures by performing a coarse grid search optimiz-\ning the average accuracy of a set of 50 randomly-sampled\narchitectures from the space. This is similar to standard\npractice in the literature (Zoph et al., 2018; Liu et al., 2018a;\nReal et al., 2018) and is further justiﬁed by our experimental\nanalysis in Section 5.1.\nImplementation details. All models are trained and evalu-\nated on CIFAR-10 (40k training examples, 10k validation\nexamples, 10k testing examples), using standard data aug-\nmentation techniques (He et al., 2016). The learning rate\nis annealed via cosine decay (Loshchilov & Hutter, 2017)\nto 0 in order to reduce the variance between multiple inde-\npendent training runs. Training is performed via RMSProp\n(Tieleman & Hinton, 2012) on the cross-entropy loss with\nL2 weight decay. All models were trained on the TPU v2 ac-\ncelerator. The code, implemented in TensorFlow, along with\nall chosen hyperparameters, is publicly available at\nhttps:\n//github.com/google-research/nasbench.\n3 repeats and 4 epoch budgets. We repeat the train-\ning and evaluation of all architectures 3 times to ob-\ntain a measure of variance. Also, in order to allow the\nevaluation of multi-ﬁdelity optimization methods, e.g.,\nHyperband (Li et al., 2018)), we trained all our archi-\ntectures with four increasing epoch budgets:\nEstop ∈\n{Emax/33,E max/32,E max/3,E max} = {4, 12, 36, 108}\nepochs. In each case, the learning rate is annealed to 0 by\nepochEstop.2 We thus trained 3× 423k∼ 1.27M models\nfor each value ofEstop, and thus 4× 1.27M∼ 5M models\noverall.\n2.5. Metrics\nWe evaluated each architectureA after training three times\nwith different random initializations, and did this for each\nof the 4 budgetsEstop above. As a result, the dataset is\na mapping from the (A,E stop, trial#) to the following\nquantities:\n• training accuracy;\n• validation accuracy;\n• testing accuracy;\n• training time in seconds; and\n• number of trainable model parameters.\nOnly metrics on the training and validation set should be\nused to search models within a single NAS algorithm, and\ntesting accuracy should only be used for an ofﬂine evalu-\nation. The training time metric allows benchmarking al-\ngorithms that optimize for accuracy while operating under\na time limit (Section 4) and also allows the evaluation of\nmulti-objective optimization methods. Other metrics that do\nnot require retraining can be computed using the released\ncode.\n2.6. Benchmarking methods\nOne of the central purposes of the dataset is to facilitate\nbenchmarking of NAS algorithms. This section establishes\nrecommended best practices for using NAS-Bench-101\nwhich we followed in our subsequent analysis; we also\nrefer to Supplement S6 for a full set of best practices in\nbenchmarking with NAS-Bench-101.\nThe goal of NAS algorithms is to ﬁnd architectures that\nhave high testing accuracy at epochEmax. To do this, we\nrepeatedly query the dataset at (A,E stop) pairs, whereA is\nan architecture in the search space andEstop is an allowed\nnumber of epochs (Estop∈{ 4, 12, 36, 108}). Each query\ndoes a look-up using a random trial index, drawn uniformly\n2 Instead of 4 epoch budgets, we could have trained single\nlong runs and used the performance at intermediate checkpoints\nas benchmarking data for early stopping algorithms. However,\nbecause of the learning rate schedule, such checkpoints would have\noccurred when the learning rates are still high, leading to noisy\naccuracies that do not correlate well with the ﬁnal performance.\nNAS-Bench-101\nat random from{1, 2, 3}, to simulate the stochasticity of\nSGD training.\nWhile searching, we keep track of the best architecture ˆAi\nthe algorithm has found after each function evaluationi, as\nranked by its validation accuracy. To best simulate real\nworld computational constratints, we stop the search run\nwhen the total “training time” exceeds a ﬁxed limit. After\neach complete search rollout, we query the corresponding\nmean test accuracy f ( ˆAi) for that model (test accuracy\nshould never be used to guide the search itself). Then we\ncompute the immediate test regret:r( ˆAi) = f ( ˆAi)−f (A∗),\nwhere A∗ denotes the model with the highest mean test\naccuracy in the entire dataset. This regret becomes the score\nfor the search run. To measure the robustness of different\nsearch algorithms, a large number of independent search\nrollouts should be conducted.\n3. NASBench as a Dataset\nIn this section, we analyze the NAS-Bench-101 dataset\nas a whole to gain some insight into the role of neural\nnetwork operations and cell topology in the performance\nof convolutional neural networks. In doing so, we hope to\nshed light on the loss landscape that is traversed by NAS\nalgorithms.\n3.1. Dataset statistics\nFirst we study the empirical cumulative distribution (ECDF)\nof various metrics across all architectures in Figure 2. Most\nof the architectures converge and reach 100% training ac-\ncuracy. The validation accuracy and test accuracy are both\nabove 90% for a majority of models. The best architec-\nture in our dataset (Figure 1) achieved a mean test ac-\ncuracy of 94.32%. For comparison, the ResNet-like and\nInception-like cells attained 93.12% and 92.95%, respec-\ntively, which is roughly in-line with the performance of the\noriginal ResNet-56 (93.03%) on CIFAR-10 (He et al., 2016).\nWe observed that the correlation between validation and test\naccuracy is extremely high (r = 0.999) at 108 epochs which\nsuggests that strong optimizers are unlikely to overﬁt on the\nvalidation error. Due to the stochastic nature of the training\nprocess, training and evaluating the same architecture will\ngenerally lead to a small amount of noise in the accuracy.\nWe also observe, as expected, that the noise between runs is\nlower at longer training epochs.\nFigure 3 investigates the relationship between the number\nof parameters, training time, and validation accuracy of\nmodels in the dataset. The left plot suggests that there is\npositive correlation between all of these quantities. However\nparameter count and training time are not the only factors\nsince the best cell in the dataset is not the most computa-\ntionally intensive one. Hand-designed cells, such as ResNet\n0.2 0.4 0.6 0.8 1.0\naccuracy\n0.0\n0.5\n1.0\nECDF\ntest\ntrain\nvalid\n10−4 10−3 10−2 10−1\nnoise\n0.0\n0.5\n1.0\nECDF\n4 epochs\n12 epochs\n36 epochs\n108 epochs\nFigure 2: The empirical cumulative distribution (ECDF)\nof all valid conﬁgurations for: (left) the train/valid/test ac-\ncuracy after training for 108 epochs and (right) the noise,\ndeﬁned as the standard deviation of the test accuracy be-\ntween the three trials, after training for 12, 36 and 108\nepochs.\nFigure 3: (left) Training time vs. trainable parameters, color-\ncoded by validation accuracy. (right) Validation accuracy\nvs. training time with select cell architectures highlighted.\nInception neighbors are the graphs which are 1-edit distance\naway from the Inception cell.\nand Inception, perform near the Pareto frontier of accuracy\nover cost, which suggests that topology and operation selec-\ntion are critical for ﬁnding both high-accuracy and low-cost\nmodels.\n3.2. Architectural design\nNAS-Bench-101 presents us with the unique opportunity to\ninvestigate the impact of various architectural choices on the\nperformance of the network. In Figure 4, we study the effect\nof replacing each of the operations in a cell with a different\noperation. Not surprisingly, replacing a 3× 3 convolution\nwith a 1× 1 convolution or 3× 3 max-pooling operation\ngenerally leads to a drop in absolute ﬁnal validation accuracy\nby 1.16% and 1.99%, respectively. This is also reﬂected\nin the relative change in training time, which decreases by\n14.11% and 9.84%. Even though 3× 3 max-pooling is\nparameter-free, it appears to be on average 5.04% more\nexpensive in training time than 1× 1 convolution and also\nhas an average absolute validation accuracy 0.81% lower.\nHowever, some of the top cells in the space (ranked by mean\ntest accuracy, i.e., Figure 1) contain max-pool operations, so\nother factors must also be at play and replacing all\n3×3 max-\nNAS-Bench-101\nFigure 4: Measuring the aggregated impact of replacing one\noperation with another on (left) absolute validation accuracy\nand (right) relative training time.\nFigure 5: Comparing mean validation accuracy and training\ntime for cells by (left) depth, measured by length of longest\npath from inpu to output, and (right) width, measured by\nmaximum directed cut on the graph.\npooling operations with 1×1 convolutions is not necessarily\na globally optimal choice.\nIn Figure 5, we also investigate the role of depth vs. width.\nIn terms of average validation accuracy, it appears that a\ndepth of 3 is optimal whereas increasing width seems to in-\ncrease the validation accuracy up to 5, the maximum width\nof networks in the dataset. The training time of networks\nincreases as networks get deeper and wider with one ex-\nception: width 1 networks are the most expensive. This\nis a consequence of the combine semantics (see Section\n2.3), which skews the training time distributions because all\nwidth 1 networks are simple feed-forward networks with no\nbranching, and thus the activation maps are never split via\ntheir channel dimension.\n3.3. Locality\nNASBench exhibits locality, a property by which architec-\ntures that are “close by” tend to have similar performance\nmetrics. This property is exploited by many search algo-\nrithms. We deﬁne “closeness” in terms of edit-distance: the\nsmallest number of changes required to turn one architecture\ninto another; one change entails ﬂipping the operation at a\nvertex or the presence/absence of an edge. A popular mea-\nsure of locality is the random-walk autocorrelation (RW A),\ndeﬁned as the autocorrelation of the accuracies of points vis-\nited as we perform a long walk of random changes through\nthe space (Weinberger, 1990; Stadler, 1996). The RWA\n(Figure 6, left) shows high correlations for lower distances,\nindicating locality. The correlations become indistinguish-\nable from noise beyond a distance of about 6.\nFigure 6: (left) RW A for the full space and the FDC relative\nto the global maximum. To plot both curves on a common\nhorizontal axis, the autocorrelation curve is drawn as a func-\ntion of the square root of the autocorrelation shift, to account\nfor the fact that a random walk reaches a mean distance\n√\nN\nafterN steps. (right) Fraction of the search space volume\nthat lies within a given distance to the closest high peak.\nWhile the RW A aggregates across the whole space, we can\nalso consider regions of particular interest. For example,\nFigure 3 (right) displays the neighbors of the Inception-\nlike cell, indicating a degree of locality too, especially in\nterms of accuracy. Another interesting region is that around\na global accuracy maximum. To measure locality within\nthis neighborhood, we used the ﬁtness-distance correlation\nmetric (FDC, Jones et al. (1995)). Figure 6 (left) shows that\nthere is locality around the global maximum as well and the\npeak also has a coarse-grained width of about 6.\nMore broadly, we can consider how rare it is to be near a\nglobal maximum. In the cell encoding described in Sec-\ntion 2.2, the best architecture (i.e., the one with the highest\nmean testing accuracy) has 4 graph isomorphisms, produc-\ning 4 distinct peaks in our encoded search space. Moreover\nthere are 11 other architectures whose mean test accuracy\nis within 2 times standard error of the mean of the best\ngraph. Including the isomorphisms of these, too, there are\n11 570 points in the 510M-point search space corresponding\nto these top graphs, meaning that the chance of hitting one\nof them with a random sample is about 1 to 50000. Figure 6\n(right) shows how much volume of the search space lies\nnear these graphs; in particular, 35.4% of the search space is\nwithin a distance of 6 from the closest top graph. Since the\nbasin of attraction for local search appears to have a width\nof about 6, this suggests that locality-based search may be a\ngood choice for this space.\nNAS-Bench-101\n4. NASBench as a Benchmark\n4.1. Comparing NAS algorithms\nIn this section we establish baselines for future work by\nusing our dataset to compare some popular algorithms for\nwhich open source code is available. Note that the intention\nis not to answer the question “ Which methods work best on\nthis benchmark?”, but rather to demonstrate the utility of a\nreproducible baseline.\nWe benchmarked a small set of NAS and hyperparame-\nter optimization (HPO) algorithms with publicly available\nimplementations: random search (RS) (Bergstra & Ben-\ngio, 2012), regularized evolution (RE) (Real et al., 2018),\nSMAC (Hutter et al., 2011), TPE (Bergstra et al., 2011),\nHyperband (HB) (Li et al., 2018), and BOHB (Falkner et al.,\n2018). We follow the guidelines established in Section 2.6.\nDue to its recent success for NAS (Zoph & Le, 2016), we\nalso include our own implementation of reinforcement learn-\ning (RL) as an additional baseline, since an ofﬁcial imple-\nmentation is not available. However, instead of using an\nLSTM controller, which we found to perform worse, we\nused a categorical distribution for each parameter and op-\ntimized the probability values directly with REINFORCE.\nSupplement S2 has additional implementation details for all\nmethods.\nNAS algorithms based on weight sharing (Pham et al.,\n2018; Liu et al., 2018b) or network morphisms (Cai et al.,\n2018; Elsken et al., 2018) cannot be directly evaluated\non the dataset, so we did not include them. We also do\nnot include Gaussian process–based HPO methods (Shahri-\nari et al., 2016), such as Spearmint (Snoek et al., 2012),\nsince they tend to have problems in high-dimensional dis-\ncrete optimization tasks (Eggensperger et al., 2013). While\nBayesian optimization methods based on Bayesian neural\nnetworks (Snoek et al., 2015; Springenberg et al., 2016)\nare generally applicable to this benchmark, we found their\ncomputational overhead compared to the other methods\nto be prohibitively expensive for an exhaustive empirical\nevaluation. The benchmarking scripts we used are publicly\navailable3. For all optimizers we investigate their own main\nmeta-parameters in Supplement S2.2 (except for TPE where\nthe open-source implementation does not allow to change\nthe meta-parameters) and report here the performance based\non the best found settings.\nFigure 7 (left) shows the mean performance of each of these\nNAS/HPO algorithms across 500 independent trials. The\nx-axis shows estimated wall-clock time, counting the evalua-\ntion of each architecture with the time that the corresponding\ntraining run took. Note that the evaluation of 500 trials of\neach NAS algorithm (for up to 10M simulated TPU sec-\nonds, i.e., 115 TPU days each) was only made possible by\n3 https://github.com/automl/nas_benchmarks\nvirtue of our tabular benchmark; without it, they would have\namounted to over 900 TPU years of computation.\nWe make the following observations:\n• RE, BOHB, and SMAC perform best and start to outper-\nform RS after roughly 50 000 TPU seconds (the equiva-\nlent of roughly 25 evaluated architectures); they achieved\nthe ﬁnal performance of RS about 5 times faster and\ncontinued to improve beyond this point.\n• SMAC, as a Bayesian optimization method, performs\nthis well despite the issue of invalid architectures; we\nbelieve that this is due to its robust random forest model.\nSMAC is slightly slower in the beginning of the search;\nwe assume that this is due to its internal incumbent esti-\nmation procedure (which evaluates the same architecture\nmultiple times).\n• The other Bayesian optimization method, TPE, struggles\nwith this benchmark, with its performance falling back to\nrandom search.\n• The multi-ﬁdelity optimization algorithms HB and BO-\nHB do not yield the speedups frequently observed com-\npared to RS or Bayesian optimization. We attribute this\nto the relatively low rank-correlation between the perfor-\nmance obtained with different budgets (see Figure 7 in\nSupplement S2).\n• BOHB achieves the same test regret as SMAC and RE\nafter recovering from misleading early evaluations; we\nattribute this to the fact, that, compared to TPE, it uses a\nmultivariate instead of a univariate kernel density estima-\ntor.\n• Even though RL starts outperforming RS at roughly the\nsame time as the other methods, it converges much slower\ntowards the global optimum.\nBesides achieving good performance, we argue that robust-\nness, i.e., how sensitive an optimizer is to the randomness\nin both the search algorithm and the training process, plays\nan important role in practice for HPO and NAS methods.\nThis aspect has been neglected in the NAS literature due\nto the extreme cost of performing many repeated runs of\nNAS experiments, but with NAS-Bench-101 performing\nmany repeats becomes trivial. Figure 7 (right) shows the\nempirical cumulative distribution of the regret after 10M\nseconds across all 500 runs of each method. For all meth-\nods, the ﬁnal test regrets ranged over roughly an order of\nmagnitude, with RE, BOHB, and SMAC showing the most\nrobust performance.\n4.2. Generalization bootstrap\nTo test the generalization of our ﬁndings on the dataset, we\nideally would need to run the benchmarked algorithms on\na larger space of architectures. However, due to computa-\ntional limitations, it is infeasible for us to run a large number\nof NAS trials on a meaningfully larger space. Instead, to\nprovide some preliminary evidence of generalization, we\nNAS-Bench-101\n101 102 103 104 105 106 107\nestimated wall-clock time (seconds)\n10−3\n10−2\ntest regret\nHB∗\nRS\nSMAC\nTPE\nRE\nBOHB∗\nRL\n10−3 10−2\nﬁnal test regret\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nCDF\nHB∗\nRS\nSMAC\nTPE\nRE\nBOHB∗\nRL\nFigure 7: (left) Comparison of the performance of various search algorithms. The plot shows the mean performance of 500\nindependent runs as a function of the estimated training time. (right) Robustness of different optimization methods with\nrespect to the seed for the random number generator. *HB and BO-HB are budget-aware algorithms which query the dataset\na shorter epoch lengths. The remaining methods only query the dataset at the longest length (108 epochs).\nperform a bootstrapped experiment: we set aside a subset\nof NAS-Bench-101, dubbed NAS-Bench-Mini, and com-\npare the outcomes of algorithms run on NAS-Bench-Mini\ncompared to the full NAS-Bench-101. NAS-Bench-Mini\ncontains all cells within the search space that utilize\n6 or\nfewer vertices ( 64.5k unique cells), compared to the full\nNAS-Bench-101 that uses up to 7 vertices (423k unique\ncells).\nWe compare two very similar algorithms (regularized evolu-\ntion, RE, and non-regularized evolution, NRE) to a baseline\n(random search, RS). RE and NRE are identical except that\nRE removes the oldest individual in a population to main-\ntain the population size whereas NRE removes the lowest\nﬁtness individual. Figure 8 (top) shows the comparison\non NAS-Bench-Mini and NAS-Bench-101 on 100 trials of\neach algorithm to a ﬁxed time budget. The plots show that\nthe rankings of the three algorithms (RS < NRE < RE)\nare consistent across the smaller dataset and the larger one.\nFurthermore, we demonstrate that NAS-Bench-Mini can\ngeneralize to NAS-Bench-101 for different hyperparame-\nter settings of a single algorithm (regularized evolution)\nin Figure 8 (middle, bottom). This suggests that conclu-\nsions drawn from NAS-Bench-101 may generalize to larger\nsearch spaces.\n5. Discussion\nIn this section, we discuss some of the choices we made\nwhen designing NAS-Bench-101.\n5.1. Relationship to hyperparameter optimization\nAll models in NAS-Bench-101 were trained with a ﬁxed\nset of hyperparameters. In this section, we justify that\nchoice. The problem of hyperparameter optimization (HPO)\nis closely intertwined with NAS. NAS aims to discover good\nneural network architectures while HPO involves ﬁnding\nthe best set of training hyperparameters for a given archi-\ntecture. HPO operates by tuning various numerical neural\nnetwork training parameters (e.g., learning rate) as well as\ncategorical choices ( e.g., optimizer type) to optimize the\ntraining process. Formally, given an architectureA, the task\nof HPO is to ﬁnd its optimal hyperparameter conﬁguration\nH∗:\nH∗(A) = arg max\nH\nf (A,H ),\nwheref is a performance metric, such as validation accu-\nracy and the arg max is over all possible hyperparameter\nconﬁgurations. The “pure” NAS problem can be formu-\nlated as ﬁnding an architecture\nA∗ when all architectures\nare evaluated under optimal hyperparameter choices:\nA∗ = arg max\nA\nf (A,H∗(A)),\nIn practice, this would involve running an inner HPO search\nfor each architecture, which is computationally intractable.\nWe therefore approximateA∗ withA†:\nA∗≈A† = arg max\nA\nf (A,H†),\nwhere H† is a set of hyperparameters that has been es-\ntimated by maximizing the average accuracy on a small\nsubsetS of the architectures:\nH†(S) = arg max\nH\n{f (A,H ) : A∈S}.\nFor example, in Section 2.4,S was a random sample of 50\narchitectures.\nTo justify the approximation above, we performed a study\non a different set of NAS-HPO-Bench (Klein & Hutter,\n2019) datasets (described in detail in Supplement S5) These\nare smaller datasets of architecture–hyperparameter pairs\nNAS-Bench-101\nFigure 8: Generalization bootstrap experiments. Each line\nmarks the median of 100 runs and the shaded region is\nthe 25% to 75% interquartile range. (top) Comparing ran-\ndom search (RS), non-regularized evolution (NRE), and\nregularized evolution (RE) against NAS-Bench-Mini and\nNAS-Bench-101. (middle) Comparing RE runs with dif-\nferent mutation rates. (bottom) Comparing RE runs with\ndifferent tournament sizes.\n(A,H ), where we computed f (A,H ) for all settings of\nA andH. This let us compute the exact hyperparameter-\noptimized accuracy,f∗(A) = max Hf (A,H ). We can also\nmeasure how well this correlates with the approximation we\nuse in NAS-Bench-101. To do this, we chose a set of hyper-\nparametersH† by optimizing the mean accuracy across all\nof the architectures for a given dataset. This allows us to\nmap each architectureA to its approximate hyperparameter-\noptimized accuracy,f†(A) = f (A,H†). (This approximate\naccuracy is analogous to the one computed in the NAS-\nBench-101 metrics, except there the average was over 50\nrandom architectures, not all of them.)\nWe ﬁnd thatf† andf∗ are quite strongly correlated across\nmodels, with a Spearman rank correlation of 0.9155; Fig-\nure 9 provides a scatter plot off∗ againstf† for the archi-\ntectures. The ranking is especially consistent for the best\narchitectures (points near the origin).\n0 25 50 75 100 125 150\nrank arch with ﬁxed hp\n0\n50\n100\n150rank arch with best hp\nFigure 9: Scatter plot between ranks of f∗ (vertical axis)\nandf† (horizontal axis) on the NAS-HPO-Bench-Protein\ndataset. Ideally, the points should be close to the diagonal.\nThe high correlation at low-rank means the best architec-\ntures are ranked identically when usingH∗ andH†.\n5.2. Absolute accuracy of models\nThe choice of search space, hyperparameters, and training\ntechniques were designed to ensure that NAS-Bench-101\nwould be feasible to compute with our resources. Unfortu-\nnately, this means that the models we evaluate do not reach\ncurrent state-of-the-art performance on CIFAR-10. This is\nprimarily because: (1) the search space is constrained in\nboth size and selection of operations and does not contain\nmore complex architectures, such as those used by NASNet\n(Zoph et al., 2018); (2) We do not apply the expensive “aug-\nmentation trick” (Zoph et al., 2018) by which models’ depth\nand width are increased by a large amount and the train-\ning lengthened to hundreds of epochs; and (3) we do not\nutilize more advanced regularization like Cutout (DeVries\n& Taylor, 2017), ScheduledDropPath (Zoph et al., 2018)\nand decoupled weight decay (Loshchilov & Hutter, 2019)\nin order to keep our training pipeline similar to previous\nstandardized models like ResNet.\n6. Conclusion\nWe introduced NAS-Bench-101, a new tabular benchmark\nfor neural architecture search that is inexpensive to evalu-\nate but still preserves the original NAS optimization prob-\nlem, enabling us to rigorously compare various algorithms\nquickly and without the enormous computational budgets\noften used by projects in the ﬁeld. Based on the data we gen-\nerated for this dataset, we were able to analyze the properties\nof an exhaustively evaluated set of convolutional neural ar-\nchitectures at unprecedented scale. In open-sourcing the\nNAS-Bench-101 data and generating code, we hope to make\nNAS research more accessible and reproducible. We also\nhope that NAS-Bench-101 will be the ﬁrst of a continu-\nally improving sequence of rigorous benchmarks for the\nemerging NAS ﬁeld.\nNAS-Bench-101\nAcknowledgements\nAaron and Frank gratefully acknowledge support by the\nEuropean Research Council (ERC) under the European\nUnion’s Horizon 2020 research and innovation programme\nunder grant no. 716721, by BMBF grant DeToL, by the\nstate of Baden-W¨urttemberg through bwHPC and the Ger-\nman Research Foundation (DFG) through grant no INST\n39/963-1 FUGG, and by a Google Faculty Research Award.\nChris, Esteban, Eric, and Kevin would like to thank Quoc\nLe, Samy Bengio, Alok Aggarwal, Barret Zoph, Jon Shlens,\nChristian Szegedy, Jascha Sohl-Dickstein; and the larger\nGoogle Brain team.\nReferences\nBaker, B., Gupta, O., Naik, N., and Raskar, R. Designing\nneural network architectures using reinforcement learn-\ning. In ICLR, 2017.\nBergstra, J. and Bengio, Y . Random search for hyper-\nparameter optimization. JMLR, 2012.\nBergstra, J. S., Bardenet, R., Bengio, Y ., and K´egl, B. Algo-\nrithms for hyper-parameter optimization. In NIPS, 2011.\nCai, H., Chen, T., Zhang, W., Yu, Y ., and Wang, J. Efﬁcient\narchitecture search by network transformation. In AAAI,\n2018.\nDeVries, T. and Taylor, G. W. Improved regularization of\nconvolutional neural networks with cutout. arXiv, 2017.\nEggensperger, K., Feurer, M., Hutter, F., Bergstra, J., Snoek,\nJ., Hoos, H., and Leyton-Brown, K. Towards an empirical\nfoundation for assessing bayesian optimization of hyper-\nparameters. In NIPS workshop on Bayesian Optimization\nin Theory and Practice, December 2013.\nElsken, T., Metzen, J. H., and Hutter, F. Multi-\nobjective architecture search for cnns. arXiv preprint\narXiv:1804.09081, 2018.\nElsken, T., Metzen, J. H., and Hutter, F. Neural architecture\nsearch: A survey. Journal of Machine Learning Research,\n20(55):1–21, April 2019.\nFalkner, S., Klein, A., and Hutter, F. Bohb: Robust and\nefﬁcient hyperparameter optimization at scale. ICML,\n2018.\nGoodfellow, I., Pouget-Abadie, J., Mirza, M., Xu, B.,\nWarde-Farley, D., Ozair, S., Courville, A., and Bengio, Y .\nGenerative adversarial nets. In NIPS, 2014.\nHe, K., Zhang, X., Ren, S., and Sun, J. Deep residual\nlearning for image recognition. In CVPR, 2016.\nHu, J., Shen, L., and Sun, G. Squeeze-and-excitation net-\nworks. CVPR, 2018.\nHuang, G., Liu, Z., Weinberger, K. Q., and van der Maaten,\nL. Densely connected convolutional networks. In CVPR,\n2017.\nHuang, Y ., Cheng, Y ., Chen, D., Lee, H., Ngiam, J., Le,\nQ. V ., and Chen, Z. Gpipe: Efﬁcient training of giant\nneural networks using pipeline parallelism.arXiv preprint\narXiv:1811.06965, 2018.\nHutter, F., Hoos, H. H., and Leyton-Brown, K. Sequential\nmodel-based optimization for general algorithm conﬁg-\nuration. In International Conference on Learning and\nIntelligent Optimization, 2011.\nJones, T. et al. Evolutionary algorithms, ﬁtness landscapes\nand search. PhD thesis, Citeseer, 1995.\nKlein, A. and Hutter, F. Tabular benchmarks for joint archi-\ntecture and hyperparameter optimization. 2019.\nKrizhevsky, A. and Hinton, G. Learning multiple layers\nof features from tiny images. Master’s thesis, Dept. of\nComputer Science, U. of Toronto, 2009.\nKrizhevsky, A., Sutskever, I., and Hinton, G. E. Imagenet\nclassiﬁcation with deep convolutional neural networks.\nIn NIPS, 2012.\nLi, L. and Talwalkar, A. Random Search and Reproducibil-\nity for Neural Architecture Search. arXiv e-prints, art.\narXiv:1902.07638, Feb 2019.\nLi, L., Jamieson, K., DeSalvo, G., Rostamizadeh, A., and\nTalwalkar, A. Hyperband: A novel bandit-based approach\nto hyperparameter optimization. JMLR, 2018.\nLiu, C., Zoph, B., Shlens, J., Hua, W., Li, L.-J., Fei-Fei, L.,\nYuille, A., Huang, J., and Murphy, K. Progressive neural\narchitecture search. ECCV, 2018a.\nLiu, H., Simonyan, K., and Yang, Y . Darts: Differentiable\narchitecture search. ICLR, 2018b.\nLoshchilov, I. and Hutter, F. Sgdr: Stochastic gradient\ndescent with warm restarts. ICLR, 2017.\nLoshchilov, I. and Hutter, F. Decoupled weight decay reg-\nularization. In International Conference on Learning\nRepresentations, 2019. URL https://openreview.\nnet/forum?id=Bkg6RiCqY7.\nPham, H., Guan, M. Y ., Zoph, B., Le, Q. V ., and Dean, J.\nEfﬁcient neural architecture search via parameter sharing.\nICML, 2018.\nNAS-Bench-101\nReal, E., Moore, S., Selle, A., Saxena, S., Suematsu, Y . L.,\nLe, Q., and Kurakin, A. Large-scale evolution of image\nclassiﬁers. In ICML, 2017.\nReal, E., Aggarwal, A., Huang, Y ., and Le, Q. V . Regu-\nlarized evolution for image classiﬁer architecture search.\narXiv preprint arXiv:1802.01548, 2018.\nSciuto, C., Yu, K., Jaggi, M., Musat, C., and Salzmann, M.\nEvaluating the search phase of neural architecture search.\nCoRR, abs/1902.08142, 2019. URL http://arxiv.\norg/abs/1902.08142.\nShahriari, B., Swersky, K., Wang, Z., Adams, R. P., and\nde Freitas, N. Taking the human out of the loop: A review\nof bayesian optimization. Proceedings of the IEEE, 104\n(1):148–175, 2016.\nSnoek, J., Larochelle, H., and Adams, R. P. Practical\nbayesian optimization of machine learning algorithms.\nIn NIPS, 2012.\nSnoek, J., Rippel, O., Swersky, K., Kiros, R., Satish, N.,\nSundaram, N., Patwary, M., Prabhat, and Adams, R. Scal-\nable Bayesian optimization using deep neural networks.\nIn Proceedings of the 32nd International Conference on\nMachine Learning (ICML’15), 2015.\nSo, D. R., Liang, C., and Le, Q. V . The evolved transformer.\nCoRR, abs/1901.11117, 2019.\nSpringenberg, J. T., Klein, A., Falkner, S., and Hutter, F.\nBayesian optimization with robust bayesian neural net-\nworks. In Proceedings of the 29th International Con-\nference on Advances in Neural Information Processing\nSystems (NIPS’16), 2016.\nStadler, P. F. Landscapes and their correlation functions.\nJournal of Mathematical chemistry, 20(1):1–45, 1996.\nSutskever, I., Vinyals, O., and Le, Q. V . Sequence to se-\nquence learning with neural networks. In NIPS, 2014.\nSzegedy, C., Liu, W., Jia, Y ., Sermanet, P., Reed, S.,\nAnguelov, D., Erhan, D., Vanhoucke, V ., and Rabinovich,\nA. Going deeper with convolutions. In CVPR, 2015.\nSzegedy, C., Vanhoucke, V ., Ioffe, S., Shlens, J., and Wojna,\nZ. Rethinking the inception architecture for computer\nvision. In CVPR, 2016.\nTieleman, T. and Hinton, G. Lecture 6.5-rmsprop: Divide\nthe gradient by a running average of its recent magni-\ntude. COURSERA: Neural networks for machine learn-\ning, 2012.\nWeinberger, E. Correlated and uncorrelated ﬁtness land-\nscapes and how to tell the difference. Biological cyber-\nnetics, 63(5):325–336, 1990.\nWilliams, R. J. Simple statistical gradient-following algo-\nrithms for connectionist reinforcement learning. Ma-\nchine Learning , 8:229–256, 1992. doi: 10.1007/\nBF00992696. URL https://doi.org/10.1007/\nBF00992696.\nYing, C. Enumerating unique computational graphs via an\niterative graph invariant. CoRR, abs/1902.06192, 2019.\nZoph, B. and Le, Q. V . Neural architecture search with\nreinforcement learning. In ICLR, 2016.\nZoph, B., Vasudevan, V ., Shlens, J., and Le, Q. V . Learning\ntransferable architectures for scalable image recognition.\nIn CVPR, 2018.",
  "values": {
    "Non-maleficence": "No",
    "Respect for Law and public interest": "No",
    "Autonomy (power to decide)": "No",
    "Beneficence": "No",
    "Critiqability": "No",
    "Explicability": "No",
    "Respect for Persons": "No",
    "Not socially biased": "No",
    "Transparent (to users)": "No",
    "Deferral to humans": "No",
    "Interpretable (to users)": "No",
    "Collective influence": "No",
    "User influence": "No",
    "Fairness": "No",
    "Privacy": "No",
    "Justice": "No"
  }
}