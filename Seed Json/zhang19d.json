{
  "pdf": "zhang19d",
  "title": "Self-Attention Generative Adversarial Networks",
  "author": "Han Zhang, Ian Goodfellow, Dimitris Metaxas, Augustus Odena",
  "paper_id": "zhang19d",
  "text": "Self-Attention Generative Adversarial Networks\nHan Zhang 1 2 Ian Goodfellow 2 Dimitris Metaxas 1 Augustus Odena 2\nAbstract\nIn this paper, we propose the Self-Attention Gen-\nerative Adversarial Network (SAGAN) which\nallows attention-driven, long-range dependency\nmodeling for image generation tasks. Traditional\nconvolutional GANs generate high-resolution de-\ntails as a function of only spatially local points\nin lower-resolution feature maps. In SAGAN, de-\ntails can be generated using cues from all feature\nlocations. Moreover, the discriminator can check\nthat highly detailed features in distant portions\nof the image are consistent with each other. Fur-\nthermore, recent work has shown that generator\nconditioning affects GAN performance. Leverag-\ning this insight, we apply spectral normalization\nto the GAN generator and ﬁnd that this improves\ntraining dynamics. The proposed SAGAN per-\nforms better than prior work1, boosting the best\npublished Inception score from 36.8 to 52.52 and\nreducing Fr´echet Inception distance from 27.62 to\n18.65 on the challenging ImageNet dataset. Visu-\nalization of the attention layers shows that the gen-\nerator leverages neighborhoods that correspond\nto object shapes rather than local regions of ﬁxed\nshape.\n1. Introduction\nImage synthesis is an important problem in computer vi-\nsion. There has been remarkable progress in this direc-\ntion with the emergence of Generative Adversarial Net-\nworks (GANs) (Goodfellow et al., 2014). GANs based on\ndeep convolutional networks (Radford et al., 2016; Kar-\nras et al., 2018; Zhang et al.) have been especially suc-\ncessful. However, by carefully examining the generated\nsamples from these models, we can observe that convo-\n1Department of Computer Science, Rutgers University 2Google\nResearch, Brain Team. Correspondence to: Han Zhang <zhang-\nhan@google.com>.\nProceedings of the 36 th International Conference on Machine\nLearning, Long Beach, California, PMLR 97, 2019. Copyright\n2019 by the author(s).\n1Brock et al. (2018), which builds heavily on this work, has\nsince improved those results substantially.\nlutional GANs (Odena et al., 2017; Miyato et al., 2018;\nMiyato & Koyama, 2018) have much more difﬁculty in\nmodeling some image classes than others when trained on\nmulti-class datasets ( e.g., ImageNet (Russakovsky et al.,\n2015)). For example, while the state-of-the-art ImageNet\nGAN model (Miyato & Koyama, 2018) excels at synthe-\nsizing image classes with few structural constraints ( e.g.,\nocean, sky and landscape classes, which are distinguished\nmore by texture than by geometry), it fails to capture geo-\nmetric or structural patterns that occur consistently in some\nclasses (for example, dogs are often drawn with realistic\nfur texture but without clearly deﬁned separate feet). One\npossible explanation for this is that previous models rely\nheavily on convolution to model the dependencies across\ndifferent image regions. Since the convolution operator has\na local receptive ﬁeld, long range dependencies can only be\nprocessed after passing through several convolutional layers.\nThis could prevent learning about long-term dependencies\nfor a variety of reasons: a small model may not be able\nto represent them, optimization algorithms may have trou-\nble discovering parameter values that carefully coordinate\nmultiple layers to capture these dependencies, and these\nparameterizations may be statistically brittle and prone to\nfailure when applied to previously unseen inputs. Increasing\nthe size of the convolution kernels can increase the represen-\ntational capacity of the network but doing so also loses the\ncomputational and statistical efﬁciency obtained by using\nlocal convolutional structure. Self-attention (Cheng et al.,\n2016; Parikh et al., 2016; Vaswani et al., 2017), on the\nother hand, exhibits a better balance between the ability to\nmodel long-range dependencies and the computational and\nstatistical efﬁciency. The self-attention module calculates\nresponse at a position as a weighted sum of the features at\nall positions, where the weights – or attention vectors – are\ncalculated with only a small computational cost.\nIn this work, we propose Self-Attention Generative Adver-\nsarial Networks (SAGANs), which introduce a self-attention\nmechanism into convolutional GANs. The self-attention\nmodule is complementary to convolutions and helps with\nmodeling long range, multi-level dependencies across image\nregions. Armed with self-attention, the generator can draw\nimages in which ﬁne details at every location are carefully\ncoordinated with ﬁne details in distant portions of the image.\nMoreover, the discriminator can also more accurately en-\nforce complicated geometric constraints on the global image\nSelf-Attention Generative Adversarial Networks\nFigure 1. The proposed SAGAN generates images by leveraging complementary features in distant portions of the image rather than local\nregions of ﬁxed shape to generate consistent objects/scenarios. In each row, the ﬁrst image shows ﬁve representative query locations with\ncolor coded dots. The other ﬁve images are attention maps for those query locations, with corresponding color coded arrows summarizing\nthe most-attended regions.\nstructure.\nIn addition to self-attention, we also incorporate recent\ninsights relating network conditioning to GAN perfor-\nmance. The work by (Odena et al., 2018) showed that\nwell-conditioned generators tend to perform better. We pro-\npose enforcing good conditioning of GAN generators using\nthe spectral normalization technique that has previously\nbeen applied only to the discriminator (Miyato et al., 2018).\nWe have conducted extensive experiments on the ImageNet\ndataset to validate the effectiveness of the proposed self-\nattention mechanism and stabilization techniques. SAGAN\nsigniﬁcantly outperforms prior work in image synthe-\nsis by boosting the best reported Inception score from\n36.8 to 52.52 and reducing Fr ´echet Inception distance\nfrom 27.62 to 18.65. Visualization of the attention layers\nshows that the generator leverages neighborhoods that cor-\nrespond to object shapes rather than local regions of ﬁxed\nshape. Our code is available at https://github.com/\nbrain-research/self-attention-gan .\n2. Related Work\nGenerative Adversarial Networks. GANs have achieved\ngreat success in various image generation tasks, including\nimage-to-image translation (Isola et al., 2017; Zhu et al.,\n2017; Taigman et al., 2017; Liu & Tuzel, 2016; Xue et al.,\n2018; Park et al., 2019), image super-resolution (Ledig\net al., 2017; Snderby et al., 2017) and text-to-image syn-\nthesis (Reed et al., 2016b;a; Zhang et al., 2017; Hong\net al., 2018). Despite this success, the training of GANs is\nknown to be unstable and sensitive to the choices of hyper-\nparameters. Several works have attempted to stabilize the\nGAN training dynamics and improve the sample diversity by\ndesigning new network architectures (Radford et al., 2016;\nZhang et al., 2017; Karras et al., 2018; 2019), modifying\nthe learning objectives and dynamics (Arjovsky et al., 2017;\nSalimans et al., 2018; Metz et al., 2017; Che et al., 2017;\nZhao et al., 2017; Jolicoeur-Martineau, 2019), adding reg-\nularization methods (Gulrajani et al., 2017; Miyato et al.,\n2018) and introducing heuristic tricks (Salimans et al., 2016;\nOdena et al., 2017). Recently, Miyato et al. (Miyato et al.,\n2018) proposed limiting the spectral norm of the weight\nmatrices in the discriminator in order to constrain the Lip-\nschitz constant of the discriminator function. Combined\nwith the projection-based discriminator (Miyato & Koyama,\n2018), the spectrally normalized model greatly improves\nclass-conditional image generation on ImageNet.\nAttention Models. Recently, attention mechanisms have\nbecome an integral part of models that must capture global\ndependencies (Bahdanau et al., 2014; Xu et al., 2015; Yang\net al., 2016; Gregor et al., 2015; Chen et al., 2018). In\nparticular, self-attention (Cheng et al., 2016; Parikh et al.,\n2016), also called intra-attention, calculates the response at\na position in a sequence by attending to all positions within\nthe same sequence. Vaswani et al. (Vaswani et al., 2017)\ndemonstrated that machine translation models could achieve\nstate-of-the-art results by solely using a self-attention model.\nParmar et al. (Parmar et al., 2018) proposed an Image Trans-\nformer model to add self-attention into an autoregressive\nmodel for image generation. Wang et al. (Wang et al., 2018)\nformalized self-attention as a non-local operation to model\nthe spatial-temporal dependencies in video sequences. In\nspite of this progress, self-attention has not yet been ex-\nplored in the context of GANs. (AttnGAN (Xu et al., 2018)\nuses attention over word embeddings within an input se-\nquence, but not self-attention over internal model states ).\nSAGAN learns to efﬁciently ﬁnd global, long-range depen-\ndencies within internal representations of images.\nSelf-Attention Generative Adversarial Networks\nFigure 2. The proposed self-attention module for the SAGAN. The ⊗ denotes matrix multiplication. The softmax operation is performed\non each row.\n3. Self-Attention Generative Adversarial\nNetworks\nMost GAN-based models (Radford et al., 2016; Salimans\net al., 2016; Karras et al., 2018) for image generation are\nbuilt using convolutional layers. Convolution processes the\ninformation in a local neighborhood, thus using convolu-\ntional layers alone is computationally inefﬁcient for model-\ning long-range dependencies in images. In this section, we\nadapt the non-local model of (Wang et al., 2018) to intro-\nduce self-attention to the GAN framework, enabling both\nthe generator and the discriminator to efﬁciently model rela-\ntionships between widely separated spatial regions. We call\nthe proposed method Self-Attention Generative Adversarial\nNetworks (SAGAN) because of its self-attention module\n(see Figure 2).\nThe image features from the previous hidden layer x∈\nRC×N are ﬁrst transformed into two feature spaces f, g\nto calculate the attention, where f (x) = Wf x, g(x) =\nWgx\nβj,i = exp(sij)∑N\ni=1 exp(sij)\n, wheresij = f(xi)T g(xj), (1)\nandβj,i indicates the extent to which the model attends to\ntheith location when synthesizing thejth region. Here,C\nis the number of channels andN is the number of feature\nlocations of features from the previous hidden layer. The out-\nput of the attention layer is o = (o1, o2,..., oj,..., oN)∈\nRC×N , where,\noj = v\n( N∑\ni=1\nβj,ih(xi)\n)\n, h(xi) = Whxi, v(xi) = Wvxi.\n(2)\nIn the above formulation, Wg ∈ R ¯C×C, Wf ∈ R ¯C×C,\nWh∈ R ¯C×C, and Wv∈ RC× ¯C are the learned weight ma-\ntrices, which are implemented as 1×1 convolutions. Since\nWe did not notice any signiﬁcant performance decrease\nwhen reducing the channel number of ¯C to be C/k, where\nk = 1, 2, 4, 8 after few training epochs on ImageNet. For\nmemory efﬁciency, we choosek = 8 (i.e., ¯C = C/8) in all\nour experiments.\nIn addition, we further multiply the output of the attention\nlayer by a scale parameter and add back the input feature\nmap. Therefore, the ﬁnal output is given by,\nyi =γoi + xi, (3)\nwhereγ is a learnable scalar and it is initialized as 0. In-\ntroducing the learnable γ allows the network to ﬁrst rely\non the cues in the local neighborhood – since this is eas-\nier – and then gradually learn to assign more weight to the\nnon-local evidence. The intuition for why we do this is\nstraightforward: we want to learn the easy task ﬁrst and then\nprogressively increase the complexity of the task. In the\nSAGAN, the proposed attention module has been applied to\nboth the generator and the discriminator, which are trained\nin an alternating fashion by minimizing the hinge version\nof the adversarial loss (Lim & Ye, 2017; Tran et al., 2017;\nMiyato et al., 2018),\nLD = − E(x,y)∼pdata[min(0,−1 +D(x,y ))]\n− Ez∼pz,y∼pdata[min(0,−1−D(G(z),y ))],\nLG = − Ez∼pz,y∼pdataD(G(z),y ),\n(4)\n4. Techniques to Stabilize the Training of\nGANs\nWe also investigate two techniques to stabilize the training\nof GANs on challenging datasets. First, we use spectral\nnormalization (Miyato et al., 2018) in the generator as well\nas in the discriminator. Second, we conﬁrm that the two-\ntimescale update rule (TTUR) (Heusel et al., 2017) is effec-\ntive, and we advocate using it speciﬁcally to address slow\nlearning in regularized discriminators.\nSelf-Attention Generative Adversarial Networks\n4.1. Spectral normalization for both generator and\ndiscriminator\nMiyato et al. (Miyato et al., 2018) originally proposed sta-\nbilizing the training of GANs by applying spectral normal-\nization to the discriminator network. Doing so constrains\nthe Lipschitz constant of the discriminator by restricting\nthe spectral norm of each layer. Compared to other normal-\nization techniques, spectral normalization does not require\nextra hyper-parameter tuning (setting the spectral norm of\nall weight layers to 1 consistently performs well in practice).\nMoreover, the computational cost is also relatively small.\nWe argue that the generator can also beneﬁt from spectral\nnormalization, based on recent evidence that the condition-\ning of the generator is an important causal factor in GANs’\nperformance (Odena et al., 2018). Spectral normalization in\nthe generator can prevent the escalation of parameter magni-\ntudes and avoid unusual gradients. We ﬁnd empirically that\nspectral normalization of both generator and discriminator\nmakes it possible to use fewer discriminator updates per\ngenerator update, thus signiﬁcantly reducing the computa-\ntional cost of training. The approach also shows more stable\ntraining behavior.\n4.2. Imbalanced learning rate for generator and\ndiscriminator updates\nIn previous work, regularization of the discriminator (Miy-\nato et al., 2018; Gulrajani et al., 2017) often slows down\nthe GANs’ learning process. In practice, methods using\nregularized discriminators typically require multiple (e.g.,\n5) discriminator update steps per generator update step dur-\ning training. Independently, Heusel et al. (Heusel et al.,\n2017) have advocated using separate learning rates (TTUR)\nfor the generator and the discriminator. We propose using\nTTUR speciﬁcally to compensate for the problem of slow\nlearning in a regularized discriminator, making it possible\nto use fewer discriminator steps per generator step. Using\nthis approach, we are able to produce better results given\nthe same wall-clock time.\n5. Experiments\nTo evaluate the proposed methods, we conducted extensive\nexperiments on the LSVRC2012 (ImageNet) dataset (Rus-\nsakovsky et al., 2015). First, in Section 5.1, we present\nexperiments designed to evaluate the effectiveness of the\ntwo proposed techniques for stabilizing GANs’ training.\nNext, the proposed self-attention mechanism is investigated\nin Section 5.2. Finally, our SAGAN is compared with state-\nof-the-art methods (Odena et al., 2017; Miyato & Koyama,\n2018) on the image generation task in Section 5.3.\nEvaluation metrics. We choose the Inception score\n(IS) (Salimans et al., 2016) and the Fr ´echet Inception dis-\ntance (FID) (Heusel et al., 2017) for quantitative evaluation.\nThe Inception score (Salimans et al., 2016) computes the KL\ndivergence between the conditional class distribution and\nthe marginal class distribution. Higher Inception score indi-\ncates better image quality. We include the Inception score\nbecause it is widely used and thus makes it possible to com-\npare our results to previous work. However, it is important\nto understand that Inception score has serious limitations—\nit is intended primarily to ensure that the model generates\nsamples that can be conﬁdently recognized as belonging to\na speciﬁc class, and that the model generates samples from\nmany classes, not necessarily to assess realism of details or\nintra-class diversity. FID is a more principled and compre-\nhensive metric, and has been shown to be more consistent\nwith human evaluation in assessing the realism and varia-\ntion of the generated samples (Heusel et al., 2017). FID\ncalculates the Wasserstein-2 distance between the gener-\nated images and the real images in the feature space of an\nInception-v3 network. Besides the FID calculated over the\nwhole data distribution (i.e.., all 1000 classes of images in\nImageNet), we also compute FID between the generated\nimages and dataset images within each class (called intra\nFID (Miyato & Koyama, 2018)). Lower FID and intra FID\nvalues mean closer distances between synthetic and real data\ndistributions. In all our experiments, 50k samples are ran-\ndomly generated for each model to compute the Inception\nscore, FID and intra FID.\nNetwork structures and implementation details. All\nthe SAGAN models we train are designed to generate\n128×128 images. By default, spectral normalization (Miy-\nato et al., 2018) is used for the layers in both the generator\nand the discriminator. Similar to (Miyato & Koyama, 2018),\nSAGAN uses conditional batch normalization in the gen-\nerator and projection in the discriminator. For all models,\nwe use the Adam optimizer (Kingma & Ba, 2015) with\nβ1 = 0 andβ2 = 0.9 for training. By default, the learning\nrate for the discriminator is 0.0004 and the learning rate for\nthe generator is 0.0001.\n5.1. Evaluating the proposed stabilization techniques\nIn this section, experiments are conducted to evaluate the\neffectiveness of the proposed stabilization techniques, i.e.,\napplying spectral normalization (SN) to the generator and\nutilizing imbalanced learning rates (TTUR). In Figure 3, our\nmodels “SN on G/D” and “SN on G/D+TTUR” are com-\npared with a baseline model, which is implemented based\non the state-of-the-art image generation method (Miyato\net al., 2018). In this baseline model, SN is only utilized\nin the discriminator. When we train it with 1:1 balanced\nupdates for the discriminator (D) and the generator (G), the\ntraining becomes very unstable, as shown in the leftmost\nsub-ﬁgures of Figure 3. It exhibits mode collapse very early\nin training. For example, the top-left sub-ﬁgure of Figure 4\nSelf-Attention Generative Adversarial Networks\nFigure 3. Training curves for the baseline model and our models with the proposed stabilization techniques, “SN on G/D” and two-\ntimescale learning rates (TTUR). All models are trained with 1:1 balanced updates for G and D.\nillustrates some images randomly generated by the baseline\nmodel at the 10k-th iteration. Although in the the original\npaper (Miyato et al., 2018) this unstable training behavior\nis greatly mitigated by using 5:1 imbalanced updates for\nD andG, the ability to be stably trained with 1:1 balanced\nupdates is desirable for improving the convergence speed\nof the model. Thus, using our proposed techniques means\nthat the model can produce better results given the same\nwall-clock time. Given this, there is no need to search for a\nsuitable update ratio for the generator and discriminator. As\nshown in the middle sub-ﬁgures of Figure 3, adding SN to\nboth the generator and the discriminator greatly stabilized\nour model “SN on G/D”, even when it was trained with\n1:1 balanced updates. However, the quality of samples does\nnot improve monotonically during training. For example,\nthe image quality as measured by FID and IS is starting to\ndrop at the 260k-th iteration. Example images randomly\ngenerated by this model at different iterations can be found\nin Figure 4. When we also apply the imbalanced learning\nrates to train the discriminator and the generator, the quality\nof images generated by our model “SN on G/D+TTUR”\nimproves monotonically during the whole training process.\nAs shown in Figure 3 and Figure 4, we do not observe any\nsigniﬁcant decrease in sample quality or in the FID or the\nInception score during one million training iterations. Thus,\nboth quantitative results and qualitative results demonstrate\nthe effectiveness of the proposed stabilization techniques\nfor GANs’ training. They also demonstrate that the effect\nof the two techniques is at least partly additive. In the rest\nof experiments, all models use spectral normalization for\nboth the generator and discriminator and use the imbalanced\nlearning rates to train the generator and the discriminator\nwith 1:1 updates.\n5.2. Self-attention mechanism.\nTo explore the effect of the proposed self-attention mecha-\nnism, we build several SAGAN models by adding the self-\nattention mechanism to different stages of the generator and\nthe discriminator. As shown in Table 1, the SAGAN mod-\nels with the self-attention mechanism at the middle-to-high\nlevel feature maps (e.g.,feat32 andfeat64) achieve better\nperformance than the models with the self-attention mecha-\nnism at the low level feature maps (e.g.,feat8 andfeat16).\nFor example, the FID of the model “SAGAN, feat8” is im-\nproved from 22.98 to 18.28 by “SAGAN, feat32”. The rea-\nson is that self-attention receives more evidence and enjoys\nmore freedom to choose conditions with larger feature maps\n(i.e., it is complementary to convolution for large feature\nmaps), however, it plays a similar role as the local convo-\nlution when modeling dependencies for small ( e.g., 8×8)\nfeature maps. It demonstrates that the attention mechanism\ngives more power to both the generator and the discrimi-\nnator to directly model the long-range dependencies in the\nfeature maps. In addition, the comparison of our SAGAN\nSelf-Attention Generative Adversarial Networks\nBaseline: SN on D\n(10k, FID=181.84)\nSN on G/D\n(10k, FID=93.52)\nSN on G/D\n(160k, FID=33.39)\nSN on G/D\n(260k, FID=72.41)\nSN on G/D+TTUR\n(10k, FID=99.04)\nSN on G/D+TTUR\n(160k, FID=40.96)\nSN on G/D+TTUR\n(260k, FID=34.62)\nSN on G/D+TTUR\n(1M, FID=22.96)\nFigure 4. 128×128 examples randomly generated by the baseline model and our models “SN on G/D” and “SN on G/D+TTUR”.\nModel no\nattention\nSAGAN Residual\nf eat8 f eat16 f eat32 f eat64 f eat8 f eat16 f eat32 f eat64\nFID 22.96 22.98 22.14 18.28 18.65 42.13 22.40 27.33 28.82\nIS 42.87 43.15 45.94 51.43 52.52 23.17 44.49 38.50 38.96\nTable 1. Comparison of Self-Attention and Residual block on GANs. These blocks are added into different layers of the network. All\nmodels have been trained for one million iterations, and the best Inception scores (IS) and Fr´echet Inception distance (FID) are reported.\nf eatk means adding self-attention to the k×k feature maps.\nand the baseline model without attention (2nd column of\nTable 1) further shows the effectiveness of the proposed\nself-attention mechanism.\nCompared with residual blocks with the same number of pa-\nrameters, the self-attention blocks also achieve better results.\nFor example, the training is not stable when we replace the\nself-attention block with the residual block in 8×8 feature\nmaps, which leads to a signiﬁcant decrease in performance\n(e.g., FID increases from 22.98 to 42.13). Even for the cases\nwhen the training goes smoothly, replacing the self-attention\nblock with the residual block still leads to worse results in\nterms of FID and Inception score. (e.g., FID 18.28 vs 27.33\nin feature map 32× 32). This comparison demonstrates that\nthe performance improvement given by using SAGAN is\nnot simply due to an increase in model depth and capacity.\nTo better understand what has been learned during the gen-\neration process, we visualize the attention weights of the\ngenerator in SAGAN for different images. Some sample\nimages with attention are shown in Figure 5 and Figure 1.\nWe observe that the network learns to allocate attention ac-\ncording to similarity of color and texture, rather than just\nspatial adjacency. For example, in the top-left cell of Fig-\nure 1, the red point attends mostly to the body of the bird\naround it, however, the green point learns to attend to other\nside of the image. In this way, the image has a consistent\nbackground (i.e., trees from the left to the right though they\nare separated by the bird). Similarly, the blue point allocates\nthe attention to the whole tail of the bird to make the gener-\nated part coherent. Those long-range dependencies could\nnot be captured by convolutions with local receptive ﬁelds.\nWe also ﬁnd that although some query points are quite close\nin spatial location, their attention maps can be very differ-\nent, as shown in the bottom-left cell. The red point attends\nmostly to the background regions, whereas the blue point,\nthough adjacent to red point, puts most of the attention on\nthe foreground object. This also reduces the chance for the\nlocal errors to propagate, since the adjacent position has\nthe freedom to choose to attend to other distant locations.\nThese observations further demonstrate that self-attention\nSelf-Attention Generative Adversarial Networks\nFigure 5. Visualization of attention maps. These images were generated by SAGAN. We visualize the attention maps of the last generator\nlayer that used attention, since this layer is the closest to the output pixels and is the most straightforward to project into pixel space\nand interpret. In each cell, the ﬁrst image shows three representative query locations with color coded dots. The other three images are\nattention maps for those query locations, with corresponding color coded arrows summarizing the most-attended regions. We observe that\nthe network learns to allocate attention according to similarity of color and texture, rather than just spatial adjacency (see the top-left cell).\nWe also ﬁnd that although some query points are quite close in spatial location, their attention maps can be very different, as shown in the\nbottom-left cell. As shown in the top-right cell, SAGAN is able to draw dogs with clearly separated legs. The blue query point shows that\nattention helps to get the structure of the joint area correct. See the text for more discussion about the properties of learned attention maps.\nis complementary to convolutions for image generation in\nGANs. As shown in the top-right cell, SAGAN is able to\ndraw dogs with clearly separated legs. The blue query point\nshows that attention helps to get the structure of the joint\narea correct.\n5.3. Comparison with the state-of-the-art\nOur SAGAN is also compared with the state-of-the-art GAN\nmodels (Odena et al., 2017; Miyato & Koyama, 2018) for\nclass conditional image generation on ImageNet. As shown\nin Table 2, our proposed SAGAN achieves the best Incep-\ntion score, intra FID and FID. The proposed SAGAN sig-\nniﬁcantly improves the best published Inception score from\n36.8 to 52.52. The lower FID (18.65) and intra FID (83.7)\nachieved by the SAGAN also indicates that the SAGAN\ncan better approximate the original image distribution by\nusing the self-attention module to model the long-range\ndependencies between image regions.\nFigure 6 shows some comparison results and generated-\nimages for representative classes of ImageNet. We observe\nthat our SAGAN achieves much better performance ( i.e.,\nlower intra FID) than the state-of-the-art GAN model (Miy-\nato & Koyama, 2018) for synthesizing image classes with\ncomplex geometric or structural patterns, such as goldﬁsh\nand Saint Bernard. For classes with few structural con-\nstraints (e.g., valley, stone wall and coral fungus, which\nare distinguished more by texture than by geometry), our\nSAGAN shows less superiority compared with the baseline\nmodel (Miyato & Koyama, 2018). Again, the reason is\nthat the self-attention in SAGAN is complementary to the\nconvolution for capturing long-range, global-level depen-\ndencies occurring consistently in geometric or structural\npatterns, but plays a similar role as the local convolution\nwhen modeling dependencies for simple texture.\n6. Conclusion\nIn this paper, we proposed Self-Attention Generative Ad-\nversarial Networks (SAGANs), which incorporate a self-\nattention mechanism into the GAN framework. The self-\nattention module is effective in modeling long-range de-\npendencies. In addition, we show that spectral normaliza-\ntion applied to the generator stabilizes GAN training and\nthat TTUR speeds up training of regularized discrimina-\ntors. SAGAN achieves the state-of-the-art performance on\nclass-conditional image generation on ImageNet.\nSelf-Attention Generative Adversarial Networks\nModel Inception Score Intra FID FID\nAC-GAN (Odena et al., 2017) 28.5 260.0 /\nSNGAN-projection (Miyato & Koyama, 2018) 36.8 92.4 27.62∗\nSAGAN 52.52 83.7 18.65\nTable 2. Comparison of the proposed SAGAN with state-of-the-art GAN models (Odena et al., 2017; Miyato & Koyama, 2018) for class\nconditional image generation on ImageNet. FID of SNGAN-projection is calculated from ofﬁcially released weights.\ngoldﬁsh\n(44.4, 58.1)\nindigo\nbunting\n(53.0, 66.8)\nredshank\n(48.9, 60.1)\nsaint\nbernard\n(35.7, 55.3)\ntiger\ncat\n(88.1, 90.2)\nstone\nwall\n(57.5, 49.3)\ngeyser\n(21.6, 19.5)\nvalley\n(39.7, 26.0)\ncoral\nfungus\n(38.0, 37.2)\nFigure 6. 128x128 example images generated by SAGAN for different classes. Each row shows examples from one class. In the leftmost\ncolumn, the intra FID of our SAGAN (left) and the state-of-the-art method (Miyato & Koyama, 2018)) (right) are listed.\nSelf-Attention Generative Adversarial Networks\nAcknowledgments\nWe thank Surya Bhupatiraju for feedback on drafts of this\narticle. We also thank David Berthelot and Tom B. Brown\nfor help with implementation details. Finally, we thank\nJakob Uszkoreit, Tao Xu, and Ashish Vaswani for helpful\ndiscussions.\nReferences\nArjovsky, M., Chintala, S., and Bottou, L. Wasserstein\nGAN. arXiv:1701.07875, 2017.\nBahdanau, D., Cho, K., and Bengio, Y . Neural machine\ntranslation by jointly learning to align and translate.\narXiv:1409.0473, 2014.\nBrock, A., Donahue, J., and Simonyan, K. Large scale gan\ntraining for high ﬁdelity natural image synthesis. arXiv\npreprint arXiv:1809.11096, 2018.\nChe, T., Li, Y ., Jacob, A. P., Bengio, Y ., and Li, W. Mode\nregularized generative adversarial networks. In ICLR,\n2017.\nChen, X., Mishra, N., Rohaninejad, M., and Abbeel, P.\nPixelsnail: An improved autoregressive generative model.\nIn ICML, 2018.\nCheng, J., Dong, L., and Lapata, M. Long short-term\nmemory-networks for machine reading. In EMNLP, 2016.\nGoodfellow, I. J., Pouget-Abadie, J., Mirza, M., Xu, B.,\nWarde-Farley, D., Ozair, S., Courville, A. C., and Bengio,\nY . Generative adversarial nets. InNIPS, 2014.\nGregor, K., Danihelka, I., Graves, A., Rezende, D. J., and\nWierstra, D. DRAW: A recurrent neural network for\nimage generation. In ICML, 2015.\nGulrajani, I., Ahmed, F., Arjovsky, M., Dumoulin, V ., and\nCourville, A. C. Improved training of wasserstein GANs.\nIn NIPS, 2017.\nHeusel, M., Ramsauer, H., Unterthiner, T., Nessler, B., and\nHochreiter, S. GANs trained by a two time-scale update\nrule converge to a local nash equilibrium. In NIPS, pp.\n6629–6640, 2017.\nHong, S., Yang, D., Choi, J., and Lee, H. Inferring semantic\nlayout for hierarchical text-to-image synthesis. In CVPR,\n2018.\nIsola, P., Zhu, J.-Y ., Zhou, T., and Efros, A. A. Image-to-\nimage translation with conditional adversarial networks.\nIn CVPR, 2017.\nJolicoeur-Martineau, A. The relativistic discriminator: a\nkey element missing from standard GAN. In ICLR, 2019.\nKarras, T., Aila, T., Laine, S., and Lehtinen, J. Progressive\ngrowing of GANs for improved quality, stability, and\nvariation. In ICLR, 2018.\nKarras, T., Laine, S., and Aila, T. A style-based genera-\ntor architecture for generative adversarial networks. In\nCVPR, 2019.\nKingma, D. P. and Ba, J. Adam: A method for stochastic\noptimization. In ICLR, 2015.\nLedig, C., Theis, L., Huszar, F., Caballero, J., Aitken, A.,\nTejani, A., Totz, J., Wang, Z., and Shi, W. Photo-realistic\nsingle image super-resolution using a generative adver-\nsarial network. In CVPR, 2017.\nLim, J. H. and Ye, J. C. Geometric GAN.arXiv:1705.02894,\n2017.\nLiu, M. and Tuzel, O. Coupled generative adversarial net-\nworks. In NIPS, 2016.\nMetz, L., Poole, B., Pfau, D., and Sohl-Dickstein, J. Un-\nrolled generative adversarial networks. In ICLR, 2017.\nMiyato, T. and Koyama, M. cGANs with projection dis-\ncriminator. In ICLR, 2018.\nMiyato, T., Kataoka, T., Koyama, M., and Yoshida, Y . Spec-\ntral normalization for generative adversarial networks. In\nICLR, 2018.\nOdena, A., Olah, C., and Shlens, J. Conditional image\nsynthesis with auxiliary classiﬁer GANs. In ICML, 2017.\nOdena, A., Buckman, J., Olsson, C., Brown, T. B., Olah, C.,\nRaffel, C., and Goodfellow, I. Is generator conditioning\ncausally related to GAN performance? In ICML, 2018.\nParikh, A. P., T ¨ackstr¨om, O., Das, D., and Uszkoreit, J.\nA decomposable attention model for natural language\ninference. In EMNLP, 2016.\nPark, T., Liu, M., Wang, T., and Zhu, J. Semantic image\nsynthesis with spatially-adaptive normalization. In CVPR,\n2019.\nSelf-Attention Generative Adversarial Networks\nParmar, N., Vaswani, A., Uszkoreit, J., ukasz Kaiser,\nShazeer, N., and Ku, A. Image transformer.\narXiv:1802.05751, 2018.\nRadford, A., Metz, L., and Chintala, S. Unsupervised rep-\nresentation learning with deep convolutional generative\nadversarial networks. In ICLR, 2016.\nReed, S., Akata, Z., Mohan, S., Tenka, S., Schiele, B., and\nLee, H. Learning what and where to draw. In NIPS,\n2016a.\nReed, S., Akata, Z., Yan, X., Logeswaran, L., Schiele, B.,\nand Lee, H. Generative adversarial text-to-image synthe-\nsis. In ICML, 2016b.\nRussakovsky, O., Deng, J., Su, H., Krause, J., Satheesh, S.,\nMa, S., Huang, Z., Karpathy, A., Khosla, A., Bernstein,\nM., Berg, A. C., and Fei-Fei, L. ImageNet large scale\nvisual recognition challenge. IJCV, 2015.\nSalimans, T., Goodfellow, I. J., Zaremba, W., Cheung, V .,\nRadford, A., and Chen, X. Improved techniques for\ntraining GANs. In NIPS, 2016.\nSalimans, T., Zhang, H., Radford, A., and Metaxas, D. N.\nImproving GANs using optimal transport. In ICLR, 2018.\nSnderby, C. K., Caballero, J., Theis, L., Shi, W., and Huszar,\nF. Amortised map inference for image super-resolution.\nIn ICLR, 2017.\nTaigman, Y ., Polyak, A., and Wolf, L. Unsupervised cross-\ndomain image generation. In ICLR, 2017.\nTran, D., Ranganath, R., and Blei, D. M. Deep and hierar-\nchical implicit models. arXiv:1702.08896, 2017.\nVaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones,\nL., Gomez, A. N., Kaiser, L., and Polosukhin, I. Attention\nis all you need. arXiv:1706.03762, 2017.\nWang, X., Girshick, R., Gupta, A., and He, K. Non-local\nneural networks. In CVPR, 2018.\nXu, K., Ba, J., Kiros, R., Cho, K., Courville, A. C., Salakhut-\ndinov, R., Zemel, R. S., and Bengio, Y . Show, attend and\ntell: Neural image caption generation with visual atten-\ntion. In ICML, 2015.\nXu, T., Zhang, P., Huang, Q., Zhang, H., Gan, Z., Huang,\nX., and He, X. AttnGAN: Fine-grained text to image gen-\neration with attentional generative adversarial networks.\nIn CVPR, 2018.\nXue, Y ., Xu, T., Zhang, H., Long, L. R., and Huang, X.\nSegAN: Adversarial network with multi-scale L1 loss\nfor medical image segmentation. Neuroinformatics, pp.\n1–10, 2018.\nYang, Z., He, X., Gao, J., Deng, L., and Smola, A. J. Stacked\nattention networks for image question answering. In\nCVPR, 2016.\nZhang, H., Xu, T., Li, H., Zhang, S., Wang, X., Huang,\nX., and Metaxas, D. N. StackGAN++: Realistic image\nsynthesis with stacked generative adversarial networks.\nTPAMI.\nZhang, H., Xu, T., Li, H., Zhang, S., Wang, X., Huang, X.,\nand Metaxas, D. StackGAN: Text to photo-realistic image\nsynthesis with stacked generative adversarial networks.\nIn ICCV, 2017.\nZhao, J., Mathieu, M., and LeCun, Y . Energy-based genera-\ntive adversarial network. In ICLR, 2017.\nZhu, J.-Y ., Park, T., Isola, P., and Efros, A. A. Unpaired\nimage-to-image translation using cycle-consistent adver-\nsarial networks. In ICCV, 2017.",
  "values": {
    "Transparent (to users)": "Yes",
    "Interpretable (to users)": "Yes",
    "Respect for Persons": "Yes",
    "Non-maleficence": "Yes",
    "Deferral to humans": "Yes",
    "Beneficence": "Yes",
    "Explicability": "Yes",
    "Critiqability": "Yes",
    "Not socially biased": "Yes",
    "Autonomy (power to decide)": "Yes",
    "Respect for Law and public interest": "Yes",
    "Justice": "Yes",
    "User influence": "Yes",
    "Fairness": "Yes",
    "Privacy": "Yes",
    "Collective influence": "Yes"
  }
}