{
  "pdf": "zhang19p",
  "title": "Theoretically Principled Trade-off between Robustness and Accuracy",
  "author": "Unknown",
  "paper_id": "zhang19p",
  "text": "Theoretically Principled Trade-off between Robustness and Accuracy\nHongyang Zhang 12 Yaodong Yu3 Jiantao Jiao 4 Eric P . Xing15 Laurent El Ghaoui 4 Michael I. Jordan 4\nAbstract\nWe identify a trade-off between robustness and\naccuracy that serves as a guiding principle in the\ndesign of defenses against adversarial examples.\nAlthough this problem has been widely studied\nempirically, much remains unknown concerning\nthe theory underlying this trade-off. In this work,\nwe decompose the prediction error for adversarial\nexamples (robust error) as the sum of the natural\n(classiﬁcation) error and boundary error, and pro-\nvide a differentiable upper bound using the theory\nof classiﬁcation-calibrated loss, which is shown to\nbe the tightest possible upper bound uniform over\nall probability distributions and measurable pre-\ndictors. Inspired by our theoretical analysis, we\nalso design a new defense method, TRADES, to\ntrade adversarial robustness off against accuracy.\nOur proposed algorithm performs well experimen-\ntally in real-world datasets. The methodology is\nthe foundation of our entry to the NeurIPS 2018\nAdversarial Vision Challenge in which we won\nthe 1st place out of ~2,000 submissions, surpass-\ning the runner-up approach by 11.41% in terms\nof mean `2 perturbation distance.\n1. Introduction\nIn response to the vulnerability of deep neural networks\nto small perturbations around input data ( Szegedy et al. ,\n2013), adversarial defenses have been an imperative object\nof study in machine learning ( Huang et al., 2017), computer\nvision (Song et al. , 2018; Xie et al. , 2017; Meng & Chen ,\n2017), natural language processing ( Jia & Liang , 2017),\nand many other domains. In machine learning, study of\nadversarial defenses has led to signiﬁcant advances in under-\nstanding and defending against adversarial threat ( He et al. ,\n2017). In computer vision and natural language process-\ning, adversarial defenses serve as indispensable building\n1Carnegie Mellon University 2Toyota Technological Insti-\ntute at Chicago 3University of Virginia 4University of California,\nBerkeley 5Petuum Inc.. Correspondence to: Hongyang Zhang\n<hongyanz@cs.cmu.edu>, Y aodong Y u <yy8ms@virginia.edu>.\nProceedings of the 36 th International Conference on Machine\nLearning, Long Beach, California, PMLR 97, 2019. Copyright\n2019 by the author(s).\nblocks for a range of security-critical systems and appli-\ncations, such as autonomous cars and speech recognition\nauthorization. The problem of adversarial defenses can be\nstated as that of learning a classiﬁer with high test accuracy\non both natural and adversarial examples. The adversarial\nexample for a given labeled data (x,y ) is a data point x0\nthat causes a classiﬁer c to output a different label on x0than\ny, but is “imperceptibly similar” to x. Given the difﬁculty\nof providing an operational deﬁnition of “imperceptible sim-\nilarity,” adversarial examples typically come in the form of\nrestricted attacks such as ✏-bounded perturbations ( Szegedy\net al., 2013), or unrestricted attacks such as adversarial ro-\ntations, translations, and deformations ( Brown et al., 2018;\nEngstrom et al., 2017; Gilmer et al., 2018; Xiao et al., 2018;\nAlaifari et al., 2019; Zhang et al. , 2019a). The focus of this\nwork is the former setting, though our framework can be\ngeneralized to the latter.\nDespite a large literature devoted to improving the robust-\nness of deep-learning models, many fundamental questions\nremain unresolved. One of the most important questions\nis how to trade off adversarial robustness against natural\naccuracy. Statistically, robustness can be be at odds with\naccuracy when no assumptions are made on the data distri-\nbution (Tsipras et al. , 2019). This has led to an empirical\nline of work on adversarial defense that incorporates var-\nious kinds of assumptions ( Su et al. , 2018; Kurakin et al. ,\n2017). On the theoretical front, methods such as relaxation\nbased defenses (Kolter & Wong, 2018; Raghunathan et al.,\n2018a) provide provable guarantees for adversarial robust-\nness. They, however, ignore the performance of classiﬁer\non the non-adversarial examples, and thus leave open the\ntheoretical treatment of the putative robustness/accuracy\ntrade-off.\nThe problem of adversarial defense becomes more challeng-\ning when computational issues are considered. For example,\nthe straightforward empirical risk minimization (ERM) for-\nmulation of robust classiﬁcation involves minimizing the\nrobust 0-1 loss maxx0 :kx0 \u0000xk✏ 1{c(x0) 6= y}, a loss which\nis NP-hard to optimize even if ✏ =0 in general. Hence, it\nis natural to expect that some prior work on adversarial de-\nfense replaced the 0-1 loss 1(·) with a surrogate loss (Madry\net al., 2018; Kurakin et al., 2017; Uesato et al. , 2018). How-\never, there is little theoretical guarantee on the tightness of\nthis approximation.\nTheoretically Principled Trade-off between Robustness and Accuracy\n\tFigure 1. Left ﬁgure: decision boundary learned by natural train-\ning method. Right ﬁgure: decision boundary learned by our\nadversarial training method, where the orange dotted line repre-\nsents the decision boundary in the left ﬁgure. It shows that both\nmethods achieve zero natural training error, while our adversar-\nial training method achieves better robust training error than the\nnatural training method.\n1.1. Our methodology and results\nWe begin with an example that illustrates the trade-off be-\ntween accuracy and adversarial robustness in Section 2.4,a\nphenomenon which has been demonstrated by Tsipras et al.\n(2019), but without theoretical guarantees. We constructed\na toy example where the Bayes optimal classiﬁer achieves\nnatural error 0% and robust error 100%, while the trivial\nall-one classiﬁer achieves both natural error and robust er-\nror 50% (Table 1). Despite a large literature on the analysis\nof robust error in terms of generalization ( Schmidt et al. ,\n2018; Cullina et al. , 2018; Yin et al. , 2018) and computa-\ntional complexity ( Bubeck et al. , 2018b;a), the trade-off\nbetween the natural error and the robust error has not been\na focus of theoretical study.\nWe show that the robust error can in general be bounded\ntightly using two terms: one corresponds to the natural er-\nror measured by a surrogate loss function, and the other\ncorresponds to how likely the input features are close to the\n✏-extension of the decision boundary, termed as the bound-\nary error. We then minimize the differentiable upper bound.\nOur theoretical analysis naturally leads to a new formulation\nof adversarial defense which has several appealing proper-\nties; in particular, it inherits the beneﬁts of scalability to\nlarge datasets exhibited by Tiny ImageNet, and the algo-\nrithm achieves state-of-the-art performance on a range of\nbenchmarks while providing theoretical guarantees. For\nexample, while the defenses overviewed in ( Athalye et al.,\n2018) achieve robust accuracy no higher than ~ 47% under\nwhite-box attacks, our method achieves robust accuracy as\nhigh as ~57% in the same setting. The methodology is the\nfoundation of our entry to the NeurIPS 2018 Adversarial\nVision Challenge where we won ﬁrst place out of ~2,000\nsubmissions, surpassing the runner-up approach by 11.41%\nin terms of mean `2 perturbation distance.\n1.2. Summary of contributions\nOur work tackles the problem of trading accuracy off against\nrobustness and advances the state-of-the-art in multiple\nways.\n• Theoretically, we characterize the trade-off between\naccuracy and robustness for classiﬁcation problems\nvia decomposing the robust error as the sum of the\nnatural error and the boundary error. We provide differ-\nentiable upper bounds on both terms using the theory\nof classiﬁcation-calibrated loss, which are shown to be\nthe tightest upper bounds uniform over all probability\ndistributions and measurable predictors.\n• Algorithmically, inspired by our theoretical analysis,\nwe propose a new formulation of adversarial defense,\nTRADES, as optimizing a regularized surrogate loss.\nThe loss consists of two terms: the term of empirical\nrisk minimization encourages the algorithm to maxi-\nmize the natural accuracy, while the regularization term\nencourages the algorithm to push the decision bound-\nary away from the data, so as to improve adversarial\nrobustness (see Figure 1).\n• Experimentally, we show that our proposed algorithm\noutperforms state-of-the-art methods under both black-\nbox and white-box threat models. In particular, the\nmethodology won the ﬁnal round of the NeurIPS 2018\nAdversarial Vision Challenge.\n2. Preliminaries\nWe illustrate our methodology using the framework of bi-\nnary classiﬁcation, but it can be generalized to other settings\nas well.\n2.1. Notation\nWe will use bold capital letters such as X and Y to repre-\nsent random vector, bold lower-case letters such as x and y\nto represent realization of random vector, capital letters such\nas X and Y to represent random variable, and lower-case\nletters such as x and y to represent realization of random\nvariable. Speciﬁcally, we denote by x 2X the sample\ninstance, and by y 2 {\u00001, +1} the label, where X✓ Rd\nindicates the instance space. sign(x) represents the sign\nof scalar x with sign(0) = +1 . Denote by f : X! R\nthe score function which maps an instance to a conﬁdence\nvalue associated with being positive. It can be parametrized,\ne.g., by deep neural networks. The associated binary clas-\nsiﬁer is sign(f (·)). We will frequently use 1{event}, the\n0-1 loss, to represent an indicator function that is 1 if an\nevent happens and 0 otherwise. For norms, we denote by\nkxka generic norm. Examples of norms include kxk1 ,\nthe inﬁnity norm of vector x, and kxk2, the `2 norm of\nTheoretically Principled Trade-off between Robustness and Accuracy\nvector x. We use B(x,✏) to represent a neighborhood of\nx: {x02X : kx0\u0000 xk ✏}. For a given score function\nf , we denote by DB(f ) the decision boundary of f ; that\nis, the set {x 2X : f (x)=0 }. The set B(DB(f ),✏)\ndenotes the neighborhood of the decision boundary of f :\n{x 2X : 9x02 B(x,✏) s.t. f (x)f (x0)  0}. For a given\nfunction  (u), we denote by  ⇤(v): =s u pu{uT v\u0000 (u)}\nthe conjugate function of  , by  ⇤⇤the bi-conjugate, and\nby  \u00001 the inverse function. We will frequently use \u0000(·) to\nindicate the surrogate of 0-1 loss.\n2.2. Robust (classiﬁcation) error\nIn the setting of adversarial learning, we are given a set of\ninstances x1,. . . ,xn 2X and labels y1,. . . ,yn 2 {\u00001, +1}.\nWe assume that the data are sampled from an unknown dis-\ntribution (X,Y ) ⇠D . To characterize the robustness of a\nscore function f : X! R, Schmidt et al. (2018); Cullina\net al. (2018); Bubeck et al. (2018b) deﬁned robust (classiﬁca-\ntion) error under the threat model of bounded ✏ perturbation:\nRrob(f ): = E(X,Y )⇠D 1{9X02 B(X,✏) s.t. f (X0)Y \n0}. This is in sharp contrast to the standard measure of\nclassiﬁer performance—the natural (classiﬁcation) error\nRnat(f ): = E(X,Y )⇠D 1{f (X)Y  0} We note that the\ntwo errors satisfy Rrob(f ) \u0000R nat(f ) for all f ; the robust\nerror is equal to the natural error when ✏ =0 .\n2.3. Boundary error\nWe introduce the boundary error deﬁned as Rbdy(f ): =\nE(X,Y )⇠D 1{X 2 B(DB(f ),✏),f (X)Y> 0}. We have\nthe following decomposition of Rrob(f ):\nRrob(f )= Rnat(f )+ Rbdy(f ). (1)\n2.4. Trade-off between natural and robust errors\nOur study is motivated by the trade-off between natural and\nrobust errors. Tsipras et al. (2019) showed that training\nrobust models may lead to a reduction of standard accuracy.\nTo illustrate the phenomenon, we provide a toy example.\nExample. Consider the case (X, Y ) ⇠D , where the\nmarginal distribution over the instance space is a uniform\ndistribution over [0, 1], and for k =0 , 1,. . . ,d1\n2✏ \u0000 1e,\n⌘(x): =P r (Y =1 |X = x)\n=\n(\n0,x 2 [2k✏,(2k + 1)✏),\n1,x 2 ((2k + 1)✏,(2k + 2)✏].\n(2)\nSee Figure 2 for the visualization of ⌘(x). We consider two\nclassiﬁers: a) the Bayes optimal classiﬁer sign(2⌘(x) \u0000 1);\nb) the all-one classiﬁer which always outputs “positive.”\nTable 1 displays the trade-off between natural and robust\nerrors: the minimal natural error is achieved by the Bayes\n\t\n!(#)\t\n0\t\n1\t\n%\t\n%\t\n1/2\t\n #\t\n1\tFigure 2. Counterexample given by Eqn. ( 2).\nTable 1. Comparisons of natural and robust errors of Bayes optimal\nclassiﬁer and all-one classiﬁer in example (2). The Bayes optimal\nclassiﬁer has the optimal natural error while the all-one classiﬁer\nhas the optimal robust error.\nBayes Optimal Classiﬁer All-One Classiﬁer\nRnat 0 (optimal) 1/2\nRbdy 1 0\nRrob 1 1/2 (optimal)\noptimal classiﬁer with large robust error, while the optimal\nrobust error is achieved by the all-one classiﬁer with large\nnatural error.\nOur goal. In practice, one may prefer to trade-off between\nrobustness and accuracy by introducing weights in ( 1) to\nbias more towards the natural error or the boundary error.\nNoting that both the natural error and the boundary error\ninvolve 0-1 loss functions, our goal is to devise tight differ-\nentiable upper bounds on both of these terms. Towards this\ngoal, we utilize the theory of classiﬁcation-calibrated loss.\n2.5. Classiﬁcation-calibrated surrogate loss\nDeﬁnition. Minimization of the 0-1 loss in the natural and\nrobust errors is computationally intractable and the demands\nof computational efﬁciency have led researchers to focus\non minimization of a tractable surrogate loss, R\u0000 (f ): =\nE(X,Y )⇠D \u0000(f (X)Y ). We then need to ﬁnd quantitative re-\nlationships between the excess errors associated with \u0000 and\nthose associated with 0–1 loss. We make a weak assumption\non \u0000: it is classiﬁcation-calibrated (Bartlett et al. , 2006).\nFormally, for ⌘ 2 [0, 1], deﬁne the conditional \u0000-risk by\nH(⌘): = i n f\n↵ 2R\nC⌘(↵ ): = i n f\n↵ 2R\n(⌘\u0000(↵ )+( 1 \u0000 ⌘)\u0000(\u0000↵ )) ,\nand deﬁne H\u0000 (⌘): =i n f ↵ (2⌘\u00001)0 C⌘(↵ ). The\nclassiﬁcation-calibrated condition requires that imposing\nthe constraint that ↵ has an inconsistent sign with the Bayes\ndecision rule sign(2⌘ \u0000 1) leads to a strictly larger \u0000-risk:\nAssumption 1 (Classiﬁcation-Calibrated Loss). We assume\nthat the surrogate loss \u0000 is classiﬁcation-calibrated, mean-\ning that for any ⌘ 6=1 /2, H\u0000 (⌘) >H (⌘).\nWe argue that Assumption 1 is indispensable for classiﬁ-\ncation problems, since without it the Bayes optimal clas-\nTheoretically Principled Trade-off between Robustness and Accuracy\nTable 2. Examples of classiﬁcation-calibrated loss \u0000 and associated\n -transform. Here  log(✓)= 1\n2 (1 \u0000 ✓)l o g2(1 \u0000 ✓)+ 1\n2 (1 +\n✓)l o g2(1 + ✓).\nLoss \u0000(↵ )  (✓)\nHinge max{1 \u0000 ↵, 0} ✓\nSigmoid 1 \u0000 tanh(↵ ) ✓\nExponential exp(\u0000↵ )1 \u0000\np\n1 \u0000 ✓2\nLogistic log2(1 + exp(\u0000↵ ))  log(✓)\nsiﬁer cannot be the minimizer of the \u0000-risk. Examples of\nclassiﬁcation-calibrated loss include hinge loss, sigmoid\nloss, exponential loss, logistic loss, and many others (see\nTable 2).\nProperties. Classiﬁcation-calibrated loss has many struc-\ntural properties that one can exploit. We begin by intro-\nducing a functional transform of classiﬁcation-calibrated\nloss \u0000 which was proposed by Bartlett et al. (2006). De-\nﬁne the function  :[ 0 , 1] ! [0, 1 ) by  = e ⇤⇤, where\ne (✓): = H\u0000 \u00001+✓\n2\n\u0000\n\u0000 H\n\u00001+✓\n2\n\u0000\n. Indeed, the function  (✓)\nis the largest convex lower bound on H\u0000 \u00001+✓\n2\n\u0000\n\u0000 H\n\u00001+✓\n2\n\u0000\n.\nThe value H\u0000 \u00001+✓\n2\n\u0000\n\u0000 H\n\u00001+✓\n2\n\u0000\ncharacterizes how close\nthe surrogate loss \u0000 is to the class of non-classiﬁcation-\ncalibrated losses.\nBelow we state useful properties of the  -transform. We\nwill frequently use the function  to bound Rrob(f ) \u0000R ⇤\nnat.\nLemma 2.1 (Bartlett et al. (2006)). Under Assumption\n1, the function  has the following properties:  is non-\ndecreasing, continuous, convex on [0, 1] and  (0) = 0 .\n3. Relating 0-1 loss to surrogate loss\nIn this section, we present our main theoretical contributions\nfor binary classiﬁcation and compare our results with prior\nliterature. Binary classiﬁcation problems have received sig-\nniﬁcant attention in recent years as many competitions eval-\nuate the performance of robust models on binary classiﬁca-\ntion problems ( Brown et al., 2018). We defer the discussion\nof multi-class problems to Section 4.\n3.1. Upper bound\nOur analysis leads to a guarantee on the performance of\nsurrogate loss minimization. Intuitively, by Eqn. (1),\nRrob(f ) \u0000R ⇤\nnat = Rnat(f ) \u0000R ⇤\nnat + Rbdy(f ) \n \u00001(R\u0000 (f ) \u0000R ⇤\n\u0000 )+ Rbdy(f ), where the last inequality\nholds because we choose \u0000 as a classiﬁcation-calibrated\nloss (Bartlett et al., 2006). This leads to the following result.\nTheorem 3.1. Let R\u0000 (f ): = E\u0000(f (X)Y ) and R⇤\n\u0000 :=\nminf R\u0000 (f ). Under Assumption 1, for any non-negative\nloss function \u0000 such that \u0000(0) \u0000 1, any measurable f :\nX! R, any probability distribution on X ⇥ {± 1}, and\nany \u0000> 0, we have 1\nRrob(f ) \u0000R ⇤\nnat\n  \u00001(R\u0000 (f )\u0000R⇤\n\u0000 )+Pr[X2B(DB(f ),✏),f (X)Y> 0]\n  \u00001(R\u0000 (f )\u0000R⇤\n\u0000 )+ E max\nX0 2B(X,✏)\n\u0000(f (X0)f (X)/\u0000).\nQuantity governing model robustness. Our result pro-\nvides a formal justiﬁcation for the existence of adversar-\nial examples: learning models are vulnerable to small\nadversarial attacks because the probability that data lie\naround the decision boundary of the model, Pr[X 2\nB(DB(f ),✏),f (X)Y> 0], is large. As a result, small\nperturbations may move the data point to the wrong side\nof the decision boundary, leading to weak robustness of\nclassiﬁcation models.\n3.2. Lower bound\nWe now establish a lower bound on Rrob(f ) \u0000R ⇤\nnat. Our\nlower bound matches our analysis of the upper bound in\nSection 3.1 up to an arbitrarily small constant.\nTheorem 3.2. Suppose that |X | \u0000 2. Under Assumption\n1, for any non-negative loss function \u0000 such that \u0000(x) ! 0\nas x ! +1 , any ⇠> 0, and any ✓ 2 [0, 1], there exists\na probability distribution on X ⇥ {± 1}, a function f :\nRd ! R, and a regularization parameter \u0000> 0 such that\nRrob(f ) \u0000R ⇤\nnat = ✓ and\n \n⇣\n✓ \u0000 E max\nX0 2B(X,✏)\n\u0000(f (X0)f (X)/\u0000)\n⌘\nR \u0000 (f ) \u0000R ⇤\n\u0000\n  \n✓\n✓ \u0000 E max\nX0 2B(X,✏)\n\u0000(f (X0)f (X)/\u0000)\n◆\n+ ⇠.\nTheorem 3.2 demonstrates that in the presence of extra\nconditions on the loss function, i.e., limx! +1 \u0000(x)=0 ,\nthe upper bound in Section 3.1 is tight. The condition holds\nfor all the losses in Table 2.\n4. Algorithmic Design for Defenses\nOptimization. Theorems 3.1 and 3.2 shed light on algorith-\nmic designs of adversarial defenses. In order to minimize\nRrob(f ) \u0000R ⇤\nnat, the theorems suggest minimizing 2\nmin\nf\nE\nn\n\u0000(f (X)Y )| {z }\nfor accuracy\n+ max\nX0 2B(X,✏)\n\u0000(f (X)f (X0)/\u0000)\n| {z }\nregularization for robustness\no\n.\n(3)\n1We study the population form of the risk functions, and\nmention that by incorporating the generalization theory for\nclassiﬁcation-calibrated losses ( Bartlett et al. , 2006) one can ex-\ntend the analysis to ﬁnite samples. We leave this analysis for future\nresearch.\n2For simplicity of implementation, we do not use the function\n \u00001 and rely on \u0000 to approximately reﬂect the effect of  \u00001, the\ntrade-off between the natural error and the boundary error, and the\ntight approximation of the boundary error using the corresponding\nsurrogate loss function.\nTheoretically Principled Trade-off between Robustness and Accuracy\nWe name our method TRADES (TRadeoff-inspired Adver-\nsarial DEfense via Surrogate-loss minimization).\nIntuition behind the optimization. Problem (3) captures\nthe trade-off between the natural and robust errors: the ﬁrst\nterm in (3) encourages the natural error to be optimized by\nminimizing the “difference” between f (X) and Y , while\nthe second regularization term encourages the output to be\nsmooth, that is, it pushes the decision boundary of classiﬁer\naway from the sample instances via minimizing the “dif-\nference” between the prediction of natural example f (X)\nand that of adversarial example f (X0). This is conceptually\nconsistent with the argument that smoothness is an indis-\npensable property of robust models ( Cisse et al., 2017). The\ntuning parameter \u0000 plays a critical role on balancing the\nimportance of natural and robust errors. To see how the \u0000\naffects the solution in the example of Section 2.4, problem\n(3) tends to the Bayes optimal classiﬁer when \u0000 ! +1 ,\nand tends to the all-one classiﬁer when \u0000 ! 0.\nComparisons with prior work. We compare our approach\nwith several related lines of research in the prior litera-\nture. One of the best known algorithms for adversarial\ndefense is based on robust optimization (Madry et al. , 2018;\nKolter & Wong, 2018; Wong et al., 2018; Raghunathan et al.,\n2018a;b). Most results in this direction involve algorithms\nthat approximately minimize\nmin\nf\nE\n⇢\nmax\nX0 2B(X,✏)\n\u0000(f (X0)Y )\n\u0000\n, (4)\nwhere the objective function in problem (4) serves as an up-\nper bound of the robust error Rrob(f ). In complex problem\ndomains, however, this objective function might not be tight\nas an upper bound of the robust error, and may not capture\nthe trade-off between natural and robust errors.\nA related line of research is adversarial training by regular-\nization (Kurakin et al. , 2017; Ross & Doshi-V elez, 2017;\nZheng et al. , 2016). There are several key differences\nbetween the results in this paper and those of ( Kurakin\net al., 2017; Ross & Doshi-V elez, 2017; Zheng et al. , 2016).\nFirstly, the optimization formulations are different. In the\nprevious works, the regularization term either measures the\n“difference” between f (X0) and Y (Kurakin et al., 2017),\nor its gradient ( Ross & Doshi-V elez, 2017). In contrast,\nour regularization term measures the “difference” between\nf (X) and f (X0). While Zheng et al. (2016) generated the\nadversarial example X0by adding random Gaussian noise\nto X, our method simulates the adversarial example by solv-\ning the inner maximization problem in Eqn. (3). Secondly,\nwe note that the losses in ( Kurakin et al. , 2017; Ross &\nDoshi-V elez, 2017; Zheng et al., 2016) lack of theoretical\nguarantees. Our loss, with the presence of the second term\nin problem (3), makes our theoretical analysis signiﬁcantly\nmore subtle. Moreover, our algorithm takes the same com-\nputational resources as ( Kurakin et al., 2017), which makes\nAlgorithm 1 Adversarial training by TRADES\ninput Step sizes ⌘1 and ⌘2, batch size m, number of iter-\nations K in inner optimization, network architecture\nparametrized by ✓\noutput Robust network f✓\n1: Randomly initialize network f✓, or initialize network\nwith pre-trained conﬁguration\n2: repeat\n3: Read mini-batch B = {x1,. . . ,xm} from training set\n4: for i =1 ,. . . ,m(in parallel) do\n5: x0\ni  xi +0 .001 ·N (0, I), where N (0, I) is the\nGaussian distribution with zero mean and identity\nvariance\n6: for k =1 ,. . . ,Kdo\n7: x0\ni  ⇧B(xi ,✏)(⌘1sign(rx0\ni\nL(f✓(xi),f ✓(x0\ni)))+\nx0\ni), where ⇧ is the projection operator\n8: end for\n9: end for\n10: ✓  ✓ \u0000 ⌘2\nP m\ni=1 r✓[L(f✓(xi), yi)+\nL(f✓(xi),f ✓(x0\ni))/\u0000]/m\n11: until training converged\nour method scalable to large-scale datasets. We defer the\nexperimental comparisons of various regularization based\nmethods to Table 5.\nHeuristic algorithm. In response to the optimization for-\nmulation (3), we use two heuristics to achieve more general\ndefenses: a) extending to multi-class problems by involv-\ning multi-class calibrated loss; b) approximately solving\nthe minimax problem via alternating gradient descent. For\nmulti-class problems, a surrogate loss is calibrated if mini-\nmizers of the surrogate risk are also minimizers of the 0-1\nrisk (Pires & Szepesvári , 2016). Examples of multi-class\ncalibrated loss include cross-entropy loss. Algorithmically,\nwe extend problem (3) to the case of multi-class classiﬁca-\ntions by replacing \u0000 with a multi-class calibrated loss L(·, ·):\nmin\nf\nE\n⇢\nL(f (X), Y ) + max\nX0 2B(X,✏)\nL(f (X),f (X0))/\u0000\n\u0000\n,\n(5)\nwhere f (X) is the output vector of learning model (with\nsoftmax operator in the top layer for the cross-entropy loss\nL(·, ·)), Y is the label-indicator vector, and \u0000> 0 is the\nregularization parameter. The pseudocode of adversarial\ntraining procedure, which aims at minimizing the empirical\nform of problem ( 5), is displayed in Algorithm 1.\nThe key ingredient of the algorithm is to approximately\nsolve the linearization of inner maximization in problem (5)\nby the projected gradient descent (see Step 7). We note that\nxi is a global minimizer with zero gradient to the objective\nfunction g(x0): = L(f (xi),f (x0)) in the inner problem.\nTherefore, we initialize x0\ni by adding a small, random per-\nturbation around xi in Step 5 to start the inner optimizer.\nTheoretically Principled Trade-off between Robustness and Accuracy\nTable 3. Theoretical veriﬁcation on the optimality of Theorem 3.1.\n\u0000 Arob(f )( % ) R\u0000 (f )\u0000 = \u0000 RHS \u0000 \u0000LHS\n2.0 99.43 0.0006728 0.006708\n3.0 99.41 0.0004067 0.005914\n4.0 99.37 0.0003746 0.006757\n5.0 99.34 0.0003430 0.005860\nMore exhaustive approximations of the inner maximization\nproblem in terms of either optimization formulations or\nsolvers would lead to better defense performance.\n5. Experimental Results\nIn this section, we verify the effectiveness of TRADES\nby numerical experiments. We denote by Arob(f )=\n1 \u0000R rob(f ) the robust accuracy, and by Anat(f )=\n1 \u0000R nat(f ) the natural accuracy on test dataset. We release\nour code and trained models at https://github.com/\nyaodongyu/TRADES.\n5.1. Optimality of Theorem 3.1\nWe verify the tightness of the established upper bound in\nTheorem 3.1 for binary classiﬁcation problem on MNIST\ndataset. The negative examples are ‘1’ and the positive\nexamples are ‘3’. Here we use a Convolutional Neural\nNetwork (CNN) with two convolutional layers, followed\nby two fully-connected layers. The output size of the last\nlayer is 1. To learn the robust classiﬁer, we minimize the\nregularized surrogate loss in Eqn. (3), and use the hinge\nloss in Table 2 as the surrogate loss \u0000, where the associated\n -transform is  (✓)= ✓.\nTo verify the tightness of our upper bound, we calculate the\nleft hand side in Theorem 3.1, i.e.,\n\u0000LHS = Rrob(f ) \u0000R ⇤\nnat,\nand the right hand side, i.e.,\n\u0000RHS =( R\u0000 (f ) \u0000R ⇤\n\u0000 )+ E max\nX0 2B(X,✏)\n\u0000(f (X0)f (X)/\u0000).\nAs we cannot have access to the unknown distribution D,\nwe approximate the above expectation terms by test dataset.\nWe ﬁrst use natural training method to train a classiﬁer so\nas to approximately estimate R⇤\nnat and R⇤\n\u0000 , where we ﬁnd\nthat the naturally trained classiﬁer can achieve natural error\nR⇤\nnat = 0%, and loss value R⇤\n\u0000 =0 .0 for the binary classi-\nﬁcation problem. Next, we optimize problem (3) to train a\nrobust classiﬁer f . We take perturbation ✏ =0 .1, number of\niterations K = 20 and run 30 epochs on the training dataset.\nFinally, to approximate the second term in \u0000RHS, we use\nFGSMk (white-box) attack (a.k.a. PGD attack) ( Kurakin\net al., 2017) with 20 iterations to approximately calculate\nthe worst-case perturbed data X0.\nThe results in Table 3 show the tightness of our upper bound\nin Theorem 3.1. It shows that the differences between \u0000RHS\nand \u0000LHS under various \u0000’s are very small.\n5.2. Sensitivity of regularization hyperparameter \u0000\nThe regularization parameter \u0000 is an important hyperparame-\nter in our proposed method. We show how the regularization\nparameter affects the performance of our robust classiﬁers\nby numerical experiments on two datasets, MNIST and CI-\nFAR10. For both datasets, we minimize the loss in Eqn. (5)\nto learn robust classiﬁers for multi-class problems, where\nwe choose L as the cross-entropy loss.\nMNIST setup. We use the CNN which has two convolu-\ntional layers, followed by two fully-connected layers. The\noutput size of the last layer is 10. We set perturbation\n✏ =0 .1, perturbation step size ⌘1 =0 .01, number of itera-\ntions K = 20, learning rate ⌘2 =0 .01, batch size m = 128,\nand run 50 epochs on the training dataset. To evaluate the\nrobust error, we apply FGSM k (white-box) attack with 40\niterations and 0.005 step size. The results are in Table 4.\nCIFAR10 setup. We apply ResNet-18 (He et al. , 2016) for\nclassiﬁcation. The output size of the last layer is 10. We set\nperturbation ✏ =0 .031, perturbation step size ⌘1 =0 .007,\nnumber of iterations K = 10, learning rate ⌘2 =0 .1, batch\nsize m = 128 , and run 100 epochs on the training dataset.\nTo evaluate the robust error, we apply FGSM k (white-box)\nattack with 20 iterations and the step size is 0.003. The\nresults are in Table 4.\nWe observe that as the regularization parameter 1/\u0000 in-\ncreases, the natural accuracy Anat(f ) decreases while the\nrobust accuracy Arob(f ) increases, which veriﬁes our the-\nory on the trade-off between robustness and accuracy. Note\nthat for MNIST dataset, the natural accuracy does not de-\ncrease too much as the regularization term 1/\u0000 increases,\nwhich is different from the results of CIFAR10. This is\nprobably because the classiﬁcation task for MNIST is easier.\nMeanwhile, our proposed method is not very sensitive to the\nchoice of \u0000. Empirically, when we set the hyperparameter\n1/\u0000 in [1, 10], our method is able to learn classiﬁers with\nboth high robustness and high accuracy. We will set 1/\u0000 as\neither 1 or 6 in the following experiments.\n5.3. Adversarial defenses under various attacks\nPreviously, Athalye et al. (2018) showed that 7 defenses in\nICLR 2018 which relied on obfuscated gradients may easily\nbreak down. In this section, we verify the effectiveness of\nour method with the same experimental setup under both\nwhite-box and black-box threat models.\nMNIST setup. We use the CNN architecture in ( Carlini &\nWagner, 2017) with four convolutional layers, followed by\nthree fully-connected layers. We set perturbation ✏ =0 .3,\nperturbation step size ⌘1 =0 .01, number of iterations K =\n40, learning rate ⌘2 =0 .01, batch size m = 128 , and run\n100 epochs on the training dataset.\nCIFAR10 setup. We use the same neural network architec-\nture as ( Madry et al. , 2018), i.e., the wide residual network\nTheoretically Principled Trade-off between Robustness and Accuracy\nTable 4. Sensitivity of regularization hyperparameter \u0000 on MNIST and CIFAR10 datasets.\n1/\u0000 Arob(f )( % ) on MNIST Anat(f )( % ) on MNIST Arob(f )( % ) on CIFAR10 Anat(f )( % ) on CIFAR10\n1.0 94.75 ± 0.0712 99.28 ± 0.0125 44.68 ± 0.3088 87.01 ± 0.2819\n2.0 95.45 ± 0.0883 99.29 ± 0.0262 48.22 ± 0.0740 85.22 ± 0.0543\n3.0 95.57 ± 0.0262 99.24 ± 0.0216 49.67 ± 0.3179 83.82 ± 0.4050\n4.0 95.65 ± 0.0340 99.16 ± 0.0205 50.25 ± 0.1883 82.90 ± 0.2217\n5.0 95.65 ± 0.1851 99.16 ± 0.0403 50.64 ± 0.3336 81.72 ± 0.0286\nWRN-34-10 (Zagoruyko & Komodakis, 2016). We set per-\nturbation ✏ =0 .031, perturbation step size ⌘1 =0 .007,\nnumber of iterations K = 10, learning rate ⌘2 =0 .1, batch\nsize m = 128, and run 100 epochs on the training dataset.\n5.3.1. W HITE -BOX A TTACKS\nWe summarize our results in Table 5 together with the re-\nsults from ( Athalye et al. , 2018). We also implement meth-\nods in ( Zheng et al. , 2016; Kurakin et al. , 2017; Ross &\nDoshi-V elez, 2017) on the CIFAR10 dataset as they are also\nregularization based methods. For MNIST dataset, we ap-\nply FGSMk (white-box) attack with 40 iterations and the\nstep size is 0.01. For CIFAR10 dataset, we apply FGSM k\n(white-box) attack with 20 iterations and the step size is\n0.003, under which the defense model in ( Madry et al. ,\n2018) achieves 47.04% robust accuracy. Table 5 shows that\nour proposed defense method can signiﬁcantly improve the\nrobust accuracy of models, which is able to achieve robust\naccuracy as high as 56.61%. We also evaluate our robust\nmodel on MNIST dataset under the same threat model as\nin (Samangouei et al., 2018) (C&W white-box attack Carlini\n& Wagner (2017)), and the robust accuracy is 99.46%. See\nappendix for detailed information of models in Table 5.\n5.3.2. B LACK -BOX A TTACKS\nWe verify the robustness of our models under black-box at-\ntacks. We ﬁrst train models without using adversarial train-\ning on the MNIST and CIFAR10 datasets. We use the same\nnetwork architectures that are speciﬁed in the beginning of\nthis section, i.e., the CNN architecture in ( Carlini & Wag-\nner, 2017) and the WRN-34-10 architecture in ( Zagoruyko\n& Komodakis , 2016). We denote these models by natu-\nrally trained models ( Natural). The accuracy of the natu-\nrally trained CNN model is 99.50% on the MNIST dataset.\nThe accuracy of the naturally trained WRN-34-10 model is\n95.29% on the CIFAR10 dataset. We also implement the\nmethod proposed in ( Madry et al. , 2018) on both datasets.\nWe denote these models by Madry’s models ( Madry). The\naccuracy of Madry et al. (2018)’s CNN model is 99.36% on\nthe MNIST dataset. The accuracy of Madry et al. (2018)’s\nWRN-34-10 model is 85.49% on the CIFAR10 dataset.\nFor both datasets, we use FGSM k (black-box) method to\nattack various defense models. For MNIST dataset, we set\nperturbation ✏ =0 .3 and apply FGSMk (black-box) attack\nwith 40 iterations and the step size is 0.01. For CIFAR10\ndataset, we set ✏ =0 .031 and apply FGSM k (black-box)\nattack with 20 iterations and the step size is 0.003. Note that\nthe setup is the same as the setup speciﬁed in Section 5.3.1.\nWe summarize our results in Table 6 and Table 7. In both\ntables, we use two source models (noted in the parentheses)\nto generate adversarial perturbations: we compute the per-\nturbation directions according to the gradients of the source\nmodels on the input images. It shows that our models are\nmore robust against black-box attacks transfered from nat-\nurally trained models and Madry et al. (2018)’s models.\nMoreover, our models can generate stronger adversarial\nexamples for black-box attacks compared with naturally\ntrained models and Madry et al. (2018)’s models.\n5.4. Case study: NeurIPS 2018 Adversarial Vision\nChallenge\nCompetition settings. In the adversarial competition, the\nadversarial attacks and defenses are under the black-box\nsetting. The dataset in this competition is Tiny ImageNet,\nwhich consists of 550,000 data (with our data augmentation)\nand 200 classes. The robust models only return label pre-\ndictions instead of explicit gradients and conﬁdence scores.\nThe task for robust models is to defend against adversarial\nexamples that are generated by the top-5 submissions in the\nun-targeted attack track. The score for each defense model\nis evaluated by the smallest perturbation distance that makes\nthe defense model fail to output correct labels.\nCompetition results. The methodology in this paper was\napplied to the competition, where our entry ranked the 1st\nplace. We implemented our method to train ResNet models.\nWe report the mean `2 perturbation distance of the top-6\nentries in Figure 3. It shows that our method outperforms\nother approaches with a large margin. In particular, we\nsurpass the runner-up submission by 11.41% in terms of\nmean `2 perturbation distance.\n6. Conclusions\nIn this paper, we study the problem of adversarial defenses\nagainst structural perturbations around input data. We focus\non the trade-off between robustness and accuracy, and show\nan upper bound on the gap between robust error and optimal\nnatural error. Our result advances the state-of-the-art work\nand matches the lower bound in the worst-case scenario.\nThe bounds motivate us to minimize a new form of regu-\nlarized surrogate loss, TRADES, for adversarial training.\nTheoretically Principled Trade-off between Robustness and Accuracy\nTable 5. Comparisons of TRADES with prior defense models under white-box attacks.\nDefense Defense type Under which attack Dataset Distance Anat(f ) Arob(f )\nBuckman et al. (2018) gradient mask Athalye et al. (2018) CIFAR10 0.031 (`1 ) - 0%\nMa et al. (2018) gradient mask Athalye et al. (2018) CIFAR10 0.031 (`1 ) - 5%\nDhillon et al. (2018) gradient mask Athalye et al. (2018) CIFAR10 0.031 (`1 ) - 0%\nSong et al. (2018) gradient mask Athalye et al. (2018) CIFAR10 0.031 (`1 ) - 9%\nNa et al. (2017) gradient mask Athalye et al. (2018) CIFAR10 0.015 (`1 ) - 15%\nWong et al. (2018) robust opt. FGSM20 (PGD) CIFAR10 0.031 (`1 ) 27.07% 23.54%\nMadry et al. (2018) robust opt. FGSM20 (PGD) CIFAR10 0.031 (`1 ) 87.30% 47.04%\nZheng et al. (2016) regularization FGSM20 (PGD) CIFAR10 0.031 (`1 ) 94.64% 0.15%\nKurakin et al. (2017) regularization FGSM20 (PGD) CIFAR10 0.031 (`1 ) 85.25% 45.89%\nRoss & Doshi-V elez(2017) regularization FGSM20 (PGD) CIFAR10 0.031 (`1 ) 95.34% 0%\nTRADES (1/\u0000 =1 .0) regularization FGSM20 (PGD) CIFAR10 0.031 (`1 ) 88.64% 49.14%\nTRADES (1/\u0000 =6 .0) regularization FGSM20 (PGD) CIFAR10 0.031 (`1 ) 84.92% 56.61%\nTRADES (1/\u0000 =1 .0) regularization DeepFool (`1 ) CIFAR10 0.031 (`1 ) 88.64% 59.10%\nTRADES (1/\u0000 =6 .0) regularization DeepFool (`1 ) CIFAR10 0.031 (`1 ) 84.92% 61.38%\nTRADES (1/\u0000 =1 .0) regularization LBFGSAttack CIFAR10 0.031 (`1 ) 88.64% 84.41%\nTRADES (1/\u0000 =6 .0) regularization LBFGSAttack CIFAR10 0.031 (`1 ) 84.92% 81.58%\nTRADES (1/\u0000 =1 .0) regularization MI-FGSM CIFAR10 0.031 (`1 ) 88.64% 51.26%\nTRADES (1/\u0000 =6 .0) regularization MI-FGSM CIFAR10 0.031 (`1 ) 84.92% 57.95%\nTRADES (1/\u0000 =1 .0) regularization C&W CIFAR10 0.031 (`1 ) 88.64% 84.03%\nTRADES (1/\u0000 =6 .0) regularization C&W CIFAR10 0.031 (`1 ) 84.92% 81.24%\nSamangouei et al. (2018) gradient mask Athalye et al. (2018) MNIST 0.005 (`2) - 55%\nMadry et al. (2018) robust opt. FGSM40 (PGD) MNIST 0.3 (`1 ) 99.36% 96.01%\nTRADES (1/\u0000 =6 .0) regularization FGSM40 (PGD) MNIST 0.3 (`1 ) 99.48% 96.07%\nTRADES (1/\u0000 =6 .0) regularization C&W MNIST 0.005 (`2) 99.48% 99.46%\nTable 6. Comparisons of TRADES with prior defenses under black-\nbox FGSM 40 attack on the MNIST dataset. The models inside\nparentheses are source models which provide gradients to adver-\nsarial attackers. We provide the average cross-entropy loss value\nL(f (X), Y ) of each defense model in the bracket. The defense\nmodel ‘Madry’ is the same model as in the antepenultimate line of\nTable 5. The defense model ‘TRADES’ is the same model as in\nthe penultimate line of Table 5.\nDefense Model Robust Accuracy Arob(f )\nMadry 97.43% [0.0078484] (Natural)\nTRADES 97.63% [0.0075324] (Natural)\nMadry 97.38% [0.0084962] (Ours)\nTRADES 97.66% [0.0073532] (Madry)\nTable 7. Comparisons of TRADES with prior defenses under black-\nbox FGSM20 attack on the CIFAR10 dataset. The models inside\nparentheses are source models which provide gradients to adver-\nsarial attackers. We provide the average cross-entropy loss value\nof each defense model in the bracket. The defense model ‘Madry’\nis implemented based on ( Madry et al. , 2018), and the defense\nmodel ‘TRADES’ is the same model as in the 11th line of Table 5.\nDefense Model Robust Accuracy Arob(f )\nMadry 84.39% [0.0519784] (Natural)\nTRADES 87.60% [0.0380258] (Natural)\nMadry 66.00% [0.1252672] (Ours)\nTRADES 70.14% [0.0885364] (Madry)\n1st\t(TRADES) 2.256\n2nd 2.025\n3rd 1.6374th 1.585\n5th 1.4766th 1.401\n2.256\n2.025\n1.637 1.585 1.476 1.401\n0\n0.5\n1\n1.5\n2\n2.5\n1st\t(TRADES)2nd 3rd 4th 5th 6th\nFigure 3. Top-6 results (out of ~2,000 submissions) in the NeurIPS\n2018 Adversarial Vision Challenge. The vertical axis represents\nthe mean `2 perturbation distance that makes robust models fail to\noutput correct labels.\nExperiments on real datasets and adversarial competition\ndemonstrate the effectiveness of our proposed algorithms.\nIt would be interesting to combine our methods with other\nrelated line of research on adversarial defenses, e.g., feature\ndenoising technique ( Xie et al. , 2018) and network archi-\ntecture design ( Cisse et al. , 2017), to achieve more robust\nlearning systems.\nAcknowledgements. We thank Maria-Florina Balcan and\nAvrim Blum for valuable discussions. Part of this work\nwas done while H. Z. was visiting Simons Institute for the\nTheory of Computing, and Y . Y . was an intern at Petuum.\nTheoretically Principled Trade-off between Robustness and Accuracy\nReferences\nAlaifari, R., Alberti, G. S., and Gauksson, T. ADef: an\niterative algorithm to construct adversarial deformations.\nIn International Conference on Learning Representations,\n2019.\nAthalye, A., Carlini, N., and Wagner, D. Obfuscated gra-\ndients give a false sense of security: Circumventing de-\nfenses to adversarial examples. In International Confer-\nence on Machine Learning , 2018.\nBarthe, F. Extremal properties of central half-spaces for\nproduct measures. Journal of Functional Analysis , 182\n(1):81–107, 2001.\nBartlett, P . L., Jordan, M. I., and McAuliffe, J. D. Convexity,\nclassiﬁcation, and risk bounds. Journal of the American\nStatistical Association, 101(473):138–156, 2006.\nBrendel, W., Rauber, J., and Bethge, M. Decision-based\nadversarial attacks: Reliable attacks against black-box\nmachine learning models. In International Conference\non Learning Representations, 2018.\nBrown, T. B., Carlini, N., Zhang, C., Olsson, C., Christiano,\nP ., and Goodfellow, I. Unrestricted adversarial examples.\narXiv preprint arXiv:1809.08352, 2018.\nBubeck, S., Lee, Y . T., Price, E., and Razenshteyn, I. Ad-\nversarial examples from cryptographic pseudo-random\ngenerators. arXiv preprint arXiv:1811.06418, 2018a.\nBubeck, S., Price, E., and Razenshteyn, I. Adversarial\nexamples from computational constraints. arXiv preprint\narXiv:1805.10204, 2018b.\nBuckman, J., Roy, A., Raffel, C., and Goodfellow, I. Ther-\nmometer encoding: One hot way to resist adversarial\nexamples. In International Conference on Learning Rep-\nresentations, 2018.\nCarlini, N. and Wagner, D. Towards evaluating the robust-\nness of neural networks. In IEEE Symposium on Security\nand Privacy, pp. 39–57, 2017.\nCisse, M., Bojanowski, P ., Grave, E., Dauphin, Y ., and\nUsunier, N. Parseval networks: Improving robustness\nto adversarial examples. In International Conference on\nMachine Learning, 2017.\nCullina, D., Bhagoji, A. N., and Mittal, P . PAC-learning\nin the presence of adversaries. In Advances in Neural\nInformation Processing Systems, pp. 228–239, 2018.\nDhillon, G. S., Azizzadenesheli, K., Lipton, Z. C., Bern-\nstein, J., Kossaiﬁ, J., Khanna, A., and Anandkumar, A.\nStochastic activation pruning for robust adversarial de-\nfense. arXiv preprint arXiv:1803.01442, 2018.\nDong, Y ., Liao, F., Pang, T., Su, H., Zhu, J., Hu, X., and Li,\nJ. Boosting adversarial attacks with momentum. In IEEE\nConference on Computer Vision and Pattern Recognition ,\npp. 9185–9193, 2018.\nEngstrom, L., Tran, B., Tsipras, D., Schmidt, L., and\nMadry, A. A rotation and a translation sufﬁce: Fool-\ning CNNs with simple transformations. arXiv preprint\narXiv:1712.02779, 2017.\nEngstrom, L., Ilyas, A., and Athalye, A. Evaluating and\nunderstanding the robustness of adversarial logit pairing.\narXiv preprint arXiv:1807.10272, 2018.\nFawzi, A., Fawzi, H., and Fawzi, O. Adversarial vulnerabil-\nity for any classiﬁer. In Advances in Neural Information\nProcessing Systems, pp. 1186–1195, 2018.\nGilmer, J., Adams, R. P ., Goodfellow, I., Andersen, D., and\nDahl, G. E. Motivating the rules of the game for adversar-\nial example research. arXiv preprint arXiv:1807.06732,\n2018.\nGoodfellow, I. J., Shlens, J., and Szegedy, C. Explaining\nand harnessing adversarial examples. In International\nConference on Learning Representations, 2015.\nHe, K., Zhang, X., Ren, S., and Sun, J. Deep residual\nlearning for image recognition. In IEEE conference on\ncomputer vision and pattern recognition , pp. 770–778,\n2016.\nHe, W., Wei, J., Chen, X., Carlini, N., and Song, D. Adver-\nsarial example defenses: Ensembles of weak defenses are\nnot strong. arXiv preprint arXiv:1706.04701, 2017.\nHuang, R., Xu, B., Schuurmans, D., and Szepesvári,\nC. Learning with a strong adversary. arXiv preprint\narXiv:1511.03034, 2015.\nHuang, S., Papernot, N., Goodfellow, I., Duan, Y ., and\nAbbeel, P . Adversarial attacks on neural network policies.\narXiv preprint arXiv:1702.02284, 2017.\nJia, R. and Liang, P . Adversarial examples for evaluating\nreading comprehension systems. In Empirical Methods\nin Natural Language Processing, 2017.\nKannan, H., Kurakin, A., and Goodfellow, I. Adversarial\nlogit pairing. arXiv preprint arXiv:1803.06373, 2018.\nKolter, J. Z. and Wong, E. Provable defenses against adver-\nsarial examples via the convex outer adversarial polytope.\nIn International Conference on Machine Learning , 2018.\nKurakin, A., Goodfellow, I., and Bengio, S. Adversarial\nmachine learning at scale. In International Conference\non Learning Representations, 2017.\nTheoretically Principled Trade-off between Robustness and Accuracy\nMa, X., Li, B., Wang, Y ., Erfani, S. M., Wijewickrema, S.,\nHoule, M. E., Schoenebeck, G., Song, D., and Bailey, J.\nCharacterizing adversarial subspaces using local intrinsic\ndimensionality. arXiv preprint arXiv:1801.02613, 2018.\nMadry, A., Makelov, A., Schmidt, L., Tsipras, D., and\nVladu, A. Towards deep learning models resistant to\nadversarial attacks. In International Conference on Learn-\ning Representations, 2018.\nMeng, D. and Chen, H. Magnet: a two-pronged defense\nagainst adversarial examples. In ACM SIGSAC Confer-\nence on Computer and Communications Security , pp.\n135–147, 2017.\nMoosavi-Dezfooli, S.-M., Fawzi, A., and Frossard, P . Deep-\nfool: a simple and accurate method to fool deep neural\nnetworks. In IEEE Conference on Computer Vision and\nPattern Recognition, pp. 2574–2582, 2016.\nNa, T., Ko, J. H., and Mukhopadhyay, S. Cascade adver-\nsarial machine learning regularized with a uniﬁed embed-\nding. arXiv preprint arXiv:1708.02582, 2017.\nPires, B. Á. and Szepesvári, C. Multiclass classiﬁcation\ncalibration functions. arXiv preprint arXiv:1609.06385,\n2016.\nRaghunathan, A., Steinhardt, J., and Liang, P . Certiﬁed\ndefenses against adversarial examples. In International\nConference on Learning Representations, 2018a.\nRaghunathan, A., Steinhardt, J., and Liang, P . S. Semidef-\ninite relaxations for certifying robustness to adversarial\nexamples. In Advances in Neural Information Processing\nSystems, pp. 10899–10909, 2018b.\nRauber, J., Brendel, W., and Bethge, M. Foolbox v0. 8.0: A\npython toolbox to benchmark the robustness of machine\nlearning models. arXiv preprint arXiv:1707.04131, 2017.\nRoss, A. S. and Doshi-V elez, F. Improving the adversarial\nrobustness and interpretability of deep neural networks\nby regularizing their input gradients. arXiv preprint\narXiv:1711.09404, 2017.\nSamangouei, P ., Kabkab, M., and Chellappa, R. Defense-\ngan: Protecting classiﬁers against adversarial attacks us-\ning generative models. arXiv preprint arXiv:1805.06605,\n2018.\nSchmidt, L., Santurkar, S., Tsipras, D., Talwar, K., and\nM ˛ adry, A. Adversarially robust generalization requires\nmore data. In Advances in Neural Information Processing\nSystems 31, pp. 5019–5031, 2018.\nShaham, U., Y amada, Y ., and Negahban, S. Understanding\nadversarial training: Increasing local stability of neu-\nral nets through robust optimization. arXiv preprint\narXiv:1511.05432, 2015.\nSinha, A., Namkoong, H., and Duchi, J. Certiﬁable distribu-\ntional robustness with principled adversarial training. In\nInternational Conference on Learning Representations ,\n2018.\nSong, Y ., Kim, T., Nowozin, S., Ermon, S., and Kushman, N.\nPixeldefend: Leveraging generative models to understand\nand defend against adversarial examples. In International\nConference on Learning Representations, 2018.\nSu, D., Zhang, H., Chen, H., Yi, J., Chen, P .-Y ., and Gao, Y .\nIs robustness the cost of accuracy? — a comprehensive\nstudy on the robustness of 18 deep image classiﬁcation\nmodels. In European Conference on Computer Vision ,\n2018.\nSzegedy, C., Zaremba, W., Sutskever, I., Bruna, J., Erhan,\nD., Goodfellow, I., and Fergus, R. Intriguing properties of\nneural networks. arXiv preprint arXiv:1312.6199, 2013.\nTabacof, P . and V alle, E. Exploring the space of adversarial\nimages. In International Joint Conference on Neural\nNetworks, pp. 426–433, 2016.\nTramèr, F., Kurakin, A., Papernot, N., Goodfellow, I.,\nBoneh, D., and McDaniel, P . Ensemble adversarial train-\ning: Attacks and defenses. In International Conference\non Learning Representations, 2018.\nTsipras, D., Santurkar, S., Engstrom, L., Turner, A., and\nMadry, A. Robustness may be at odds with accuracy. In\nInternational Conference on Learning Representations ,\n2019.\nUesato, J., O’Donoghue, B., Kohli, P ., and van den Oord,\nA. Adversarial risk and the dangers of evaluating against\nweak attacks. In International Conference on Machine\nLearning, pp. 5025–5034, 2018.\nV olpi, R., Namkoong, H., Sener, O., Duchi, J. C., Murino,\nV ., and Savarese, S. Generalizing to unseen domains via\nadversarial data augmentation. In Advances in Neural\nInformation Processing Systems, pp. 5339–5349, 2018.\nWong, E., Schmidt, F., Metzen, J., and Kolter, J. Scaling\nprovable adversarial defenses. In Advances in Neural\nInformation Processing Systems, 2018.\nXiao, C., Zhu, J.-Y ., Li, B., He, W., Liu, M., and Song, D.\nSpatially transformed adversarial examples. In Interna-\ntional Conference on Learning Representations , 2018.\nXie, C., Wang, J., Zhang, Z., Zhou, Y ., Xie, L., and Y uille, A.\nAdversarial examples for semantic segmentation and ob-\nject detection. In International Conference on Computer\nVision, 2017.\nTheoretically Principled Trade-off between Robustness and Accuracy\nXie, C., Wu, Y ., van der Maaten, L., Y uille, A., and He, K.\nFeature denoising for improving adversarial robustness.\narXiv preprint arXiv:1812.03411, 2018.\nYin, D., Ramchandran, K., and Bartlett, P . Rademacher\ncomplexity for adversarially robust generalization. arXiv\npreprint arXiv:1810.11914, 2018.\nZagoruyko, S. and Komodakis, N. Wide residual networks.\nIn British Machine Vision Conference, 2016.\nZhang, H., Xu, S., Jiao, J., Xie, P ., Salakhutdinov, R., and\nXing, E. P . Stackelberg GAN: Towards provable mini-\nmax equilibrium via multi-generator architectures. arXiv\npreprint arXiv:1811.08010, 2018.\nZhang, H., Chen, H., Song, Z., Boning, D., Dhillon, I. S.,\nand Hsieh, C.-J. The limitations of adversarial training\nand the blind-spot attack. In International Conference on\nLearning Representations, 2019a.\nZhang, H., Shao, J., and Salakhutdinov, R. Deep neural\nnetworks with multi-branch architectures are intrinsically\nless non-convex. In International Conference on Artiﬁcial\nIntelligence and Statistics, pp. 1099–1109, 2019b.\nZhang, T. Covering number bounds of certain regularized\nlinear function classes. Journal of Machine Learning\nResearch, 2:527–550, 2002.\nZheng, S., Song, Y ., Leung, T., and Goodfellow, I. Improv-\ning the robustness of deep neural networks via stability\ntraining. In IEEE Conference on Computer Vision and\nPattern Recognition, pp. 4480–4488, 2016.",
  "values": {
    "Privacy": "No",
    "Collective influence": "No",
    "Justice": "No",
    "Critiqability": "No",
    "Respect for Persons": "No",
    "Fairness": "No",
    "Deferral to humans": "No",
    "User influence": "No",
    "Beneficence": "No",
    "Explicability": "No",
    "Respect for Law and public interest": "No",
    "Non-maleficence": "No",
    "Not socially biased": "No",
    "Autonomy (power to decide)": "No",
    "Interpretable (to users)": "No",
    "Transparent (to users)": "No"
  }
}